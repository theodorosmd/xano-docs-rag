<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>RAGArch: Building a No-Code RAG Pipeline Configuration &amp; One-Click RAG Code Generation Tool Powered by LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="RAGArch: Building a No-Code RAG Pipeline Configuration &amp; One-Click RAG Code Generation Tool Powered by LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="RAGArch: Building a No-Code RAG Pipeline Configuration &amp; One-Click RAG Code Generation Tool Powered by LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="RAGArch: Building a No-Code RAG Pipeline Configuration &amp; One-Click RAG Code Generation Tool Powered by LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="512" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/harshad-suryawanshi">Harshad Suryawanshi</a> <!-- -->•<!-- --> <!-- -->2024-02-02</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">RAGArch: Building a No-Code RAG Pipeline Configuration &amp; One-Click RAG Code Generation Tool Powered by LlamaIndex</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/rag"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">RAG</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/no-code"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">No Code</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/openai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">OpenAI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/code-generation"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Code Generation</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>Unlocking the power of AI should be as intuitive as using your favorite apps. That’s the philosophy behind RAGArch, my latest creation designed to demystify and streamline the process of setting up Retrieval-Augmented Generation (RAG) pipelines. This tool is born from a simple vision: to provide a straightforward, no-code platform that empowers both seasoned developers and curious explorers in the world of AI to craft, test, and implement RAG pipelines with confidence and ease.</p><h1>Features</h1><p>RAGArch leverages LlamaIndex’s powerful LLM orchestration capabilities, to provide a seamless experience and granular control over your RAG pipeline.</p><ul><li><strong>Intuitive Interface:</strong> RAGArch’s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively.</li><li><strong>Custom Configuration</strong>: The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project’s needs.</li><li><strong>Live Testing:</strong> Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome.</li><li><strong>One-Click Code Generation</strong>: Once you’re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application.</li></ul><iframe width="560" height="315" src="https://www.youtube.com/embed/3kfUmxFyuPA?si=c2x717Th_xVfEQpn" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><h1>Tools and Technologies</h1><p>The creation of RAGArch was made possible by integrating a variety of powerful tools and technologies:</p><ul><li><strong>UI: </strong>Streamlit</li><li><strong>Hosting:</strong> Hugging Face Spaces</li><li><strong>LLMs: </strong>OpenAI GPT 3.5 and 4, Cohere API, Gemini Pro</li><li><strong>LLM Orchestration: </strong>Llamaindex</li><li><strong>Embedding Models:</strong> “BAAI/bge-small-en-v1.5”, “WhereIsAI/UAE-Large-V1”, “BAAI/bge-large-en-v1.5”, “khoa-klaytn/bge-small-en-v1.5-angle”, “BAAI/bge-base-en-v1.5”, “llmrails/ember-v1”, “jamesgpt1/sf_model_e5”, “thenlper/gte-large”, “infgrad/stella-base-en-v2” and “thenlper/gte-base”</li><li><strong>Vector Stores:</strong> Simple (Llamaindex default), Pinecone and Qdrant</li></ul><h1>Deep Dive into the Code</h1><p>The <code class="cw qa qb qc qd b">app.py</code> script is the backbone of RAGArch, integrating various components to provide a cohesive experience. The following are the key functions of app.py</p><h2><code class="cw qa qb qc qd b"><strong>upload_file</strong></code></h2><p>This function manages file uploads and uses Llamaindex's <code class="cw qa qb qc qd b">SimpleDirectoryReader</code> to load documents into the system. It supports a wide array of document types, including PDFs, text files, HTML, JSON files, and more, making it versatile for processing diverse data sources.</p><figure><img src="/blog/images/1*hrekcMeqyyEIQbiO1GB8NQ.png" alt="" width="700" height="169"></figure><pre><span id="891b" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">upload_file</span>():
    file = st.file_uploader(<span class="hljs-string">"Upload a file"</span>, on_change=reset_pipeline_generated)
    <span class="hljs-keyword">if</span> file <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        file_path = save_uploaded_file(file)
        
        <span class="hljs-keyword">if</span> file_path:
            loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Total documents: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(loaded_file)}</span>"</span>)

            st.success(<span class="hljs-string">f"File uploaded successfully. Total documents loaded: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(loaded_file)}</span>"</span>)
        <span class="hljs-keyword">return</span> loaded_file
    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span></span></pre><h2>save_uploaded_file</h2><p>This utility function saves the uploaded file to a temporary location on the server, making it accessible for further processing. It’s a crucial part of the file handling process, ensuring data integrity and availability.</p><pre><span id="d86e" class="qx on gt qd b bf qy qz l ra rb">def <span class="hljs-title function_">save_uploaded_file</span>(uploaded_file):
    <span class="hljs-attr">try</span>:
        <span class="hljs-keyword">with</span> tempfile.<span class="hljs-title class_">NamedTemporaryFile</span>(<span class="hljs-keyword">delete</span>=<span class="hljs-title class_">False</span>, suffix=os.<span class="hljs-property">path</span>.<span class="hljs-title function_">splitext</span>(uploaded_file.<span class="hljs-property">name</span>)[<span class="hljs-number">1</span>]) <span class="hljs-keyword">as</span> <span class="hljs-attr">tmp_file</span>:
            tmp_file.<span class="hljs-title function_">write</span>(uploaded_file.<span class="hljs-title function_">getvalue</span>())
            <span class="hljs-keyword">return</span> tmp_file.<span class="hljs-property">name</span>
    except <span class="hljs-title class_">Exception</span> <span class="hljs-keyword">as</span> <span class="hljs-attr">e</span>:
        st.<span class="hljs-title function_">error</span>(f<span class="hljs-string">"Error saving file: {e}"</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-title class_">None</span></span></pre><h2><code class="cw qa qb qc qd b"><strong>select_llm</strong></code></h2><p>Allows users to select a Large Language Model and initializes it for use. You can choose from Google’s Gemini Pro, Cohere, OpenAI’s GPT 3.5 and GPT 4.</p><figure><img src="/blog/images/1*O2HndW3Ow_OgBHOABLOCLA.png" alt="" width="700" height="286"></figure><pre><span id="c7d6" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_llm</span>():
    st.header(<span class="hljs-string">"Choose LLM"</span>)
    llm_choice = st.selectbox(<span class="hljs-string">"Select LLM"</span>, [<span class="hljs-string">"Gemini"</span>, <span class="hljs-string">"Cohere"</span>, <span class="hljs-string">"GPT-3.5"</span>, <span class="hljs-string">"GPT-4"</span>], on_change=reset_pipeline_generated)
    
    <span class="hljs-keyword">if</span> llm_choice == <span class="hljs-string">"GPT-3.5"</span>:
        llm = OpenAI(temperature=<span class="hljs-number">0.1</span>, model=<span class="hljs-string">"gpt-3.5-turbo-1106"</span>)
        st.write(<span class="hljs-string">f"<span class="hljs-subst">{llm_choice}</span> selected"</span>)
    <span class="hljs-keyword">elif</span> llm_choice == <span class="hljs-string">"GPT-4"</span>:
        llm = OpenAI(temperature=<span class="hljs-number">0.1</span>, model=<span class="hljs-string">"gpt-4-1106-preview"</span>)
        st.write(<span class="hljs-string">f"<span class="hljs-subst">{llm_choice}</span> selected"</span>)
    <span class="hljs-keyword">elif</span> llm_choice == <span class="hljs-string">"Gemini"</span>:
        llm = Gemini(model=<span class="hljs-string">"models/gemini-pro"</span>)
        st.write(<span class="hljs-string">f"<span class="hljs-subst">{llm_choice}</span> selected"</span>)
    <span class="hljs-keyword">elif</span> llm_choice == <span class="hljs-string">"Cohere"</span>:
        llm = Cohere(model=<span class="hljs-string">"command"</span>, api_key=os.environ[<span class="hljs-string">'COHERE_API_TOKEN'</span>])
        st.write(<span class="hljs-string">f"<span class="hljs-subst">{llm_choice}</span> selected"</span>)
    <span class="hljs-keyword">return</span> llm, llm_choice</span></pre><h2><code class="cw qa qb qc qd b">select_embedding_model</code></h2><p>Offers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face’s MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models.</p><figure><img src="/blog/images/1*1HZdInelOskY3EWY1SmWkw.png" alt="" width="700" height="462"></figure><pre><span id="6c63" class="qx on gt qd b bf qy qz l ra rb">def <span class="hljs-title function_">select_embedding_model</span>():
    st.<span class="hljs-title function_">header</span>(<span class="hljs-string">"Choose Embedding Model"</span>)
    col1, col2 = st.<span class="hljs-title function_">columns</span>([<span class="hljs-number">2</span>,<span class="hljs-number">1</span>])
    <span class="hljs-keyword">with</span> <span class="hljs-attr">col2</span>:
        st.<span class="hljs-title function_">markdown</span>(<span class="hljs-string">""</span><span class="hljs-string">"
                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
                    "</span><span class="hljs-string">""</span>)
    model_names = [
        <span class="hljs-string">"BAAI/bge-small-en-v1.5"</span>,
        <span class="hljs-string">"WhereIsAI/UAE-Large-V1"</span>,
        <span class="hljs-string">"BAAI/bge-large-en-v1.5"</span>,
        <span class="hljs-string">"khoa-klaytn/bge-small-en-v1.5-angle"</span>,
        <span class="hljs-string">"BAAI/bge-base-en-v1.5"</span>,
        <span class="hljs-string">"llmrails/ember-v1"</span>,
        <span class="hljs-string">"jamesgpt1/sf_model_e5"</span>,
        <span class="hljs-string">"thenlper/gte-large"</span>,
        <span class="hljs-string">"infgrad/stella-base-en-v2"</span>,
        <span class="hljs-string">"thenlper/gte-base"</span>
    ]
    selected_model = st.<span class="hljs-title function_">selectbox</span>(<span class="hljs-string">"Select Embedding Model"</span>, model_names,  on_change=reset_pipeline_generated)
    <span class="hljs-keyword">with</span> st.<span class="hljs-title function_">spinner</span>(<span class="hljs-string">"Please wait"</span>) <span class="hljs-keyword">as</span> <span class="hljs-attr">status</span>:
        embed_model = <span class="hljs-title class_">HuggingFaceEmbedding</span>(model_name=selected_model)
        st.<span class="hljs-property">session_state</span>[<span class="hljs-string">'embed_model'</span>] = embed_model
        st.<span class="hljs-title function_">markdown</span>(F<span class="hljs-string">"Embedding Model: {embed_model.model_name}"</span>)
        st.<span class="hljs-title function_">markdown</span>(F<span class="hljs-string">"Embed Batch Size: {embed_model.embed_batch_size}"</span>)
        st.<span class="hljs-title function_">markdown</span>(F<span class="hljs-string">"Embed Batch Size: {embed_model.max_length}"</span>)


    <span class="hljs-keyword">return</span> embed_model, selected_model</span></pre><h2>select_node_parser Function</h2><p>This function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. I have included some of the most commonly used node parsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser and MarkdownNodeParser.</p><figure><img src="/blog/images/1*DR2jdaguScUtE6Kaxiw56A.png" alt="" width="700" height="552"></figure><pre><span id="756c" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_node_parser</span>():
    st.header(<span class="hljs-string">"Choose Node Parser"</span>)
    col1, col2 = st.columns([<span class="hljs-number">4</span>,<span class="hljs-number">1</span>])
    <span class="hljs-keyword">with</span> col2:
        st.markdown(<span class="hljs-string">"""
                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)
                    """</span>)
    parser_types = [<span class="hljs-string">"SentenceSplitter"</span>, <span class="hljs-string">"CodeSplitter"</span>, <span class="hljs-string">"SemanticSplitterNodeParser"</span>,
                    <span class="hljs-string">"TokenTextSplitter"</span>, <span class="hljs-string">"HTMLNodeParser"</span>, <span class="hljs-string">"JSONNodeParser"</span>, <span class="hljs-string">"MarkdownNodeParser"</span>]
    parser_type = st.selectbox(<span class="hljs-string">"Select Node Parser"</span>, parser_types, on_change=reset_pipeline_generated)
    
    parser_params = {}
    <span class="hljs-keyword">if</span> parser_type == <span class="hljs-string">"HTMLNodeParser"</span>:
        tags = st.text_input(<span class="hljs-string">"Enter tags separated by commas"</span>, <span class="hljs-string">"p, h1"</span>)
        tag_list = tags.split(<span class="hljs-string">','</span>)
        parser = HTMLNodeParser(tags=tag_list)
        parser_params = {<span class="hljs-string">'tags'</span>: tag_list}
        
    <span class="hljs-keyword">elif</span> parser_type == <span class="hljs-string">"JSONNodeParser"</span>:
        parser = JSONNodeParser()
        
    <span class="hljs-keyword">elif</span> parser_type == <span class="hljs-string">"MarkdownNodeParser"</span>:
        parser = MarkdownNodeParser()
        
    <span class="hljs-keyword">elif</span> parser_type == <span class="hljs-string">"CodeSplitter"</span>:
        language = st.text_input(<span class="hljs-string">"Language"</span>, <span class="hljs-string">"python"</span>)
        chunk_lines = st.number_input(<span class="hljs-string">"Chunk Lines"</span>, min_value=<span class="hljs-number">1</span>, value=<span class="hljs-number">40</span>)
        chunk_lines_overlap = st.number_input(<span class="hljs-string">"Chunk Lines Overlap"</span>, min_value=<span class="hljs-number">0</span>, value=<span class="hljs-number">15</span>)
        max_chars = st.number_input(<span class="hljs-string">"Max Chars"</span>, min_value=<span class="hljs-number">1</span>, value=<span class="hljs-number">1500</span>)
        parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)
        parser_params = {<span class="hljs-string">'language'</span>: language, <span class="hljs-string">'chunk_lines'</span>: chunk_lines, <span class="hljs-string">'chunk_lines_overlap'</span>: chunk_lines_overlap, <span class="hljs-string">'max_chars'</span>: max_chars}
        
    <span class="hljs-keyword">elif</span> parser_type == <span class="hljs-string">"SentenceSplitter"</span>:
        chunk_size = st.number_input(<span class="hljs-string">"Chunk Size"</span>, min_value=<span class="hljs-number">1</span>, value=<span class="hljs-number">1024</span>)
        chunk_overlap = st.number_input(<span class="hljs-string">"Chunk Overlap"</span>, min_value=<span class="hljs-number">0</span>, value=<span class="hljs-number">20</span>)
        parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        parser_params = {<span class="hljs-string">'chunk_size'</span>: chunk_size, <span class="hljs-string">'chunk_overlap'</span>: chunk_overlap}
        
    <span class="hljs-keyword">elif</span> parser_type == <span class="hljs-string">"SemanticSplitterNodeParser"</span>:
        <span class="hljs-keyword">if</span> <span class="hljs-string">'embed_model'</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> st.session_state:
            st.warning(<span class="hljs-string">"Please select an embedding model first."</span>)
            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>
        
        embed_model = st.session_state[<span class="hljs-string">'embed_model'</span>]
        buffer_size = st.number_input(<span class="hljs-string">"Buffer Size"</span>, min_value=<span class="hljs-number">1</span>, value=<span class="hljs-number">1</span>)
        breakpoint_percentile_threshold = st.number_input(<span class="hljs-string">"Breakpoint Percentile Threshold"</span>, min_value=<span class="hljs-number">0</span>, max_value=<span class="hljs-number">100</span>, value=<span class="hljs-number">95</span>)
        parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)
        parser_params = {<span class="hljs-string">'buffer_size'</span>: buffer_size, <span class="hljs-string">'breakpoint_percentile_threshold'</span>: breakpoint_percentile_threshold}
        
    <span class="hljs-keyword">elif</span> parser_type == <span class="hljs-string">"TokenTextSplitter"</span>:
        chunk_size = st.number_input(<span class="hljs-string">"Chunk Size"</span>, min_value=<span class="hljs-number">1</span>, value=<span class="hljs-number">1024</span>)
        chunk_overlap = st.number_input(<span class="hljs-string">"Chunk Overlap"</span>, min_value=<span class="hljs-number">0</span>, value=<span class="hljs-number">20</span>)
        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        parser_params = {<span class="hljs-string">'chunk_size'</span>: chunk_size, <span class="hljs-string">'chunk_overlap'</span>: chunk_overlap}

    <span class="hljs-comment"># Save the parser type and parameters to the session state</span>
    st.session_state[<span class="hljs-string">'node_parser_type'</span>] = parser_type
    st.session_state[<span class="hljs-string">'node_parser_params'</span>] = parser_params
    
    <span class="hljs-keyword">return</span> parser, parser_type</span></pre><p>Below the node parser selection, I have also included a preview of the first node of the text after splitting/parsing, just to give the users an idea of how the chunking is actually happening based the selected node parser and the relevant parameters.</p><h2><code class="cw qa qb qc qd b">select_response_synthesis_method</code></h2><p>This function allows users to choose how the RAG pipeline synthesizes responses. I have included varioud response synthesis methods supported by Llamaindex including <em class="re">refine</em>, <em class="re">tree_summarize</em>, <em class="re">compact</em>, <em class="re">simple_summarize</em>, <em class="re">accumulate </em>and<em class="re"> compact_accumulate.</em></p><p>Users can click on the more information link to get more details about response synthesis and the different types.</p><figure><img src="/blog/images/1*5OhOU25529Z08s_fart_DA.png" alt="" width="700" height="410"></figure><pre><span id="d551" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_response_synthesis_method</span>():
    st.header(<span class="hljs-string">"Choose Response Synthesis Method"</span>)
    col1, col2 = st.columns([<span class="hljs-number">4</span>,<span class="hljs-number">1</span>])
    <span class="hljs-keyword">with</span> col2:
        st.markdown(<span class="hljs-string">"""
                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)
                    """</span>)
    response_modes = [
        <span class="hljs-string">"refine"</span>,
        <span class="hljs-string">"tree_summarize"</span>,  
        <span class="hljs-string">"compact"</span>, 
        <span class="hljs-string">"simple_summarize"</span>, 
        <span class="hljs-string">"accumulate"</span>, 
        <span class="hljs-string">"compact_accumulate"</span>
    ]
    selected_mode = st.selectbox(<span class="hljs-string">"Select Response Mode"</span>, response_modes, on_change=reset_pipeline_generated)
    response_mode = selected_mode
    <span class="hljs-keyword">return</span> response_mode, selected_mode</span></pre><h2><code class="cw qa qb qc qd b">select_vector_store</code></h2><p>Enables users to choose a vector store, which is a critical component for storing and retrieving embeddings in the RAG pipeline. This function supports the selection from multiple vector store options including Simple (Llamaindex default), Pinecone and Qdrant.</p><figure><img src="/blog/images/1*bDOo67R7yzPKFQX_TMR56w.png" alt="" width="700" height="240"></figure><pre><span id="3b35" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_vector_store</span>():
    st.header(<span class="hljs-string">"Choose Vector Store"</span>)
    vector_stores = [<span class="hljs-string">"Simple"</span>, <span class="hljs-string">"Pinecone"</span>, <span class="hljs-string">"Qdrant"</span>]
    selected_store = st.selectbox(<span class="hljs-string">"Select Vector Store"</span>, vector_stores, on_change=reset_pipeline_generated)

    vector_store = <span class="hljs-literal">None</span>
    <span class="hljs-keyword">if</span> selected_store == <span class="hljs-string">"Pinecone"</span>:
        pc = Pinecone(api_key=os.environ[<span class="hljs-string">'PINECONE_API_KEY'</span>])
        index = pc.Index(<span class="hljs-string">"test"</span>)
        vector_store = PineconeVectorStore(pinecone_index=index)
    <span class="hljs-keyword">elif</span> selected_store == <span class="hljs-string">"Qdrant"</span>:
        client = qdrant_client.QdrantClient(location=<span class="hljs-string">":memory:"</span>)
        vector_store = QdrantVectorStore(client=client, collection_name=<span class="hljs-string">"sampledata"</span>)
    st.write(selected_store)
    <span class="hljs-keyword">return</span> vector_store, selected_store</span></pre><h2>generate_rag_pipeline Function</h2><p>This core function ties together the selected components to generate a RAG pipeline. It initializes the pipeline with the chosen LLM, embedding model, node parser, response synthesis method, and vector store. It is triggered by pressing the ‘Generate RAG Pipeline’ button.</p><pre><span id="0fa5" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_rag_pipeline</span>(<span class="hljs-params">file, llm, embed_model, node_parser, response_mode, vector_store</span>):
    <span class="hljs-keyword">if</span> vector_store <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        <span class="hljs-comment"># Set storage context if vector_store is not None</span>
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
    <span class="hljs-keyword">else</span>:
        storage_context = <span class="hljs-literal">None</span>

    <span class="hljs-comment"># Create the service context</span>
    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)

    <span class="hljs-comment"># Create the vector index</span>
    vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">if</span> storage_context:
        vector_index.storage_context.persist(persist_dir=<span class="hljs-string">"persist_dir"</span>)

    <span class="hljs-comment"># Create the query engine</span>
    query_engine = vector_index.as_query_engine(
        response_mode=response_mode,
        verbose=<span class="hljs-literal">True</span>,
    )

    <span class="hljs-keyword">return</span> query_engine</span></pre><h2>generate_code_snippet Function</h2><p>This function is the culmination of the user’s selections, generating the Python code necessary to implement the configured RAG pipeline. It dynamically constructs the code snippet based on the chosen LLM, embedding model, node parser, response synthesis method, and vector store, including the parameters set for the node parser.</p><pre><span id="b12e" class="qx on gt qd b bf qy qz l ra rb"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_code_snippet</span>(<span class="hljs-params">llm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice</span>):
    node_parser_params = st.session_state.get(<span class="hljs-string">'node_parser_params'</span>, {})
    <span class="hljs-built_in">print</span>(node_parser_params)
    code_snippet = <span class="hljs-string">"from llama_index.llms import OpenAI, Gemini, Cohere\n"</span>
    code_snippet += <span class="hljs-string">"from llama_index.embeddings import HuggingFaceEmbedding\n"</span>
    code_snippet += <span class="hljs-string">"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n"</span>
    code_snippet += <span class="hljs-string">"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\n"</span>
    code_snippet += <span class="hljs-string">"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\n"</span>
    code_snippet += <span class="hljs-string">"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\n"</span>
    code_snippet += <span class="hljs-string">"import qdrant_client\n\n"</span>

    <span class="hljs-comment"># LLM initialization</span>
    <span class="hljs-keyword">if</span> llm_choice == <span class="hljs-string">"GPT-3.5"</span>:
        code_snippet += <span class="hljs-string">"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\n"</span>
    <span class="hljs-keyword">elif</span> llm_choice == <span class="hljs-string">"GPT-4"</span>:
        code_snippet += <span class="hljs-string">"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\n"</span>
    <span class="hljs-keyword">elif</span> llm_choice == <span class="hljs-string">"Gemini"</span>:
        code_snippet += <span class="hljs-string">"llm = Gemini(model='models/gemini-pro')\n"</span>
    <span class="hljs-keyword">elif</span> llm_choice == <span class="hljs-string">"Cohere"</span>:
        code_snippet += <span class="hljs-string">"llm = Cohere(model='command', api_key='&amp;lt;YOUR_API_KEY&amp;gt;')  # Replace &amp;lt;YOUR_API_KEY&amp;gt; with your actual API key\n"</span>

    <span class="hljs-comment"># Embedding model initialization</span>
    code_snippet += <span class="hljs-string">f"embed_model = HuggingFaceEmbedding(model_name='<span class="hljs-subst">{embed_model_choice}</span>')\n\n"</span>

    <span class="hljs-comment"># Node parser initialization</span>
    node_parsers = {
        <span class="hljs-string">"SentenceSplitter"</span>: <span class="hljs-string">f"SentenceSplitter(chunk_size=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'chunk_size'</span>, <span class="hljs-number">1024</span>)}</span>, chunk_overlap=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'chunk_overlap'</span>, <span class="hljs-number">20</span>)}</span>)"</span>,
        <span class="hljs-string">"CodeSplitter"</span>: <span class="hljs-string">f"CodeSplitter(language=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'language'</span>, <span class="hljs-string">'python'</span>)}</span>, chunk_lines=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'chunk_lines'</span>, <span class="hljs-number">40</span>)}</span>, chunk_lines_overlap=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'chunk_lines_overlap'</span>, <span class="hljs-number">15</span>)}</span>, max_chars=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'max_chars'</span>, <span class="hljs-number">1500</span>)}</span>)"</span>,
        <span class="hljs-string">"SemanticSplitterNodeParser"</span>: <span class="hljs-string">f"SemanticSplitterNodeParser(buffer_size=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'buffer_size'</span>, <span class="hljs-number">1</span>)}</span>, breakpoint_percentile_threshold=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'breakpoint_percentile_threshold'</span>, <span class="hljs-number">95</span>)}</span>, embed_model=embed_model)"</span>,
        <span class="hljs-string">"TokenTextSplitter"</span>: <span class="hljs-string">f"TokenTextSplitter(chunk_size=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'chunk_size'</span>, <span class="hljs-number">1024</span>)}</span>, chunk_overlap=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'chunk_overlap'</span>, <span class="hljs-number">20</span>)}</span>)"</span>,
        <span class="hljs-string">"HTMLNodeParser"</span>: <span class="hljs-string">f"HTMLNodeParser(tags=<span class="hljs-subst">{node_parser_params.get(<span class="hljs-string">'tags'</span>, [<span class="hljs-string">'p'</span>, <span class="hljs-string">'h1'</span>])}</span>)"</span>,  
        <span class="hljs-string">"JSONNodeParser"</span>: <span class="hljs-string">"JSONNodeParser()"</span>,
        <span class="hljs-string">"MarkdownNodeParser"</span>: <span class="hljs-string">"MarkdownNodeParser()"</span>
    }
    code_snippet += <span class="hljs-string">f"node_parser = <span class="hljs-subst">{node_parsers[node_parser_choice]}</span>\n\n"</span>

    <span class="hljs-comment"># Response mode</span>
    code_snippet += <span class="hljs-string">f"response_mode = '<span class="hljs-subst">{response_mode}</span>'\n\n"</span>

    <span class="hljs-comment"># Vector store initialization</span>
    <span class="hljs-keyword">if</span> vector_store_choice == <span class="hljs-string">"Pinecone"</span>:
        code_snippet += <span class="hljs-string">"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n"</span>
        code_snippet += <span class="hljs-string">"index = pc.Index('test')\n"</span>
        code_snippet += <span class="hljs-string">"vector_store = PineconeVectorStore(pinecone_index=index)\n"</span>
    <span class="hljs-keyword">elif</span> vector_store_choice == <span class="hljs-string">"Qdrant"</span>:
        code_snippet += <span class="hljs-string">"client = qdrant_client.QdrantClient(location=':memory:')\n"</span>
        code_snippet += <span class="hljs-string">"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\n"</span>
    <span class="hljs-keyword">elif</span> vector_store_choice == <span class="hljs-string">"Simple"</span>:
        code_snippet += <span class="hljs-string">"vector_store = None  # Simple in-memory vector store selected\n"</span>

    code_snippet += <span class="hljs-string">"\n# Finalizing the RAG pipeline setup\n"</span>
    code_snippet += <span class="hljs-string">"if vector_store is not None:\n"</span>
    code_snippet += <span class="hljs-string">"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"</span>
    code_snippet += <span class="hljs-string">"else:\n"</span>
    code_snippet += <span class="hljs-string">"    storage_context = None\n\n"</span>

    code_snippet += <span class="hljs-string">"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n\n"</span>

    code_snippet += <span class="hljs-string">"_file = 'path_to_your_file'  # Replace with the path to your file\n"</span>
    code_snippet += <span class="hljs-string">"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\n"</span>
    code_snippet += <span class="hljs-string">"if storage_context:\n"</span>
    code_snippet += <span class="hljs-string">"    vector_index.storage_context.persist(persist_dir='persist_dir')\n\n"</span>

    code_snippet += <span class="hljs-string">"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\n"</span>

    <span class="hljs-keyword">return</span> code_snippet</span></pre><h1>Conclusion</h1><p>RAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It’s designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation.</p><p>Your insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let’s start a conversation on Linkedin. I’m always eager to collaborate and share knowledge with fellow tech adventurers.</p><p><a href="https://github.com/AI-ANK/RAGArch" rel="noopener ugc nofollow" target="_blank">GitHub Repo</a></p><p><a href="https://www.linkedin.com/in/harshadsuryawanshi/" rel="noopener ugc nofollow" target="_blank">Connect with Me on LinkedIn</a></p><p><a href="https://huggingface.co/spaces/AI-ANK/RAGArch" rel="noopener ugc nofollow" target="_blank">Live Demo</a></p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/rag-is-dead-long-live-agentic-retrieval">RAG is dead, long live agentic retrieval</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-29</p></div></li><li><div class="CardBlog_card__mm0Zw CardBlog_featuredCard__5FPeD"><div class="CardBlog_grid__5PeSv"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><div class="CardBlog_thumbnailGradient__x5CbY"><p class="Text_text__zPO0D Text_text-size-36__cH7Hj Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/beyond-chatbots-adopting-agentic-document-workflows-for-enterprises">Beyond chatbots: adopting Agentic Document Workflows for enterprises</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu CardBlog_date__E1rJK">2025-04-23</p></div></div></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/building-blocks-of-llm-report-generation-beyond-basic-rag">Building Blocks of LLM Report Generation: Beyond Basic RAG</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-11-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/building-a-serverless-rag-application-with-llamaindex-and-azure-openai">Building a serverless RAG application with LlamaIndex and Azure OpenAI</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-08-27</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"10424e80-5f28-40bb-98f4-0750649bbe27","_rev":"05dtDS0H5iRVsxYMarYz2O","_type":"blogPost","_updatedAt":"2025-05-21T20:36:34Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:51:08Z","_id":"e1ef8fc3-74dc-41a2-864d-4ae53df698e3","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:05Z","name":"Harshad Suryawanshi","slug":{"_type":"slug","current":"harshad-suryawanshi"}}],"featured":false,"htmlContent":"\u003cp\u003eUnlocking the power of AI should be as intuitive as using your favorite apps. That’s the philosophy behind RAGArch, my latest creation designed to demystify and streamline the process of setting up Retrieval-Augmented Generation (RAG) pipelines. This tool is born from a simple vision: to provide a straightforward, no-code platform that empowers both seasoned developers and curious explorers in the world of AI to craft, test, and implement RAG pipelines with confidence and ease.\u003c/p\u003e\u003ch1\u003eFeatures\u003c/h1\u003e\u003cp\u003eRAGArch leverages LlamaIndex’s powerful LLM orchestration capabilities, to provide a seamless experience and granular control over your RAG pipeline.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eIntuitive Interface:\u003c/strong\u003e RAGArch’s user-friendly interface, built with Streamlit, allows you to test different RAG pipeline components interactively.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCustom Configuration\u003c/strong\u003e: The app provides a wide range of options to configure Language Models, Embedding Models, Node Parsers, Response Synthesis Methods, and Vector Stores to suit your project’s needs.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLive Testing:\u003c/strong\u003e Instantly test your RAG pipeline with your own data and see how different configurations affect the outcome.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eOne-Click Code Generation\u003c/strong\u003e: Once you’re satisfied with the configuration, the app can generate the Python code for your custom RAG pipeline, ready to be integrated into your application.\u003c/li\u003e\u003c/ul\u003e\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/3kfUmxFyuPA?si=c2x717Th_xVfEQpn\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\u003ch1\u003eTools and Technologies\u003c/h1\u003e\u003cp\u003eThe creation of RAGArch was made possible by integrating a variety of powerful tools and technologies:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eUI: \u003c/strong\u003eStreamlit\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eHosting:\u003c/strong\u003e Hugging Face Spaces\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLLMs: \u003c/strong\u003eOpenAI GPT 3.5 and 4, Cohere API, Gemini Pro\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLLM Orchestration: \u003c/strong\u003eLlamaindex\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEmbedding Models:\u003c/strong\u003e “BAAI/bge-small-en-v1.5”, “WhereIsAI/UAE-Large-V1”, “BAAI/bge-large-en-v1.5”, “khoa-klaytn/bge-small-en-v1.5-angle”, “BAAI/bge-base-en-v1.5”, “llmrails/ember-v1”, “jamesgpt1/sf_model_e5”, “thenlper/gte-large”, “infgrad/stella-base-en-v2” and “thenlper/gte-base”\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVector Stores:\u003c/strong\u003e Simple (Llamaindex default), Pinecone and Qdrant\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eDeep Dive into the Code\u003c/h1\u003e\u003cp\u003eThe \u003ccode class=\"cw qa qb qc qd b\"\u003eapp.py\u003c/code\u003e script is the backbone of RAGArch, integrating various components to provide a cohesive experience. The following are the key functions of app.py\u003c/p\u003e\u003ch2\u003e\u003ccode class=\"cw qa qb qc qd b\"\u003e\u003cstrong\u003eupload_file\u003c/strong\u003e\u003c/code\u003e\u003c/h2\u003e\u003cp\u003eThis function manages file uploads and uses Llamaindex's \u003ccode class=\"cw qa qb qc qd b\"\u003eSimpleDirectoryReader\u003c/code\u003e to load documents into the system. It supports a wide array of document types, including PDFs, text files, HTML, JSON files, and more, making it versatile for processing diverse data sources.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*hrekcMeqyyEIQbiO1GB8NQ.png\" alt=\"\" width=\"700\" height=\"169\"\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"891b\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eupload_file\u003c/span\u003e():\n    file = st.file_uploader(\u003cspan class=\"hljs-string\"\u003e\"Upload a file\"\u003c/span\u003e, on_change=reset_pipeline_generated)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e file \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        file_path = save_uploaded_file(file)\n        \n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e file_path:\n            loaded_file = SimpleDirectoryReader(input_files=[file_path]).load_data()\n            \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Total documents: \u003cspan class=\"hljs-subst\"\u003e{\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(loaded_file)}\u003c/span\u003e\"\u003c/span\u003e)\n\n            st.success(\u003cspan class=\"hljs-string\"\u003ef\"File uploaded successfully. Total documents loaded: \u003cspan class=\"hljs-subst\"\u003e{\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(loaded_file)}\u003c/span\u003e\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e loaded_file\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003esave_uploaded_file\u003c/h2\u003e\u003cp\u003eThis utility function saves the uploaded file to a temporary location on the server, making it accessible for further processing. It’s a crucial part of the file handling process, ensuring data integrity and availability.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d86e\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003edef \u003cspan class=\"hljs-title function_\"\u003esave_uploaded_file\u003c/span\u003e(uploaded_file):\n    \u003cspan class=\"hljs-attr\"\u003etry\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e tempfile.\u003cspan class=\"hljs-title class_\"\u003eNamedTemporaryFile\u003c/span\u003e(\u003cspan class=\"hljs-keyword\"\u003edelete\u003c/span\u003e=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e, suffix=os.\u003cspan class=\"hljs-property\"\u003epath\u003c/span\u003e.\u003cspan class=\"hljs-title function_\"\u003esplitext\u003c/span\u003e(uploaded_file.\u003cspan class=\"hljs-property\"\u003ename\u003c/span\u003e)[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003etmp_file\u003c/span\u003e:\n            tmp_file.\u003cspan class=\"hljs-title function_\"\u003ewrite\u003c/span\u003e(uploaded_file.\u003cspan class=\"hljs-title function_\"\u003egetvalue\u003c/span\u003e())\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e tmp_file.\u003cspan class=\"hljs-property\"\u003ename\u003c/span\u003e\n    except \u003cspan class=\"hljs-title class_\"\u003eException\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ee\u003c/span\u003e:\n        st.\u003cspan class=\"hljs-title function_\"\u003eerror\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"Error saving file: {e}\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003e\u003ccode class=\"cw qa qb qc qd b\"\u003e\u003cstrong\u003eselect_llm\u003c/strong\u003e\u003c/code\u003e\u003c/h2\u003e\u003cp\u003eAllows users to select a Large Language Model and initializes it for use. You can choose from Google’s Gemini Pro, Cohere, OpenAI’s GPT 3.5 and GPT 4.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*O2HndW3Ow_OgBHOABLOCLA.png\" alt=\"\" width=\"700\" height=\"286\"\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"c7d6\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eselect_llm\u003c/span\u003e():\n    st.header(\u003cspan class=\"hljs-string\"\u003e\"Choose LLM\"\u003c/span\u003e)\n    llm_choice = st.selectbox(\u003cspan class=\"hljs-string\"\u003e\"Select LLM\"\u003c/span\u003e, [\u003cspan class=\"hljs-string\"\u003e\"Gemini\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"Cohere\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"GPT-3.5\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"GPT-4\"\u003c/span\u003e], on_change=reset_pipeline_generated)\n    \n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"GPT-3.5\"\u003c/span\u003e:\n        llm = OpenAI(temperature=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, model=\u003cspan class=\"hljs-string\"\u003e\"gpt-3.5-turbo-1106\"\u003c/span\u003e)\n        st.write(\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{llm_choice}\u003c/span\u003e selected\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"GPT-4\"\u003c/span\u003e:\n        llm = OpenAI(temperature=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e, model=\u003cspan class=\"hljs-string\"\u003e\"gpt-4-1106-preview\"\u003c/span\u003e)\n        st.write(\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{llm_choice}\u003c/span\u003e selected\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"Gemini\"\u003c/span\u003e:\n        llm = Gemini(model=\u003cspan class=\"hljs-string\"\u003e\"models/gemini-pro\"\u003c/span\u003e)\n        st.write(\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{llm_choice}\u003c/span\u003e selected\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"Cohere\"\u003c/span\u003e:\n        llm = Cohere(model=\u003cspan class=\"hljs-string\"\u003e\"command\"\u003c/span\u003e, api_key=os.environ[\u003cspan class=\"hljs-string\"\u003e'COHERE_API_TOKEN'\u003c/span\u003e])\n        st.write(\u003cspan class=\"hljs-string\"\u003ef\"\u003cspan class=\"hljs-subst\"\u003e{llm_choice}\u003c/span\u003e selected\"\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e llm, llm_choice\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003e\u003ccode class=\"cw qa qb qc qd b\"\u003eselect_embedding_model\u003c/code\u003e\u003c/h2\u003e\u003cp\u003eOffers a dropdown for users to select the embedding model of their choice from a predefined list. I have included some of the top embedding models from Hugging Face’s MTEB leaderboard. Near the dropdown I have also included a handy link to the leaderboard where users can get more information about the embedding models.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*1HZdInelOskY3EWY1SmWkw.png\" alt=\"\" width=\"700\" height=\"462\"\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"6c63\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003edef \u003cspan class=\"hljs-title function_\"\u003eselect_embedding_model\u003c/span\u003e():\n    st.\u003cspan class=\"hljs-title function_\"\u003eheader\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Choose Embedding Model\"\u003c/span\u003e)\n    col1, col2 = st.\u003cspan class=\"hljs-title function_\"\u003ecolumns\u003c/span\u003e([\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003ecol2\u003c/span\u003e:\n        st.\u003cspan class=\"hljs-title function_\"\u003emarkdown\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n                    [Embedding Models Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n                    \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e)\n    model_names = [\n        \u003cspan class=\"hljs-string\"\u003e\"BAAI/bge-small-en-v1.5\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"WhereIsAI/UAE-Large-V1\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"BAAI/bge-large-en-v1.5\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"khoa-klaytn/bge-small-en-v1.5-angle\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"BAAI/bge-base-en-v1.5\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"llmrails/ember-v1\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"jamesgpt1/sf_model_e5\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"thenlper/gte-large\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"infgrad/stella-base-en-v2\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"thenlper/gte-base\"\u003c/span\u003e\n    ]\n    selected_model = st.\u003cspan class=\"hljs-title function_\"\u003eselectbox\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Select Embedding Model\"\u003c/span\u003e, model_names,  on_change=reset_pipeline_generated)\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e st.\u003cspan class=\"hljs-title function_\"\u003espinner\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Please wait\"\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003estatus\u003c/span\u003e:\n        embed_model = \u003cspan class=\"hljs-title class_\"\u003eHuggingFaceEmbedding\u003c/span\u003e(model_name=selected_model)\n        st.\u003cspan class=\"hljs-property\"\u003esession_state\u003c/span\u003e[\u003cspan class=\"hljs-string\"\u003e'embed_model'\u003c/span\u003e] = embed_model\n        st.\u003cspan class=\"hljs-title function_\"\u003emarkdown\u003c/span\u003e(F\u003cspan class=\"hljs-string\"\u003e\"Embedding Model: {embed_model.model_name}\"\u003c/span\u003e)\n        st.\u003cspan class=\"hljs-title function_\"\u003emarkdown\u003c/span\u003e(F\u003cspan class=\"hljs-string\"\u003e\"Embed Batch Size: {embed_model.embed_batch_size}\"\u003c/span\u003e)\n        st.\u003cspan class=\"hljs-title function_\"\u003emarkdown\u003c/span\u003e(F\u003cspan class=\"hljs-string\"\u003e\"Embed Batch Size: {embed_model.max_length}\"\u003c/span\u003e)\n\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e embed_model, selected_model\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eselect_node_parser Function\u003c/h2\u003e\u003cp\u003eThis function allows users to choose a node parser, which is instrumental in breaking down documents into manageable chunks or nodes, facilitating better handling and processing. I have included some of the most commonly used node parsers supported by Llamaindex, which include SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter, HTMLNodeParser, JSONNodeParser and MarkdownNodeParser.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*DR2jdaguScUtE6Kaxiw56A.png\" alt=\"\" width=\"700\" height=\"552\"\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"756c\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eselect_node_parser\u003c/span\u003e():\n    st.header(\u003cspan class=\"hljs-string\"\u003e\"Choose Node Parser\"\u003c/span\u003e)\n    col1, col2 = st.columns([\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e col2:\n        st.markdown(\u003cspan class=\"hljs-string\"\u003e\"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/root.html)\n                    \"\"\"\u003c/span\u003e)\n    parser_types = [\u003cspan class=\"hljs-string\"\u003e\"SentenceSplitter\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"CodeSplitter\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"SemanticSplitterNodeParser\"\u003c/span\u003e,\n                    \u003cspan class=\"hljs-string\"\u003e\"TokenTextSplitter\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"HTMLNodeParser\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"JSONNodeParser\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"MarkdownNodeParser\"\u003c/span\u003e]\n    parser_type = st.selectbox(\u003cspan class=\"hljs-string\"\u003e\"Select Node Parser\"\u003c/span\u003e, parser_types, on_change=reset_pipeline_generated)\n    \n    parser_params = {}\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"HTMLNodeParser\"\u003c/span\u003e:\n        tags = st.text_input(\u003cspan class=\"hljs-string\"\u003e\"Enter tags separated by commas\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"p, h1\"\u003c/span\u003e)\n        tag_list = tags.split(\u003cspan class=\"hljs-string\"\u003e','\u003c/span\u003e)\n        parser = HTMLNodeParser(tags=tag_list)\n        parser_params = {\u003cspan class=\"hljs-string\"\u003e'tags'\u003c/span\u003e: tag_list}\n        \n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"JSONNodeParser\"\u003c/span\u003e:\n        parser = JSONNodeParser()\n        \n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"MarkdownNodeParser\"\u003c/span\u003e:\n        parser = MarkdownNodeParser()\n        \n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"CodeSplitter\"\u003c/span\u003e:\n        language = st.text_input(\u003cspan class=\"hljs-string\"\u003e\"Language\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"python\"\u003c/span\u003e)\n        chunk_lines = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Chunk Lines\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e40\u003c/span\u003e)\n        chunk_lines_overlap = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Chunk Lines Overlap\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e)\n        max_chars = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Max Chars\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e1500\u003c/span\u003e)\n        parser = CodeSplitter(language=language, chunk_lines=chunk_lines, chunk_lines_overlap=chunk_lines_overlap, max_chars=max_chars)\n        parser_params = {\u003cspan class=\"hljs-string\"\u003e'language'\u003c/span\u003e: language, \u003cspan class=\"hljs-string\"\u003e'chunk_lines'\u003c/span\u003e: chunk_lines, \u003cspan class=\"hljs-string\"\u003e'chunk_lines_overlap'\u003c/span\u003e: chunk_lines_overlap, \u003cspan class=\"hljs-string\"\u003e'max_chars'\u003c/span\u003e: max_chars}\n        \n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"SentenceSplitter\"\u003c/span\u003e:\n        chunk_size = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Chunk Size\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e)\n        chunk_overlap = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Chunk Overlap\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {\u003cspan class=\"hljs-string\"\u003e'chunk_size'\u003c/span\u003e: chunk_size, \u003cspan class=\"hljs-string\"\u003e'chunk_overlap'\u003c/span\u003e: chunk_overlap}\n        \n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"SemanticSplitterNodeParser\"\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'embed_model'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e st.session_state:\n            st.warning(\u003cspan class=\"hljs-string\"\u003e\"Please select an embedding model first.\"\u003c/span\u003e)\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n        \n        embed_model = st.session_state[\u003cspan class=\"hljs-string\"\u003e'embed_model'\u003c/span\u003e]\n        buffer_size = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Buffer Size\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)\n        breakpoint_percentile_threshold = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Breakpoint Percentile Threshold\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, max_value=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e95\u003c/span\u003e)\n        parser = SemanticSplitterNodeParser(buffer_size=buffer_size, breakpoint_percentile_threshold=breakpoint_percentile_threshold, embed_model=embed_model)\n        parser_params = {\u003cspan class=\"hljs-string\"\u003e'buffer_size'\u003c/span\u003e: buffer_size, \u003cspan class=\"hljs-string\"\u003e'breakpoint_percentile_threshold'\u003c/span\u003e: breakpoint_percentile_threshold}\n        \n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e parser_type == \u003cspan class=\"hljs-string\"\u003e\"TokenTextSplitter\"\u003c/span\u003e:\n        chunk_size = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Chunk Size\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e)\n        chunk_overlap = st.number_input(\u003cspan class=\"hljs-string\"\u003e\"Chunk Overlap\"\u003c/span\u003e, min_value=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, value=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        parser = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n        parser_params = {\u003cspan class=\"hljs-string\"\u003e'chunk_size'\u003c/span\u003e: chunk_size, \u003cspan class=\"hljs-string\"\u003e'chunk_overlap'\u003c/span\u003e: chunk_overlap}\n\n    \u003cspan class=\"hljs-comment\"\u003e# Save the parser type and parameters to the session state\u003c/span\u003e\n    st.session_state[\u003cspan class=\"hljs-string\"\u003e'node_parser_type'\u003c/span\u003e] = parser_type\n    st.session_state[\u003cspan class=\"hljs-string\"\u003e'node_parser_params'\u003c/span\u003e] = parser_params\n    \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e parser, parser_type\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eBelow the node parser selection, I have also included a preview of the first node of the text after splitting/parsing, just to give the users an idea of how the chunking is actually happening based the selected node parser and the relevant parameters.\u003c/p\u003e\u003ch2\u003e\u003ccode class=\"cw qa qb qc qd b\"\u003eselect_response_synthesis_method\u003c/code\u003e\u003c/h2\u003e\u003cp\u003eThis function allows users to choose how the RAG pipeline synthesizes responses. I have included varioud response synthesis methods supported by Llamaindex including \u003cem class=\"re\"\u003erefine\u003c/em\u003e, \u003cem class=\"re\"\u003etree_summarize\u003c/em\u003e, \u003cem class=\"re\"\u003ecompact\u003c/em\u003e, \u003cem class=\"re\"\u003esimple_summarize\u003c/em\u003e, \u003cem class=\"re\"\u003eaccumulate \u003c/em\u003eand\u003cem class=\"re\"\u003e compact_accumulate.\u003c/em\u003e\u003c/p\u003e\u003cp\u003eUsers can click on the more information link to get more details about response synthesis and the different types.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*5OhOU25529Z08s_fart_DA.png\" alt=\"\" width=\"700\" height=\"410\"\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"d551\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eselect_response_synthesis_method\u003c/span\u003e():\n    st.header(\u003cspan class=\"hljs-string\"\u003e\"Choose Response Synthesis Method\"\u003c/span\u003e)\n    col1, col2 = st.columns([\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e,\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e])\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e col2:\n        st.markdown(\u003cspan class=\"hljs-string\"\u003e\"\"\"\n                    [More Information](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/response_synthesizers.html)\n                    \"\"\"\u003c/span\u003e)\n    response_modes = [\n        \u003cspan class=\"hljs-string\"\u003e\"refine\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"tree_summarize\"\u003c/span\u003e,  \n        \u003cspan class=\"hljs-string\"\u003e\"compact\"\u003c/span\u003e, \n        \u003cspan class=\"hljs-string\"\u003e\"simple_summarize\"\u003c/span\u003e, \n        \u003cspan class=\"hljs-string\"\u003e\"accumulate\"\u003c/span\u003e, \n        \u003cspan class=\"hljs-string\"\u003e\"compact_accumulate\"\u003c/span\u003e\n    ]\n    selected_mode = st.selectbox(\u003cspan class=\"hljs-string\"\u003e\"Select Response Mode\"\u003c/span\u003e, response_modes, on_change=reset_pipeline_generated)\n    response_mode = selected_mode\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e response_mode, selected_mode\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003e\u003ccode class=\"cw qa qb qc qd b\"\u003eselect_vector_store\u003c/code\u003e\u003c/h2\u003e\u003cp\u003eEnables users to choose a vector store, which is a critical component for storing and retrieving embeddings in the RAG pipeline. This function supports the selection from multiple vector store options including Simple (Llamaindex default), Pinecone and Qdrant.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*bDOo67R7yzPKFQX_TMR56w.png\" alt=\"\" width=\"700\" height=\"240\"\u003e\u003c/figure\u003e\u003cpre\u003e\u003cspan id=\"3b35\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eselect_vector_store\u003c/span\u003e():\n    st.header(\u003cspan class=\"hljs-string\"\u003e\"Choose Vector Store\"\u003c/span\u003e)\n    vector_stores = [\u003cspan class=\"hljs-string\"\u003e\"Simple\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"Pinecone\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"Qdrant\"\u003c/span\u003e]\n    selected_store = st.selectbox(\u003cspan class=\"hljs-string\"\u003e\"Select Vector Store\"\u003c/span\u003e, vector_stores, on_change=reset_pipeline_generated)\n\n    vector_store = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e selected_store == \u003cspan class=\"hljs-string\"\u003e\"Pinecone\"\u003c/span\u003e:\n        pc = Pinecone(api_key=os.environ[\u003cspan class=\"hljs-string\"\u003e'PINECONE_API_KEY'\u003c/span\u003e])\n        index = pc.Index(\u003cspan class=\"hljs-string\"\u003e\"test\"\u003c/span\u003e)\n        vector_store = PineconeVectorStore(pinecone_index=index)\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e selected_store == \u003cspan class=\"hljs-string\"\u003e\"Qdrant\"\u003c/span\u003e:\n        client = qdrant_client.QdrantClient(location=\u003cspan class=\"hljs-string\"\u003e\":memory:\"\u003c/span\u003e)\n        vector_store = QdrantVectorStore(client=client, collection_name=\u003cspan class=\"hljs-string\"\u003e\"sampledata\"\u003c/span\u003e)\n    st.write(selected_store)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e vector_store, selected_store\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003egenerate_rag_pipeline Function\u003c/h2\u003e\u003cp\u003eThis core function ties together the selected components to generate a RAG pipeline. It initializes the pipeline with the chosen LLM, embedding model, node parser, response synthesis method, and vector store. It is triggered by pressing the ‘Generate RAG Pipeline’ button.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0fa5\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003egenerate_rag_pipeline\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efile, llm, embed_model, node_parser, response_mode, vector_store\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e vector_store \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        \u003cspan class=\"hljs-comment\"\u003e# Set storage context if vector_store is not None\u003c/span\u003e\n        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        storage_context = \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# Create the service context\u003c/span\u003e\n    service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\n\n    \u003cspan class=\"hljs-comment\"\u003e# Create the vector index\u003c/span\u003e\n    vector_index = VectorStoreIndex.from_documents(documents=file, storage_context=storage_context, service_context=service_context, show_progress=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e storage_context:\n        vector_index.storage_context.persist(persist_dir=\u003cspan class=\"hljs-string\"\u003e\"persist_dir\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-comment\"\u003e# Create the query engine\u003c/span\u003e\n    query_engine = vector_index.as_query_engine(\n        response_mode=response_mode,\n        verbose=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    )\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e query_engine\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003egenerate_code_snippet Function\u003c/h2\u003e\u003cp\u003eThis function is the culmination of the user’s selections, generating the Python code necessary to implement the configured RAG pipeline. It dynamically constructs the code snippet based on the chosen LLM, embedding model, node parser, response synthesis method, and vector store, including the parameters set for the node parser.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b12e\" class=\"qx on gt qd b bf qy qz l ra rb\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003egenerate_code_snippet\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003ellm_choice, embed_model_choice, node_parser_choice, response_mode, vector_store_choice\u003c/span\u003e):\n    node_parser_params = st.session_state.get(\u003cspan class=\"hljs-string\"\u003e'node_parser_params'\u003c/span\u003e, {})\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(node_parser_params)\n    code_snippet = \u003cspan class=\"hljs-string\"\u003e\"from llama_index.llms import OpenAI, Gemini, Cohere\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"from llama_index.embeddings import HuggingFaceEmbedding\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"from llama_index import ServiceContext, VectorStoreIndex, StorageContext\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"from llama_index.node_parser import SentenceSplitter, CodeSplitter, SemanticSplitterNodeParser, TokenTextSplitter\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"from llama_index.node_parser.file import HTMLNodeParser, JSONNodeParser, MarkdownNodeParser\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"from llama_index.vector_stores import MilvusVectorStore, QdrantVectorStore\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"import qdrant_client\\n\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# LLM initialization\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"GPT-3.5\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"llm = OpenAI(temperature=0.1, model='gpt-3.5-turbo-1106')\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"GPT-4\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"llm = OpenAI(temperature=0.1, model='gpt-4-1106-preview')\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"Gemini\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"llm = Gemini(model='models/gemini-pro')\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e llm_choice == \u003cspan class=\"hljs-string\"\u003e\"Cohere\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"llm = Cohere(model='command', api_key='\u0026amp;lt;YOUR_API_KEY\u0026amp;gt;')  # Replace \u0026amp;lt;YOUR_API_KEY\u0026amp;gt; with your actual API key\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# Embedding model initialization\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003ef\"embed_model = HuggingFaceEmbedding(model_name='\u003cspan class=\"hljs-subst\"\u003e{embed_model_choice}\u003c/span\u003e')\\n\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# Node parser initialization\u003c/span\u003e\n    node_parsers = {\n        \u003cspan class=\"hljs-string\"\u003e\"SentenceSplitter\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003ef\"SentenceSplitter(chunk_size=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'chunk_size'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e)}\u003c/span\u003e, chunk_overlap=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'chunk_overlap'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)}\u003c/span\u003e)\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"CodeSplitter\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003ef\"CodeSplitter(language=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'language'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'python'\u003c/span\u003e)}\u003c/span\u003e, chunk_lines=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'chunk_lines'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e40\u003c/span\u003e)}\u003c/span\u003e, chunk_lines_overlap=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'chunk_lines_overlap'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e15\u003c/span\u003e)}\u003c/span\u003e, max_chars=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'max_chars'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1500\u003c/span\u003e)}\u003c/span\u003e)\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"SemanticSplitterNodeParser\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003ef\"SemanticSplitterNodeParser(buffer_size=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'buffer_size'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e)}\u003c/span\u003e, breakpoint_percentile_threshold=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'breakpoint_percentile_threshold'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e95\u003c/span\u003e)}\u003c/span\u003e, embed_model=embed_model)\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"TokenTextSplitter\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003ef\"TokenTextSplitter(chunk_size=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'chunk_size'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e)}\u003c/span\u003e, chunk_overlap=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'chunk_overlap'\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)}\u003c/span\u003e)\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"HTMLNodeParser\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003ef\"HTMLNodeParser(tags=\u003cspan class=\"hljs-subst\"\u003e{node_parser_params.get(\u003cspan class=\"hljs-string\"\u003e'tags'\u003c/span\u003e, [\u003cspan class=\"hljs-string\"\u003e'p'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'h1'\u003c/span\u003e])}\u003c/span\u003e)\"\u003c/span\u003e,  \n        \u003cspan class=\"hljs-string\"\u003e\"JSONNodeParser\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"JSONNodeParser()\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"MarkdownNodeParser\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"MarkdownNodeParser()\"\u003c/span\u003e\n    }\n    code_snippet += \u003cspan class=\"hljs-string\"\u003ef\"node_parser = \u003cspan class=\"hljs-subst\"\u003e{node_parsers[node_parser_choice]}\u003c/span\u003e\\n\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# Response mode\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003ef\"response_mode = '\u003cspan class=\"hljs-subst\"\u003e{response_mode}\u003c/span\u003e'\\n\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# Vector store initialization\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e vector_store_choice == \u003cspan class=\"hljs-string\"\u003e\"Pinecone\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\\n\"\u003c/span\u003e\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"index = pc.Index('test')\\n\"\u003c/span\u003e\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"vector_store = PineconeVectorStore(pinecone_index=index)\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e vector_store_choice == \u003cspan class=\"hljs-string\"\u003e\"Qdrant\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"client = qdrant_client.QdrantClient(location=':memory:')\\n\"\u003c/span\u003e\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"vector_store = QdrantVectorStore(client=client, collection_name='sampledata')\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelif\u003c/span\u003e vector_store_choice == \u003cspan class=\"hljs-string\"\u003e\"Simple\"\u003c/span\u003e:\n        code_snippet += \u003cspan class=\"hljs-string\"\u003e\"vector_store = None  # Simple in-memory vector store selected\\n\"\u003c/span\u003e\n\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"\\n# Finalizing the RAG pipeline setup\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"if vector_store is not None:\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"    storage_context = StorageContext.from_defaults(vector_store=vector_store)\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"else:\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"    storage_context = None\\n\\n\"\u003c/span\u003e\n\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, node_parser=node_parser)\\n\\n\"\u003c/span\u003e\n\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"_file = 'path_to_your_file'  # Replace with the path to your file\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"vector_index = VectorStoreIndex.from_documents(documents=_file, storage_context=storage_context, service_context=service_context, show_progress=True)\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"if storage_context:\\n\"\u003c/span\u003e\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"    vector_index.storage_context.persist(persist_dir='persist_dir')\\n\\n\"\u003c/span\u003e\n\n    code_snippet += \u003cspan class=\"hljs-string\"\u003e\"query_engine = vector_index.as_query_engine(response_mode=response_mode, verbose=True)\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e code_snippet\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eRAGArch stands at the intersection of innovation and practicality, offering a streamlined no-code approach to RAG pipeline development. It’s designed to demystify the complexities of AI configurations. With RAGArch, both seasoned developers and AI enthusiasts can craft custom pipelines with ease, accelerating the journey from idea to implementation.\u003c/p\u003e\u003cp\u003eYour insights and contributions are invaluable as I continue to evolve this tool. Check out RAGArch on Github and let’s start a conversation on Linkedin. I’m always eager to collaborate and share knowledge with fellow tech adventurers.\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/AI-ANK/RAGArch\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGitHub Repo\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://www.linkedin.com/in/harshadsuryawanshi/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eConnect with Me on LinkedIn\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://huggingface.co/spaces/AI-ANK/RAGArch\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLive Demo\u003c/a\u003e\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/96bf01e854f82bcd0499b3c23e7f07370c3cf73d-1024x1024.png","publishedDate":"2024-02-02","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562-png","_type":"reference"}},"publishedDate":"2025-05-29","slug":"rag-is-dead-long-live-agentic-retrieval","title":"RAG is dead, long live agentic retrieval"},{"featured":true,"image":{"_type":"image","asset":{"_ref":"image-13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631-png","_type":"reference"}},"publishedDate":"2025-04-23","slug":"beyond-chatbots-adopting-agentic-document-workflows-for-enterprises","title":"Beyond chatbots: adopting Agentic Document Workflows for enterprises"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647-png","_type":"reference"}},"publishedDate":"2024-11-05","slug":"building-blocks-of-llm-report-generation-beyond-basic-rag","title":"Building Blocks of LLM Report Generation: Beyond Basic RAG"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-ecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800-png","_type":"reference"}},"publishedDate":"2024-08-27","slug":"building-a-serverless-rag-application-with-llamaindex-and-azure-openai","title":"Building a serverless RAG application with LlamaIndex and Azure OpenAI"}],"slug":{"_type":"slug","current":"ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"893258fa-46ae-4ae2-b1a3-acb12849ab60","_rev":"RDEDF5eNko8cW03GEH0cXj","_type":"blogTag","_updatedAt":"2024-08-21T19:17:20Z","slug":{"_type":"slug","current":"rag"},"title":"RAG"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"0bb86b28-2181-4c07-89d2-d63594e7fb5c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"no-code"},"title":"No Code"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"e171aa9d-bc85-4645-8a08-eabe04c530c7","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"openai"},"title":"OpenAI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"3c300a42-e3f3-4c5e-974f-1e6c77e5bd8d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"code-generation"},"title":"Code Generation"}],"title":"RAGArch: Building a No-Code RAG Pipeline Configuration \u0026 One-Click RAG Code Generation Tool Powered by LlamaIndex"},"publishedDate":"Invalid Date"},"params":{"slug":"ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"ragarch-building-a-no-code-rag-pipeline-configuration-one-click-rag-code-generation-tool-powered-b6e8eeb70089"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>