<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Becoming Proficient in Document Extraction — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Becoming Proficient in Document Extraction — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Becoming Proficient in Document Extraction — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/c624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Becoming Proficient in Document Extraction — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/c624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="557" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/ankush-k-singal">Ankush k Singal</a> <!-- -->•<!-- --> <!-- -->2023-11-20</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Becoming Proficient in Document Extraction</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/technology"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Technology</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/software-development"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Software Development</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/software-engineering"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Software Engineering</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/tech"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Tech</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/artificial-intelligence"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Artificial Intelligence</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><h2>
  <strong>Introduction</strong>
</h2>
<p>
  In the domain of document handling, accurately extracting crucial information
  from images has posed an enduring obstacle. Despite Optical Character
  Recognition (OCR) advancements in converting images to editable text, it faces
  numerous intricacies with diverse document formats and quality. Here enters
  Zephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly
  addresses these hurdles, heralding a transformative era in image-based
  document extraction.
</p>
<figure>
  <img
    src="/blog/images/1*JFum54ZY63bo7QfJOKi1WQ.png"
    alt=""
    width="700"
    height="401"
    loading="lazy"
    role="presentation"
  />
  <figcaption>
    Source:
    <a
      href="https://corca.substack.com/p/top-llm-papers-of-the-week-95e?utm_source=profile&amp;utm_medium=reader2"
      rel="noopener ugc nofollow"
      target="_blank"
      >Zephyr-llama-index</a
    >
  </figcaption>
</figure>
<h2>
  <strong>The OCR Dilemma: Obstacles and Constraints Optical Character</strong>
</h2>
<p>Recognition (OCR), though potent, faces impediments such as:</p>
<ol>
  <li>
    <strong>Diverse Document Formats</strong>: Documents exhibit intricate
    layouts, fonts, and structures, posing challenges for traditional OCR to
    precisely interpret and extract information.
  </li>
  <li>
    <strong>Quality and Clarity</strong>: Images with low resolution,
    blurriness, or skewed angles hinder OCR’s accuracy in deciphering text.
  </li>
  <li>
    <strong>Handwritten and Cursive Content</strong>: OCR often struggles with
    handwritten text or cursive fonts, resulting in errors or incomplete
    extraction.
  </li>
  <li>
    <strong>Multilingual Complexity</strong>: Processing documents in multiple
    languages poses a challenge for OCR systems lacking proficiency in
    recognizing and extracting varied linguistic content.
  </li>
</ol>
<figure>
  <img
    alt=""
    src="/blog/images/1*Wjgk1UrzmpXz83skj2TxpQ.png"
    width="700"
    height="717"
    loading="lazy"
    role="presentation"
  />
  <figcaption>Source: Created by Author using MidJourney</figcaption>
</figure>
<h2>
  <strong>Zephyr 7b LLM: Narrowing the Divide</strong>
</h2>
<p>
  Zephyr 7b LLM revolutionizes the landscape by tackling these inherent
  constraints of OCR technology:
</p>
<ol>
  <li>
    <strong>Advanced Machine Learning Algorithms:</strong>
  </li>
</ol>
<p>
  Employing state-of-the-art machine learning algorithms, Zephyr 7b LLM
  undergoes extensive training with diverse document formats and languages. This
  equips it to adapt and learn from various document structures, resulting in
  heightened accuracy and robust extraction capabilities.
</p>
<p>
  <strong>2. Contextual Comprehension:</strong>
</p>
<p>
  Diverging from conventional OCR, Zephyr 7b LLM doesn’t merely identify
  individual characters; it comprehends the context in which these characters
  exist. This contextual understanding significantly reduces errors, ensuring
  precise extraction even from intricate document layouts.
</p>
<p>
  <strong>3. Adaptive Image Processing:</strong>
</p>
<p>
  The fusion with LlamaIndex amplifies Zephyr 7b LLM’s ability to handle images
  of varying resolutions or qualities. Leveraging adaptive image processing
  techniques, it rectifies distortions, enhances clarity, and optimizes images
  for meticulous OCR analysis.
</p>
<p>
  <strong>4. Multilingual Proficiency:</strong>
</p>
<p>
  Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency
  facilitates seamless content extraction from documents in various languages,
  extending global accessibility for businesses dealing with multilingual
  documentation.
</p>
<figure>
  <img
    alt=""
    src="/blog/images/1*1DVGFV2TzRGzsbKfb54zwQ.png"
    width="700"
    height="697"
    loading="lazy"
    role="presentation"
  />
  <figcaption>Source: Created by Author using MidJourney</figcaption>
</figure>
<h2>
  <strong>Implementation of Code</strong>
</h2>
<p>
  The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal
  transformation in document extraction. By merging Zephyr’s advanced OCR
  capabilities with LlamaIndex’s image enhancement and data organization
  features, this integration presents a comprehensive solution:
</p>
<ol>
  <li>
    <strong>Augmented Precision</strong>: The fusion of Zephyr’s machine
    learning expertise and LlamaIndex’s image enhancement markedly heightens the
    accuracy of extracted data, diminishing errors and enhancing overall
    efficiency.
  </li>
  <li>
    <strong>Efficient Workflow</strong>: Users experience an optimized workflow,
    enabling swift extraction and conversion of image-based documents into
    structured, actionable data, facilitating expedited decision-making
    processes.
  </li>
  <li>
    <strong>Adaptability Across Document Varieties</strong>: This integration
    empowers users to handle diverse document formats and languages
    effortlessly, granting access to previously challenging document types for
    extraction and analysis.
  </li>
</ol>
<figure>
  <img
    src="/blog/images/1*m3B-1XOL6t_5nojWWOBtgw.png"
    alt=""
    width="700"
    height="699"
    loading="lazy"
    role="presentation"
  />
  <figcaption>Source: Image created by Author using MidJourney</figcaption>
</figure>
<p>
  <strong>Step 1: Install and Import Libraries</strong>
</p>
<pre><span  >!pip install llama-index transformers accelerate sentencepiece bitsandbytes -q</span></pre>
<p>
  <strong>Step 2: Load the Model</strong>
</p>
<pre><span  >import torch<br>from transformers import BitsAndBytesConfig<br>from llama_index.prompts import PromptTemplate<br>from llama_index.llms import HuggingFaceLLM<br><br>quantization_config = BitsAndBytesConfig(<br>    load_in_4bit=True,<br>    bnb_4bit_compute_dtype=torch.float16,<br>    bnb_4bit_quant_type="nf4",<br>    bnb_4bit_use_double_quant=True,<br>)<br><br>def messages_to_prompt(messages):<br>  prompt = ""<br>  for message in messages:<br>    if message.role == 'system':<br>      prompt += f"&lt;|system|&gt;\n{message.content}&lt;/s&gt;\n"<br>    elif message.role == 'user':<br>      prompt += f"&lt;|user|&gt;\n{message.content}&lt;/s&gt;\n"<br>    elif message.role == 'assistant':<br>      prompt += f"&lt;|assistant|&gt;\n{message.content}&lt;/s&gt;\n"<br><br>  # ensure we start with a system prompt, insert blank if needed<br>  if not prompt.startswith("&lt;|system|&gt;\n"):<br>    prompt = "&lt;|system|&gt;\n&lt;/s&gt;\n" + prompt<br><br>  # add final assistant prompt<br>  prompt = prompt + "&lt;|assistant|&gt;\n"<br><br>  return prompt<br><br><br>llm = HuggingFaceLLM(<br>    model_name="HuggingFaceH4/zephyr-7b-alpha",<br>    tokenizer_name="HuggingFaceH4/zephyr-7b-alpha",<br>    query_wrapper_prompt=PromptTemplate("&lt;|system|&gt;\n&lt;/s&gt;\n&lt;|user|&gt;\n{query_str}&lt;/s&gt;\n&lt;|assistant|&gt;\n"),<br>    context_window=3900,<br>    max_new_tokens=2000,<br>    model_kwargs={"quantization_config": quantization_config},<br>    # tokenizer_kwargs={},<br>    generate_kwargs={"temperature": 0.7, "top_k": 50, "top_p": 0.95},<br>    messages_to_prompt=messages_to_prompt,<br>    device_map="auto",<br>)</span></pre>
<pre><span  >from llama_index import ServiceContext, set_global_service_context<br><br>service_context = ServiceContext.from_defaults(llm=llm, embed_model="local:BAAI/bge-small-en-v1.5")</span></pre>
<pre><span  >set_global_service_context(service_context)</span></pre>
<p>
  <strong>Step 3: Storing your index</strong>
</p>
<pre><span  >from llama_index import SimpleDirectoryReader, VectorStoreIndex<br>from llama_index.readers.file.base import (<br>    DEFAULT_FILE_READER_CLS,<br>    ImageReader,<br>)<br>from llama_index.response.notebook_utils import (<br>    display_response,<br>    display_image,<br>)<br>from llama_index.indices.query.query_transform.base import (<br>    ImageOutputQueryTransform,<br>)<br><br>filename_fn = lambda filename: {"file_name": filename}<br><br>llama_reader = SimpleDirectoryReader(<br>    input_dir="/content/llama",<br>    file_metadata=filename_fn,<br>)<br>llama_documents = llama_reader.load_data()<br><br>llama_index = VectorStoreIndex.from_documents(llama_documents)</span></pre>
<p>
  <strong>Step 4: Query </strong
  ><a
    href="https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha"
    rel="noopener ugc nofollow"
    target="_blank"
    ><strong>Transformations</strong></a
  >
</p>
<pre><span  >from llama_index.query_engine import TransformQueryEngine<br><br><br>query_engine = llama_index.as_query_engine(similarity_top_k=2)<br>query_engine = TransformQueryEngine(<br>    query_engine, query_transform=ImageOutputQueryTransform(width=400)<br>)<br>llama_response = query_engine.query(<br>    "Show an image to illustrate how tree index works and explain briefly",<br>)<br><br>display_response(llama_response)<br><br>#Output<br>Final Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works.<br><br>tree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance.<br><br>when querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter.<br><br>for example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent.<br><br>the following image illustrates how a tree index works:<br><br>! Tree Index Example<br><br>in this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value "x", we would start at the root node and follow the left branch (since "x" is less than "a") to the next level. we would then follow the left branch again to reach the leaf node with the value "x".<br><br>i hope this helps clarify how tree index works!</span></pre>
<p>
  <strong>Step 5: Lets read the </strong
  ><a
    href="https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha"
    rel="noopener ugc nofollow"
    target="_blank"
    ><strong>receipts</strong></a
  >
</p>
<pre><span  >from llama_index.readers.file.base import DEFAULT_FILE_READER_CLS<br>from llama_index.readers.file.image_reader import ImageReader<br><br>image_parser =ImageReader(<br>    keep_image=True,<br>    parse_text=True<br>    )<br>file_extractor = DEFAULT_FILE_READER_CLS<br>file_extractor.update({<br>    ".jpg": image_parser,<br>    ".png": image_parser,<br>    ".jpeg": image_parser,<br>    })<br><br>receipt_reader = SimpleDirectoryReader(<br>    input_dir="/content/data",<br>    file_metadata=filename_fn,<br>    file_extractor=file_extractor,<br>)<br>receipt_documents = receipt_reader.load_data()<br>print(len(receipt_documents))<br><br>#Output<br>3</span></pre>
<pre><span  >receipts_index = VectorStoreIndex.from_documents(receipt_documents)<br><br>from llama_index.query_engine import TransformQueryEngine<br>query_engine = receipts_index.as_query_engine()<br><br>receipts_response = query_engine.query(<br>    "When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.",<br>)<br><br>display_response(receipts_response)<br><br># Output <br>Final Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.</span></pre>
<h2>Conclusion</h2>
<p>
  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter
  in image-based document extraction. Beyond addressing OCR’s inherent
  challenges, it enhances the precision and efficiency of data extraction from
  images, fostering improved productivity and decision-making in
  document-focused workflows.
</p>
<p>“Stay connected and support my work through various platforms:</p>
<ul>
  <li>
    <a href="https://medium.com/u/8df3bf3c40ae" rel="noopener">GitHub</a>: For
    all my open-source projects and Notebooks, you can visit my GitHub profile
    at
    <a
      href="https://github.com/andysingal"
      rel="noopener ugc nofollow"
      target="_blank"
      >https://github.com/andysingal</a
    >. If you find my content valuable, don’t hesitate to leave a star.
  </li>
  <li>
    Patreon: If you’d like to provide additional support, you can consider
    becoming a patron on my Patreon page at
    <a
      href="https://www.patreon.com/AndyShanu"
      rel="noopener ugc nofollow"
      target="_blank"
      >https://www.patreon.com/AndyShanu</a
    >.
  </li>
  <li>
    <a href="https://medium.com/u/504c7870fdb6" rel="noopener">Medium</a>: You
    can read my latest articles and insights on Medium at
    <a href="https://medium.com/@andysingal" rel="noopener"
      >https://medium.com/@andysingal</a
    >.
  </li>
  <li>
    <a href="https://medium.com/u/29b47aa8cce3" rel="noopener">The Kaggle</a>:
    Check out my Kaggle profile for data science and machine learning projects
    at
    <a
      href="https://www.kaggle.com/alphasingal"
      rel="noopener ugc nofollow"
      target="_blank"
      >https://www.kaggle.com/alphasingal</a
    >.
  </li>
  <li>
    <a href="https://medium.com/u/b1574f0c6c5e" rel="noopener">Hugging Face</a>:
    For natural language processing and AI-related projects, you can explore my
    Huggingface profile at
    <a
      href="https://huggingface.co/Andyrasika"
      rel="noopener ugc nofollow"
      target="_blank"
      >https://huggingface.co/Andyrasika</a
    >.
  </li>
  <li>
    YouTube: To watch my video content, visit my YouTube channel at
    <a
      href="https://www.youtube.com/@andy111007"
      rel="noopener ugc nofollow"
      target="_blank"
      >https://www.youtube.com/@andy111007</a
    >.
  </li>
  <li>
    LinkedIn: To stay updated on my latest projects and posts, you can follow me
    on LinkedIn. Here is the link to my profile:
    <a
      href="https://www.linkedin.com/in/ankushsingal/.%22"
      rel="noopener ugc nofollow"
      target="_blank"
      >https://www.linkedin.com/in/ankushsingal/."</a
    >
  </li>
</ul>
<p>
  Requests and questions: If you have a project in mind that you’d like me to
  work on or if you have any questions about the concepts I’ve explained, don’t
  hesitate to let me know. I’m always looking for new ideas for future Notebooks
  and I love helping to resolve any doubts you might have.
</p>
<p>
  Remember, each “Like”, “Share”, and “Star” greatly contributes to my work and
  motivates me to continue producing more quality content. Thank you for your
  support!
</p>
<p>
  If you enjoyed this story, feel free
  <a href="https://medium.com/@andysingal" rel="noopener">to subscribe</a>
  to Medium, and you will get notifications when my new articles will be
  published, as well as full access to thousands of stories from other authors.
</p>
<p>Resource:</p>
<ul>
  <li>
    <a
      href="https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha"
      rel="noopener ugc nofollow"
      target="_blank"
      >Data used for above code</a
    >
  </li>
  <li>
    <a
      href="https://gpt-index.readthedocs.io/en/stable/"
      rel="noopener ugc nofollow"
      target="_blank"
      >llama-index</a
    >
  </li>
</ul>
</div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5072f44cf25db4b603e6a20487d2b46085ef6f05-1536x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5072f44cf25db4b603e6a20487d2b46085ef6f05-1536x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5072f44cf25db4b603e6a20487d2b46085ef6f05-1536x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/python-tooling-at-scale-llamaindex-s-monorepo-overhaul">Python Tooling at Scale: LlamaIndex’s Monorepo Overhaul</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-21</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F8890c5098dbde95bd57d8c230fb46ee60bede234-2400x1420.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F8890c5098dbde95bd57d8c230fb46ee60bede234-2400x1420.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F8890c5098dbde95bd57d8c230fb46ee60bede234-2400x1420.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/agentic-rag-with-llamaindex-2721b8a49ff6">Agentic RAG With LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-01-30</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F1f5feeb7c7670e074431b6788725fd576d8b4663-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F1f5feeb7c7670e074431b6788725fd576d8b4663-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F1f5feeb7c7670e074431b6788725fd576d8b4663-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070">How to train a custom GPT on your data with EmbedAI + LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2023-12-14</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F396424d55af5fa49327b9f248c5b5ce3025cce30-3648x1271.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F396424d55af5fa49327b9f248c5b5ce3025cce30-3648x1271.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F396424d55af5fa49327b9f248c5b5ce3025cce30-3648x1271.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af">LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2023-08-29</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"64ad0ec2-3bf9-4cc8-a6e4-1026a61dcf3e","_rev":"05dtDS0H5iRVsxYMarZBDu","_type":"blogPost","_updatedAt":"2025-05-21T20:37:56Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:51:08Z","_id":"27a2a493-cc3b-4dc4-8853-58da5b684075","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Ankush k Singal","slug":{"_type":"slug","current":"ankush-k-singal"}}],"featured":false,"htmlContent":"\u003ch2\u003e\n  \u003cstrong\u003eIntroduction\u003c/strong\u003e\n\u003c/h2\u003e\n\u003cp\u003e\n  In the domain of document handling, accurately extracting crucial information\n  from images has posed an enduring obstacle. Despite Optical Character\n  Recognition (OCR) advancements in converting images to editable text, it faces\n  numerous intricacies with diverse document formats and quality. Here enters\n  Zephyr 7b LLM, a pioneering remedy that, coupled with LlamaIndex, directly\n  addresses these hurdles, heralding a transformative era in image-based\n  document extraction.\n\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg\n    src=\"/blog/images/1*JFum54ZY63bo7QfJOKi1WQ.png\"\n    alt=\"\"\n    width=\"700\"\n    height=\"401\"\n    loading=\"lazy\"\n    role=\"presentation\"\n  /\u003e\n  \u003cfigcaption\u003e\n    Source:\n    \u003ca\n      href=\"https://corca.substack.com/p/top-llm-papers-of-the-week-95e?utm_source=profile\u0026amp;utm_medium=reader2\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003eZephyr-llama-index\u003c/a\n    \u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2\u003e\n  \u003cstrong\u003eThe OCR Dilemma: Obstacles and Constraints Optical Character\u003c/strong\u003e\n\u003c/h2\u003e\n\u003cp\u003eRecognition (OCR), though potent, faces impediments such as:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eDiverse Document Formats\u003c/strong\u003e: Documents exhibit intricate\n    layouts, fonts, and structures, posing challenges for traditional OCR to\n    precisely interpret and extract information.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eQuality and Clarity\u003c/strong\u003e: Images with low resolution,\n    blurriness, or skewed angles hinder OCR’s accuracy in deciphering text.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eHandwritten and Cursive Content\u003c/strong\u003e: OCR often struggles with\n    handwritten text or cursive fonts, resulting in errors or incomplete\n    extraction.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eMultilingual Complexity\u003c/strong\u003e: Processing documents in multiple\n    languages poses a challenge for OCR systems lacking proficiency in\n    recognizing and extracting varied linguistic content.\n  \u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\n  \u003cimg\n    alt=\"\"\n    src=\"/blog/images/1*Wjgk1UrzmpXz83skj2TxpQ.png\"\n    width=\"700\"\n    height=\"717\"\n    loading=\"lazy\"\n    role=\"presentation\"\n  /\u003e\n  \u003cfigcaption\u003eSource: Created by Author using MidJourney\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2\u003e\n  \u003cstrong\u003eZephyr 7b LLM: Narrowing the Divide\u003c/strong\u003e\n\u003c/h2\u003e\n\u003cp\u003e\n  Zephyr 7b LLM revolutionizes the landscape by tackling these inherent\n  constraints of OCR technology:\n\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eAdvanced Machine Learning Algorithms:\u003c/strong\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\n  Employing state-of-the-art machine learning algorithms, Zephyr 7b LLM\n  undergoes extensive training with diverse document formats and languages. This\n  equips it to adapt and learn from various document structures, resulting in\n  heightened accuracy and robust extraction capabilities.\n\u003c/p\u003e\n\u003cp\u003e\n  \u003cstrong\u003e2. Contextual Comprehension:\u003c/strong\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  Diverging from conventional OCR, Zephyr 7b LLM doesn’t merely identify\n  individual characters; it comprehends the context in which these characters\n  exist. This contextual understanding significantly reduces errors, ensuring\n  precise extraction even from intricate document layouts.\n\u003c/p\u003e\n\u003cp\u003e\n  \u003cstrong\u003e3. Adaptive Image Processing:\u003c/strong\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  The fusion with LlamaIndex amplifies Zephyr 7b LLM’s ability to handle images\n  of varying resolutions or qualities. Leveraging adaptive image processing\n  techniques, it rectifies distortions, enhances clarity, and optimizes images\n  for meticulous OCR analysis.\n\u003c/p\u003e\n\u003cp\u003e\n  \u003cstrong\u003e4. Multilingual Proficiency:\u003c/strong\u003e\n\u003c/p\u003e\n\u003cp\u003e\n  Zephyr 7b LLM surpasses language barriers. Its multilingual proficiency\n  facilitates seamless content extraction from documents in various languages,\n  extending global accessibility for businesses dealing with multilingual\n  documentation.\n\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg\n    alt=\"\"\n    src=\"/blog/images/1*1DVGFV2TzRGzsbKfb54zwQ.png\"\n    width=\"700\"\n    height=\"697\"\n    loading=\"lazy\"\n    role=\"presentation\"\n  /\u003e\n  \u003cfigcaption\u003eSource: Created by Author using MidJourney\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003ch2\u003e\n  \u003cstrong\u003eImplementation of Code\u003c/strong\u003e\n\u003c/h2\u003e\n\u003cp\u003e\n  The collaboration between Zephyr 7b LLM and LlamaIndex signifies a pivotal\n  transformation in document extraction. By merging Zephyr’s advanced OCR\n  capabilities with LlamaIndex’s image enhancement and data organization\n  features, this integration presents a comprehensive solution:\n\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eAugmented Precision\u003c/strong\u003e: The fusion of Zephyr’s machine\n    learning expertise and LlamaIndex’s image enhancement markedly heightens the\n    accuracy of extracted data, diminishing errors and enhancing overall\n    efficiency.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eEfficient Workflow\u003c/strong\u003e: Users experience an optimized workflow,\n    enabling swift extraction and conversion of image-based documents into\n    structured, actionable data, facilitating expedited decision-making\n    processes.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cstrong\u003eAdaptability Across Document Varieties\u003c/strong\u003e: This integration\n    empowers users to handle diverse document formats and languages\n    effortlessly, granting access to previously challenging document types for\n    extraction and analysis.\n  \u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\n  \u003cimg\n    src=\"/blog/images/1*m3B-1XOL6t_5nojWWOBtgw.png\"\n    alt=\"\"\n    width=\"700\"\n    height=\"699\"\n    loading=\"lazy\"\n    role=\"presentation\"\n  /\u003e\n  \u003cfigcaption\u003eSource: Image created by Author using MidJourney\u003c/figcaption\u003e\n\u003c/figure\u003e\n\u003cp\u003e\n  \u003cstrong\u003eStep 1: Install and Import Libraries\u003c/strong\u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003cspan  \u003e!pip install llama-index transformers accelerate sentencepiece bitsandbytes -q\u003c/span\u003e\u003c/pre\u003e\n\u003cp\u003e\n  \u003cstrong\u003eStep 2: Load the Model\u003c/strong\u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003cspan  \u003eimport torch\u003cbr\u003efrom transformers import BitsAndBytesConfig\u003cbr\u003efrom llama_index.prompts import PromptTemplate\u003cbr\u003efrom llama_index.llms import HuggingFaceLLM\u003cbr\u003e\u003cbr\u003equantization_config = BitsAndBytesConfig(\u003cbr\u003e    load_in_4bit=True,\u003cbr\u003e    bnb_4bit_compute_dtype=torch.float16,\u003cbr\u003e    bnb_4bit_quant_type=\"nf4\",\u003cbr\u003e    bnb_4bit_use_double_quant=True,\u003cbr\u003e)\u003cbr\u003e\u003cbr\u003edef messages_to_prompt(messages):\u003cbr\u003e  prompt = \"\"\u003cbr\u003e  for message in messages:\u003cbr\u003e    if message.role == 'system':\u003cbr\u003e      prompt += f\"\u0026lt;|system|\u0026gt;\\n{message.content}\u0026lt;/s\u0026gt;\\n\"\u003cbr\u003e    elif message.role == 'user':\u003cbr\u003e      prompt += f\"\u0026lt;|user|\u0026gt;\\n{message.content}\u0026lt;/s\u0026gt;\\n\"\u003cbr\u003e    elif message.role == 'assistant':\u003cbr\u003e      prompt += f\"\u0026lt;|assistant|\u0026gt;\\n{message.content}\u0026lt;/s\u0026gt;\\n\"\u003cbr\u003e\u003cbr\u003e  # ensure we start with a system prompt, insert blank if needed\u003cbr\u003e  if not prompt.startswith(\"\u0026lt;|system|\u0026gt;\\n\"):\u003cbr\u003e    prompt = \"\u0026lt;|system|\u0026gt;\\n\u0026lt;/s\u0026gt;\\n\" + prompt\u003cbr\u003e\u003cbr\u003e  # add final assistant prompt\u003cbr\u003e  prompt = prompt + \"\u0026lt;|assistant|\u0026gt;\\n\"\u003cbr\u003e\u003cbr\u003e  return prompt\u003cbr\u003e\u003cbr\u003e\u003cbr\u003ellm = HuggingFaceLLM(\u003cbr\u003e    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\u003cbr\u003e    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\u003cbr\u003e    query_wrapper_prompt=PromptTemplate(\"\u0026lt;|system|\u0026gt;\\n\u0026lt;/s\u0026gt;\\n\u0026lt;|user|\u0026gt;\\n{query_str}\u0026lt;/s\u0026gt;\\n\u0026lt;|assistant|\u0026gt;\\n\"),\u003cbr\u003e    context_window=3900,\u003cbr\u003e    max_new_tokens=2000,\u003cbr\u003e    model_kwargs={\"quantization_config\": quantization_config},\u003cbr\u003e    # tokenizer_kwargs={},\u003cbr\u003e    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\u003cbr\u003e    messages_to_prompt=messages_to_prompt,\u003cbr\u003e    device_map=\"auto\",\u003cbr\u003e)\u003c/span\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003cspan  \u003efrom llama_index import ServiceContext, set_global_service_context\u003cbr\u003e\u003cbr\u003eservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\u003c/span\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003cspan  \u003eset_global_service_context(service_context)\u003c/span\u003e\u003c/pre\u003e\n\u003cp\u003e\n  \u003cstrong\u003eStep 3: Storing your index\u003c/strong\u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003cspan  \u003efrom llama_index import SimpleDirectoryReader, VectorStoreIndex\u003cbr\u003efrom llama_index.readers.file.base import (\u003cbr\u003e    DEFAULT_FILE_READER_CLS,\u003cbr\u003e    ImageReader,\u003cbr\u003e)\u003cbr\u003efrom llama_index.response.notebook_utils import (\u003cbr\u003e    display_response,\u003cbr\u003e    display_image,\u003cbr\u003e)\u003cbr\u003efrom llama_index.indices.query.query_transform.base import (\u003cbr\u003e    ImageOutputQueryTransform,\u003cbr\u003e)\u003cbr\u003e\u003cbr\u003efilename_fn = lambda filename: {\"file_name\": filename}\u003cbr\u003e\u003cbr\u003ellama_reader = SimpleDirectoryReader(\u003cbr\u003e    input_dir=\"/content/llama\",\u003cbr\u003e    file_metadata=filename_fn,\u003cbr\u003e)\u003cbr\u003ellama_documents = llama_reader.load_data()\u003cbr\u003e\u003cbr\u003ellama_index = VectorStoreIndex.from_documents(llama_documents)\u003c/span\u003e\u003c/pre\u003e\n\u003cp\u003e\n  \u003cstrong\u003eStep 4: Query \u003c/strong\n  \u003e\u003ca\n    href=\"https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha\"\n    rel=\"noopener ugc nofollow\"\n    target=\"_blank\"\n    \u003e\u003cstrong\u003eTransformations\u003c/strong\u003e\u003c/a\n  \u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003cspan  \u003efrom llama_index.query_engine import TransformQueryEngine\u003cbr\u003e\u003cbr\u003e\u003cbr\u003equery_engine = llama_index.as_query_engine(similarity_top_k=2)\u003cbr\u003equery_engine = TransformQueryEngine(\u003cbr\u003e    query_engine, query_transform=ImageOutputQueryTransform(width=400)\u003cbr\u003e)\u003cbr\u003ellama_response = query_engine.query(\u003cbr\u003e    \"Show an image to illustrate how tree index works and explain briefly\",\u003cbr\u003e)\u003cbr\u003e\u003cbr\u003edisplay_response(llama_response)\u003cbr\u003e\u003cbr\u003e#Output\u003cbr\u003eFinal Response: I am not capable of displaying images. however, i can provide you with an explanation of how tree index works.\u003cbr\u003e\u003cbr\u003etree index is a data structure that organizes data in a hierarchical manner, similar to a tree. it is commonly used in databases to improve query performance.\u003cbr\u003e\u003cbr\u003ewhen querying a tree index, the process involves traversing from the root node down to the leaf nodes. the number of child nodes chosen per parent node is determined by the child_branch_factor parameter.\u003cbr\u003e\u003cbr\u003efor example, if child_branch_factor=1, a query will choose one child node given a parent node. if child_branch_factor=2, a query will choose two child nodes per parent.\u003cbr\u003e\u003cbr\u003ethe following image illustrates how a tree index works:\u003cbr\u003e\u003cbr\u003e! Tree Index Example\u003cbr\u003e\u003cbr\u003ein this example, the tree index is built from a set of nodes (which become leaf nodes in this tree). when querying this index, the process involves traversing from the root node down to the leaf nodes. for instance, if we want to find a specific node with the value \"x\", we would start at the root node and follow the left branch (since \"x\" is less than \"a\") to the next level. we would then follow the left branch again to reach the leaf node with the value \"x\".\u003cbr\u003e\u003cbr\u003ei hope this helps clarify how tree index works!\u003c/span\u003e\u003c/pre\u003e\n\u003cp\u003e\n  \u003cstrong\u003eStep 5: Lets read the \u003c/strong\n  \u003e\u003ca\n    href=\"https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha\"\n    rel=\"noopener ugc nofollow\"\n    target=\"_blank\"\n    \u003e\u003cstrong\u003ereceipts\u003c/strong\u003e\u003c/a\n  \u003e\n\u003c/p\u003e\n\u003cpre\u003e\u003cspan  \u003efrom llama_index.readers.file.base import DEFAULT_FILE_READER_CLS\u003cbr\u003efrom llama_index.readers.file.image_reader import ImageReader\u003cbr\u003e\u003cbr\u003eimage_parser =ImageReader(\u003cbr\u003e    keep_image=True,\u003cbr\u003e    parse_text=True\u003cbr\u003e    )\u003cbr\u003efile_extractor = DEFAULT_FILE_READER_CLS\u003cbr\u003efile_extractor.update({\u003cbr\u003e    \".jpg\": image_parser,\u003cbr\u003e    \".png\": image_parser,\u003cbr\u003e    \".jpeg\": image_parser,\u003cbr\u003e    })\u003cbr\u003e\u003cbr\u003ereceipt_reader = SimpleDirectoryReader(\u003cbr\u003e    input_dir=\"/content/data\",\u003cbr\u003e    file_metadata=filename_fn,\u003cbr\u003e    file_extractor=file_extractor,\u003cbr\u003e)\u003cbr\u003ereceipt_documents = receipt_reader.load_data()\u003cbr\u003eprint(len(receipt_documents))\u003cbr\u003e\u003cbr\u003e#Output\u003cbr\u003e3\u003c/span\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003cspan  \u003ereceipts_index = VectorStoreIndex.from_documents(receipt_documents)\u003cbr\u003e\u003cbr\u003efrom llama_index.query_engine import TransformQueryEngine\u003cbr\u003equery_engine = receipts_index.as_query_engine()\u003cbr\u003e\u003cbr\u003ereceipts_response = query_engine.query(\u003cbr\u003e    \"When was the last time I went to RESTAURANT and how much did I spend? this data is in your latest vector index.\",\u003cbr\u003e)\u003cbr\u003e\u003cbr\u003edisplay_response(receipts_response)\u003cbr\u003e\u003cbr\u003e# Output \u003cbr\u003eFinal Response: Based on the given context information, the last time the querying individual went to RESTAURANT was on July 5, 2019, and they spent $164.00.\u003c/span\u003e\u003c/pre\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003e\n  In summary, the fusion of Zephyr 7b LLM and LlamaIndex initiates a new chapter\n  in image-based document extraction. Beyond addressing OCR’s inherent\n  challenges, it enhances the precision and efficiency of data extraction from\n  images, fostering improved productivity and decision-making in\n  document-focused workflows.\n\u003c/p\u003e\n\u003cp\u003e“Stay connected and support my work through various platforms:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003ca href=\"https://medium.com/u/8df3bf3c40ae\" rel=\"noopener\"\u003eGitHub\u003c/a\u003e: For\n    all my open-source projects and Notebooks, you can visit my GitHub profile\n    at\n    \u003ca\n      href=\"https://github.com/andysingal\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ehttps://github.com/andysingal\u003c/a\n    \u003e. If you find my content valuable, don’t hesitate to leave a star.\n  \u003c/li\u003e\n  \u003cli\u003e\n    Patreon: If you’d like to provide additional support, you can consider\n    becoming a patron on my Patreon page at\n    \u003ca\n      href=\"https://www.patreon.com/AndyShanu\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ehttps://www.patreon.com/AndyShanu\u003c/a\n    \u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003ca href=\"https://medium.com/u/504c7870fdb6\" rel=\"noopener\"\u003eMedium\u003c/a\u003e: You\n    can read my latest articles and insights on Medium at\n    \u003ca href=\"https://medium.com/@andysingal\" rel=\"noopener\"\n      \u003ehttps://medium.com/@andysingal\u003c/a\n    \u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003ca href=\"https://medium.com/u/29b47aa8cce3\" rel=\"noopener\"\u003eThe Kaggle\u003c/a\u003e:\n    Check out my Kaggle profile for data science and machine learning projects\n    at\n    \u003ca\n      href=\"https://www.kaggle.com/alphasingal\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ehttps://www.kaggle.com/alphasingal\u003c/a\n    \u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003ca href=\"https://medium.com/u/b1574f0c6c5e\" rel=\"noopener\"\u003eHugging Face\u003c/a\u003e:\n    For natural language processing and AI-related projects, you can explore my\n    Huggingface profile at\n    \u003ca\n      href=\"https://huggingface.co/Andyrasika\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ehttps://huggingface.co/Andyrasika\u003c/a\n    \u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    YouTube: To watch my video content, visit my YouTube channel at\n    \u003ca\n      href=\"https://www.youtube.com/@andy111007\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ehttps://www.youtube.com/@andy111007\u003c/a\n    \u003e.\n  \u003c/li\u003e\n  \u003cli\u003e\n    LinkedIn: To stay updated on my latest projects and posts, you can follow me\n    on LinkedIn. Here is the link to my profile:\n    \u003ca\n      href=\"https://www.linkedin.com/in/ankushsingal/.%22\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ehttps://www.linkedin.com/in/ankushsingal/.\"\u003c/a\n    \u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n  Requests and questions: If you have a project in mind that you’d like me to\n  work on or if you have any questions about the concepts I’ve explained, don’t\n  hesitate to let me know. I’m always looking for new ideas for future Notebooks\n  and I love helping to resolve any doubts you might have.\n\u003c/p\u003e\n\u003cp\u003e\n  Remember, each “Like”, “Share”, and “Star” greatly contributes to my work and\n  motivates me to continue producing more quality content. Thank you for your\n  support!\n\u003c/p\u003e\n\u003cp\u003e\n  If you enjoyed this story, feel free\n  \u003ca href=\"https://medium.com/@andysingal\" rel=\"noopener\"\u003eto subscribe\u003c/a\u003e\n  to Medium, and you will get notifications when my new articles will be\n  published, as well as full access to thousands of stories from other authors.\n\u003c/p\u003e\n\u003cp\u003eResource:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003ca\n      href=\"https://github.com/andysingal/CV_public/tree/main/zephyr-7b-alpha\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003eData used for above code\u003c/a\n    \u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003ca\n      href=\"https://gpt-index.readthedocs.io/en/stable/\"\n      rel=\"noopener ugc nofollow\"\n      target=\"_blank\"\n      \u003ellama-index\u003c/a\n    \u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n","image":{"_type":"image","asset":{"_ref":"image-c624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/c624648e0fa8046ed57f89eb7efb46cc83e9a6f9-1526x1114.png","publishedDate":"2023-11-20","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-5072f44cf25db4b603e6a20487d2b46085ef6f05-1536x1024-png","_type":"reference"}},"publishedDate":"2025-05-21","slug":"python-tooling-at-scale-llamaindex-s-monorepo-overhaul","title":"Python Tooling at Scale: LlamaIndex’s Monorepo Overhaul"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-8890c5098dbde95bd57d8c230fb46ee60bede234-2400x1420-png","_type":"reference"}},"publishedDate":"2024-01-30","slug":"agentic-rag-with-llamaindex-2721b8a49ff6","title":"Agentic RAG With LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-1f5feeb7c7670e074431b6788725fd576d8b4663-1024x1024-png","_type":"reference"}},"publishedDate":"2023-12-14","slug":"how-to-train-a-custom-gpt-on-your-data-with-embedai-llamaindex-8a701d141070","title":"How to train a custom GPT on your data with EmbedAI + LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-396424d55af5fa49327b9f248c5b5ce3025cce30-3648x1271-png","_type":"reference"}},"publishedDate":"2023-08-29","slug":"llamaindex-automatic-knowledge-transfer-kt-generation-for-code-bases-f3d91f21b7af","title":"LlamaIndex: Automatic Knowledge Transfer (KT) Generation for Code Bases"}],"slug":{"_type":"slug","current":"becoming-proficient-in-document-extraction-32aa13046ed5"},"tags":[{"_createdAt":"2024-02-22T20:19:13Z","_id":"741df249-4b52-4d22-899a-3d4806361f3d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"technology"},"title":"Technology"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"d08c6abc-df26-4b4b-96e5-8e9b6006c606","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"software-development"},"title":"Software Development"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"6651472f-2627-4a13-b737-005ba3f55928","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"software-engineering"},"title":"Software Engineering"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"76ea148a-b132-446e-bfb5-9167669603ab","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"tech"},"title":"Tech"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"3d5b5d70-cd50-435c-86e7-3ba4c4c44d61","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"artificial-intelligence"},"title":"Artificial Intelligence"}],"title":"Becoming Proficient in Document Extraction"},"publishedDate":"Invalid Date"},"params":{"slug":"becoming-proficient-in-document-extraction-32aa13046ed5"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"becoming-proficient-in-document-extraction-32aa13046ed5"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>