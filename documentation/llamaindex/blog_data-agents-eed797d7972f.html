<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Data Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Data Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Data Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/b8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Data Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/b8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="913.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2023-07-12</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Data Agents</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/nlp"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">NLP</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/agents"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Agents</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/data"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Data</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>Today we’re incredibly excited to announce the launch of a big new capability within LlamaIndex: <strong>Data Agents</strong>.</p><p>Data Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a “read” and “write” function. They are capable of the following:</p><ul><li>Perform automated search and retrieval over different types of data — unstructured, semi-structured, and structured.</li><li>Calling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use.</li><li>Storing conversation history.</li><li>Using all of the above to fulfill both simple and complex data tasks.</li></ul><figure><img src="/blog/images/1*cWwW01Ez_JIS2hcMJwcV8Q.png" alt="" width="700" height="513"></figure><p>We’ve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Today’s launch consists of the following key components:</p><ul><li><a href="https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html" rel="noopener ugc nofollow" target="_blank"><strong>General Agent/Tool Abstractions</strong></a><strong>:</strong> a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition.</li><li><a href="https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/root.html" rel="noopener ugc nofollow" target="_blank"><strong>LlamaHub Tool Repository</strong></a><strong>:</strong> A <a href="https://llamahub.ai/" rel="noopener ugc nofollow" target="_blank">brand-new section within LlamaHub</a> that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to <a href="https://github.com/emptycrown/llama-hub" rel="noopener ugc nofollow" target="_blank">community contributions</a>!</li></ul><p>See below for full details. <strong>We show you how to build a Gmail agent that’s able to automatically create/send emails in &lt;10 lines of code!</strong></p><h1>Context</h1><p>Our core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define “state” (how to parse/structure your data), and “compute” (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality <a href="https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html" rel="noopener ugc nofollow" target="_blank">beyond that</a>.</p><p>A lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the “reasoning” capabilities of these query engines. As a result some of our existing query capabilities contain “agent-like” components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.</p><p>But there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both “read” and “write” capabilities — the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source.</p><p>Some existing <a href="https://openai.com/blog/chatgpt-plugins" rel="noopener ugc nofollow" target="_blank">services</a>, <a href="https://python.langchain.com/docs/modules/agents/" rel="noopener ugc nofollow" target="_blank">toolkits</a>, and <a href="https://arxiv.org/abs/2302.04761" rel="noopener ugc nofollow" target="_blank">research</a> <a href="https://arxiv.org/abs/2210.03629" rel="noopener ugc nofollow" target="_blank">papers</a> have already demonstrated the possibilities of LLM-powered “agents” that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.</p><h1>Core Components of Data Agents</h1><p>Building a data agent requires the following core components:</p><ul><li>A reasoning loop</li><li>Tool abstractions</li></ul><p>At a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).</p><p>Given an input task, the data agent uses a <strong>reasoning loop</strong> to decide which tools to use, in which sequence, and the parameters to call each tool. The “loop” can conceptually be very simple (a one-step tool selection process), or complex (a multi-step selection process, where a multitude of tools are picked at each step).</p><figure><img src="/blog/images/1*WPOS7tiljXCrd3IkJy84CQ.png" alt="" width="700" height="411"></figure><p>These components are described in more detail below.</p><h2>Agent Abstraction + Reasoning Loop</h2><p>We have support for the following agents:</p><ul><li>OpenAI Function agent (built on top of the OpenAI Function API)</li><li>a ReAct agent (which works across any chat/text completion endpoint).</li></ul><p>You can use them as the following:</p><pre><span id="d7cf" class="qs ow gt qp b bf qt qu l qv qw"><span class="hljs-keyword">from</span> llama_index.agent <span class="hljs-keyword">import</span> OpenAIAgent, ReActAgent
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI

<span class="hljs-comment"># import and define tools</span>
...
<span class="hljs-comment"># initialize llm</span>
llm = OpenAI(model=<span class="hljs-string">"gpt-3.5-turbo-0613"</span>)
<span class="hljs-comment"># initialize openai agent</span>
agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># initialize ReAct agent</span>
agent = ReActAgent.from_tools(tools, llm=llm, verbose=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># use agent</span>
response = agent.chat(<span class="hljs-string">"What is (121 * 3) + 42?"</span>)</span></pre><p>Each agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task — <code class="cw qx qy qz qp b">chat</code> and <code class="cw qx qy qz qp b">query</code>. Note that these are the core methods used in our <code class="cw qx qy qz qp b">ChatEngine</code> and <code class="cw qx qy qz qp b">QueryEngine</code> respectively. In fact that our base agent class (<code class="cw qx qy qz qp b">BaseAgent</code>) simply inherits from <code class="cw qx qy qz qp b">BaseChatEngine</code> and <code class="cw qx qy qz qp b">BaseQueryEngine</code>. <code class="cw qx qy qz qp b">chat</code> allows the agent to utilize previously stored conversation history, whereas <code class="cw qx qy qz qp b">query</code> is a stateless call - history/state is not preserved over time.</p><p>The reasoning loop depends on the type of agent. The OpenAI agent calls the OpenAI function API in a while loop, since the tool decision logic is baked into the function API. Given an input prompt and previous chat history (which includes previous function calls), the function API will decide whether to make another function call (pick a Tool), or return an assistant message. If the API returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. If the API returns an assistant message, then the loop is complete (we assume the task is solved).</p><p>The ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str → output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this:</p><pre><span id="c804" class="qs ow gt qp b bf qt qu l qv qw">...
You have access to the following tools:
{tool_desc}

To answer the question, please use the following format.

```
Thought: I need to use a tool to help me answer the question.
Action: tool name (one of {tool_names})
Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{"text": "hello world", "num_beams": 5}})
```
Please use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.

If this format is used, you will receive a response in the following format:

```
Observation: tool response
```
...</span></pre><p>We implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message.</p><p><strong>Note:</strong> the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API — the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting.</p><h2>Tool Abstractions</h2><p>Having proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a single Tool as well as a “ToolSpec” containing a series of functions under the hood.</p><p>We describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools.</p><p><strong>Base Tool Abstraction</strong></p><p>The base tool defines a very generic interface. The <code class="cw qx qy qz qp b">__call__</code> function can take in any series of arguments, and return a generic <code class="cw qx qy qz qp b">ToolOutput</code> container that can capture any response. A tool also has metadata containing its name, description, and function schema.</p><pre><span id="9ffe" class="qs ow gt qp b bf qt qu l qv qw"><span class="hljs-variable">@dataclass</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">ToolMetadata</span>:
    <span class="hljs-symbol">description:</span> str
    <span class="hljs-symbol">name:</span> <span class="hljs-title class_">Optional</span>[str] = <span class="hljs-title class_">None</span>
    <span class="hljs-symbol">fn_schema:</span> <span class="hljs-title class_">Optional</span>[<span class="hljs-title class_">Type</span>[<span class="hljs-title class_">BaseModel</span>]] = <span class="hljs-title class_">DefaultToolFnSchema</span>

<span class="hljs-keyword">class</span> <span class="hljs-title class_">BaseTool</span>:
    <span class="hljs-variable">@property</span>
    <span class="hljs-variable">@abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">metadata</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span></span>) -&amp;gt; <span class="hljs-title class_">ToolMetadata</span>:
        pass
    <span class="hljs-variable">@abstractmethod</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params"><span class="hljs-variable language_">self</span>, <span class="hljs-symbol">input:</span> <span class="hljs-title class_">Any</span></span>) -&amp;gt; <span class="hljs-title class_">ToolOutput</span>:
        pass</span></pre><p><strong>Function Tool</strong></p><p>A function tool allows users to easily convert any function into a Tool. It takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface. It can also “auto-infer” the function schema if it isn’t specified beforehand.</p><p>Our <code class="cw qx qy qz qp b">ToolSpec</code> classes make use of this <code class="cw qx qy qz qp b">FunctionTool</code> abstraction to convert functions defined in the tool spec into a set of agent tools (see below).</p><p>Here’s a trivial example of defining a FunctionTool.</p><pre><span id="d7f8" class="qs ow gt qp b bf qt qu l qv qw">from llama_index.tools.function_tool import FunctionTool

def <span class="hljs-title function_">multiply</span><span class="hljs-params">(a: <span class="hljs-type">int</span>, b: <span class="hljs-type">int</span>)</span> -&amp;gt; <span class="hljs-type">int</span>:
    <span class="hljs-string">""</span><span class="hljs-string">"Multiple two integers and returns the result integer"</span><span class="hljs-string">""</span>
    <span class="hljs-keyword">return</span> a * b
multiply_tool = FunctionTool.from_defaults(fn=multiply)</span></pre><p><strong>QueryEngineTool</strong></p><p>Of course, we also provide Tool abstractions to wrap our existing query engines. This provides a seamless transition from working on query engines to working on agents. Our query engines can be thought of “constrained” agents meant for the read/write setting and centered around retrieval purposes. These query engines can be used in an overall agent setting.</p><pre><span id="1349" class="qs ow gt qp b bf qt qu l qv qw"><span class="hljs-keyword">from</span> llama_index.tools <span class="hljs-keyword">import</span> QueryEngineTool

query_engine_tools = [
    QueryEngineTool(
        query_engine=query_engine, 
        metadata=ToolMetadata(
            name=<span class="hljs-string">'&amp;lt;tool_name&amp;gt;'</span>, 
            description=<span class="hljs-string">"Queries over X data source."</span>
        )
    ),
 ...
]</span></pre><p><strong>Tool Specs</strong></p><p>A <strong>tool spec</strong> is a Python class that represents a full API specification that an agent can interact with, and a tool spec can be converted into a list of tools that an agent can be initialized with.</p><p>This class allows users to define entire services, not just single tools that perform individual tasks. Each tool spec may contain read/write endpoints that allow an agent to interact with a service in meaningful ways. For instance, a Slack tool spec could allow the user to both read existing messages and channels (<code class="cw qx qy qz qp b">load_data</code>, <code class="cw qx qy qz qp b">fetch_channels</code>) as well as write messages (<code class="cw qx qy qz qp b">send_message</code>). It would be roughly defined as the following:</p><pre><span id="424d" class="qs ow gt qp b bf qt qu l qv qw"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SlackToolSpec</span>(<span class="hljs-title class_ inherited__">BaseToolSpec</span>):
    <span class="hljs-string">"""Slack tool spec."""</span>
    spec_functions = [<span class="hljs-string">"load_data"</span>, <span class="hljs-string">"send_message"</span>, <span class="hljs-string">"fetch_channels"</span>]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">
          self,
          channel_ids: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>],
          reverse_chronological: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,
      </span>) -&amp;gt; <span class="hljs-type">List</span>[Document]:
          <span class="hljs-string">"""Load data from the input directory."""</span>
          ...
      <span class="hljs-keyword">def</span> <span class="hljs-title function_">send_message</span>(<span class="hljs-params">
          self,
          channel_id: <span class="hljs-built_in">str</span>,
          message: <span class="hljs-built_in">str</span>,
      </span>) -&amp;gt; <span class="hljs-literal">None</span>:
          <span class="hljs-string">"""Send a message to a channel given the channel ID."""</span>
          ...
      <span class="hljs-keyword">def</span> <span class="hljs-title function_">fetch_channels</span>(<span class="hljs-params">
          self,
      </span>) -&amp;gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:
          <span class="hljs-string">"""Fetch a list of relevant channels."""</span>
          ...</span></pre><p>If a tool spec is initialized, it can be converted into a list of tools that can be fed into an agent with <code class="cw qx qy qz qp b">to_tool_list</code>. For instance,</p><pre><span id="ec2b" class="qs ow gt qp b bf qt qu l qv qw">tool_spec = SlackToolSpec()
# initialize openai agent
agent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True)</span></pre><p>Defining a tool spec is not that different than defining a Python class. Each function becomes converted into a tool, and by default the docstring for each function gets used as the tool description (though you can customize names/description in <code class="cw qx qy qz qp b">to_tool_list(func_to_metadata_mapping=...)</code>.</p><p>We also made the intentional choice that the input arguments and return types can be anything. The primary reason is to preserve the generality of the tool interface for subsequent iterations of agents. Even if current iterations of agents expect tool outputs to be in string format, that may change in the future, and we didn’t want to arbitrarily restrict the types of tool interface.</p><h1>LlamaHub Tool Repository</h1><p>A huge component of our launch is a brand-new addition to <a href="https://llamahub.ai/" rel="noopener ugc nofollow" target="_blank">LlamaHub</a>: a Tool Repository. The Tool Repository consists of <strong>15+ Tool Specs</strong> that an agent can use. These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.</p><p>Among others, they include the following specs:</p><ul><li>Gmail Spec</li><li>Zapier Spec</li><li>Google Calendar Spec</li><li>OpenAPI Spec</li><li>SQL + Vector Database Spec</li></ul><p>We also provide a list of <strong>utility tools</strong> that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.</p><p>For instance, our Gmail Tool Spec allows an agent to search existing emails, create drafts, update drafts, and send emails. Our Zapier Spec allows an agent to perform any natural language query to Zapier through their <a href="https://nla.zapier.com/start/" rel="noopener ugc nofollow" target="_blank">Natural Language Actions</a> interface.</p><p>Best of all, you don’t need to spend a lot of time figuring out how to use these tools — we have <a href="https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks" rel="noopener ugc nofollow" target="_blank"><strong>10+ notebooks</strong></a> showing how you can build agents for each service, or even build agents that use a combination of services (e.g. Gmail, Google Calendar, and Search).</p><h2>Example Walkthrough</h2><p>Let’s take a look at a few examples! We initialize an OpenAIAgent with the Gmail Spec. As mentioned above, the spec consists of tools to search emails, create/update drafts, and send emails.</p><figure><img src="/blog/images/1*SH6bJecf_3sGRTQ9kJX4HA.png" alt="" width="599" height="158"></figure><p>Now let’s give the agent a sequence of commands so that it can create an email draft, make a few edits to it, and then send it off.</p><p>First, let’s create an initial email draft. Note that the agent chooses the <code class="cw qx qy qz qp b">create_draft</code> tool, which takes in the “to”, “subject”, and “message” parameters. The agent is able to infer the parameters simultaneously while picking the tool.</p><figure><img src="/blog/images/1*KjzdLLPbJUILWRO69pRtGQ.png" alt="" width="700" height="235"></figure><p>Next, let’s update the draft with a slight modification:</p><figure><img src="/blog/images/1*eZo-dEDJihP3koZthDMXJA.png" alt="" width="700" height="222"></figure><p>Next, let’s show the current state of the draft.</p><figure><img src="/blog/images/1*2GLPDhnxAmBhVrSSIlESHg.png" alt="" width="700" height="328"></figure><p>Finally, let’s send the email!</p><figure><img src="/blog/images/1*29cEek1EU0jP5j_HjL8fVA.png" alt="" width="700" height="140"></figure><p>This is a good start, but this is just the beginning. We are actively working on contributing more tools to this repository, and we’re also opening this up to community contributions. If you’re interested in contributing a Tool to LlamaHub, please feel free to open a PR in this repo.</p><h2>Utility Tools</h2><p>Oftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).</p><p>To tackle this, we’ve provided an initial set of “utility tools” in the core LlamaIndex repo — utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that’s returned from any API request.</p><p>Let’s walk through our two main utility tools below.</p><p><strong>OnDemandLoaderTool</strong></p><p>This tool turns any existing LlamaIndex data loader ( <code class="cw qx qy qz qp b">BaseReader</code> class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger <code class="cw qx qy qz qp b">load_data</code> from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it “on-demand”. All three of these steps happen in a single tool call.</p><figure><img src="/blog/images/1*gtJHjmMl9kVcjmbL2wx8Pw.png" alt="" width="700" height="209"></figure><p>Oftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.</p><p>A usage example is given below:</p><pre><span id="3359" class="qs ow gt qp b bf qt qu l qv qw"><span class="hljs-keyword">from</span> llama_hub.wikipedia.base <span class="hljs-keyword">import</span> WikipediaReader
<span class="hljs-keyword">from</span> llama_index.tools.on_demand_loader_tool <span class="hljs-keyword">import</span> OnDemandLoaderTool

tool = OnDemandLoaderTool.from_defaults(
 reader,
 name=<span class="hljs-string">"Wikipedia Tool"</span>,
 description=<span class="hljs-string">"A tool for loading data and querying articles from Wikipedia"</span>
)</span></pre><p><strong>LoadAndSearchToolSpec</strong></p><p>The LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements <code class="cw qx qy qz qp b">to_tool_list</code> , and when that function is called, two tools are returned: a <code class="cw qx qy qz qp b">load</code> tool and then a <code class="cw qx qy qz qp b">search</code> tool.</p><p>The <code class="cw qx qy qz qp b">load</code> Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The <code class="cw qx qy qz qp b">search</code> Tool execution would take in a query string as input and call the underlying index.</p><figure><img src="/blog/images/1*oU9riMUVakKA_gmTVFNVXQ.png" alt="" width="700" height="336"></figure><p>This is helpful for any API endpoint that will by default return large volumes of data — for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.</p><p>Example usage is shown below:</p><pre><span id="acf7" class="qs ow gt qp b bf qt qu l qv qw"><span class="hljs-keyword">from</span> llama_hub.tools.wikipedia.base <span class="hljs-keyword">import</span> WikipediaToolSpec
<span class="hljs-keyword">from</span> llama_index.tools.tool_spec.load_and_search.base <span class="hljs-keyword">import</span> LoadAndSearchToolSpec

wiki_spec = WikipediaToolSpec()
<span class="hljs-comment"># Get the search wikipedia tool</span>
tool = wiki_spec.to_tool_list()[<span class="hljs-number">1</span>]
<span class="hljs-comment"># Create the Agent with load/search tools</span>
agent = OpenAIAgent.from_tools(
 LoadAndSearchToolSpec.from_defaults(
    tool
 ).to_tool_list(), verbose=<span class="hljs-literal">True</span>
)</span></pre><p>This is the output when we run an input prompt</p><pre><span id="13c2" class="qs ow gt qp b bf qt qu l qv qw">agent.chat('what is the capital of poland')</span></pre><p>Output:</p><pre><span id="2bda" class="qs ow gt qp b bf qt qu l qv qw">=== <span class="hljs-title class_">Calling</span> <span class="hljs-title class_">Function</span> ===
<span class="hljs-title class_">Calling</span> <span class="hljs-attr">function</span>: search_data <span class="hljs-keyword">with</span> <span class="hljs-attr">args</span>: {
  <span class="hljs-string">"query"</span>: <span class="hljs-string">"capital of Poland"</span>
}
<span class="hljs-title class_">Got</span> <span class="hljs-attr">output</span>: <span class="hljs-title class_">Content</span> loaded! <span class="hljs-title class_">You</span> can now search the information using read_search_data
========================
=== <span class="hljs-title class_">Calling</span> <span class="hljs-title class_">Function</span> ===
<span class="hljs-title class_">Calling</span> <span class="hljs-attr">function</span>: read_search_data <span class="hljs-keyword">with</span> <span class="hljs-attr">args</span>: {
  <span class="hljs-string">"query"</span>: <span class="hljs-string">"What is the capital of Poland?"</span>
}
<span class="hljs-title class_">Got</span> <span class="hljs-attr">output</span>: 
<span class="hljs-title class_">The</span> capital <span class="hljs-keyword">of</span> <span class="hljs-title class_">Poland</span> is <span class="hljs-title class_">Warsaw</span>.
========================
<span class="hljs-title class_">AgentChatResponse</span>(response=<span class="hljs-string">'The capital of Poland is Warsaw.'</span>, sources=[])</span></pre><p>Note that the agent figures out that it first needs to first call the “load” tool (denoted by the original name of the tool, “search_data”). This load tool will load the Wikipedia page and index under the hood. The output just mentions that the “content is loaded, and tells the agent that the next step is to use <code class="cw qx qy qz qp b">read_search_data</code>. The agent then reasons that it needs to call the <code class="cw qx qy qz qp b">read_search_data</code> tool, which will query the index for the right answer.</p><h1>FAQ</h1><p><strong>Should I use Data Agents for search and retrieval, or continue to use Query Engines?</strong></p><p>Short answer: both are possible. Query engines give you the ability to define your own workflows over your data, in both a constrained reasoning fashion as well as unconstrained fashion. For instance, you may want to define a specific workflow over text-to-SQL with our <code class="cw qx qy qz qp b">NLStructStoreQueryEngine</code> (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our <code class="cw qx qy qz qp b">SubQuestionQueryEngine</code> to decompose a question among sub-documents (even less constrained).</p><p>By default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities — for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too.</p><p><strong>How are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)?</strong></p><p>Most of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our “data agents” design, we’ve tried our best to answer the following key questions well:</p><ul><li>How do we effectively index/query and retrieve data beforehand?</li><li>How do we effectively index/query and retrieve data on the fly?</li><li>How do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?</li><li>How do we properly get sources in citations?</li></ul><p>Our goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data — these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to “cache” API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.</p><p><strong>Can I use Tools with LangChain agents? 
</strong>You can easily use any of our tools with LangChain agents as well.</p><pre><span id="0804" class="qs ow gt qp b bf qt qu l qv qw">tools = tool_spec.to_tool_list()
langchain_tools = [t.to_langchain_tool() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tools]</span></pre><p>See our <a href="https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/usage_pattern.html#using-with-langchain" rel="noopener ugc nofollow" target="_blank">tools usage guide</a> for more details!</p><h1>Conclusion</h1><p>In summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository.</p><h2>Resources</h2><p>We’ve written a comprehensive section in the docs — take a look here: <a href="https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html" rel="noopener ugc nofollow" target="_blank">https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html</a></p><p>Take a look at our LlamaHub Tools section: <a href="https://llamahub.ai/" rel="noopener ugc nofollow" target="_blank">https://llamahub.ai/</a></p><p>Notebook Tutorials for LlamaHub Tools: <a href="https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks" rel="noopener ugc nofollow" target="_blank">https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks</a></p><p>If you have questions, please hop on our Discord: <a href="https://discord.gg/dGcwcsnxhU" rel="noopener ugc nofollow" target="_blank">https://discord.gg/dGcwcsnxhU</a></p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/introducing-the-spreadsheet-agent-in-private-preview">Introducing the Spreadsheet Agent, in private preview</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-06-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/rag-is-dead-long-live-agentic-retrieval">RAG is dead, long live agentic retrieval</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-29</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/improved-long-and-short-term-memory-for-llamaindex-agents">Improved Long &amp; Short-Term Memory for LlamaIndex Agents</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-13</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/bending-without-breaking-optimal-design-patterns-for-effective-agents">Bending without breaking: optimal design patterns for effective agents</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-04-25</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"8728072d-b020-4b41-8631-c3ef45540fe7","_rev":"05dtDS0H5iRVsxYMarZJIB","_type":"blogPost","_updatedAt":"2025-05-21T20:38:30Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"htmlContent":"\u003cp\u003eToday we’re incredibly excited to announce the launch of a big new capability within LlamaIndex: \u003cstrong\u003eData Agents\u003c/strong\u003e.\u003c/p\u003e\u003cp\u003eData Agents are LLM-powered knowledge workers that can intelligently perform various tasks over your data, in both a “read” and “write” function. They are capable of the following:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePerform automated search and retrieval over different types of data — unstructured, semi-structured, and structured.\u003c/li\u003e\u003cli\u003eCalling any external service API in a structured fashion. They can either process the response immediately, or index/cache this data for future use.\u003c/li\u003e\u003cli\u003eStoring conversation history.\u003c/li\u003e\u003cli\u003eUsing all of the above to fulfill both simple and complex data tasks.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*cWwW01Ez_JIS2hcMJwcV8Q.png\" alt=\"\" width=\"700\" height=\"513\"\u003e\u003c/figure\u003e\u003cp\u003eWe’ve worked hard to provide abstractions, services, and guides on both the agents side and tools side in order to build data agents. Today’s launch consists of the following key components:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eGeneral Agent/Tool Abstractions\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e:\u003c/strong\u003e a set of abstractions to build agent loops, and to have those loops interact with tools according to a structured API definition.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eLlamaHub Tool Repository\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e:\u003c/strong\u003e A \u003ca href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ebrand-new section within LlamaHub\u003c/a\u003e that consists of 15+ Tools (e.g. Google Calendar, Notion, SQL, OpenAPI) that can be connected. Opening to \u003ca href=\"https://github.com/emptycrown/llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecommunity contributions\u003c/a\u003e!\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee below for full details. \u003cstrong\u003eWe show you how to build a Gmail agent that’s able to automatically create/send emails in \u0026lt;10 lines of code!\u003c/strong\u003e\u003c/p\u003e\u003ch1\u003eContext\u003c/h1\u003e\u003cp\u003eOur core mission at LlamaIndex is to unlock the full capabilities of LLMs over your external sources of data. It provides a set of tools to both define “state” (how to parse/structure your data), and “compute” (how to query your data). Up until now, our framework has primarily focused on search and retrieval use case. We have an incredible suite of tools and capabilities that not only allow you to create the basic RAG stack around a vector database + top-k retrieval, but also offer much greater functionality \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/query_engine/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ebeyond that\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eA lot of that technology used to lie in our query engines. Our goal was to increase the capability of query engines to answer a wide range of different queries. In order to do this, we had to improve the “reasoning” capabilities of these query engines. As a result some of our existing query capabilities contain “agent-like” components: we have query engines capable of chain-of-thought reasoning, query decomposition, and routing. In the process, users had the option of choosing from a spectrum of query engines that had more constrained reasoning capabilities to less constrained capabilities.\u003c/p\u003e\u003cp\u003eBut there was a huge opportunity for LLMs to have an even richer set of interactions with data; they should be capable of general reasoning over any set of tools, whether from a database or an API. They should also be capable of both “read” and “write” capabilities — the ability to not only understand state but also modify it. As a result they should be able to do more than search and retrieval from a static knowledge source.\u003c/p\u003e\u003cp\u003eSome existing \u003ca href=\"https://openai.com/blog/chatgpt-plugins\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eservices\u003c/a\u003e, \u003ca href=\"https://python.langchain.com/docs/modules/agents/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003etoolkits\u003c/a\u003e, and \u003ca href=\"https://arxiv.org/abs/2302.04761\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eresearch\u003c/a\u003e \u003ca href=\"https://arxiv.org/abs/2210.03629\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003epapers\u003c/a\u003e have already demonstrated the possibilities of LLM-powered “agents” that can interact with the external environment. Using these existing approaches as inspiration, we saw an opportunity to build a principled series of abstractions enabling anyone to build knowledge workers over their data.\u003c/p\u003e\u003ch1\u003eCore Components of Data Agents\u003c/h1\u003e\u003cp\u003eBuilding a data agent requires the following core components:\u003c/p\u003e\u003cul\u003e\u003cli\u003eA reasoning loop\u003c/li\u003e\u003cli\u003eTool abstractions\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eAt a high-level, a data agent is provided with a set of APIs, or Tools, to interact with. These APIs can return information about the world, or perform an action that modifies state. Each Tool exposes a request/response interface. The request is a set of structured parameters, and the response can be any format (at least conceptually, in most cases the response here is a text string of some form).\u003c/p\u003e\u003cp\u003eGiven an input task, the data agent uses a \u003cstrong\u003ereasoning loop\u003c/strong\u003e to decide which tools to use, in which sequence, and the parameters to call each tool. The “loop” can conceptually be very simple (a one-step tool selection process), or complex (a multi-step selection process, where a multitude of tools are picked at each step).\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*WPOS7tiljXCrd3IkJy84CQ.png\" alt=\"\" width=\"700\" height=\"411\"\u003e\u003c/figure\u003e\u003cp\u003eThese components are described in more detail below.\u003c/p\u003e\u003ch2\u003eAgent Abstraction + Reasoning Loop\u003c/h2\u003e\u003cp\u003eWe have support for the following agents:\u003c/p\u003e\u003cul\u003e\u003cli\u003eOpenAI Function agent (built on top of the OpenAI Function API)\u003c/li\u003e\u003cli\u003ea ReAct agent (which works across any chat/text completion endpoint).\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou can use them as the following:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d7cf\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.agent \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAIAgent, ReActAgent\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\n\u003cspan class=\"hljs-comment\"\u003e# import and define tools\u003c/span\u003e\n...\n\u003cspan class=\"hljs-comment\"\u003e# initialize llm\u003c/span\u003e\nllm = OpenAI(model=\u003cspan class=\"hljs-string\"\u003e\"gpt-3.5-turbo-0613\"\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# initialize openai agent\u003c/span\u003e\nagent = OpenAIAgent.from_tools(tools, llm=llm, verbose=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# initialize ReAct agent\u003c/span\u003e\nagent = ReActAgent.from_tools(tools, llm=llm, verbose=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\u003cspan class=\"hljs-comment\"\u003e# use agent\u003c/span\u003e\nresponse = agent.chat(\u003cspan class=\"hljs-string\"\u003e\"What is (121 * 3) + 42?\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eEach agent takes in a set of Tools. The details behind our tool abstractions are provided below. Each agent also supports two main methods for taking in an input task — \u003ccode class=\"cw qx qy qz qp b\"\u003echat\u003c/code\u003e and \u003ccode class=\"cw qx qy qz qp b\"\u003equery\u003c/code\u003e. Note that these are the core methods used in our \u003ccode class=\"cw qx qy qz qp b\"\u003eChatEngine\u003c/code\u003e and \u003ccode class=\"cw qx qy qz qp b\"\u003eQueryEngine\u003c/code\u003e respectively. In fact that our base agent class (\u003ccode class=\"cw qx qy qz qp b\"\u003eBaseAgent\u003c/code\u003e) simply inherits from \u003ccode class=\"cw qx qy qz qp b\"\u003eBaseChatEngine\u003c/code\u003e and \u003ccode class=\"cw qx qy qz qp b\"\u003eBaseQueryEngine\u003c/code\u003e. \u003ccode class=\"cw qx qy qz qp b\"\u003echat\u003c/code\u003e allows the agent to utilize previously stored conversation history, whereas \u003ccode class=\"cw qx qy qz qp b\"\u003equery\u003c/code\u003e is a stateless call - history/state is not preserved over time.\u003c/p\u003e\u003cp\u003eThe reasoning loop depends on the type of agent. The OpenAI agent calls the OpenAI function API in a while loop, since the tool decision logic is baked into the function API. Given an input prompt and previous chat history (which includes previous function calls), the function API will decide whether to make another function call (pick a Tool), or return an assistant message. If the API returns a function call, then we are responsible for executing the function and passing in a function message in the chat history. If the API returns an assistant message, then the loop is complete (we assume the task is solved).\u003c/p\u003e\u003cp\u003eThe ReAct agent uses general text completion endpoints, so it can be used with any LLM. A text completion endpoint has a simple input str → output str format, which means that the reasoning logic must be encoded in the prompt. The ReAct agent uses an input prompt inspired by the ReAct paper (and adapted into other versions), in order to decide which tool to pick. It looks something like this:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c804\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e...\nYou have access to the following tools:\n{tool_desc}\n\nTo answer the question, please use the following format.\n\n```\nThought: I need to use a tool to help me answer the question.\nAction: tool name (one of {tool_names})\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"text\": \"hello world\", \"num_beams\": 5}})\n```\nPlease use a valid JSON format for the action input. Do NOT do this {{'text': 'hello world', 'num_beams': 5}}.\n\nIf this format is used, you will receive a response in the following format:\n\n```\nObservation: tool response\n```\n...\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe implement ReAct natively over chat prompts; the reasoning loop is implemented as an alternating series of assistant and user messages. The Thought/Action/Action Input section is represented as an assistant message, and the Observation section is implemented as a user message.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e the ReAct prompt expects not only the name of the tool to pick, but also the parameters to fill in the tool in a JSON format. This makes the output not dissimilar from the output of the OpenAI Function API — the main difference is that in the case of the function API, the tool-picking logic is baked into the API itself (through a finetuned model), whereas here it is elicited through explicit prompting.\u003c/p\u003e\u003ch2\u003eTool Abstractions\u003c/h2\u003e\u003cp\u003eHaving proper tool abstractions is at the core of building data agents. Defining a set of Tools is similar to defining any API interface, with the exception that these Tools are meant for agent rather than human use. We allow users to define both a single Tool as well as a “ToolSpec” containing a series of functions under the hood.\u003c/p\u003e\u003cp\u003eWe describe the base tool abstraction, as well as how you can easily define tools over existing query engines, other tools.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBase Tool Abstraction\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe base tool defines a very generic interface. The \u003ccode class=\"cw qx qy qz qp b\"\u003e__call__\u003c/code\u003e function can take in any series of arguments, and return a generic \u003ccode class=\"cw qx qy qz qp b\"\u003eToolOutput\u003c/code\u003e container that can capture any response. A tool also has metadata containing its name, description, and function schema.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9ffe\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e\u003cspan class=\"hljs-variable\"\u003e@dataclass\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eToolMetadata\u003c/span\u003e:\n    \u003cspan class=\"hljs-symbol\"\u003edescription:\u003c/span\u003e str\n    \u003cspan class=\"hljs-symbol\"\u003ename:\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[str] = \u003cspan class=\"hljs-title class_\"\u003eNone\u003c/span\u003e\n    \u003cspan class=\"hljs-symbol\"\u003efn_schema:\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eOptional\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eType\u003c/span\u003e[\u003cspan class=\"hljs-title class_\"\u003eBaseModel\u003c/span\u003e]] = \u003cspan class=\"hljs-title class_\"\u003eDefaultToolFnSchema\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eBaseTool\u003c/span\u003e:\n    \u003cspan class=\"hljs-variable\"\u003e@property\u003c/span\u003e\n    \u003cspan class=\"hljs-variable\"\u003e@abstractmethod\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003emetadata\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003cspan class=\"hljs-variable language_\"\u003eself\u003c/span\u003e\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-title class_\"\u003eToolMetadata\u003c/span\u003e:\n        pass\n    \u003cspan class=\"hljs-variable\"\u003e@abstractmethod\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\u003cspan class=\"hljs-variable language_\"\u003eself\u003c/span\u003e, \u003cspan class=\"hljs-symbol\"\u003einput:\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAny\u003c/span\u003e\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-title class_\"\u003eToolOutput\u003c/span\u003e:\n        pass\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eFunction Tool\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA function tool allows users to easily convert any function into a Tool. It takes in a user-defined function (that can take in any inputs/outputs), and wraps it into a tool interface. It can also “auto-infer” the function schema if it isn’t specified beforehand.\u003c/p\u003e\u003cp\u003eOur \u003ccode class=\"cw qx qy qz qp b\"\u003eToolSpec\u003c/code\u003e classes make use of this \u003ccode class=\"cw qx qy qz qp b\"\u003eFunctionTool\u003c/code\u003e abstraction to convert functions defined in the tool spec into a set of agent tools (see below).\u003c/p\u003e\u003cp\u003eHere’s a trivial example of defining a FunctionTool.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d7f8\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003efrom llama_index.tools.function_tool import FunctionTool\n\ndef \u003cspan class=\"hljs-title function_\"\u003emultiply\u003c/span\u003e\u003cspan class=\"hljs-params\"\u003e(a: \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e, b: \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e)\u003c/span\u003e -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eint\u003c/span\u003e:\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"Multiple two integers and returns the result integer\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e a * b\nmultiply_tool = FunctionTool.from_defaults(fn=multiply)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eQueryEngineTool\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eOf course, we also provide Tool abstractions to wrap our existing query engines. This provides a seamless transition from working on query engines to working on agents. Our query engines can be thought of “constrained” agents meant for the read/write setting and centered around retrieval purposes. These query engines can be used in an overall agent setting.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"1349\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.tools \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e QueryEngineTool\n\nquery_engine_tools = [\n    QueryEngineTool(\n        query_engine=query_engine, \n        metadata=ToolMetadata(\n            name=\u003cspan class=\"hljs-string\"\u003e'\u0026amp;lt;tool_name\u0026amp;gt;'\u003c/span\u003e, \n            description=\u003cspan class=\"hljs-string\"\u003e\"Queries over X data source.\"\u003c/span\u003e\n        )\n    ),\n ...\n]\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eTool Specs\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA \u003cstrong\u003etool spec\u003c/strong\u003e is a Python class that represents a full API specification that an agent can interact with, and a tool spec can be converted into a list of tools that an agent can be initialized with.\u003c/p\u003e\u003cp\u003eThis class allows users to define entire services, not just single tools that perform individual tasks. Each tool spec may contain read/write endpoints that allow an agent to interact with a service in meaningful ways. For instance, a Slack tool spec could allow the user to both read existing messages and channels (\u003ccode class=\"cw qx qy qz qp b\"\u003eload_data\u003c/code\u003e, \u003ccode class=\"cw qx qy qz qp b\"\u003efetch_channels\u003c/code\u003e) as well as write messages (\u003ccode class=\"cw qx qy qz qp b\"\u003esend_message\u003c/code\u003e). It would be roughly defined as the following:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"424d\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eSlackToolSpec\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eBaseToolSpec\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"Slack tool spec.\"\"\"\u003c/span\u003e\n    spec_functions = [\u003cspan class=\"hljs-string\"\u003e\"load_data\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"send_message\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"fetch_channels\"\u003c/span\u003e]\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eload_data\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n          self,\n          channel_ids: \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e],\n          reverse_chronological: \u003cspan class=\"hljs-built_in\"\u003ebool\u003c/span\u003e = \u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n      \u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[Document]:\n          \u003cspan class=\"hljs-string\"\u003e\"\"\"Load data from the input directory.\"\"\"\u003c/span\u003e\n          ...\n      \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003esend_message\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n          self,\n          channel_id: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e,\n          message: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e,\n      \u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n          \u003cspan class=\"hljs-string\"\u003e\"\"\"Send a message to a channel given the channel ID.\"\"\"\u003c/span\u003e\n          ...\n      \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003efetch_channels\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n          self,\n      \u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e]:\n          \u003cspan class=\"hljs-string\"\u003e\"\"\"Fetch a list of relevant channels.\"\"\"\u003c/span\u003e\n          ...\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eIf a tool spec is initialized, it can be converted into a list of tools that can be fed into an agent with \u003ccode class=\"cw qx qy qz qp b\"\u003eto_tool_list\u003c/code\u003e. For instance,\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ec2b\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003etool_spec = SlackToolSpec()\n# initialize openai agent\nagent = OpenAIAgent.from_tools(tool_spec.to_tool_list(), llm=llm, verbose=True)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eDefining a tool spec is not that different than defining a Python class. Each function becomes converted into a tool, and by default the docstring for each function gets used as the tool description (though you can customize names/description in \u003ccode class=\"cw qx qy qz qp b\"\u003eto_tool_list(func_to_metadata_mapping=...)\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eWe also made the intentional choice that the input arguments and return types can be anything. The primary reason is to preserve the generality of the tool interface for subsequent iterations of agents. Even if current iterations of agents expect tool outputs to be in string format, that may change in the future, and we didn’t want to arbitrarily restrict the types of tool interface.\u003c/p\u003e\u003ch1\u003eLlamaHub Tool Repository\u003c/h1\u003e\u003cp\u003eA huge component of our launch is a brand-new addition to \u003ca href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaHub\u003c/a\u003e: a Tool Repository. The Tool Repository consists of \u003cstrong\u003e15+ Tool Specs\u003c/strong\u003e that an agent can use. These tool specs represent an initial curated list of services that an agent can interact with and enrich its capability to perform different actions.\u003c/p\u003e\u003cp\u003eAmong others, they include the following specs:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGmail Spec\u003c/li\u003e\u003cli\u003eZapier Spec\u003c/li\u003e\u003cli\u003eGoogle Calendar Spec\u003c/li\u003e\u003cli\u003eOpenAPI Spec\u003c/li\u003e\u003cli\u003eSQL + Vector Database Spec\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe also provide a list of \u003cstrong\u003eutility tools\u003c/strong\u003e that help to abstract away pain points when designing agents to interact with different API services that return large amounts of data.\u003c/p\u003e\u003cp\u003eFor instance, our Gmail Tool Spec allows an agent to search existing emails, create drafts, update drafts, and send emails. Our Zapier Spec allows an agent to perform any natural language query to Zapier through their \u003ca href=\"https://nla.zapier.com/start/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNatural Language Actions\u003c/a\u003e interface.\u003c/p\u003e\u003cp\u003eBest of all, you don’t need to spend a lot of time figuring out how to use these tools — we have \u003ca href=\"https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003e10+ notebooks\u003c/strong\u003e\u003c/a\u003e showing how you can build agents for each service, or even build agents that use a combination of services (e.g. Gmail, Google Calendar, and Search).\u003c/p\u003e\u003ch2\u003eExample Walkthrough\u003c/h2\u003e\u003cp\u003eLet’s take a look at a few examples! We initialize an OpenAIAgent with the Gmail Spec. As mentioned above, the spec consists of tools to search emails, create/update drafts, and send emails.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*SH6bJecf_3sGRTQ9kJX4HA.png\" alt=\"\" width=\"599\" height=\"158\"\u003e\u003c/figure\u003e\u003cp\u003eNow let’s give the agent a sequence of commands so that it can create an email draft, make a few edits to it, and then send it off.\u003c/p\u003e\u003cp\u003eFirst, let’s create an initial email draft. Note that the agent chooses the \u003ccode class=\"cw qx qy qz qp b\"\u003ecreate_draft\u003c/code\u003e tool, which takes in the “to”, “subject”, and “message” parameters. The agent is able to infer the parameters simultaneously while picking the tool.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*KjzdLLPbJUILWRO69pRtGQ.png\" alt=\"\" width=\"700\" height=\"235\"\u003e\u003c/figure\u003e\u003cp\u003eNext, let’s update the draft with a slight modification:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*eZo-dEDJihP3koZthDMXJA.png\" alt=\"\" width=\"700\" height=\"222\"\u003e\u003c/figure\u003e\u003cp\u003eNext, let’s show the current state of the draft.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*2GLPDhnxAmBhVrSSIlESHg.png\" alt=\"\" width=\"700\" height=\"328\"\u003e\u003c/figure\u003e\u003cp\u003eFinally, let’s send the email!\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*29cEek1EU0jP5j_HjL8fVA.png\" alt=\"\" width=\"700\" height=\"140\"\u003e\u003c/figure\u003e\u003cp\u003eThis is a good start, but this is just the beginning. We are actively working on contributing more tools to this repository, and we’re also opening this up to community contributions. If you’re interested in contributing a Tool to LlamaHub, please feel free to open a PR in this repo.\u003c/p\u003e\u003ch2\u003eUtility Tools\u003c/h2\u003e\u003cp\u003eOftentimes, directly querying an API can return a massive volume of data, which on its own may overflow the context window of the LLM (or at the very least unnecessarily increase the number of tokens that you are using).\u003c/p\u003e\u003cp\u003eTo tackle this, we’ve provided an initial set of “utility tools” in the core LlamaIndex repo — utility tools are not conceptually tied to a given service (e.g. Gmail, Notion), but rather can augment the capabilities of existing Tools. In this particular case, utility tools help to abstract away common patterns of needing to cache/index and query data that’s returned from any API request.\u003c/p\u003e\u003cp\u003eLet’s walk through our two main utility tools below.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOnDemandLoaderTool\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThis tool turns any existing LlamaIndex data loader ( \u003ccode class=\"cw qx qy qz qp b\"\u003eBaseReader\u003c/code\u003e class) into a tool that an agent can use. The tool can be called with all the parameters needed to trigger \u003ccode class=\"cw qx qy qz qp b\"\u003eload_data\u003c/code\u003e from the data loader, along with a natural language query string. During execution, we first load data from the data loader, index it (for instance with a vector store), and then query it “on-demand”. All three of these steps happen in a single tool call.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*gtJHjmMl9kVcjmbL2wx8Pw.png\" alt=\"\" width=\"700\" height=\"209\"\u003e\u003c/figure\u003e\u003cp\u003eOftentimes this can be preferable to figuring out how to load and index API data yourself. While this may allow for data reusability, oftentimes users just need an ad-hoc index to abstract away prompt window limitations for any API call.\u003c/p\u003e\u003cp\u003eA usage example is given below:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"3359\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_hub.wikipedia.base \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e WikipediaReader\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.tools.on_demand_loader_tool \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OnDemandLoaderTool\n\ntool = OnDemandLoaderTool.from_defaults(\n reader,\n name=\u003cspan class=\"hljs-string\"\u003e\"Wikipedia Tool\"\u003c/span\u003e,\n description=\u003cspan class=\"hljs-string\"\u003e\"A tool for loading data and querying articles from Wikipedia\"\u003c/span\u003e\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eLoadAndSearchToolSpec\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe LoadAndSearchToolSpec takes in any existing Tool as input. As a tool spec, it implements \u003ccode class=\"cw qx qy qz qp b\"\u003eto_tool_list\u003c/code\u003e , and when that function is called, two tools are returned: a \u003ccode class=\"cw qx qy qz qp b\"\u003eload\u003c/code\u003e tool and then a \u003ccode class=\"cw qx qy qz qp b\"\u003esearch\u003c/code\u003e tool.\u003c/p\u003e\u003cp\u003eThe \u003ccode class=\"cw qx qy qz qp b\"\u003eload\u003c/code\u003e Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The \u003ccode class=\"cw qx qy qz qp b\"\u003esearch\u003c/code\u003e Tool execution would take in a query string as input and call the underlying index.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*oU9riMUVakKA_gmTVFNVXQ.png\" alt=\"\" width=\"700\" height=\"336\"\u003e\u003c/figure\u003e\u003cp\u003eThis is helpful for any API endpoint that will by default return large volumes of data — for instance our WikipediaToolSpec will by default return entire Wikipedia pages, which will easily overflow most LLM context windows.\u003c/p\u003e\u003cp\u003eExample usage is shown below:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"acf7\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_hub.tools.wikipedia.base \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e WikipediaToolSpec\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.tools.tool_spec.load_and_search.base \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e LoadAndSearchToolSpec\n\nwiki_spec = WikipediaToolSpec()\n\u003cspan class=\"hljs-comment\"\u003e# Get the search wikipedia tool\u003c/span\u003e\ntool = wiki_spec.to_tool_list()[\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e]\n\u003cspan class=\"hljs-comment\"\u003e# Create the Agent with load/search tools\u003c/span\u003e\nagent = OpenAIAgent.from_tools(\n LoadAndSearchToolSpec.from_defaults(\n    tool\n ).to_tool_list(), verbose=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis is the output when we run an input prompt\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"13c2\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003eagent.chat('what is the capital of poland')\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOutput:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"2bda\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003e=== \u003cspan class=\"hljs-title class_\"\u003eCalling\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFunction\u003c/span\u003e ===\n\u003cspan class=\"hljs-title class_\"\u003eCalling\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efunction\u003c/span\u003e: search_data \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eargs\u003c/span\u003e: {\n  \u003cspan class=\"hljs-string\"\u003e\"query\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"capital of Poland\"\u003c/span\u003e\n}\n\u003cspan class=\"hljs-title class_\"\u003eGot\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eoutput\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eContent\u003c/span\u003e loaded! \u003cspan class=\"hljs-title class_\"\u003eYou\u003c/span\u003e can now search the information using read_search_data\n========================\n=== \u003cspan class=\"hljs-title class_\"\u003eCalling\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eFunction\u003c/span\u003e ===\n\u003cspan class=\"hljs-title class_\"\u003eCalling\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efunction\u003c/span\u003e: read_search_data \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eargs\u003c/span\u003e: {\n  \u003cspan class=\"hljs-string\"\u003e\"query\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"What is the capital of Poland?\"\u003c/span\u003e\n}\n\u003cspan class=\"hljs-title class_\"\u003eGot\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eoutput\u003c/span\u003e: \n\u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e capital \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePoland\u003c/span\u003e is \u003cspan class=\"hljs-title class_\"\u003eWarsaw\u003c/span\u003e.\n========================\n\u003cspan class=\"hljs-title class_\"\u003eAgentChatResponse\u003c/span\u003e(response=\u003cspan class=\"hljs-string\"\u003e'The capital of Poland is Warsaw.'\u003c/span\u003e, sources=[])\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNote that the agent figures out that it first needs to first call the “load” tool (denoted by the original name of the tool, “search_data”). This load tool will load the Wikipedia page and index under the hood. The output just mentions that the “content is loaded, and tells the agent that the next step is to use \u003ccode class=\"cw qx qy qz qp b\"\u003eread_search_data\u003c/code\u003e. The agent then reasons that it needs to call the \u003ccode class=\"cw qx qy qz qp b\"\u003eread_search_data\u003c/code\u003e tool, which will query the index for the right answer.\u003c/p\u003e\u003ch1\u003eFAQ\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eShould I use Data Agents for search and retrieval, or continue to use Query Engines?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eShort answer: both are possible. Query engines give you the ability to define your own workflows over your data, in both a constrained reasoning fashion as well as unconstrained fashion. For instance, you may want to define a specific workflow over text-to-SQL with our \u003ccode class=\"cw qx qy qz qp b\"\u003eNLStructStoreQueryEngine\u003c/code\u003e (constrained), or a router module to decide between semantic search or summarization (less constrained), or use our \u003ccode class=\"cw qx qy qz qp b\"\u003eSubQuestionQueryEngine\u003c/code\u003e to decompose a question among sub-documents (even less constrained).\u003c/p\u003e\u003cp\u003eBy default, agent loops are unconstrained, and can theoretically reason over any set of tools that you give them. This means that you can get out-of-the-box advanced search/retrieval capabilities — for instance, in our OpenAI cookbook we show that you can get joint text-to-SQL capabilities by simply providing a SQL query engine and Vector Store Query engine as tools. But on the other hand, agents built in this fashion can be quite unreliable (see our blog post for more insights). If you are using agents for search/retrieval, be mindful of the 1) LLM you pick, and the 2) set of tools you pick too.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHow are LlamaIndex data agents different than existing agent frameworks (LangChain, Hugging Face, etc.)?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eMost of these core concepts are not new. Our overall design has taken inspiration from popular tools and frameworks for building agents. But in our “data agents” design, we’ve tried our best to answer the following key questions well:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHow do we effectively index/query and retrieve data beforehand?\u003c/li\u003e\u003cli\u003eHow do we effectively index/query and retrieve data on the fly?\u003c/li\u003e\u003cli\u003eHow do we design API interfaces for read/writes that are simultaneously rich (can take in structured inputs), but also easy for agents to understand?\u003c/li\u003e\u003cli\u003eHow do we properly get sources in citations?\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOur goal with data agents is to create automated knowledge workers that can reason over and interact with data. Our core toolkit provides the foundations for properly indexing, retrieving, and querying data — these can be easily integrated as tools. We provide some additional tool abstractions to handle the cases where you want to “cache” API outputs on the fly (see above). Finally, we provide principled tool abstractions and design principles so that agents can interface with external services in a structured manner.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCan I use Tools with LangChain agents? \n\u003c/strong\u003eYou can easily use any of our tools with LangChain agents as well.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0804\" class=\"qs ow gt qp b bf qt qu l qv qw\"\u003etools = tool_spec.to_tool_list()\nlangchain_tools = [t.to_langchain_tool() \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e t \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e tools]\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eSee our \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/usage_pattern.html#using-with-langchain\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003etools usage guide\u003c/a\u003e for more details!\u003c/p\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eIn summary, today we launched two key items: Data Agent components (incl. agent reasoning loop and tool abstractions) and the LlamaHub Tool repository.\u003c/p\u003e\u003ch2\u003eResources\u003c/h2\u003e\u003cp\u003eWe’ve written a comprehensive section in the docs — take a look here: \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/agents/root.html\u003c/a\u003e\u003c/p\u003e\u003cp\u003eTake a look at our LlamaHub Tools section: \u003ca href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://llamahub.ai/\u003c/a\u003e\u003c/p\u003e\u003cp\u003eNotebook Tutorials for LlamaHub Tools: \u003ca href=\"https://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/emptycrown/llama-hub/tree/main/llama_hub/tools/notebooks\u003c/a\u003e\u003c/p\u003e\u003cp\u003eIf you have questions, please hop on our Discord: \u003ca href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://discord.gg/dGcwcsnxhU\u003c/a\u003e\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-b8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/b8dc3145a4ecdb74e5972ca24e010ba403437818-2496x1827.png","publishedDate":"2023-07-12","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-89380b5946452462d6c4a91f8109f705dc044c82-1080x1080-png","_type":"reference"}},"publishedDate":"2025-06-05","slug":"introducing-the-spreadsheet-agent-in-private-preview","title":"Introducing the Spreadsheet Agent, in private preview"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562-png","_type":"reference"}},"publishedDate":"2025-05-29","slug":"rag-is-dead-long-live-agentic-retrieval","title":"RAG is dead, long live agentic retrieval"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379-png","_type":"reference"}},"publishedDate":"2025-05-13","slug":"improved-long-and-short-term-memory-for-llamaindex-agents","title":"Improved Long \u0026 Short-Term Memory for LlamaIndex Agents"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-dde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536-png","_type":"reference"}},"publishedDate":"2025-04-25","slug":"bending-without-breaking-optimal-design-patterns-for-effective-agents","title":"Bending without breaking: optimal design patterns for effective agents"}],"slug":{"_type":"slug","current":"data-agents-eed797d7972f"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"78713226-8bff-400f-bbfe-fd8a3d90be1d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"nlp"},"title":"NLP"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"248fed26-a405-4fa0-ba99-8f1af0df185c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"agents"},"title":"Agents"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d05727dc-d53f-4ac2-b076-f4af681ec188","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"data"},"title":"Data"}],"title":"Data Agents"},"publishedDate":"Invalid Date"},"params":{"slug":"data-agents-eed797d7972f"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"data-agents-eed797d7972f"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>