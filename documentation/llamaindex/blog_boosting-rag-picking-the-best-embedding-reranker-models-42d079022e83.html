<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Boosting RAG: Picking the Best Embedding &amp; Reranker models — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Boosting RAG: Picking the Best Embedding &amp; Reranker models — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Boosting RAG: Picking the Best Embedding &amp; Reranker models — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Boosting RAG: Picking the Best Embedding &amp; Reranker models — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="153" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/ravi-theja">Ravi Theja</a> <!-- -->•<!-- --> <!-- -->2023-11-03</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Boosting RAG: Picking the Best Embedding &amp; Reranker models</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/embedding"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Embedding</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/openai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">OpenAI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/search"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Search</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p><strong>UPDATE</strong>: The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the <code class="cw oi oj ok ol b">JinaAI-v2-base-en</code> with <code class="cw oi oj ok ol b">bge-reranker-large</code>now exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with<code class="cw oi oj ok ol b">CohereRerank</code> exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.</p><p>When building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.</p><p>But with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?</p><p>In this blog post, we’ll use the <code class="cw oi oj ok ol b">Retrieval Evaluation</code> module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in!</p><p>Let’s first start with understanding the metrics available in <code class="cw oi oj ok ol b">Retrieval Evaluation</code></p><h1>Understanding Metrics in Retrieval Evaluation:</h1><p>To gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: <strong>Hit Rate</strong> and <strong>Mean Reciprocal Rank (MRR)</strong>. Let’s delve into these metrics to understand their significance and how they operate.</p><p><strong>Hit Rate:</strong></p><p>Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.</p><p><strong>Mean Reciprocal Rank (MRR):</strong></p><p>For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.</p><p>Now that we’ve established the scope and familiarized ourselves with the metrics, it’s time to dive into the experiment. For a hands-on experience, you can also follow along using our <a href="https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing" rel="noopener ugc nofollow" target="_blank">Google Colab Notebook</a></p><h1>Setting Up the Environment</h1><pre><span id="df6f" class="py on gt ol b bf pz qa l qb qc">!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf</span></pre><h1>Setting Up the Keys</h1><pre><span id="6659" class="py on gt ol b bf pz qa l qb qc">openai_api_key = <span class="hljs-string">'YOUR OPENAI API KEY'</span>
cohere_api_key = <span class="hljs-string">'YOUR COHEREAI API KEY'</span>
anthropic_api_key = <span class="hljs-string">'YOUR ANTHROPIC API KEY'</span>
openai.api_key = openai_api_key</span></pre><h1>Download the Data</h1><p>We will use Llama2 paper for this experiment. Let’s download the paper.</p><pre><span id="525a" class="py on gt ol b bf pz qa l qb qc">!wget --user-agent "Mozilla" "https://arxiv.org/pdf/2307.09288.pdf" -O "llama2.pdf"</span></pre><h1>Load the Data</h1><p>Let’s load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.</p><p>This data was then parsed by converted to nodes, which represent chunks of data we’d like to retrieve. We did use chunk_size as 512.</p><pre><span id="dfd8" class="py on gt ol b bf pz qa l qb qc">documents = SimpleDirectoryReader(input_files=["llama2.pdf"]).load_data()

node_parser = SimpleNodeParser.from_defaults(chunk_size=512)
nodes = node_parser.get_nodes_from_documents(documents)</span></pre><h1>Generating Question-Context Pairs:</h1><p>For evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.</p><p>Let’s initialize a prompt template to generate question-context pairs.</p><pre><span id="6e11" class="py on gt ol b bf pz qa l qb qc"># Prompt to generate questions
qa_generate_prompt_tmpl = """\
Context information is below.

---------------------
{context_str}
---------------------

Given the context information and not prior knowledge.
generate only questions based on the below query.

You are a Professor. Your task is to setup \
{num_questions_per_chunk} questions for an upcoming \
quiz/examination. The questions should be diverse in nature \
across the document. The questions should not contain options, not start with Q1/ Q2. \
Restrict the questions to the context information provided.\
"""</span></pre><pre><span id="13b2" class="py on gt ol b bf pz qa l qb qc">llm = Anthropic(api_key=anthropic_api_key)
qa_dataset = generate_question_context_pairs(
    nodes, llm=llm, num_questions_per_chunk=<span class="hljs-number">2</span>
)</span></pre><p>Function to filter out sentences such as — <code class="cw oi oj ok ol b">Here are 2 questions based on provided context</code></p><pre><span id="b03a" class="py on gt ol b bf pz qa l qb qc"><span class="hljs-comment"># function to clean the dataset</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_qa_dataset</span>(<span class="hljs-params">qa_dataset</span>):
    <span class="hljs-string">"""
    Filters out queries from the qa_dataset that contain certain phrases and the corresponding
    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with
    the filtered data.

    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.
    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.
    """</span>

    <span class="hljs-comment"># Extract keys from queries and relevant_docs that need to be removed</span>
    queries_relevant_docs_keys_to_remove = {
        k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> qa_dataset.queries.items()
        <span class="hljs-keyword">if</span> <span class="hljs-string">'Here are 2'</span> <span class="hljs-keyword">in</span> v <span class="hljs-keyword">or</span> <span class="hljs-string">'Here are two'</span> <span class="hljs-keyword">in</span> v
    }

    <span class="hljs-comment"># Filter queries and relevant_docs using dictionary comprehensions</span>
    filtered_queries = {
        k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> qa_dataset.queries.items()
        <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> queries_relevant_docs_keys_to_remove
    }
    filtered_relevant_docs = {
        k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> qa_dataset.relevant_docs.items()
        <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> queries_relevant_docs_keys_to_remove
    }

    <span class="hljs-comment"># Create a new instance of EmbeddingQAFinetuneDataset with the filtered data</span>
    <span class="hljs-keyword">return</span> EmbeddingQAFinetuneDataset(
        queries=filtered_queries,
        corpus=qa_dataset.corpus,
        relevant_docs=filtered_relevant_docs
    )

<span class="hljs-comment"># filter out pairs with phrases `Here are 2 questions based on provided context`</span>
qa_dataset = filter_qa_dataset(qa_dataset)</span></pre><h1>Custom Retriever:</h1><p>To identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base <code class="cw oi oj ok ol b">VectorIndexRetriever</code>. Upon retrieving the nodes, we then introduce a reranker to further refine the results. It’s worth noting that for this particular experiment, we’ve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with <code class="cw oi oj ok ol b">OpenAIEmbedding</code>, please refer to the <a href="https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing" rel="noopener ugc nofollow" target="_blank">notebook</a> for code with other embeddings.</p><pre><span id="d790" class="py on gt ol b bf pz qa l qb qc">embed_model = OpenAIEmbedding()
service_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)
vector_index = VectorStoreIndex(nodes, service_context=service_context)
vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10)</span></pre><pre><span id="89b3" class="py on gt ol b bf pz qa l qb qc"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomRetriever</span>(<span class="hljs-title class_ inherited__">BaseRetriever</span>):
    <span class="hljs-string">"""Custom retriever that performs both Vector search and Knowledge Graph search"""</span>

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">
        self,
        vector_retriever: VectorIndexRetriever,
    </span>) -&amp;gt; <span class="hljs-literal">None</span>:
        <span class="hljs-string">"""Init params."""</span>

        self._vector_retriever = vector_retriever

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_retrieve</span>(<span class="hljs-params">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class="hljs-type">List</span>[NodeWithScore]:
        <span class="hljs-string">"""Retrieve nodes given query."""</span>

    retrieved_nodes = self._vector_retriever.retrieve(query_bundle)

    <span class="hljs-keyword">if</span> reranker != <span class="hljs-string">'None'</span>:
      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)
       <span class="hljs-keyword">else</span>:
          retrieved_nodes = retrieved_nodes[:<span class="hljs-number">5</span>]
         
       <span class="hljs-keyword">return</span> retrieved_nodes

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_aretrieve</span>(<span class="hljs-params">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class="hljs-type">List</span>[NodeWithScore]:
        <span class="hljs-string">"""Asynchronously retrieve nodes given query.

        Implemented by the user.

        """</span>
        <span class="hljs-keyword">return</span> self._retrieve(query_bundle)

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">aretrieve</span>(<span class="hljs-params">self, str_or_query_bundle: QueryType</span>) -&amp;gt; <span class="hljs-type">List</span>[NodeWithScore]:
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(str_or_query_bundle, <span class="hljs-built_in">str</span>):
            str_or_query_bundle = QueryBundle(str_or_query_bundle)
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> self._aretrieve(str_or_query_bundle)

custom_retriever = CustomRetriever(vector_retriever)</span></pre><h1>Evaluation:</h1><p>To evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics:</p><pre><span id="cfb0" class="py on gt ol b bf pz qa l qb qc">retriever_evaluator = RetrieverEvaluator.from_metric_names(
    [<span class="hljs-string">"mrr"</span>, <span class="hljs-string">"hit_rate"</span>], retriever=custom_retriever
)
eval_results = <span class="hljs-keyword">await</span> retriever_evaluator.aevaluate_dataset(qa_dataset)</span></pre><h1>Results:</h1><p>We put various embedding models and rerankers to the test. Here are the models we considered:</p><p><strong>Embedding Models</strong>:</p><ul><li><a href="https://platform.openai.com/docs/guides/embeddings" rel="noopener ugc nofollow" target="_blank">OpenAI Embedding</a></li><li><a href="https://www.voyageai.com/" rel="noopener ugc nofollow" target="_blank">Voyage Embedding</a></li><li><a href="https://txt.cohere.com/introducing-embed-v3/" rel="noopener ugc nofollow" target="_blank">CohereAI Embedding</a> (v2.0/ v3.0)</li><li><a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en" rel="noopener ugc nofollow" target="_blank">Jina Embeddings</a> (small/ base)</li><li><a href="https://huggingface.co/BAAI/bge-large-en" rel="noopener ugc nofollow" target="_blank">BAAI/bge-large-en</a></li><li><a href="https://developers.generativeai.google/tutorials/embeddings_quickstart" rel="noopener ugc nofollow" target="_blank">Google PaLM Embedding</a></li></ul><p><strong>Rerankers</strong>:</p><ul><li><a href="https://txt.cohere.com/rerank/" rel="noopener ugc nofollow" target="_blank">CohereAI</a></li><li><a href="https://huggingface.co/BAAI/bge-reranker-base" rel="noopener ugc nofollow" target="_blank">bge-reranker-base</a></li><li><a href="https://huggingface.co/BAAI/bge-reranker-large" rel="noopener ugc nofollow" target="_blank">bge-reranker-large</a></li></ul><blockquote><p id="015c" class="nk nl qp nm b nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh gm bj">It’s worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on.</p></blockquote><p>The table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR):</p><figure><img src="/blog/images/1*tCBbIjV_jLZP1AKLTX7rAw.png" alt="" width="700" height="198"></figure><h1>Analysis:</h1><h2><strong>Performance by Embedding:</strong></h2><ul><li><strong>OpenAI</strong>: Showcases top-tier performance, especially with the <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code> (0.926966 hit rate, 0.86573 MRR) and <code class="cw oi oj ok ol b"><strong>bge-reranker-large</strong></code> (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools.</li><li><strong>bge-large</strong>: Experiences significant improvement with rerankers, with the best results from <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code> (0.876404 hit rate, 0.822753 MRR).</li><li><strong>llm-embedder</strong>: Benefits greatly from reranking, particularly with <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code> (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost.</li><li><strong>Cohere</strong>: Cohere’s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR.</li><li><strong>Voyage</strong>: Has strong initial performance that is further amplified by <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code> (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking.</li><li><strong>JinaAI</strong>: Very strong performance, sees notable gains with <code class="cw oi oj ok ol b"><strong>bge-reranker-large</strong></code> (0.938202 hit rate, 0.868539 MRR) and <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code> (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance.</li><li><strong>Google-PaLM</strong>: The model demonstrates strong performance, with measurable gains when using the <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code>(0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results.</li></ul><h2><strong>Impact of Rerankers</strong>:</h2><ul><li><strong>WithoutReranker</strong>: This provides the baseline performance for each embedding.</li><li><strong>bge-reranker-base</strong>: Generally improves both hit rate and MRR across embeddings.</li><li><strong>bge-reranker-large</strong>: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code>.</li><li><strong>CohereRerank</strong>: Consistently enhances performance across all embeddings, often providing the best or near-best results.</li></ul><h2><strong>Necessity of Rerankers</strong>:</h2><ul><li>The data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs.</li><li>Rerankers, especially <code class="cw oi oj ok ol b"><strong>CohereRerank</strong></code>, have demonstrated their capability to transform any embedding into a competitive one.</li></ul><h2><strong>Overall Superiority</strong>:</h2><ul><li>When considering both hit rate and MRR, the combinations of <code class="cw oi oj ok ol b"><strong>OpenAI + CohereRerank</strong></code> and <code class="cw oi oj ok ol b"><strong>JinaAI-Base + bge-reranker-large/ CohereRerank</strong></code> emerge as top contenders.</li><li>However, the consistent improvement brought by the <code class="cw oi oj ok ol b"><strong>CohereRerank/ bge-reranker-large</strong></code> rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use.</li></ul><p>In summary, to achieve the peak performance in both hit rate and MRR, the combination of <code class="cw oi oj ok ol b"><strong>OpenAI</strong></code> or <code class="cw oi oj ok ol b"><strong>JinaAI-Base</strong></code> embeddings with the <code class="cw oi oj ok ol b"><strong>CohereRerank/bge-reranker-large</strong></code> reranker stands out.</p><blockquote><p id="4791" class="rl rm gt be rn ro rp rq rr rs rt oh dt">Please be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them.</p></blockquote><h1>Conclusions:</h1><p>In this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions.</p><ul><li><strong>Embeddings</strong>: The <code class="cw oi oj ok ol b"><strong>OpenAI</strong></code> and <code class="cw oi oj ok ol b"><strong>JinaAI-Base</strong></code> embeddings, especially when paired with the <code class="cw oi oj ok ol b"><strong>CohereRerank/bge-reranker-large</strong></code> reranker, set the gold standard for both hit rate and MRR.</li><li><strong>Rerankers</strong>: The influence of rerankers, particularly <code class="cw oi oj ok ol b"><strong>CohereRerank/bge-reranker-large</strong></code>, cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.</li><li><strong>Foundation is Key</strong>: Choosing the right embedding for the initial search is essential; even the best reranker can’t help much if the basic search results aren’t good.</li><li><strong>Working Together:</strong> To get the best out of retrievers, it’s important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing.</li></ul></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"e16cd3e1-043c-4aa3-addb-52c9a22734ab","_rev":"TLgH6AcXrxoqw75SBDhni6","_type":"blogPost","_updatedAt":"2025-05-21T20:40:13Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:58:55Z","_id":"60575af5-a5c2-40f6-9aab-d5e02da9c000","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Ravi Theja","slug":{"_type":"slug","current":"ravi-theja"}}],"featured":false,"htmlContent":"\u003cp\u003e\u003cstrong\u003eUPDATE\u003c/strong\u003e: The pooling method for the Jina AI embeddings has been adjusted to use mean pooling, and the results have been updated accordingly. Notably, the \u003ccode class=\"cw oi oj ok ol b\"\u003eJinaAI-v2-base-en\u003c/code\u003e with \u003ccode class=\"cw oi oj ok ol b\"\u003ebge-reranker-large\u003c/code\u003enow exhibits a Hit Rate of 0.938202 and an MRR (Mean Reciprocal Rank) of 0.868539 and with\u003ccode class=\"cw oi oj ok ol b\"\u003eCohereRerank\u003c/code\u003e exhibits a Hit Rate of 0.932584, and an MRR of 0.873689.\u003c/p\u003e\u003cp\u003eWhen building a Retrieval Augmented Generation (RAG) pipeline, one key component is the Retriever. We have a variety of embedding models to choose from, including OpenAI, CohereAI, and open-source sentence transformers. Additionally, there are several rerankers available from CohereAI and sentence transformers.\u003c/p\u003e\u003cp\u003eBut with all these options, how do we determine the best mix for top-notch retrieval performance? How do we know which embedding model fits our data best? Or which reranker boosts our results the most?\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll use the \u003ccode class=\"cw oi oj ok ol b\"\u003eRetrieval Evaluation\u003c/code\u003e module from LlamaIndex to swiftly determine the best combination of embedding and reranker models. Let's dive in!\u003c/p\u003e\u003cp\u003eLet’s first start with understanding the metrics available in \u003ccode class=\"cw oi oj ok ol b\"\u003eRetrieval Evaluation\u003c/code\u003e\u003c/p\u003e\u003ch1\u003eUnderstanding Metrics in Retrieval Evaluation:\u003c/h1\u003e\u003cp\u003eTo gauge the efficacy of our retrieval system, we primarily relied on two widely accepted metrics: \u003cstrong\u003eHit Rate\u003c/strong\u003e and \u003cstrong\u003eMean Reciprocal Rank (MRR)\u003c/strong\u003e. Let’s delve into these metrics to understand their significance and how they operate.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eHit Rate:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eHit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eMean Reciprocal Rank (MRR):\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\u003c/p\u003e\u003cp\u003eNow that we’ve established the scope and familiarized ourselves with the metrics, it’s time to dive into the experiment. For a hands-on experience, you can also follow along using our \u003ca href=\"https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Colab Notebook\u003c/a\u003e\u003c/p\u003e\u003ch1\u003eSetting Up the Environment\u003c/h1\u003e\u003cpre\u003e\u003cspan id=\"df6f\" class=\"py on gt ol b bf pz qa l qb qc\"\u003e!pip install llama-index sentence-transformers cohere anthropic voyageai protobuf pypdf\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eSetting Up the Keys\u003c/h1\u003e\u003cpre\u003e\u003cspan id=\"6659\" class=\"py on gt ol b bf pz qa l qb qc\"\u003eopenai_api_key = \u003cspan class=\"hljs-string\"\u003e'YOUR OPENAI API KEY'\u003c/span\u003e\ncohere_api_key = \u003cspan class=\"hljs-string\"\u003e'YOUR COHEREAI API KEY'\u003c/span\u003e\nanthropic_api_key = \u003cspan class=\"hljs-string\"\u003e'YOUR ANTHROPIC API KEY'\u003c/span\u003e\nopenai.api_key = openai_api_key\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eDownload the Data\u003c/h1\u003e\u003cp\u003eWe will use Llama2 paper for this experiment. Let’s download the paper.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"525a\" class=\"py on gt ol b bf pz qa l qb qc\"\u003e!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"llama2.pdf\"\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eLoad the Data\u003c/h1\u003e\u003cp\u003eLet’s load the data. We will use Pages from start to 36 for the experiment which excludes table of contents, references, and appendix.\u003c/p\u003e\u003cp\u003eThis data was then parsed by converted to nodes, which represent chunks of data we’d like to retrieve. We did use chunk_size as 512.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"dfd8\" class=\"py on gt ol b bf pz qa l qb qc\"\u003edocuments = SimpleDirectoryReader(input_files=[\"llama2.pdf\"]).load_data()\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=512)\nnodes = node_parser.get_nodes_from_documents(documents)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eGenerating Question-Context Pairs:\u003c/h1\u003e\u003cp\u003eFor evaluation purposes, we created a dataset of question-context pairs. This dataset can be seen as a set of questions and their corresponding context from our data. To remove bias for the evaluation of embedding(OpenAI/ CohereAI) and Reranker (CohereAI), we use Anthropic LLM to generate Question-Context Pairs.\u003c/p\u003e\u003cp\u003eLet’s initialize a prompt template to generate question-context pairs.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6e11\" class=\"py on gt ol b bf pz qa l qb qc\"\u003e# Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\"\u003c/span\u003e\u003c/pre\u003e\u003cpre\u003e\u003cspan id=\"13b2\" class=\"py on gt ol b bf pz qa l qb qc\"\u003ellm = Anthropic(api_key=anthropic_api_key)\nqa_dataset = generate_question_context_pairs(\n    nodes, llm=llm, num_questions_per_chunk=\u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eFunction to filter out sentences such as — \u003ccode class=\"cw oi oj ok ol b\"\u003eHere are 2 questions based on provided context\u003c/code\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b03a\" class=\"py on gt ol b bf pz qa l qb qc\"\u003e\u003cspan class=\"hljs-comment\"\u003e# function to clean the dataset\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003efilter_qa_dataset\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eqa_dataset\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    Filters out queries from the qa_dataset that contain certain phrases and the corresponding\n    entries in the relevant_docs, and creates a new EmbeddingQAFinetuneDataset object with\n    the filtered data.\n\n    :param qa_dataset: An object that has 'queries', 'corpus', and 'relevant_docs' attributes.\n    :return: An EmbeddingQAFinetuneDataset object with the filtered queries, corpus and relevant_docs.\n    \"\"\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# Extract keys from queries and relevant_docs that need to be removed\u003c/span\u003e\n    queries_relevant_docs_keys_to_remove = {\n        k \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e qa_dataset.queries.items()\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'Here are 2'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e v \u003cspan class=\"hljs-keyword\"\u003eor\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'Here are two'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e v\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e# Filter queries and relevant_docs using dictionary comprehensions\u003c/span\u003e\n    filtered_queries = {\n        k: v \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e qa_dataset.queries.items()\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e queries_relevant_docs_keys_to_remove\n    }\n    filtered_relevant_docs = {\n        k: v \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e k, v \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e qa_dataset.relevant_docs.items()\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e k \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e queries_relevant_docs_keys_to_remove\n    }\n\n    \u003cspan class=\"hljs-comment\"\u003e# Create a new instance of EmbeddingQAFinetuneDataset with the filtered data\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e EmbeddingQAFinetuneDataset(\n        queries=filtered_queries,\n        corpus=qa_dataset.corpus,\n        relevant_docs=filtered_relevant_docs\n    )\n\n\u003cspan class=\"hljs-comment\"\u003e# filter out pairs with phrases `Here are 2 questions based on provided context`\u003c/span\u003e\nqa_dataset = filter_qa_dataset(qa_dataset)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eCustom Retriever:\u003c/h1\u003e\u003cp\u003eTo identify the optimal retriever, we employ a combination of an embedding model and a reranker. Initially, we establish a base \u003ccode class=\"cw oi oj ok ol b\"\u003eVectorIndexRetriever\u003c/code\u003e. Upon retrieving the nodes, we then introduce a reranker to further refine the results. It’s worth noting that for this particular experiment, we’ve set similarity_top_k to 10 and picked top-5 with reranker. However, feel free to adjust this parameter based on the needs of your specific experiment. We are showing the code here with \u003ccode class=\"cw oi oj ok ol b\"\u003eOpenAIEmbedding\u003c/code\u003e, please refer to the \u003ca href=\"https://colab.research.google.com/drive/1TxDVA__uimVPOJiMEQgP5fwHiqgKqm4-?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003enotebook\u003c/a\u003e for code with other embeddings.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d790\" class=\"py on gt ol b bf pz qa l qb qc\"\u003eembed_model = OpenAIEmbedding()\nservice_context = ServiceContext.from_defaults(llm=None, embed_model = embed_model)\nvector_index = VectorStoreIndex(nodes, service_context=service_context)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k = 10)\u003c/span\u003e\u003c/pre\u003e\u003cpre\u003e\u003cspan id=\"89b3\" class=\"py on gt ol b bf pz qa l qb qc\"\u003e\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCustomRetriever\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eBaseRetriever\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n        self,\n        vector_retriever: VectorIndexRetriever,\n    \u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\"Init params.\"\"\"\u003c/span\u003e\n\n        self._vector_retriever = vector_retriever\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e_retrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, query_bundle: QueryBundle\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[NodeWithScore]:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\"Retrieve nodes given query.\"\"\"\u003c/span\u003e\n\n    retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e reranker != \u003cspan class=\"hljs-string\"\u003e'None'\u003c/span\u003e:\n      retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n       \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n          retrieved_nodes = retrieved_nodes[:\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e]\n         \n       \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e retrieved_nodes\n\n    \u003cspan class=\"hljs-keyword\"\u003easync\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e_aretrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, query_bundle: QueryBundle\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[NodeWithScore]:\n        \u003cspan class=\"hljs-string\"\u003e\"\"\"Asynchronously retrieve nodes given query.\n\n        Implemented by the user.\n\n        \"\"\"\u003c/span\u003e\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self._retrieve(query_bundle)\n\n    \u003cspan class=\"hljs-keyword\"\u003easync\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003earetrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, str_or_query_bundle: QueryType\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[NodeWithScore]:\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eisinstance\u003c/span\u003e(str_or_query_bundle, \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e):\n            str_or_query_bundle = QueryBundle(str_or_query_bundle)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eawait\u003c/span\u003e self._aretrieve(str_or_query_bundle)\n\ncustom_retriever = CustomRetriever(vector_retriever)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eEvaluation:\u003c/h1\u003e\u003cp\u003eTo evaluate our retriever, we computed the Mean Reciprocal Rank (MRR) and Hit Rate metrics:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"cfb0\" class=\"py on gt ol b bf pz qa l qb qc\"\u003eretriever_evaluator = RetrieverEvaluator.from_metric_names(\n    [\u003cspan class=\"hljs-string\"\u003e\"mrr\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"hit_rate\"\u003c/span\u003e], retriever=custom_retriever\n)\neval_results = \u003cspan class=\"hljs-keyword\"\u003eawait\u003c/span\u003e retriever_evaluator.aevaluate_dataset(qa_dataset)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eResults:\u003c/h1\u003e\u003cp\u003eWe put various embedding models and rerankers to the test. Here are the models we considered:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eEmbedding Models\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://platform.openai.com/docs/guides/embeddings\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI Embedding\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.voyageai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eVoyage Embedding\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://txt.cohere.com/introducing-embed-v3/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCohereAI Embedding\u003c/a\u003e (v2.0/ v3.0)\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/jinaai/jina-embeddings-v2-small-en\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJina Embeddings\u003c/a\u003e (small/ base)\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/BAAI/bge-large-en\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eBAAI/bge-large-en\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://developers.generativeai.google/tutorials/embeddings_quickstart\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle PaLM Embedding\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eRerankers\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://txt.cohere.com/rerank/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCohereAI\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/BAAI/bge-reranker-base\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ebge-reranker-base\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/BAAI/bge-reranker-large\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ebge-reranker-large\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cblockquote\u003e\u003cp id=\"015c\" class=\"nk nl qp nm b nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh gm bj\"\u003eIt’s worth mentioning that these results provide a solid insight into performance for this particular dataset and task. However, actual outcomes may differ based on data characteristics, dataset size, and other variables like chunk_size, similarity_top_k, and so on.\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eThe table below showcases the evaluation results based on the metrics of Hit Rate and Mean Reciprocal Rank (MRR):\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*tCBbIjV_jLZP1AKLTX7rAw.png\" alt=\"\" width=\"700\" height=\"198\"\u003e\u003c/figure\u003e\u003ch1\u003eAnalysis:\u003c/h1\u003e\u003ch2\u003e\u003cstrong\u003ePerformance by Embedding:\u003c/strong\u003e\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e: Showcases top-tier performance, especially with the \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e (0.926966 hit rate, 0.86573 MRR) and \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003ebge-reranker-large\u003c/strong\u003e\u003c/code\u003e (0.910112 hit rate, 0.855805 MRR), indicating strong compatibility with reranking tools.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ebge-large\u003c/strong\u003e: Experiences significant improvement with rerankers, with the best results from \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e (0.876404 hit rate, 0.822753 MRR).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ellm-embedder\u003c/strong\u003e: Benefits greatly from reranking, particularly with \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e (0.882022 hit rate, 0.830243 MRR), which offers a substantial performance boost.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCohere\u003c/strong\u003e: Cohere’s latest v3.0 embeddings outperform v2.0 and, with the integration of native CohereRerank, significantly improve its metrics, boasting a 0.88764 hit rate and a 0.836049 MRR.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eVoyage\u003c/strong\u003e: Has strong initial performance that is further amplified by \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e (0.91573 hit rate, 0.851217 MRR), suggesting high responsiveness to reranking.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eJinaAI\u003c/strong\u003e: Very strong performance, sees notable gains with \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003ebge-reranker-large\u003c/strong\u003e\u003c/code\u003e (0.938202 hit rate, 0.868539 MRR) and \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e (0.932584 hit rate, 0.873689), indicating that reranking significantly boosts its performance.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGoogle-PaLM\u003c/strong\u003e: The model demonstrates strong performance, with measurable gains when using the \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e(0.910112 hit rate, 0.855712 MRR). This indicates that reranking provides a clear boost to its overall results.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eImpact of Rerankers\u003c/strong\u003e:\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eWithoutReranker\u003c/strong\u003e: This provides the baseline performance for each embedding.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ebge-reranker-base\u003c/strong\u003e: Generally improves both hit rate and MRR across embeddings.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ebge-reranker-large\u003c/strong\u003e: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e: Consistently enhances performance across all embeddings, often providing the best or near-best results.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eNecessity of Rerankers\u003c/strong\u003e:\u003c/h2\u003e\u003cul\u003e\u003cli\u003eThe data clearly indicates the significance of rerankers in refining search results. Nearly all embeddings benefit from reranking, showing improved hit rates and MRRs.\u003c/li\u003e\u003cli\u003eRerankers, especially \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank\u003c/strong\u003e\u003c/code\u003e, have demonstrated their capability to transform any embedding into a competitive one.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eOverall Superiority\u003c/strong\u003e:\u003c/h2\u003e\u003cul\u003e\u003cli\u003eWhen considering both hit rate and MRR, the combinations of \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eOpenAI + CohereRerank\u003c/strong\u003e\u003c/code\u003e and \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eJinaAI-Base + bge-reranker-large/ CohereRerank\u003c/strong\u003e\u003c/code\u003e emerge as top contenders.\u003c/li\u003e\u003cli\u003eHowever, the consistent improvement brought by the \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank/ bge-reranker-large\u003c/strong\u003e\u003c/code\u003e rerankers across various embeddings make them the standout choice for enhancing search quality, regardless of the embedding in use.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn summary, to achieve the peak performance in both hit rate and MRR, the combination of \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003c/code\u003e or \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eJinaAI-Base\u003c/strong\u003e\u003c/code\u003e embeddings with the \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank/bge-reranker-large\u003c/strong\u003e\u003c/code\u003e reranker stands out.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"4791\" class=\"rl rm gt be rn ro rp rq rr rs rt oh dt\"\u003ePlease be aware that our benchmarks are intended to offer a reproducible script for your own data. Nevertheless, treat these figures as estimates and proceed with caution when interpreting them.\u003c/p\u003e\u003c/blockquote\u003e\u003ch1\u003eConclusions:\u003c/h1\u003e\u003cp\u003eIn this blog post, we have demonstrated how to evaluate and enhance retriever performance using various embeddings and rerankers. Below are our final conclusions.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eEmbeddings\u003c/strong\u003e: The \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eOpenAI\u003c/strong\u003e\u003c/code\u003e and \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eJinaAI-Base\u003c/strong\u003e\u003c/code\u003e embeddings, especially when paired with the \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank/bge-reranker-large\u003c/strong\u003e\u003c/code\u003e reranker, set the gold standard for both hit rate and MRR.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRerankers\u003c/strong\u003e: The influence of rerankers, particularly \u003ccode class=\"cw oi oj ok ol b\"\u003e\u003cstrong\u003eCohereRerank/bge-reranker-large\u003c/strong\u003e\u003c/code\u003e, cannot be overstated. They play a key role in improving the MRR for many embeddings, showing their importance in making search results better.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFoundation is Key\u003c/strong\u003e: Choosing the right embedding for the initial search is essential; even the best reranker can’t help much if the basic search results aren’t good.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eWorking Together:\u003c/strong\u003e To get the best out of retrievers, it’s important to find the right mix of embeddings and rerankers. This study shows how important it is to carefully test and find the best pairing.\u003c/li\u003e\u003c/ul\u003e","image":{"_type":"image","asset":{"_ref":"image-6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/6021f28e6e7b1835a2d69a5ebe0f793b62a0c91f-432x306.png","publishedDate":"2023-11-03","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"}],"slug":{"_type":"slug","current":"boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"},"tags":[{"_createdAt":"2024-02-22T20:19:13Z","_id":"07efcc54-5cb2-4e7e-b6d7-a6275265898f","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"embedding"},"title":"Embedding"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"e171aa9d-bc85-4645-8a08-eabe04c530c7","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"openai"},"title":"OpenAI"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"e6e1133b-d909-40ec-9295-ac77ae3fafcb","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"search"},"title":"Search"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"}],"title":"Boosting RAG: Picking the Best Embedding \u0026 Reranker models"},"publishedDate":"Invalid Date"},"params":{"slug":"boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>