<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="280.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/uptrain">Uptrain</a> <!-- -->•<!-- --> <!-- -->2024-03-19</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/evaluation"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Evaluation</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/rag"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">RAG</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>This is a guest post from Uptrain.</em></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‘vibes’. By combining LlamaIndex&#x27;s flexibility and UpTrain&#x27;s evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q"><strong>About UpTrain</strong></h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>UpTrain</strong> [<a href="https://github.com/uptrain-ai/uptrain" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">github</a> ||<a href="https://uptrain.ai/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"> website</a> ||<a href="https://docs.uptrain.ai/getting-started/introduction" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"> docs</a>] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Key Highlights:</strong></p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Data Security:</strong> As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls).</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Custom Evaluator LLMs:</strong> UpTrain allows for <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/open_source_evaluator_tutorial.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">customisation of your evaluator LLM</a>, offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Insights that help with model improvement: </strong>Beyond mere evaluation, UpTrain performs <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/root_cause_analysis/rag_with_citation.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">root cause analysis</a> to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Diverse Experimentations:</strong> The platform enables <a href="https://github.com/uptrain-ai/uptrain/tree/main/examples/experiments" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">experimentation</a> with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Compare open-source LLMs:</strong> With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality.</li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what’s affecting the quality of your responses, allowing you to take appropriate corrective actions.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q"><strong>LlamaIndex x UpTrain Callback Handler</strong></h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">If you wish to skip right ahead to the tutorial, check it out <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here.</a></p><figure><img alt="" loading="lazy" width="512" height="75.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F99c0abd796d383476d12ae5d7f9f0057f2a4f559-1024x151.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F99c0abd796d383476d12ae5d7f9f0057f2a4f559-1024x151.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F99c0abd796d383476d12ae5d7f9f0057f2a4f559-1024x151.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75"/></figure><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q"><strong>Evals across the board: From Vanilla to Advanced RAG</strong></h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">But as Uncle Ben famously said to Peter Parker in the GenAI universe:</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>“With increased complexity comes more points of failure.”.</em></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let’s look at all the evaluations provided by UpTrain.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q"><strong>Addressing Points of Failure in RAG Pipelines</strong></h2><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA"><strong>1. RAG Query Engine Evaluation</strong></h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let&#x27;s first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM&#x27;s response.</p><figure><img alt="" loading="lazy" width="999.5" height="396.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc83a698ff896c10a72106117c0ef4eeeeeeba2f5-1999x793.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc83a698ff896c10a72106117c0ef4eeeeeeba2f5-1999x793.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=2048&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc83a698ff896c10a72106117c0ef4eeeeeeba2f5-1999x793.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=2048&amp;q=75"/></figure><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><strong>Context Relevance</strong></a>: However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><strong>Factual Accuracy</strong></a>: Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><strong>Response Completeness</strong></a>: Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query.</li></ul><figure><img alt="" loading="lazy" width="512" height="267" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ce17ffc09640913e3618f407c94abaaade7ad3e-1024x534.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ce17ffc09640913e3618f407c94abaaade7ad3e-1024x534.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ce17ffc09640913e3618f407c94abaaade7ad3e-1024x534.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75"/></figure><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA"><strong>2. Sub-Question Query Engine Evaluation</strong></h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let&#x27;s say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine.</p><figure><img alt="" loading="lazy" width="721" height="548" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb76ccf0716315cb28ada74e8511397b562731e6e-1442x1096.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb76ccf0716315cb28ada74e8511397b562731e6e-1442x1096.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb76ccf0716315cb28ada74e8511397b562731e6e-1442x1096.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren&#x27;t good representations of it? UpTrain automatically adds new evaluations to check how well the module performs:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.uptrain.ai/predefined-evaluations/sub-query/sub-query-completeness" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><strong>Sub Query Completeness</strong></a>: It evaluates whether the sub-questions accurately and comprehensively cover the original query.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries.</li></ul><figure><img alt="" loading="lazy" width="512" height="350" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F44b83d738895c7039da2d5a22816c77bb318981c-1024x700.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F44b83d738895c7039da2d5a22816c77bb318981c-1024x700.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F44b83d738895c7039da2d5a22816c77bb318981c-1024x700.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75"/></figure><figure><img alt="" loading="lazy" width="512" height="271.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6e416477999fd036c611dcc3c3ab16b2a3f2f957-1024x543.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6e416477999fd036c611dcc3c3ab16b2a3f2f957-1024x543.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6e416477999fd036c611dcc3c3ab16b2a3f2f957-1024x543.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75"/></figure><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA"><strong>3. Reranking Evaluations</strong></h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We looked at a way of dealing with low Response Completeness scores. Now, let&#x27;s look at a way of dealing with low Context Relevance scores.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research [<a href="https://arxiv.org/pdf/2307.03172.pdf" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Lost in the Middle: How Language Model Uses Long Contexts</a>] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity.</p><figure><img alt="" loading="lazy" width="707" height="194" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81d01b5aa94e0a19d1472a6199ac4a5f83955f34-1414x388.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81d01b5aa94e0a19d1472a6199ac4a5f83955f34-1414x388.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81d01b5aa94e0a19d1472a6199ac4a5f83955f34-1414x388.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process:</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>a. Same Number of Nodes Before and After Reranking:</strong></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator.</p><figure><img alt="" loading="lazy" width="909" height="694" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa5594a51ff90029842b6057e49df6414fcdf7c82-1818x1388.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa5594a51ff90029842b6057e49df6414fcdf7c82-1818x1388.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa5594a51ff90029842b6057e49df6414fcdf7c82-1818x1388.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/></figure><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><strong>Context Reranking</strong></a>: Checks if the order of reranked nodes is more relevant to the query than the original order.</li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>b. Fewer Number of Nodes After Reranking:</strong></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don&#x27;t lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator.</p><figure><img alt="" loading="lazy" width="910" height="686" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F537f92f50da39fa870e5f4f0df30a0a72dcb44f2-1820x1372.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F537f92f50da39fa870e5f4f0df30a0a72dcb44f2-1820x1372.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F537f92f50da39fa870e5f4f0df30a0a72dcb44f2-1820x1372.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/></figure><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><strong>Context Conciseness</strong></a>: Examines whether the reduced number of nodes still provides all the required information.</li></ul><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q"><strong>Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation</strong></h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let&#x27;s do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this <a href="https://docs.uptrain.ai/getting-started/quickstart" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">quickstart tutorial</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q"><strong>References</strong></h2><ol><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">UpTrain Callback Handler Tutorial</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://github.com/uptrain-ai/uptrain" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">UpTrain GitHub Repository</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Advanced RAG Techniques: an Illustrated Overview</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://arxiv.org/pdf/2307.03172.pdf" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Lost in the Middle: How Language Models Use Long Contexts</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.llamaindex.ai/en/stable/community/integrations/uptrain.html" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">UpTrainCallbackHandler documentation</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://uptrain.ai/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">UpTrain Website</a></li></ol><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/rag-is-dead-long-live-agentic-retrieval">RAG is dead, long live agentic retrieval</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-29</p></div></li><li><div class="CardBlog_card__mm0Zw CardBlog_featuredCard__5FPeD"><div class="CardBlog_grid__5PeSv"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><div class="CardBlog_thumbnailGradient__x5CbY"><p class="Text_text__zPO0D Text_text-size-36__cH7Hj Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/beyond-chatbots-adopting-agentic-document-workflows-for-enterprises">Beyond chatbots: adopting Agentic Document Workflows for enterprises</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu CardBlog_date__E1rJK">2025-04-23</p></div></div></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/building-blocks-of-llm-report-generation-beyond-basic-rag">Building Blocks of LLM Report Generation: Beyond Basic RAG</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-11-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/building-a-serverless-rag-application-with-llamaindex-and-azure-openai">Building a serverless RAG application with LlamaIndex and Azure OpenAI</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-08-27</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-03-19T17:31:54Z","_id":"f0b32928-06e1-4c22-a474-daf578e56409","_rev":"05dtDS0H5iRVsxYMarZyEz","_type":"blogPost","_updatedAt":"2025-05-21T20:40:30Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-03-19T17:33:40Z","_id":"ffcb7db9-c91e-4606-b461-13cb476d18b7","_rev":"eVvFdyqjsRJTP2Pq1ijt1t","_type":"people","_updatedAt":"2024-03-19T17:33:40Z","name":"Uptrain","slug":{"_type":"slug","current":"uptrain"}}],"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png","publishedDate":"2024-03-19","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562-png","_type":"reference"}},"publishedDate":"2025-05-29","slug":"rag-is-dead-long-live-agentic-retrieval","title":"RAG is dead, long live agentic retrieval"},{"featured":true,"image":{"_type":"image","asset":{"_ref":"image-13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631-png","_type":"reference"}},"publishedDate":"2025-04-23","slug":"beyond-chatbots-adopting-agentic-document-workflows-for-enterprises","title":"Beyond chatbots: adopting Agentic Document Workflows for enterprises"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-5033e2512495122c811ac69425cc77a83c7fa00a-3311x1647-png","_type":"reference"}},"publishedDate":"2024-11-05","slug":"building-blocks-of-llm-report-generation-beyond-basic-rag","title":"Building Blocks of LLM Report Generation: Beyond Basic RAG"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-ecd41ae473c595aa2602aa86e7031c2dc79103b2-2978x1800-png","_type":"reference"}},"publishedDate":"2024-08-27","slug":"building-a-serverless-rag-application-with-llamaindex-and-azure-openai","title":"Building a serverless RAG application with LlamaIndex and Azure OpenAI"}],"slug":{"_type":"slug","current":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"b3e76bd9-2198-4a5d-85aa-5f2033b04955","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"evaluation"},"title":"Evaluation"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"893258fa-46ae-4ae2-b1a3-acb12849ab60","_rev":"RDEDF5eNko8cW03GEH0cXj","_type":"blogTag","_updatedAt":"2024-08-21T19:17:20Z","slug":{"_type":"slug","current":"rag"},"title":"RAG"}],"text":[{"_key":"b10f90530ae2","_type":"block","children":[{"_key":"c017245675580","_type":"span","marks":["em"],"text":"This is a guest post from Uptrain."}],"markDefs":[],"style":"normal"},{"_key":"b7b77a15ce15","_type":"block","children":[{"_key":"22dc4da932a8","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"9aff95f24650","_type":"block","children":[{"_key":"db3c4128ccb7","_type":"span","marks":[],"text":"We are excited to announce the recent integration of LlamaIndex with UpTrain - an open-source LLM evaluation framework to evaluate your RAG pipelines and experiment with different configurations. As an increasing number of companies are graduating their LLM prototypes to production-ready systems, robust evaluations provide a systematic framework to make decisions rather than going with the ‘vibes’. By combining LlamaIndex's flexibility and UpTrain's evaluation framework, developers can experiment with different configurations, fine-tuning their LLM-based applications for optimal performance."}],"markDefs":[],"style":"normal"},{"_key":"32d1152ad9a2","_type":"block","children":[{"_key":"884b71427b660","_type":"span","marks":["strong"],"text":"About UpTrain"}],"markDefs":[],"style":"h2"},{"_key":"2901f21f7ba6","_type":"block","children":[{"_key":"69bd6711532e0","_type":"span","marks":["strong"],"text":"UpTrain"},{"_key":"69bd6711532e1","_type":"span","marks":[],"text":" ["},{"_key":"69bd6711532e2","_type":"span","marks":["116347832813"],"text":"github"},{"_key":"69bd6711532e3","_type":"span","marks":[],"text":" ||"},{"_key":"69bd6711532e4","_type":"span","marks":["4a1b1fa6589f"],"text":" website"},{"_key":"69bd6711532e5","_type":"span","marks":[],"text":" ||"},{"_key":"69bd6711532e6","_type":"span","marks":["10f6ec10f5d6"],"text":" docs"},{"_key":"69bd6711532e7","_type":"span","marks":[],"text":"] is an open-source platform to evaluate and improve LLM applications. It provides grades for 20+ preconfigured checks (covering language, code, embedding use cases), performs root cause analyses on instances of failure cases and provides guidance for resolving them."}],"markDefs":[{"_key":"116347832813","_type":"link","href":"https://github.com/uptrain-ai/uptrain"},{"_key":"4a1b1fa6589f","_type":"link","href":"https://uptrain.ai/"},{"_key":"10f6ec10f5d6","_type":"link","href":"https://docs.uptrain.ai/getting-started/introduction"}],"style":"normal"},{"_key":"e9606491bb15","_type":"block","children":[{"_key":"758583b3b4f20","_type":"span","marks":["strong"],"text":"Key Highlights:"}],"markDefs":[],"style":"normal"},{"_key":"026377af2c01","_type":"block","children":[{"_key":"8ce8c30e6d920","_type":"span","marks":["strong"],"text":"Data Security:"},{"_key":"8ce8c30e6d921","_type":"span","marks":[],"text":" As an open-source solution, UpTrain conducts all evaluations and analyses locally, ensuring that your data remains within your secure environment (except for the LLM calls)."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"9007ca3eb9d8","_type":"block","children":[{"_key":"cc964169d68a0","_type":"span","marks":["strong"],"text":"Custom Evaluator LLMs:"},{"_key":"cc964169d68a1","_type":"span","marks":[],"text":" UpTrain allows for "},{"_key":"cc964169d68a2","_type":"span","marks":["48cab2f964f5"],"text":"customisation of your evaluator LLM"},{"_key":"cc964169d68a3","_type":"span","marks":[],"text":", offering options among several endpoints, including OpenAI, Anthropic, Llama, Mistral, or Azure."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"48cab2f964f5","_type":"link","href":"https://github.com/uptrain-ai/uptrain/blob/main/examples/open_source_evaluator_tutorial.ipynb"}],"style":"normal"},{"_key":"6bbedcf95445","_type":"block","children":[{"_key":"3b183fa506950","_type":"span","marks":["strong"],"text":"Insights that help with model improvement: "},{"_key":"3b183fa506951","_type":"span","marks":[],"text":"Beyond mere evaluation, UpTrain performs "},{"_key":"3b183fa506952","_type":"span","marks":["6d5a2fb90ba7"],"text":"root cause analysis"},{"_key":"3b183fa506953","_type":"span","marks":[],"text":" to pinpoint the specific components of your LLM pipeline, that are underperforming, as well as identifying common patterns among failure cases, thereby helping in their resolution."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"6d5a2fb90ba7","_type":"link","href":"https://github.com/uptrain-ai/uptrain/blob/main/examples/root_cause_analysis/rag_with_citation.ipynb"}],"style":"normal"},{"_key":"534cc57ade9a","_type":"block","children":[{"_key":"838142b7de1b0","_type":"span","marks":["strong"],"text":"Diverse Experimentations:"},{"_key":"838142b7de1b1","_type":"span","marks":[],"text":" The platform enables "},{"_key":"838142b7de1b2","_type":"span","marks":["a18146c5c1a6"],"text":"experimentation"},{"_key":"838142b7de1b3","_type":"span","marks":[],"text":" with different prompts, LLM models, RAG modules, embedding models, etc. and helps you find the best fit for your specific use case."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"a18146c5c1a6","_type":"link","href":"https://github.com/uptrain-ai/uptrain/tree/main/examples/experiments"}],"style":"normal"},{"_key":"be8e5af8e5d8","_type":"block","children":[{"_key":"4297563e20190","_type":"span","marks":["strong"],"text":"Compare open-source LLMs:"},{"_key":"4297563e20191","_type":"span","marks":[],"text":" With UpTrain, you can compare your fine-tuned open-source LLMs against proprietary ones (such as GPT-4), helping you to find the most cost-effective model without compromising quality."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"f47d1b41f99c","_type":"block","children":[{"_key":"933092a3a56d0","_type":"span","marks":[],"text":"In the following sections, we will illustrate how you can use UpTrain to evaluate your LlamaIndex pipeline. The evaluations demonstrated here will help you quickly find what’s affecting the quality of your responses, allowing you to take appropriate corrective actions."}],"markDefs":[],"style":"normal"},{"_key":"6dbe0b2466a9","_type":"block","children":[{"_key":"79294e8b32370","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"b757609dd75a","_type":"block","children":[{"_key":"300188ceb6e30","_type":"span","marks":["strong"],"text":"LlamaIndex x UpTrain Callback Handler"}],"markDefs":[],"style":"h2"},{"_key":"8d020692c4bf","_type":"block","children":[{"_key":"e1bfc64ae05e0","_type":"span","marks":[],"text":"We introduce an UpTrain Callback Handler which makes evaluating your existing LlamaIndex Pipeline seamless. By adding just a few lines of code, UpTrain will automatically perform a series of checks - evaluating the quality of generated responses, the quality of contextual data retrieved by the RAG pipeline as well as the performance of all the interim steps."}],"markDefs":[],"style":"normal"},{"_key":"6d41e58a397d","_type":"block","children":[{"_key":"fd153f36ab320","_type":"span","marks":[],"text":"If you wish to skip right ahead to the tutorial, check it out "},{"_key":"fd153f36ab321","_type":"span","marks":["6fc6f9d9b083"],"text":"here."}],"markDefs":[{"_key":"6fc6f9d9b083","_type":"link","href":"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb"}],"style":"normal"},{"_key":"a8abe5525c4c","_type":"image","asset":{"_ref":"image-99c0abd796d383476d12ae5d7f9f0057f2a4f559-1024x151-png","_type":"reference"}},{"_key":"9626b8b874e4","_type":"block","children":[{"_key":"41e0e86bb70d0","_type":"span","marks":["strong"],"text":"Evals across the board: From Vanilla to Advanced RAG"}],"markDefs":[],"style":"h2"},{"_key":"1c6f24833a66","_type":"block","children":[{"_key":"ec6b7852631d0","_type":"span","marks":[],"text":"Vanilla RAG involves a few steps. You need to embed the documents and store them in a vector database. When the user asks questions, the framework embeds them and uses similarity search to find the most relevant documents. The content of these retrieved documents, and the original query, are then passed on to the LLM to generate the final response."}],"markDefs":[],"style":"normal"},{"_key":"c91d46880ffd","_type":"block","children":[{"_key":"63a9630aeda60","_type":"span","marks":[],"text":"While the above is a great starting point, there have been a lot of improvements to achieve better results. Advanced RAG applications have many additional steps that improve the quality of the retrieved documents, which in turn improve the quality of your responses."}],"markDefs":[],"style":"normal"},{"_key":"01b94aaa8158","_type":"block","children":[{"_key":"0665a24d5a2a0","_type":"span","marks":[],"text":"But as Uncle Ben famously said to Peter Parker in the GenAI universe:"}],"markDefs":[],"style":"normal"},{"_key":"866520335a17","_type":"block","children":[{"_key":"be0eb63d86310","_type":"span","marks":["em"],"text":"“With increased complexity comes more points of failure.”."}],"markDefs":[],"style":"normal"},{"_key":"f4709872acae","_type":"block","children":[{"_key":"42e0a5e137540","_type":"span","marks":[],"text":"Most of the LLM evaluation tools only evaluate the final context-response pair and fail to take into consideration the intermediary steps of an advanced RAG pipeline. Let’s look at all the evaluations provided by UpTrain."}],"markDefs":[],"style":"normal"},{"_key":"34a4bd474464","_type":"block","children":[{"_key":"74fd87be460f0","_type":"span","marks":["strong"],"text":"Addressing Points of Failure in RAG Pipelines"}],"markDefs":[],"style":"h2"},{"_key":"df09f7370597","_type":"block","children":[{"_key":"6dd63ce8c8910","_type":"span","marks":["strong"],"text":"1. RAG Query Engine Evaluation"}],"markDefs":[],"style":"h3"},{"_key":"a15fe85e6670","_type":"block","children":[{"_key":"58501369be470","_type":"span","marks":[],"text":"Let's first take a Vanilla RAG Pipeline and see how you can test its performance. UpTrain provides three operators curated for testing both the retrieved context as well as the LLM's response."}],"markDefs":[],"style":"normal"},{"_key":"21010e65a9e8","_type":"image","asset":{"_ref":"image-c83a698ff896c10a72106117c0ef4eeeeeeba2f5-1999x793-png","_type":"reference"}},{"_key":"fa09b424489f","_type":"block","children":[{"_key":"6138e2a5ddca0","_type":"span","marks":["d982d2483df3","strong"],"text":"Context Relevance"},{"_key":"6138e2a5ddca1","_type":"span","marks":[],"text":": However informative the documents retrieved might be, if they are not relevant to your query, you will likely not get a response that answers your query. The Context Relevance operator determines if the documents fetched from the vector store contain information that can be used to answer your query."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"d982d2483df3","_type":"link","href":"https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance"}],"style":"normal"},{"_key":"d2572581b236","_type":"block","children":[{"_key":"ffe77e56df440","_type":"span","marks":["379fdcca7b04","strong"],"text":"Factual Accuracy"},{"_key":"ffe77e56df441","_type":"span","marks":[],"text":": Now that we have checked if the context contains information to answer our query, we will check if the response provided by the LLM is backed by the information present in the context. The Factual Accuracy operator assesses if the LLM is hallucinating or providing information that is not present in the context."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"379fdcca7b04","_type":"link","href":"https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy"}],"style":"normal"},{"_key":"b2889f8b9ea8","_type":"block","children":[{"_key":"5922722eb44c0","_type":"span","marks":["9e72f1580430","strong"],"text":"Response Completeness"},{"_key":"5922722eb44c1","_type":"span","marks":[],"text":": Not all queries are straightforward. Some of them have multiple parts to them. A good response should be able to answer all the aspects of the query. The Response Completeness operator checks if the response contains all the information requested by the query."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"9e72f1580430","_type":"link","href":"https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness"}],"style":"normal"},{"_key":"05dc1c0af58b","_type":"image","asset":{"_ref":"image-3ce17ffc09640913e3618f407c94abaaade7ad3e-1024x534-png","_type":"reference"}},{"_key":"1ad348bba4f8","_type":"block","children":[{"_key":"74ad4902f9380","_type":"span","marks":["strong"],"text":"2. Sub-Question Query Engine Evaluation"}],"markDefs":[],"style":"h3"},{"_key":"d40833019ce1","_type":"block","children":[{"_key":"b7e54332b1910","_type":"span","marks":[],"text":"Let's say you tried out a Vanilla RAG pipeline and got consistently low Response Completeness scores. This means that the LLM is not answering all aspects of your query. One of the ways to solve this is by splitting the query into multiple smaller sub-queries that the LLM can answer more easily. To do this, you can use the SubQuestionQueryGeneration operator provided by LlamaIndex. This operator decomposes a question into sub-questions, generating responses for each using an RAG query engine."}],"markDefs":[],"style":"normal"},{"_key":"83be840a453e","_type":"image","asset":{"_ref":"image-b76ccf0716315cb28ada74e8511397b562731e6e-1442x1096-png","_type":"reference"}},{"_key":"e1a129e5d2b1","_type":"block","children":[{"_key":"30d840a6b53d0","_type":"span","marks":[],"text":"If you include this SubQuery module in your RAG pipeline, it introduces another point of failure, e.g. what if the sub-questions that we split our original question aren't good representations of it? UpTrain automatically adds new evaluations to check how well the module performs:"}],"markDefs":[],"style":"normal"},{"_key":"2de30ee42d70","_type":"block","children":[{"_key":"410fbf78cce30","_type":"span","marks":["bec7f9d04714","strong"],"text":"Sub Query Completeness"},{"_key":"410fbf78cce31","_type":"span","marks":[],"text":": It evaluates whether the sub-questions accurately and comprehensively cover the original query."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"bec7f9d04714","_type":"link","href":"https://docs.uptrain.ai/predefined-evaluations/sub-query/sub-query-completeness"}],"style":"normal"},{"_key":"8281f1ae1ce3","_type":"block","children":[{"_key":"d5737ce12b400","_type":"span","marks":[],"text":"Context Relevance, Factual Accuracy and Response Completeness for each of the sub-queries."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"6fd20baaa3c2","_type":"image","asset":{"_ref":"image-44b83d738895c7039da2d5a22816c77bb318981c-1024x700-png","_type":"reference"}},{"_key":"1dfcc4171785","_type":"image","asset":{"_ref":"image-6e416477999fd036c611dcc3c3ab16b2a3f2f957-1024x543-png","_type":"reference"}},{"_key":"d342fd30387b","_type":"block","children":[{"_key":"e971ff73ad06","_type":"span","marks":["strong"],"text":"3. Reranking Evaluations"}],"markDefs":[],"style":"h3"},{"_key":"6006841a5cfd","_type":"block","children":[{"_key":"f91f0d389a0d0","_type":"span","marks":[],"text":"We looked at a way of dealing with low Response Completeness scores. Now, let's look at a way of dealing with low Context Relevance scores."}],"markDefs":[],"style":"normal"},{"_key":"fa20d74149f4","_type":"block","children":[{"_key":"6170ca1632a30","_type":"span","marks":[],"text":"RAG pipelines retrieve documents based on semantic similarity. These documents are ordered based on how similar they are to the query asked. However, recent research ["},{"_key":"6170ca1632a31","_type":"span","marks":["cfffb7c25fd5"],"text":"Lost in the Middle: How Language Model Uses Long Contexts"},{"_key":"6170ca1632a32","_type":"span","marks":[],"text":"] has shown that the LLMs are sensitive to the placement of the most critical information within the retrieved context. To solve this, you might want to add a reranking block."}],"markDefs":[{"_key":"cfffb7c25fd5","_type":"link","href":"https://arxiv.org/pdf/2307.03172.pdf"}],"style":"normal"},{"_key":"ca69e90875b0","_type":"block","children":[{"_key":"0ef18d9b8eab0","_type":"span","marks":[],"text":"Reranking involves using a semantic search model (specially tuned for the reranking task) that breaks down the retrieved context into smaller chunks, finds the semantic similarity between them and the query and rewrites the context by ranking them in order of their similarity."}],"markDefs":[],"style":"normal"},{"_key":"475c7856dac3","_type":"image","asset":{"_ref":"image-81d01b5aa94e0a19d1472a6199ac4a5f83955f34-1414x388-png","_type":"reference"}},{"_key":"cc0e82cf0e70","_type":"block","children":[{"_key":"1fb45f23a66f0","_type":"span","marks":[],"text":"We observed that when using the reranking operators in LlamaIndex, two scenarios can occur. These scenarios differ based on the number of nodes before and after the reranking process:"}],"markDefs":[],"style":"normal"},{"_key":"575012a5b5f1","_type":"block","children":[{"_key":"0874fd60cd140","_type":"span","marks":["strong"],"text":"a. Same Number of Nodes Before and After Reranking:"}],"markDefs":[],"style":"normal"},{"_key":"3b522813548c","_type":"block","children":[{"_key":"ae0b22e1b0f00","_type":"span","marks":[],"text":"If the number of nodes after the reranking remains the same, then we need to check if the new order is such that nodes higher in rank are more relevant to the query as compared to the older order. To check for this, UpTrain provides a Context Reranking operator."}],"markDefs":[],"style":"normal"},{"_key":"76b8b21d97f0","_type":"image","asset":{"_ref":"image-a5594a51ff90029842b6057e49df6414fcdf7c82-1818x1388-png","_type":"reference"}},{"_key":"1eafa42a8c81","_type":"block","children":[{"_key":"646edeff211b0","_type":"span","marks":["c28db23d869b","strong"],"text":"Context Reranking"},{"_key":"646edeff211b1","_type":"span","marks":[],"text":": Checks if the order of reranked nodes is more relevant to the query than the original order."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"c28db23d869b","_type":"link","href":"https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-reranking"}],"style":"normal"},{"_key":"966b33ec6cc5","_type":"block","children":[{"_key":"e171f4a3ee310","_type":"span","marks":["strong"],"text":"b. Fewer Number of Nodes After Reranking:"}],"markDefs":[],"style":"normal"},{"_key":"304c83bdd540","_type":"block","children":[{"_key":"c3506d1ea3dc0","_type":"span","marks":[],"text":"Reducing the number of nodes can help the LLM give better responses. This is because the LLMs process smaller context lengths better. However, we need to make sure that we don't lose information that would have been useful in answering the question. Therefore, during the process of reranking, if the number of nodes in the output is reduced, we provide a Context Conciseness operator."}],"markDefs":[],"style":"normal"},{"_key":"4c69982529cc","_type":"image","asset":{"_ref":"image-537f92f50da39fa870e5f4f0df30a0a72dcb44f2-1820x1372-png","_type":"reference"}},{"_key":"96424493c9d4","_type":"block","children":[{"_key":"17af3a52a66e0","_type":"span","marks":["abc5e84dc613","strong"],"text":"Context Conciseness"},{"_key":"17af3a52a66e1","_type":"span","marks":[],"text":": Examines whether the reduced number of nodes still provides all the required information."}],"level":1,"listItem":"bullet","markDefs":[{"_key":"abc5e84dc613","_type":"link","href":"https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-conciseness"}],"style":"normal"},{"_key":"f609f66ef16b","_type":"block","children":[{"_key":"104cec28b2280","_type":"span","marks":["strong"],"text":"Key Takeaways: Enhancing RAG Pipelines Through Advanced Techniques and Evaluation"}],"markDefs":[],"style":"h2"},{"_key":"03b8b68a36fd","_type":"block","children":[{"_key":"134222734f230","_type":"span","marks":[],"text":"Let's do a quick recap here. We started off with a Vanilla RAG pipeline and evaluated the quality of the generated response and retrieved context. Then, we moved to advanced RAG concepts like the SubQuery technique (used to combat cases with low Response Completeness scores) and the Reranking technique (used to improve the quality of retrieved context) and looked at advanced evaluations to quantify their performance."}],"markDefs":[],"style":"normal"},{"_key":"05fe8111a064","_type":"block","children":[{"_key":"908ef8cdb19d0","_type":"span","marks":[],"text":"This essentially provides a framework to systematically test the performance of different modules as well as evaluate if they actually lead to better quality responses by making data-driven decisions."}],"markDefs":[],"style":"normal"},{"_key":"5841b88fb484","_type":"block","children":[{"_key":"3ad298b9b1ab0","_type":"span","marks":[],"text":"Much of the success in the field of Artificial intelligence can be attributed to experimentation with different architectures, hyperparameters, datasets, etc., and our integration with UpTrain allows you to import those best practices while building RAG pipelines. Get started with uptrain with this "},{"_key":"3ad298b9b1ab1","_type":"span","marks":["44e98969cb0b"],"text":"quickstart tutorial"},{"_key":"3ad298b9b1ab2","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"44e98969cb0b","_type":"link","href":"https://docs.uptrain.ai/getting-started/quickstart"}],"style":"normal"},{"_key":"082665cb3af8","_type":"block","children":[{"_key":"f83f95b753800","_type":"span","marks":["strong"],"text":"References"}],"markDefs":[],"style":"h2"},{"_key":"d1510c0ad96b","_type":"block","children":[{"_key":"af3880af2f9a0","_type":"span","marks":["bcc112f9aef0"],"text":"UpTrain Callback Handler Tutorial"}],"level":1,"listItem":"number","markDefs":[{"_key":"bcc112f9aef0","_type":"link","href":"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/callbacks/UpTrainCallback.ipynb"}],"style":"normal"},{"_key":"51d96f3906b3","_type":"block","children":[{"_key":"8b23b9eeb9d70","_type":"span","marks":["45189444712c"],"text":"UpTrain GitHub Repository"}],"level":1,"listItem":"number","markDefs":[{"_key":"45189444712c","_type":"link","href":"https://github.com/uptrain-ai/uptrain"}],"style":"normal"},{"_key":"6cebd81a45d7","_type":"block","children":[{"_key":"96684e8b0cb20","_type":"span","marks":["ac8589576e78"],"text":"Advanced RAG Techniques: an Illustrated Overview"}],"level":1,"listItem":"number","markDefs":[{"_key":"ac8589576e78","_type":"link","href":"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6"}],"style":"normal"},{"_key":"715c334c0ad8","_type":"block","children":[{"_key":"b82b2f0767050","_type":"span","marks":["cbc5aed1fb06"],"text":"Lost in the Middle: How Language Models Use Long Contexts"}],"level":1,"listItem":"number","markDefs":[{"_key":"cbc5aed1fb06","_type":"link","href":"https://arxiv.org/pdf/2307.03172.pdf"}],"style":"normal"},{"_key":"be299e0a2c9b","_type":"block","children":[{"_key":"035c454ce9fc0","_type":"span","marks":["268b1da75d27"],"text":"UpTrainCallbackHandler documentation"}],"level":1,"listItem":"number","markDefs":[{"_key":"268b1da75d27","_type":"link","href":"https://docs.llamaindex.ai/en/stable/community/integrations/uptrain.html"}],"style":"normal"},{"_key":"ce2dfd668d81","_type":"block","children":[{"_key":"463af695453a0","_type":"span","marks":["195f1641bd53"],"text":"UpTrain Website"}],"level":1,"listItem":"number","markDefs":[{"_key":"195f1641bd53","_type":"link","href":"https://uptrain.ai/"}],"style":"normal"},{"_key":"7d4dfb19e604","_type":"block","children":[{"_key":"775a631e9e350","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"}],"title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"},"publishedDate":"Invalid Date"},"params":{"slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>