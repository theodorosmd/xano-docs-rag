<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Improved Long &amp; Short-Term Memory for LlamaIndex Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Improved Long &amp; Short-Term Memory for LlamaIndex Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Improved Long &amp; Short-Term Memory for LlamaIndex Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Improved Long &amp; Short-Term Memory for LlamaIndex Agents — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="189.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/tuana-celik">Tuana Çelik</a> <!-- -->•<!-- --> <!-- -->2025-05-13</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Improved Long &amp; Short-Term Memory for LlamaIndex Agents</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/agents"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Agents</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><blockquote class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>co-authored with Logan Markewich</em></blockquote><p class="Text_text__zPO0D Text_text-size-16__PkjFu">For any agentic application that needs to retain basic information about past conversations, about its users, their past interactions and so on, memory is unsurprisingly a core component. So, as part of our latest round of improvements to LlamaIndex, we’ve introduced a <a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/#memory" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">new and improved <code class="SanityPortableText_inlineCode__cI85z">Memory</code> component</a>.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this article, we will walk through some of the core features of the new LlamaIndex memory component, and how you can start using it in your own agentic applications. First, we’ll look into the most basic implementation of memory, where we simply store chat message history. Then, we’ll also look into more advanced implementations with our latest additions of long-term memory blocks.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">When to Use Memory</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Not all AI applications need a memory implementation. A lot of agentic applications we see today don’t necessarily rely on chat history, or persistent information about their users. For example, our very own <a href="/llamaextract" class="SanityPortableText_link__QA4Ze">LlamaExtract</a> is an agentic document extraction application, that uses LLMs to extract structured information from complex files. It doesn’t rely on retaining any sort of memory while doing so.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">But, if the essence of an application relies on persistent information that you’d rather not repeat (someone’s name and age for example), and/or any form of conversation flow (so virtually any application with a chat interface), memory quickly becomes quite a crucial component.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Think of this simple (and annoying) chat flow where no form of memory has been implemented:</p><figure><img alt="" loading="lazy" width="452" height="426" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa5ede0e6c636ad4ff324c7e143f622b97f17fd53-904x852.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa5ede0e6c636ad4ff324c7e143f622b97f17fd53-904x852.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa5ede0e6c636ad4ff324c7e143f622b97f17fd53-904x852.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1080&amp;q=75"/><figcaption>Conversation with no memory</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Notice how by the time I get to my second question, the agent is not able to provide me with any answers. The simple reason is that it has no context as to what ‘there’ is in the sentence ‘Can you tell me what the weather is like there?’.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This can be solved with a super simple step where we store chat message history, and send that to the LLM as context. That’s exactly how the most basic form of the LlamaIndex Memory starts:</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">The Basic Usage of Memory</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In the most simple form, the LlamaIndex <code class="SanityPortableText_inlineCode__cI85z">Memory</code> will store any number of chat messages that fit into a given token limit, into a SQL database, which defaults to an in-memory SQLite database. In short, this memory will be able to store chat messages, up to a limit. Once that limit is reached (depending on how the memory is configured), the oldest messages in chat history will either get discarded, or flushed to long-term memory.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">So, in its most basic form, the LlamaIndex Memory component allows you to build agentic workflows that are able to retain past conversations. You can store chat history into this memory either by initializing an agent with memory:</p><pre><code><span class="hljs-keyword">from</span> llama_index.core.agent.workflow <span class="hljs-keyword">import</span> FunctionAgent
<span class="hljs-keyword">from</span> llama_index.core.memory <span class="hljs-keyword">import</span> Memory

memory = Memory.from_defaults(session_id=<span class="hljs-string">&quot;my_session&quot;</span>, token_limit=<span class="hljs-number">40000</span>)

agent = FunctionAgent(llm=llm, tools=tools)

response = <span class="hljs-keyword">await</span> agent.run(<span class="hljs-string">&quot;&lt;question that invokes tool&gt;&quot;</span>, memory=memory)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Or, by using <code class="SanityPortableText_inlineCode__cI85z">memory.put_messages()</code> to add specific chat messages into memory:</p><pre><code>memory = Memory.from_defaults(session_id=<span class="hljs-string">&quot;my_session&quot;</span>,
                              token_limit=<span class="hljs-number">40000</span>)
memory.put_messages(
    [
        ChatMessage(role=<span class="hljs-string">&quot;user&quot;</span>, content=<span class="hljs-string">&quot;Hello, world!&quot;</span>),
        ChatMessage(role=<span class="hljs-string">&quot;assistant&quot;</span>, content=<span class="hljs-string">&quot;Hello, world to you too!&quot;</span>),
    ]
)
chat_history = memory.get()</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><br/></p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Long-Term Memory Blocks</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">For some use-cases, the simple chat history implementation above might be enough. But for others, it makes sense to implement something a bit more advanced. We introduce 3 new “memory blocks” that act as long-term memory:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Static memory block: which keeps track of static, non-changing information</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Fact extraction memory block: that populates a list of facts extracted from the conversation</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Vector memory block: which allows us to make use of classic embedding generation vector search as a way to both store chat history and fetch relevant parts of old conversations.</li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu">A memory component can be initialized with any number of these long-term memory blocks. Then, every time we reach the token limit for short-term memory, it will write the relevant information to these long-term memory blocks. To add long-term memory blocks we initialize memory with <code class="SanityPortableText_inlineCode__cI85z">memory_blocks</code>:</p><pre><code>blocks = [&lt;<span class="hljs-built_in">list</span> of long-term memory blocks&gt;]

memory = Memory.from_defaults(
    session_id=<span class="hljs-string">&quot;my_session&quot;</span>,
    memory_blocks=blocks,
    insert_method=<span class="hljs-string">&quot;system&quot;</span>)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let’s see in more detail how we can initialize each block.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Static Information as long-term memory</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Imagine information that might be relevant to your agentic application that is less likely to change over time. For example, your name, where you live, where you work etc. The <code class="SanityPortableText_inlineCode__cI85z">StaticMemoryBlock</code> allows you to provide this information as added context for your agent:</p><pre><code><span class="hljs-keyword">from</span> llama_index.core.memory <span class="hljs-keyword">import</span> StaticMemoryBlock

static_info_block = StaticMemoryBlock(
    name=<span class="hljs-string">&quot;core_info&quot;</span>, 
	static_content=<span class="hljs-string">&quot;My name is Tuana, and I live in Amsterdam. I work at LlamaIndex.&quot;</span>,
	priority=<span class="hljs-number">0</span>
)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">You may think that the information here are facts, so are more appropriate for the fact extraction block. The main difference is that the fact extraction block is in itself an LLM powered extraction system, that is designed to extract facts as the conversation is happening.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Facts as long-term memory</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">My personal favorite. The <code class="SanityPortableText_inlineCode__cI85z">FactExtractionMemoryBlock</code> is a unique long-term memory block that is initialized with a default prompt (which you can override), that instructs an LLM to extract a list of facts from ongoing conversations. To initialize this block:</p><pre><code><span class="hljs-keyword">from</span> llama_index.core.memory <span class="hljs-keyword">import</span> FactExtractionMemoryBlock
<span class="hljs-keyword">from</span> llama_index.llms.openai <span class="hljs-keyword">import</span> OpenAI

llm = OpenAI(model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>)
 
facts_block = FactExtractionMemoryBlock(
    name=<span class="hljs-string">&quot;extracted_info&quot;</span>,
	llm=llm,
    max_facts=<span class="hljs-number">50</span>,
    priority=<span class="hljs-number">1</span>
)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><br/>Then, for example, after a long conversation with an agent where I provide some information about myself, we may get the following:</p><pre><code>memory.memory_blocks[<span class="hljs-number">1</span>].facts

<span class="hljs-comment"># [&#x27;User is 29 years old.&#x27;, &#x27;User has a sister.&#x27;]</span></code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Vector search as long-term memory</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Finally, we also have the <code class="SanityPortableText_inlineCode__cI85z">VectorMemoryBlock</code> which has to be initialized with <a href="https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">a vector store</a> like Weaviate, Qdrant or similar. The idea here is simple: once we reach the token limit for short-term memory, the memory component will write all the chat messages that have occurred to the vector store we initialized this block with. Once we do that, any ongoing conversations, where appropriate, can fetch relevant conversations from history and use that as context before replying to the user.</p><pre><code><span class="hljs-keyword">from</span> llama_index.core.memory <span class="hljs-keyword">import</span> VectorMemoryBlock

vector_block = VectorMemoryBlock(
    name=<span class="hljs-string">&quot;vector_memory&quot;</span>,
    <span class="hljs-comment"># required: pass in a vector store like qdrant, chroma, weaviate, milvus, etc.</span>
    vector_store=vector_store,
    priority=<span class="hljs-number">2</span>,
    embed_model=embed_model,
    similarity_top_k=<span class="hljs-number">2</span>,
)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><br/></p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Customizing Memory</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">On top of all of these, we are also making it possible for you to introduce your own memory blocks by extending the <code class="SanityPortableText_inlineCode__cI85z">BaseMemoryBlock</code> class. For example, below we define a memory block that counts the mention of a given name in the conversation:</p><pre><code><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Any</span>
<span class="hljs-keyword">from</span> llama_index.core.llms <span class="hljs-keyword">import</span> ChatMessage
<span class="hljs-keyword">from</span> llama_index.core.memory.memory <span class="hljs-keyword">import</span> BaseMemoryBlock

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MentionCounter</span>(BaseMemoryBlock[<span class="hljs-built_in">str</span>]):
    <span class="hljs-string">&quot;&quot;&quot;
    A memory block that counts the number of times a user mentions a specific name.
    &quot;&quot;&quot;</span>
    mention_name: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;Logan&quot;</span>
    mention_count: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_aget</span>(<span class="hljs-params">self, messages: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[ChatMessage]] = <span class="hljs-literal">None</span>, **block_kwargs: <span class="hljs-type">Any</span></span>) -&gt; <span class="hljs-built_in">str</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;Logan was mentioned <span class="hljs-subst">{self.mention_count}</span> times.&quot;</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_aput</span>(<span class="hljs-params">self, messages: <span class="hljs-type">List</span>[ChatMessage]</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> messages:
            <span class="hljs-keyword">if</span> self.mention_name <span class="hljs-keyword">in</span> message.content:
                self.mention_count += <span class="hljs-number">1</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">atruncate</span>(<span class="hljs-params">self, content: <span class="hljs-built_in">str</span>, tokens_to_truncate: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>]:
        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span></code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><br/></p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Future Improvements</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We would love for you to try out these latest improvements to the memory component for agents. If you do, please let us know how you got on in our community Discord server.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">But, that being said, we see a few improvements coming for this component. For example, the current implementation only allows you to make use of memory as part of the backend of an agent. We’d love to provide the option of using memory as one of the suite of tools for a tool calling agent.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Additionally, we currently only support SQL databases (<a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/#remote-memory" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">including remote databases which you can configure</a>). But, we would also like to add support for NoSQL databases like MongoDB, Redis and so on.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Finally, we can imagine plenty of scenarios where the <code class="SanityPortableText_inlineCode__cI85z">FactExtractionMemoryBlock</code> might benefit from structured outputs. This would allow us to initialize the block with a predefined set of fields (facts) that the agent will fill in along the way.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">To get started and learn more about how to build agents with both short-term and long-term memory:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Visit the Memory documentation</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.llamaindex.ai/en/latest/api_reference/memory/memory/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">See the API docs</a></li></ul></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/introducing-the-spreadsheet-agent-in-private-preview">Introducing the Spreadsheet Agent, in private preview</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-06-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/rag-is-dead-long-live-agentic-retrieval">RAG is dead, long live agentic retrieval</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-29</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/bending-without-breaking-optimal-design-patterns-for-effective-agents">Bending without breaking: optimal design patterns for effective agents</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-04-25</p></div></li><li><div class="CardBlog_card__mm0Zw CardBlog_featuredCard__5FPeD"><div class="CardBlog_grid__5PeSv"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><div class="CardBlog_thumbnailGradient__x5CbY"><p class="Text_text__zPO0D Text_text-size-36__cH7Hj Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/beyond-chatbots-adopting-agentic-document-workflows-for-enterprises">Beyond chatbots: adopting Agentic Document Workflows for enterprises</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu CardBlog_date__E1rJK">2025-04-23</p></div></div></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2025-05-13T21:29:30Z","_id":"ba7d3c4f-07fe-4f19-8a2a-a7fb6daa17eb","_rev":"TLgH6AcXrxoqw75SBDhlGa","_type":"blogPost","_updatedAt":"2025-05-21T20:39:33Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2025-05-07T12:18:39Z","_id":"ba523b73-6ac5-4c88-8a84-7f0d7385d95b","_rev":"43Uccs5lLp9XEh0FNAyS56","_type":"people","_updatedAt":"2025-05-07T12:21:25Z","image":{"_type":"image","asset":{"_ref":"image-3b849830b84583f97bcc1d8f4bf9db0c7bfaf6a4-416x416-jpg","_type":"reference"}},"name":"Tuana Çelik","position":"Developer Relations Engineer","slug":{"_type":"slug","current":"tuana-celik"}}],"featured":false,"image":{"_type":"image","asset":{"_ref":"image-add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png","publishedDate":"2025-05-13","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-89380b5946452462d6c4a91f8109f705dc044c82-1080x1080-png","_type":"reference"}},"publishedDate":"2025-06-05","slug":"introducing-the-spreadsheet-agent-in-private-preview","title":"Introducing the Spreadsheet Agent, in private preview"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562-png","_type":"reference"}},"publishedDate":"2025-05-29","slug":"rag-is-dead-long-live-agentic-retrieval","title":"RAG is dead, long live agentic retrieval"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-dde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536-png","_type":"reference"}},"publishedDate":"2025-04-25","slug":"bending-without-breaking-optimal-design-patterns-for-effective-agents","title":"Bending without breaking: optimal design patterns for effective agents"},{"featured":true,"image":{"_type":"image","asset":{"_ref":"image-13ef1e27c4ec6c9a72d2ce1fae36f5acac0062ba-1263x631-png","_type":"reference"}},"publishedDate":"2025-04-23","slug":"beyond-chatbots-adopting-agentic-document-workflows-for-enterprises","title":"Beyond chatbots: adopting Agentic Document Workflows for enterprises"}],"slug":{"_type":"slug","current":"improved-long-and-short-term-memory-for-llamaindex-agents"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"248fed26-a405-4fa0-ba99-8f1af0df185c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"agents"},"title":"Agents"}],"text":[{"_key":"049a67aa8541","_type":"block","children":[{"_key":"049ca6ea8126","_type":"span","marks":["em"],"text":"co-authored with Logan Markewich"}],"markDefs":[],"style":"blockquote"},{"_key":"a5b3bf9f7245","_type":"block","children":[{"_key":"33d65271871a0","_type":"span","marks":[],"text":"For any agentic application that needs to retain basic information about past conversations, about its users, their past interactions and so on, memory is unsurprisingly a core component. So, as part of our latest round of improvements to LlamaIndex, we’ve introduced a "},{"_key":"33d65271871a1","_type":"span","marks":["05e880b9e240"],"text":"new and improved "},{"_key":"33d65271871a2","_type":"span","marks":["05e880b9e240","code"],"text":"Memory"},{"_key":"33d65271871a3","_type":"span","marks":["05e880b9e240"],"text":" component"},{"_key":"33d65271871a4","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"05e880b9e240","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/#memory"}],"style":"normal"},{"_key":"456085951c08","_type":"block","children":[{"_key":"96aeb8ddad240","_type":"span","marks":[],"text":"In this article, we will walk through some of the core features of the new LlamaIndex memory component, and how you can start using it in your own agentic applications. First, we’ll look into the most basic implementation of memory, where we simply store chat message history. Then, we’ll also look into more advanced implementations with our latest additions of long-term memory blocks."}],"markDefs":[],"style":"normal"},{"_key":"11f516cb20b8","_type":"block","children":[{"_key":"283fddd543fa0","_type":"span","marks":[],"text":"When to Use Memory"}],"markDefs":[],"style":"h2"},{"_key":"a4e681fe15a2","_type":"block","children":[{"_key":"13ffee5e85b70","_type":"span","marks":[],"text":"Not all AI applications need a memory implementation. A lot of agentic applications we see today don’t necessarily rely on chat history, or persistent information about their users. For example, our very own "},{"_key":"13ffee5e85b71","_type":"span","marks":["2adeb294750b"],"text":"LlamaExtract"},{"_key":"13ffee5e85b72","_type":"span","marks":[],"text":" is an agentic document extraction application, that uses LLMs to extract structured information from complex files. It doesn’t rely on retaining any sort of memory while doing so."}],"markDefs":[{"_key":"2adeb294750b","_type":"link","href":"/llamaextract"}],"style":"normal"},{"_key":"9268d25b6646","_type":"block","children":[{"_key":"bef935fa99410","_type":"span","marks":[],"text":"But, if the essence of an application relies on persistent information that you’d rather not repeat (someone’s name and age for example), and/or any form of conversation flow (so virtually any application with a chat interface), memory quickly becomes quite a crucial component."}],"markDefs":[],"style":"normal"},{"_key":"decae85008c7","_type":"block","children":[{"_key":"be1c6964b1950","_type":"span","marks":[],"text":"Think of this simple (and annoying) chat flow where no form of memory has been implemented:"}],"markDefs":[],"style":"normal"},{"_key":"869dd24f6f86","_type":"image","asset":{"_ref":"image-a5ede0e6c636ad4ff324c7e143f622b97f17fd53-904x852-png","_type":"reference"},"caption":"Conversation with no memory"},{"_key":"12ff7ca15acc","_type":"block","children":[{"_key":"d92f910062c50","_type":"span","marks":[],"text":"Notice how by the time I get to my second question, the agent is not able to provide me with any answers. The simple reason is that it has no context as to what ‘there’ is in the sentence ‘Can you tell me what the weather is like there?’."}],"markDefs":[],"style":"normal"},{"_key":"d03082c65777","_type":"block","children":[{"_key":"8649942bb3600","_type":"span","marks":[],"text":"This can be solved with a super simple step where we store chat message history, and send that to the LLM as context. That’s exactly how the most basic form of the LlamaIndex Memory starts:"}],"markDefs":[],"style":"normal"},{"_key":"84c32d60efbd","_type":"block","children":[{"_key":"bfcc64efc0170","_type":"span","marks":[],"text":"The Basic Usage of Memory"}],"markDefs":[],"style":"h2"},{"_key":"2f6a26446372","_type":"block","children":[{"_key":"7e73f1f205eb0","_type":"span","marks":[],"text":"In the most simple form, the LlamaIndex "},{"_key":"7e73f1f205eb1","_type":"span","marks":["code"],"text":"Memory"},{"_key":"7e73f1f205eb2","_type":"span","marks":[],"text":" will store any number of chat messages that fit into a given token limit, into a SQL database, which defaults to an in-memory SQLite database. In short, this memory will be able to store chat messages, up to a limit. Once that limit is reached (depending on how the memory is configured), the oldest messages in chat history will either get discarded, or flushed to long-term memory."}],"markDefs":[],"style":"normal"},{"_key":"eb781eedd78d","_type":"block","children":[{"_key":"cb4317b0d3c90","_type":"span","marks":[],"text":"So, in its most basic form, the LlamaIndex Memory component allows you to build agentic workflows that are able to retain past conversations. You can store chat history into this memory either by initializing an agent with memory:"}],"markDefs":[],"style":"normal"},{"_key":"68d3d1fadf83","_type":"codeBlock","code":"from llama_index.core.agent.workflow import FunctionAgent\nfrom llama_index.core.memory import Memory\n\nmemory = Memory.from_defaults(session_id=\"my_session\", token_limit=40000)\n\nagent = FunctionAgent(llm=llm, tools=tools)\n\nresponse = await agent.run(\"\u003cquestion that invokes tool\u003e\", memory=memory)","language":"python"},{"_key":"7547b3e7d342","_type":"block","children":[{"_key":"ff51b9dd42d60","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"f0cdf670b897","_type":"block","children":[{"_key":"b6d2dadb4e550","_type":"span","marks":[],"text":"Or, by using "},{"_key":"b6d2dadb4e551","_type":"span","marks":["code"],"text":"memory.put_messages()"},{"_key":"b6d2dadb4e552","_type":"span","marks":[],"text":" to add specific chat messages into memory:"}],"markDefs":[],"style":"normal"},{"_key":"be3a002e5408","_type":"codeBlock","code":"memory = Memory.from_defaults(session_id=\"my_session\",\n                              token_limit=40000)\nmemory.put_messages(\n    [\n        ChatMessage(role=\"user\", content=\"Hello, world!\"),\n        ChatMessage(role=\"assistant\", content=\"Hello, world to you too!\"),\n    ]\n)\nchat_history = memory.get()","language":"python"},{"_key":"34e161518ba9","_type":"block","children":[{"_key":"4dcd457c6e500","_type":"span","marks":[],"text":"\n"}],"markDefs":[],"style":"normal"},{"_key":"524f06df5529","_type":"block","children":[{"_key":"aabae9bd1dce0","_type":"span","marks":[],"text":"Long-Term Memory Blocks"}],"markDefs":[],"style":"h2"},{"_key":"99578ec08279","_type":"block","children":[{"_key":"e9c38eadee1a0","_type":"span","marks":[],"text":"For some use-cases, the simple chat history implementation above might be enough. But for others, it makes sense to implement something a bit more advanced. We introduce 3 new “memory blocks” that act as long-term memory:"}],"markDefs":[],"style":"normal"},{"_key":"be02cf4299af","_type":"block","children":[{"_key":"7eae05cffaf70","_type":"span","marks":[],"text":"Static memory block: which keeps track of static, non-changing information"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"5688fb7eca8f","_type":"block","children":[{"_key":"35a40ace09850","_type":"span","marks":[],"text":"Fact extraction memory block: that populates a list of facts extracted from the conversation"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"249518fdd14c","_type":"block","children":[{"_key":"6bb33f6c70aa0","_type":"span","marks":[],"text":"Vector memory block: which allows us to make use of classic embedding generation vector search as a way to both store chat history and fetch relevant parts of old conversations."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"1a0559286469","_type":"block","children":[{"_key":"511436057ff70","_type":"span","marks":[],"text":"A memory component can be initialized with any number of these long-term memory blocks. Then, every time we reach the token limit for short-term memory, it will write the relevant information to these long-term memory blocks. To add long-term memory blocks we initialize memory with "},{"_key":"511436057ff71","_type":"span","marks":["code"],"text":"memory_blocks"},{"_key":"511436057ff72","_type":"span","marks":[],"text":":"}],"markDefs":[],"style":"normal"},{"_key":"0c7a62a64ec6","_type":"codeBlock","code":"blocks = [\u003clist of long-term memory blocks\u003e]\n\nmemory = Memory.from_defaults(\n    session_id=\"my_session\",\n    memory_blocks=blocks,\n    insert_method=\"system\")","language":"python"},{"_key":"c3d27c28e30e","_type":"block","children":[{"_key":"5064b64999930","_type":"span","marks":[],"text":"Let’s see in more detail how we can initialize each block."}],"markDefs":[],"style":"normal"},{"_key":"e77176f4708f","_type":"block","children":[{"_key":"405bfce94f7e0","_type":"span","marks":[],"text":"Static Information as long-term memory"}],"markDefs":[],"style":"h3"},{"_key":"c1711e80ec16","_type":"block","children":[{"_key":"50b98a8a27a20","_type":"span","marks":[],"text":"Imagine information that might be relevant to your agentic application that is less likely to change over time. For example, your name, where you live, where you work etc. The "},{"_key":"50b98a8a27a21","_type":"span","marks":["code"],"text":"StaticMemoryBlock"},{"_key":"50b98a8a27a22","_type":"span","marks":[],"text":" allows you to provide this information as added context for your agent:"}],"markDefs":[],"style":"normal"},{"_key":"973a1d3c2973","_type":"codeBlock","code":"from llama_index.core.memory import StaticMemoryBlock\n\nstatic_info_block = StaticMemoryBlock(\n    name=\"core_info\", \n\tstatic_content=\"My name is Tuana, and I live in Amsterdam. I work at LlamaIndex.\",\n\tpriority=0\n)","language":"python"},{"_key":"466a1d586c7d","_type":"block","children":[{"_key":"49266defd4a70","_type":"span","marks":[],"text":"You may think that the information here are facts, so are more appropriate for the fact extraction block. The main difference is that the fact extraction block is in itself an LLM powered extraction system, that is designed to extract facts as the conversation is happening."}],"markDefs":[],"style":"normal"},{"_key":"8a3a0ba48511","_type":"block","children":[{"_key":"08434717c0ee0","_type":"span","marks":[],"text":"Facts as long-term memory"}],"markDefs":[],"style":"h3"},{"_key":"2442cfcd394f","_type":"block","children":[{"_key":"a7b818eb10c10","_type":"span","marks":[],"text":"My personal favorite. The "},{"_key":"a7b818eb10c11","_type":"span","marks":["code"],"text":"FactExtractionMemoryBlock"},{"_key":"a7b818eb10c12","_type":"span","marks":[],"text":" is a unique long-term memory block that is initialized with a default prompt (which you can override), that instructs an LLM to extract a list of facts from ongoing conversations. To initialize this block:"}],"markDefs":[],"style":"normal"},{"_key":"a007c89c50ca","_type":"codeBlock","code":"from llama_index.core.memory import FactExtractionMemoryBlock\nfrom llama_index.llms.openai import OpenAI\n\nllm = OpenAI(model=\"gpt-4o-mini\")\n \nfacts_block = FactExtractionMemoryBlock(\n    name=\"extracted_info\",\n\tllm=llm,\n    max_facts=50,\n    priority=1\n)","language":"python"},{"_key":"5001079b17b8","_type":"block","children":[{"_key":"d6fa913ec1530","_type":"span","marks":[],"text":"\nThen, for example, after a long conversation with an agent where I provide some information about myself, we may get the following:"}],"markDefs":[],"style":"normal"},{"_key":"089a31639e50","_type":"codeBlock","code":"memory.memory_blocks[1].facts\n\n# ['User is 29 years old.', 'User has a sister.']","language":"python"},{"_key":"fc277e14440d","_type":"block","children":[{"_key":"8d74767ea1350","_type":"span","marks":[],"text":"Vector search as long-term memory"}],"markDefs":[],"style":"h3"},{"_key":"3322c3a76ca1","_type":"block","children":[{"_key":"d95347ef09bc0","_type":"span","marks":[],"text":"Finally, we also have the "},{"_key":"d95347ef09bc1","_type":"span","marks":["code"],"text":"VectorMemoryBlock"},{"_key":"d95347ef09bc2","_type":"span","marks":[],"text":" which has to be initialized with "},{"_key":"d95347ef09bc3","_type":"span","marks":["0b657d3e9c5d"],"text":"a vector store"},{"_key":"d95347ef09bc4","_type":"span","marks":[],"text":" like Weaviate, Qdrant or similar. The idea here is simple: once we reach the token limit for short-term memory, the memory component will write all the chat messages that have occurred to the vector store we initialized this block with. Once we do that, any ongoing conversations, where appropriate, can fetch relevant conversations from history and use that as context before replying to the user."}],"markDefs":[{"_key":"0b657d3e9c5d","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/storing/vector_stores/"}],"style":"normal"},{"_key":"cd542b65245d","_type":"codeBlock","code":"from llama_index.core.memory import VectorMemoryBlock\n\nvector_block = VectorMemoryBlock(\n    name=\"vector_memory\",\n    # required: pass in a vector store like qdrant, chroma, weaviate, milvus, etc.\n    vector_store=vector_store,\n    priority=2,\n    embed_model=embed_model,\n    similarity_top_k=2,\n)","language":"python"},{"_key":"27496edc9121","_type":"block","children":[{"_key":"35cb233f1de20","_type":"span","marks":[],"text":"\n"}],"markDefs":[],"style":"normal"},{"_key":"72eff8a826bc","_type":"block","children":[{"_key":"297a9c6ffed40","_type":"span","marks":[],"text":"Customizing Memory"}],"markDefs":[],"style":"h2"},{"_key":"e472121a02cd","_type":"block","children":[{"_key":"3719bf21968f0","_type":"span","marks":[],"text":"On top of all of these, we are also making it possible for you to introduce your own memory blocks by extending the "},{"_key":"3719bf21968f1","_type":"span","marks":["code"],"text":"BaseMemoryBlock"},{"_key":"3719bf21968f2","_type":"span","marks":[],"text":" class. For example, below we define a memory block that counts the mention of a given name in the conversation:"}],"markDefs":[],"style":"normal"},{"_key":"1473208b1a98","_type":"codeBlock","code":"from typing import Optional, List, Any\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.memory.memory import BaseMemoryBlock\n\nclass MentionCounter(BaseMemoryBlock[str]):\n    \"\"\"\n    A memory block that counts the number of times a user mentions a specific name.\n    \"\"\"\n    mention_name: str = \"Logan\"\n    mention_count: int = 0\n\n    async def _aget(self, messages: Optional[List[ChatMessage]] = None, **block_kwargs: Any) -\u003e str:\n        return f\"Logan was mentioned {self.mention_count} times.\"\n\n    async def _aput(self, messages: List[ChatMessage]) -\u003e None:\n        for message in messages:\n            if self.mention_name in message.content:\n                self.mention_count += 1\n\n    async def atruncate(self, content: str, tokens_to_truncate: int) -\u003e Optional[str]:\n        return \"\"","language":"python"},{"_key":"0fe3c5a4feea","_type":"block","children":[{"_key":"4bbff2795fcc0","_type":"span","marks":[],"text":"\n"}],"markDefs":[],"style":"normal"},{"_key":"1da37de4dcde","_type":"block","children":[{"_key":"ec87e4023a050","_type":"span","marks":[],"text":"Future Improvements"}],"markDefs":[],"style":"h2"},{"_key":"79c5fcfce485","_type":"block","children":[{"_key":"74e145d6001c0","_type":"span","marks":[],"text":"We would love for you to try out these latest improvements to the memory component for agents. If you do, please let us know how you got on in our community Discord server."}],"markDefs":[],"style":"normal"},{"_key":"99a3bf350e5c","_type":"block","children":[{"_key":"d2f785057c540","_type":"span","marks":[],"text":"But, that being said, we see a few improvements coming for this component. For example, the current implementation only allows you to make use of memory as part of the backend of an agent. We’d love to provide the option of using memory as one of the suite of tools for a tool calling agent."}],"markDefs":[],"style":"normal"},{"_key":"2ca44e14fc21","_type":"block","children":[{"_key":"102a28dddbfd0","_type":"span","marks":[],"text":"Additionally, we currently only support SQL databases ("},{"_key":"102a28dddbfd1","_type":"span","marks":["e8e770738295"],"text":"including remote databases which you can configure"},{"_key":"102a28dddbfd2","_type":"span","marks":[],"text":"). But, we would also like to add support for NoSQL databases like MongoDB, Redis and so on."}],"markDefs":[{"_key":"e8e770738295","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory/#remote-memory"}],"style":"normal"},{"_key":"4554ed0006e6","_type":"block","children":[{"_key":"e683e9dda0420","_type":"span","marks":[],"text":"Finally, we can imagine plenty of scenarios where the "},{"_key":"e683e9dda0421","_type":"span","marks":["code"],"text":"FactExtractionMemoryBlock"},{"_key":"e683e9dda0422","_type":"span","marks":[],"text":" might benefit from structured outputs. This would allow us to initialize the block with a predefined set of fields (facts) that the agent will fill in along the way."}],"markDefs":[],"style":"normal"},{"_key":"7865a89e4f47","_type":"block","children":[{"_key":"2a6aa5e55f320","_type":"span","marks":[],"text":"To get started and learn more about how to build agents with both short-term and long-term memory:"}],"markDefs":[],"style":"normal"},{"_key":"9ab1e0411103","_type":"block","children":[{"_key":"edb3d5b1f9340","_type":"span","marks":["601238926d24"],"text":"Visit the Memory documentation"}],"level":1,"listItem":"bullet","markDefs":[{"_key":"601238926d24","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/memory"}],"style":"normal"},{"_key":"08cc17f2177d","_type":"block","children":[{"_key":"86bad730bd1e0","_type":"span","marks":["d8df0d628317"],"text":"See the API docs"}],"level":1,"listItem":"bullet","markDefs":[{"_key":"d8df0d628317","_type":"link","href":"https://docs.llamaindex.ai/en/latest/api_reference/memory/memory/"}],"style":"normal"}],"title":"Improved Long \u0026 Short-Term Memory for LlamaIndex Agents"},"publishedDate":"Invalid Date"},"params":{"slug":"improved-long-and-short-term-memory-for-llamaindex-agents"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"improved-long-and-short-term-memory-for-llamaindex-agents"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>