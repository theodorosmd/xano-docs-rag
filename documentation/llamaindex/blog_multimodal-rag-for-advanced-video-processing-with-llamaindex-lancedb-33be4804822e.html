<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>MultiModal RAG for Advanced Video Processing with LlamaIndex &amp; LanceDB — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="MultiModal RAG for Advanced Video Processing with LlamaIndex &amp; LanceDB — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="MultiModal RAG for Advanced Video Processing with LlamaIndex &amp; LanceDB — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="MultiModal RAG for Advanced Video Processing with LlamaIndex &amp; LanceDB — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="512" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/raghav-dixit">Raghav Dixit</a> <!-- -->•<!-- --> <!-- -->2024-02-17</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">MultiModal RAG for Advanced Video Processing with LlamaIndex &amp; LanceDB</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/generative-ai-tools"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Generative Ai Tools</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/vector-database"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Vector Database</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/large-language-models"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Large Language Models</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/open-source"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Open Source</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>The widespread consumption of videos on platforms like YouTube, Instagram, and others highlights the importance of efficiently processing and analyzing video content. This capability unlocks vast opportunities across various sectors, including media and entertainment, security, and education. However, the main challenge is effectively extracting meaningful information from videos, which are inherently complex and multimodal data streams.</p><p>This blog post introduces a solution that leverages the LlamaIndex <a href="https://docs.llamaindex.ai/en/latest/index.html#" rel="noopener ugc nofollow" target="_blank">Python API</a> for using the advanced capabilities of OpenAI’s <a href="https://help.openai.com/en/articles/8555496-gpt-4v-api" rel="noopener ugc nofollow" target="_blank">GPT4V</a>, combined with the efficient data management by <a href="https://lancedb.github.io/lancedb/" rel="noopener ugc nofollow" target="_blank">LanceDB</a> across all data formats, to process videos.</p><p>…But what does ‘RAG’ even mean?</p><p>Retrieval-augmented generation (RAG) is a technique that merges information retrieval with generative AI to produce systems capable of generating precise and contextually relevant responses by tapping into large data repositories.</p><h1>Core Concept of RAG</h1><p>RAG operates in two stages:</p><ol><li><strong>Retrieval</strong>: Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords.</li><li><strong>Generation</strong>: Integrates retrieved information to produce coherent responses, allowing the AI to “learn” from a wide range of content dynamically.</li></ol><h1>RAG Architecture</h1><p>The architecture typically involves a dense vector search engine for retrieval and a transformer model for generation. The process:</p><ul><li>Performs a semantic search to fetch relevant documents.</li><li>Processes these documents with the query to create a comprehensive context.</li><li>The generative model then crafts a detailed response based on this enriched context.</li></ul><h1>Extending to Multimodality</h1><p>Multimodal RAG integrates various data types (text, images, audio, video) in both retrieval and generation phases, enabling richer information sourcing. For example, responding to queries about “climate change impacts on polar bears” might involve retrieving scientific texts, images, and videos to produce an enriched, multi-format response.</p><p>Let’s return to our use case and dive into how it’s all done. Moving forward, you can access the full code on <a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_video_RAG.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab</a>.</p><p>The solution is divided into the following sections. Click on the topic to skip to a specific part:</p><ol><li><a href="#e9e9" rel="noopener ugc nofollow">Video Downloading</a></li><li><a href="#fceb" rel="noopener ugc nofollow">Video Processing</a></li><li><a href="#a0b8" rel="noopener ugc nofollow">Building the Multi-Modal Index and Vector Store</a></li><li><a href="#21e4" rel="noopener ugc nofollow">Retrieving Relevant Images and Context</a></li><li><a href="#6d77" rel="noopener ugc nofollow">Reasoning and Response Generation</a></li></ol><h1>1. Video Downloading</h1><p>To begin, we need to locally download multimodal content from a publicly available source, I used pytube to download a YouTube video by 3Blue1Brown on the Gaussian function.</p><pre><span id="0992" class="qi oo gt qf b bf qj qk l ql qm"># SET CONFIG
video_url = "https://www.youtube.com/watch?v=d_qvLDhkg00"
output_video_path = "./video_data/"
output_folder = "./mixed_data/"
output_audio_path = "./mixed_data/output_audio.wav"

filepath = output_video_path + "input_vid.mp4"
Path(output_folder).mkdir(parents=True, exist_ok=True)</span></pre><pre><span id="7909" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">def</span> <span class="hljs-title function_">download_video</span>(<span class="hljs-params">url, output_path</span>):
    <span class="hljs-string">"""
    Download a video from a given url and save it to the output path.

    Parameters:
    url (str): The url of the video to download.
    output_path (str): The path to save the video to.

    Returns:
    dict: A dictionary containing the metadata of the video.
    """</span>
  <span class="hljs-keyword">from</span> pytube <span class="hljs-keyword">import</span> YouTube

    yt = YouTube(url)
    metadata = {<span class="hljs-string">"Author"</span>: yt.author, <span class="hljs-string">"Title"</span>: yt.title, <span class="hljs-string">"Views"</span>: yt.views}
    yt.streams.get_highest_resolution().download(
        output_path=output_path, filename=<span class="hljs-string">"input_vid.mp4"</span>
    )
    <span class="hljs-keyword">return</span> metadata
</span></pre><p>Run <code class="cw qo qp qq qf b"><strong>metadata_vid = download_video(video_url, output_video_path)</strong></code> to invoke the function and store the video locally.</p><h1><em class="qr">2. Video Processing</em></h1><p>We need to now extract multimodal content — Images, Text(via Audio). I extracted 1 frame every 5 seconds of the video (~160 frames) using <code class="cw qo qp qq qf b">moviepy</code> .</p><pre><span id="6811" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">def</span> <span class="hljs-title function_">video_to_images</span>(<span class="hljs-params">video_path, output_folder</span>):
    <span class="hljs-string">"""
    Convert a video to a sequence of images and save them to the output folder.

    Parameters:
    video_path (str): The path to the video file.
    output_folder (str): The path to the folder to save the images to.

    """</span>
    clip = VideoFileClip(video_path)
    clip.write_images_sequence(
        os.path.join(output_folder, <span class="hljs-string">"frame%04d.png"</span>), fps=<span class="hljs-number">0.2</span> <span class="hljs-comment">#configure this for controlling frame rate.</span>
    )</span></pre><p>Following this, we extract the audio component:</p><pre><span id="1960" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">def</span> <span class="hljs-title function_">video_to_audio</span>(<span class="hljs-params">video_path, output_audio_path</span>):
    <span class="hljs-string">"""
    Convert a video to audio and save it to the output path.

    Parameters:
    video_path (str): The path to the video file.
    output_audio_path (str): The path to save the audio to.

    """</span>
    clip = VideoFileClip(video_path)
    audio = clip.audio
    audio.write_audiofile(output_audio_path)</span></pre><p>Next, let’s extract text from the audio using the SpeechRecognition library:</p><pre><span id="fdfd" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">def</span> <span class="hljs-title function_">audio_to_text</span>(<span class="hljs-params">audio_path</span>):
    <span class="hljs-string">"""
    Convert an audio file to text.

    Parameters:
    audio_path (str): The path to the audio file.

    Returns:
    test (str): The text recognized from the audio.

    """</span>
    recognizer = sr.Recognizer()
    audio = sr.AudioFile(audio_path)

    <span class="hljs-keyword">with</span> audio <span class="hljs-keyword">as</span> source:
        <span class="hljs-comment"># Record the audio data</span>
        audio_data = recognizer.record(source)

        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># Recognize the speech</span>
            text = recognizer.recognize_whisper(audio_data)
        <span class="hljs-keyword">except</span> sr.UnknownValueError:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">"Speech recognition could not understand the audio."</span>)
        <span class="hljs-keyword">except</span> sr.RequestError <span class="hljs-keyword">as</span> e:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Could not request results from service; <span class="hljs-subst">{e}</span>"</span>)

    <span class="hljs-keyword">return</span> text</span></pre><p>Run the below chunk to complete the extraction and storage process:</p><pre><span id="c99a" class="qi oo gt qf b bf qj qk l ql qm">video_to_images(filepath, output_folder)
video_to_audio(filepath, output_audio_path)
text_data = audio_to_text(output_audio_path)

<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(output_folder + <span class="hljs-string">"output_text.txt"</span>, <span class="hljs-string">"w"</span>) <span class="hljs-keyword">as</span> file:
    file.write(text_data)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Text data saved to file"</span>)
file.close()
os.remove(output_audio_path)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Audio file removed"</span>)</span></pre><h1>3. Building the Multi-Modal Index and Vector Store</h1><p>After processing the video, we proceed to construct a multi-modal index and vector store. This entails generating embeddings for both textual and visual data using OpenAI’s CLIP model, subsequently storing and managing these embeddings in LanceDB VectorStore via the <code class="cw qo qp qq qf b"><strong>LanceDBVectorStore</strong></code> class.</p><pre><span id="78c3" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">from</span> llama_index.indices.multi_modal.base <span class="hljs-keyword">import</span> MultiModalVectorStoreIndex
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> SimpleDirectoryReader, StorageContext

<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> SimpleDirectoryReader, StorageContext
<span class="hljs-keyword">from</span> llama_index.vector_stores <span class="hljs-keyword">import</span> LanceDBVectorStore


<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    SimpleDirectoryReader,
)

text_store = LanceDBVectorStore(uri=<span class="hljs-string">"lancedb"</span>, table_name=<span class="hljs-string">"text_collection"</span>)
image_store = LanceDBVectorStore(uri=<span class="hljs-string">"lancedb"</span>, table_name=<span class="hljs-string">"image_collection"</span>)
storage_context = StorageContext.from_defaults(
    vector_store=text_store, image_store=image_store
)

<span class="hljs-comment"># Create the MultiModal index</span>
documents = SimpleDirectoryReader(output_folder).load_data()

index = MultiModalVectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
)</span></pre><h1>4. Retrieving Relevant Images and Context</h1><p>With the index in place, the system can then retrieve pertinent images and contextual information based on input queries. This enhances the prompt with precise and relevant multimodal data, anchoring the analysis in the video’s content.</p><p>Lets set up the engine for retrieving, I am fetching top 5 most relevant <code class="cw qo qp qq qf b">Nodes</code> from the vectordb based on the similarity score:</p><pre><span id="72d0" class="qi oo gt qf b bf qj qk l ql qm">retriever_engine = index.as_retriever(
    similarity_top_k=<span class="hljs-number">5</span>, image_similarity_top_k=<span class="hljs-number">5</span>
)</span></pre><blockquote><p id="090b" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><em class="gt">A </em><code class="cw qo qp qq qf b"><em class="gt">Node</em></code><em class="gt"> object is a “chunk” of any source Document, whether it’s text, an image, or other. It contains embeddings as well as meta information of the chunk of data.</em></p><p id="d183" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj">By default, LanceDB uses <code class="cw qo qp qq qf b">l2</code> as metric type for evaluating similarity. You can specify the metric type as <code class="cw qo qp qq qf b">cosine</code> or <code class="cw qo qp qq qf b">dot</code> if required.</p></blockquote><p>Next, we create a helper function for executing the retrieval logic:</p><pre><span id="d602" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">from</span> llama_index.response.notebook_utils <span class="hljs-keyword">import</span> display_source_node
<span class="hljs-keyword">from</span> llama_index.schema <span class="hljs-keyword">import</span> ImageNode


<span class="hljs-keyword">def</span> <span class="hljs-title function_">retrieve</span>(<span class="hljs-params">retriever_engine, query_str</span>):
    retrieval_results = retriever_engine.retrieve(query_str)

    retrieved_image = []
    retrieved_text = []
    <span class="hljs-keyword">for</span> res_node <span class="hljs-keyword">in</span> retrieval_results:
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(res_node.node, ImageNode):
            retrieved_image.append(res_node.node.metadata[<span class="hljs-string">"file_path"</span>])
        <span class="hljs-keyword">else</span>:
            display_source_node(res_node, source_length=<span class="hljs-number">200</span>)
            retrieved_text.append(res_node.text)

    <span class="hljs-keyword">return</span> retrieved_image, retrieved_textdef retrieve(retriever_engine, query_str):
    retrieval_results = retriever_engine.retrieve(query_str)</span></pre><p>Lets input the query now and then move on to complete the process by retrieving and visualizing the data :</p><pre><span id="eead" class="qi oo gt qf b bf qj qk l ql qm">query_str = """
Using examples from the video, explain all things covered regarding
the Gaussian function
"""</span></pre><pre><span id="b939" class="qi oo gt qf b bf qj qk l ql qm">
img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)
image_documents = SimpleDirectoryReader(
    input_dir=output_folder, input_files=img
).load_data()
context_str = <span class="hljs-string">""</span>.join(txt)
plot_images(img)</span></pre><p>You should see something similar to the example below (note that the output will vary depending on your query):</p><figure><figcaption class="qx fe qy na nb qz ra be b bf z dt">Displaying the similar Text objects (nodes)</figcaption><img src="/blog/images/1*huhFfhzTx8o8xAIPqXKwAQ.png" alt="" width="700" height="259"></figure><figure><figcaption class="qx fe qy na nb qz ra be b bf z dt">Retrieved Images</figcaption><img src="/blog/images/1*LEQOVC_m2CJGTzzEuSWXDw.png" alt="" width="700" height="335"></figure><p>Observe that the <code class="cw qo qp qq qf b">node</code> object displayed shows the <code class="cw qo qp qq qf b">Id</code> of the data chunk , its similarity score and the source text of the chunk that was matched (for images we get the filepath instead of text).</p><h1>5. Reasoning and Response Generation</h1><p>The final step leverages GPT4V to reason about the correlations between the input query and the augmented data. Below is the prompt template :</p><pre><span id="392a" class="qi oo gt qf b bf qj qk l ql qm">qa_tmpl_str = (
    <span class="hljs-string">""</span><span class="hljs-string">"
 Given the provided information, including relevant images and retrieved context from the video, \
 accurately and precisely answer the query without any additional prior knowledge.\n"</span>
    <span class="hljs-string">"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\n"</span>
    <span class="hljs-string">"---------------------\n"</span>
    <span class="hljs-string">"Context: {context_str}\n"</span>
    <span class="hljs-string">"Metadata for video: {metadata_str} \n"</span>
    <span class="hljs-string">"---------------------\n"</span>
    <span class="hljs-string">"Query: {query_str}\n"</span>
    <span class="hljs-string">"Answer: "</span>
<span class="hljs-string">""</span><span class="hljs-string">"
)</span></span></pre><p>The <code class="cw qo qp qq qf b">OpenAIMultiModal</code> class from LlamaIndex enables us to incorporate image data directly into our prompt object. Thus, in the final step, we enhance the query and contextual elements within the template to produce the response as follows:</p><pre><span id="902c" class="qi oo gt qf b bf qj qk l ql qm"><span class="hljs-keyword">from</span> llama_index.multi_modal_llms.openai <span class="hljs-keyword">import</span> OpenAIMultiModal

openai_mm_llm = OpenAIMultiModal(
    model=<span class="hljs-string">"gpt-4-vision-preview"</span>, api_key=OPENAI_API_TOKEN, max_new_tokens=<span class="hljs-number">1500</span>
)


response_1 = openai_mm_llm.complete(
    prompt=qa_tmpl_str.<span class="hljs-built_in">format</span>(
        context_str=context_str, query_str=query_str, metadata_str=metadata_str
    ),
    image_documents=image_documents,
)

pprint(response_1.text)</span></pre><blockquote><p id="e779" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><em class="gt">The generated response captures the context pretty well and structures the answer correctly :</em></p><p id="f18e" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj">The video “A pretty reason why Gaussian + Gaussian = Gaussian” by 3Blue1Brown delves into the Gaussian function or normal distribution, highlighting several critical aspects:</p><p id="e128" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Central Limit Theorem:</strong> It starts with the central limit theorem, illustrating how the sum of multiple random variable copies tends toward a normal distribution, improving with more variables.</p><p id="2c09" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Convolution of Random Variables:</strong> Explains the addition of two random variables as their distributions’ convolution, focusing on visualizing this through diagonal slices.</p><p id="eed4" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Gaussian Function:</strong> Details the Gaussian function, emphasizing the normalization factor for a valid probability distribution, and describes the distribution’s spread and center with standard deviation (σ) and mean (μ).</p><p id="da5d" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Convolution of Two Gaussians:</strong> Discusses adding two normally distributed variables, equivalent to convolving two Gaussian functions, and visualizes this using the graph’s rotational symmetry.</p><p id="df0b" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Rotational Symmetry and Slices:</strong> Shows the rotational symmetry of e^(-x²) * e^(-y²) around the origin, a unique Gaussian function property. It explains computing the area under diagonal slices, equating to the functions’ convolution.</p><p id="36bd" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Resulting Distribution:</strong> Demonstrates the convolution of two Gaussian functions yielding another Gaussian, a notable exception in convolutions usually resulting in a different function type.</p><p id="3053" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Standard Deviation of the Result:</strong> Concludes that convolving two normal distributions with mean 0 and standard deviation (σ) produces a normal distribution with a standard deviation of sqrt(2) * σ.</p><p id="1def" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj"><strong>Implications for the Central Limit Theorem:</strong> Highlights the convolution of two Gaussians’ role in the central limit theorem, positioning the Gaussian distribution as a distribution space fixed point.</p><p id="f19c" class="no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj">The author uses visual examples and explanations throughout to clarify the mathematical concepts related to the Gaussian function and its significance in probability and statistics.</p></blockquote><h1>Conclusion</h1><p>The Multimodal RAG architecture offers a powerful and efficient solution for processing and analyzing video content. By leveraging the capabilities of OpenAI’s GPT4V and LanceDB, this approach not only simplifies the video analysis process but also enhances its accuracy and relevance. Whether for content creation, security surveillance, or educational purposes, the potential applications of this technology are vast and varied. As we continue to explore and refine these tools, the future of video analysis looks promising, with AI-driven solutions leading the way towards more insightful and actionable interpretations of video data.</p><p>Stay tuned for upcoming projects !</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-05">LlamaIndex Newsletter 2024-03-05</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f">Querying a network of knowledge with llama-index-networks</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-27</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"b7bdc78d-63a0-4ebd-a57d-aa75f4211a00","_rev":"05dtDS0H5iRVsxYMarZZyl","_type":"blogPost","_updatedAt":"2025-05-21T20:39:30Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:51:07Z","_id":"4c0f674f-a623-4c2d-af85-6e2e55b77a97","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Raghav Dixit","slug":{"_type":"slug","current":"raghav-dixit"}}],"featured":false,"htmlContent":"\u003cp\u003eThe widespread consumption of videos on platforms like YouTube, Instagram, and others highlights the importance of efficiently processing and analyzing video content. This capability unlocks vast opportunities across various sectors, including media and entertainment, security, and education. However, the main challenge is effectively extracting meaningful information from videos, which are inherently complex and multimodal data streams.\u003c/p\u003e\u003cp\u003eThis blog post introduces a solution that leverages the LlamaIndex \u003ca href=\"https://docs.llamaindex.ai/en/latest/index.html#\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePython API\u003c/a\u003e for using the advanced capabilities of OpenAI’s \u003ca href=\"https://help.openai.com/en/articles/8555496-gpt-4v-api\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGPT4V\u003c/a\u003e, combined with the efficient data management by \u003ca href=\"https://lancedb.github.io/lancedb/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLanceDB\u003c/a\u003e across all data formats, to process videos.\u003c/p\u003e\u003cp\u003e…But what does ‘RAG’ even mean?\u003c/p\u003e\u003cp\u003eRetrieval-augmented generation (RAG) is a technique that merges information retrieval with generative AI to produce systems capable of generating precise and contextually relevant responses by tapping into large data repositories.\u003c/p\u003e\u003ch1\u003eCore Concept of RAG\u003c/h1\u003e\u003cp\u003eRAG operates in two stages:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eRetrieval\u003c/strong\u003e: Utilizes semantic search to find documents related to a query, leveraging the context and meaning beyond mere keywords.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGeneration\u003c/strong\u003e: Integrates retrieved information to produce coherent responses, allowing the AI to “learn” from a wide range of content dynamically.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003eRAG Architecture\u003c/h1\u003e\u003cp\u003eThe architecture typically involves a dense vector search engine for retrieval and a transformer model for generation. The process:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePerforms a semantic search to fetch relevant documents.\u003c/li\u003e\u003cli\u003eProcesses these documents with the query to create a comprehensive context.\u003c/li\u003e\u003cli\u003eThe generative model then crafts a detailed response based on this enriched context.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eExtending to Multimodality\u003c/h1\u003e\u003cp\u003eMultimodal RAG integrates various data types (text, images, audio, video) in both retrieval and generation phases, enabling richer information sourcing. For example, responding to queries about “climate change impacts on polar bears” might involve retrieving scientific texts, images, and videos to produce an enriched, multi-format response.\u003c/p\u003e\u003cp\u003eLet’s return to our use case and dive into how it’s all done. Moving forward, you can access the full code on \u003ca href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/multi_modal/multi_modal_video_RAG.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Colab\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe solution is divided into the following sections. Click on the topic to skip to a specific part:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"#e9e9\" rel=\"noopener ugc nofollow\"\u003eVideo Downloading\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#fceb\" rel=\"noopener ugc nofollow\"\u003eVideo Processing\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#a0b8\" rel=\"noopener ugc nofollow\"\u003eBuilding the Multi-Modal Index and Vector Store\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#21e4\" rel=\"noopener ugc nofollow\"\u003eRetrieving Relevant Images and Context\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"#6d77\" rel=\"noopener ugc nofollow\"\u003eReasoning and Response Generation\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003e1. Video Downloading\u003c/h1\u003e\u003cp\u003eTo begin, we need to locally download multimodal content from a publicly available source, I used pytube to download a YouTube video by 3Blue1Brown on the Gaussian function.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0992\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e# SET CONFIG\nvideo_url = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\noutput_video_path = \"./video_data/\"\noutput_folder = \"./mixed_data/\"\noutput_audio_path = \"./mixed_data/output_audio.wav\"\n\nfilepath = output_video_path + \"input_vid.mp4\"\nPath(output_folder).mkdir(parents=True, exist_ok=True)\u003c/span\u003e\u003c/pre\u003e\u003cpre\u003e\u003cspan id=\"7909\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003edownload_video\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eurl, output_path\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    Download a video from a given url and save it to the output path.\n\n    Parameters:\n    url (str): The url of the video to download.\n    output_path (str): The path to save the video to.\n\n    Returns:\n    dict: A dictionary containing the metadata of the video.\n    \"\"\"\u003c/span\u003e\n  \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pytube \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e YouTube\n\n    yt = YouTube(url)\n    metadata = {\u003cspan class=\"hljs-string\"\u003e\"Author\"\u003c/span\u003e: yt.author, \u003cspan class=\"hljs-string\"\u003e\"Title\"\u003c/span\u003e: yt.title, \u003cspan class=\"hljs-string\"\u003e\"Views\"\u003c/span\u003e: yt.views}\n    yt.streams.get_highest_resolution().download(\n        output_path=output_path, filename=\u003cspan class=\"hljs-string\"\u003e\"input_vid.mp4\"\u003c/span\u003e\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e metadata\n\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eRun \u003ccode class=\"cw qo qp qq qf b\"\u003e\u003cstrong\u003emetadata_vid = download_video(video_url, output_video_path)\u003c/strong\u003e\u003c/code\u003e to invoke the function and store the video locally.\u003c/p\u003e\u003ch1\u003e\u003cem class=\"qr\"\u003e2. Video Processing\u003c/em\u003e\u003c/h1\u003e\u003cp\u003eWe need to now extract multimodal content — Images, Text(via Audio). I extracted 1 frame every 5 seconds of the video (~160 frames) using \u003ccode class=\"cw qo qp qq qf b\"\u003emoviepy\u003c/code\u003e .\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6811\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003evideo_to_images\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003evideo_path, output_folder\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    Convert a video to a sequence of images and save them to the output folder.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_folder (str): The path to the folder to save the images to.\n\n    \"\"\"\u003c/span\u003e\n    clip = VideoFileClip(video_path)\n    clip.write_images_sequence(\n        os.path.join(output_folder, \u003cspan class=\"hljs-string\"\u003e\"frame%04d.png\"\u003c/span\u003e), fps=\u003cspan class=\"hljs-number\"\u003e0.2\u003c/span\u003e \u003cspan class=\"hljs-comment\"\u003e#configure this for controlling frame rate.\u003c/span\u003e\n    )\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eFollowing this, we extract the audio component:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"1960\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003evideo_to_audio\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003evideo_path, output_audio_path\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    Convert a video to audio and save it to the output path.\n\n    Parameters:\n    video_path (str): The path to the video file.\n    output_audio_path (str): The path to save the audio to.\n\n    \"\"\"\u003c/span\u003e\n    clip = VideoFileClip(video_path)\n    audio = clip.audio\n    audio.write_audiofile(output_audio_path)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNext, let’s extract text from the audio using the SpeechRecognition library:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"fdfd\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eaudio_to_text\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eaudio_path\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    Convert an audio file to text.\n\n    Parameters:\n    audio_path (str): The path to the audio file.\n\n    Returns:\n    test (str): The text recognized from the audio.\n\n    \"\"\"\u003c/span\u003e\n    recognizer = sr.Recognizer()\n    audio = sr.AudioFile(audio_path)\n\n    \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e audio \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e source:\n        \u003cspan class=\"hljs-comment\"\u003e# Record the audio data\u003c/span\u003e\n        audio_data = recognizer.record(source)\n\n        \u003cspan class=\"hljs-keyword\"\u003etry\u003c/span\u003e:\n            \u003cspan class=\"hljs-comment\"\u003e# Recognize the speech\u003c/span\u003e\n            text = recognizer.recognize_whisper(audio_data)\n        \u003cspan class=\"hljs-keyword\"\u003eexcept\u003c/span\u003e sr.UnknownValueError:\n            \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Speech recognition could not understand the audio.\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003eexcept\u003c/span\u003e sr.RequestError \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e e:\n            \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Could not request results from service; \u003cspan class=\"hljs-subst\"\u003e{e}\u003c/span\u003e\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e text\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eRun the below chunk to complete the extraction and storage process:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c99a\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003evideo_to_images(filepath, output_folder)\nvideo_to_audio(filepath, output_audio_path)\ntext_data = audio_to_text(output_audio_path)\n\n\u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(output_folder + \u003cspan class=\"hljs-string\"\u003e\"output_text.txt\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"w\"\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e file:\n    file.write(text_data)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Text data saved to file\"\u003c/span\u003e)\nfile.close()\nos.remove(output_audio_path)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Audio file removed\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003e3. Building the Multi-Modal Index and Vector Store\u003c/h1\u003e\u003cp\u003eAfter processing the video, we proceed to construct a multi-modal index and vector store. This entails generating embeddings for both textual and visual data using OpenAI’s CLIP model, subsequently storing and managing these embeddings in LanceDB VectorStore via the \u003ccode class=\"cw qo qp qq qf b\"\u003e\u003cstrong\u003eLanceDBVectorStore\u003c/strong\u003e\u003c/code\u003e class.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"78c3\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.indices.multi_modal.base \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e MultiModalVectorStoreIndex\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SimpleDirectoryReader, StorageContext\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SimpleDirectoryReader, StorageContext\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e LanceDBVectorStore\n\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    SimpleDirectoryReader,\n)\n\ntext_store = LanceDBVectorStore(uri=\u003cspan class=\"hljs-string\"\u003e\"lancedb\"\u003c/span\u003e, table_name=\u003cspan class=\"hljs-string\"\u003e\"text_collection\"\u003c/span\u003e)\nimage_store = LanceDBVectorStore(uri=\u003cspan class=\"hljs-string\"\u003e\"lancedb\"\u003c/span\u003e, table_name=\u003cspan class=\"hljs-string\"\u003e\"image_collection\"\u003c/span\u003e)\nstorage_context = StorageContext.from_defaults(\n    vector_store=text_store, image_store=image_store\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# Create the MultiModal index\u003c/span\u003e\ndocuments = SimpleDirectoryReader(output_folder).load_data()\n\nindex = MultiModalVectorStoreIndex.from_documents(\n    documents,\n    storage_context=storage_context,\n)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003e4. Retrieving Relevant Images and Context\u003c/h1\u003e\u003cp\u003eWith the index in place, the system can then retrieve pertinent images and contextual information based on input queries. This enhances the prompt with precise and relevant multimodal data, anchoring the analysis in the video’s content.\u003c/p\u003e\u003cp\u003eLets set up the engine for retrieving, I am fetching top 5 most relevant \u003ccode class=\"cw qo qp qq qf b\"\u003eNodes\u003c/code\u003e from the vectordb based on the similarity score:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"72d0\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003eretriever_engine = index.as_retriever(\n    similarity_top_k=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, image_similarity_top_k=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e\n)\u003c/span\u003e\u003c/pre\u003e\u003cblockquote\u003e\u003cp id=\"090b\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cem class=\"gt\"\u003eA \u003c/em\u003e\u003ccode class=\"cw qo qp qq qf b\"\u003e\u003cem class=\"gt\"\u003eNode\u003c/em\u003e\u003c/code\u003e\u003cem class=\"gt\"\u003e object is a “chunk” of any source Document, whether it’s text, an image, or other. It contains embeddings as well as meta information of the chunk of data.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"d183\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003eBy default, LanceDB uses \u003ccode class=\"cw qo qp qq qf b\"\u003el2\u003c/code\u003e as metric type for evaluating similarity. You can specify the metric type as \u003ccode class=\"cw qo qp qq qf b\"\u003ecosine\u003c/code\u003e or \u003ccode class=\"cw qo qp qq qf b\"\u003edot\u003c/code\u003e if required.\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eNext, we create a helper function for executing the retrieval logic:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d602\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.response.notebook_utils \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e display_source_node\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.schema \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ImageNode\n\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eretrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eretriever_engine, query_str\u003c/span\u003e):\n    retrieval_results = retriever_engine.retrieve(query_str)\n\n    retrieved_image = []\n    retrieved_text = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e res_node \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e retrieval_results:\n        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eisinstance\u003c/span\u003e(res_node.node, ImageNode):\n            retrieved_image.append(res_node.node.metadata[\u003cspan class=\"hljs-string\"\u003e\"file_path\"\u003c/span\u003e])\n        \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n            display_source_node(res_node, source_length=\u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e)\n            retrieved_text.append(res_node.text)\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e retrieved_image, retrieved_textdef retrieve(retriever_engine, query_str):\n    retrieval_results = retriever_engine.retrieve(query_str)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eLets input the query now and then move on to complete the process by retrieving and visualizing the data :\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"eead\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003equery_str = \"\"\"\nUsing examples from the video, explain all things covered regarding\nthe Gaussian function\n\"\"\"\u003c/span\u003e\u003c/pre\u003e\u003cpre\u003e\u003cspan id=\"b939\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\nimg, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\nimage_documents = SimpleDirectoryReader(\n    input_dir=output_folder, input_files=img\n).load_data()\ncontext_str = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e.join(txt)\nplot_images(img)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eYou should see something similar to the example below (note that the output will vary depending on your query):\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"qx fe qy na nb qz ra be b bf z dt\"\u003eDisplaying the similar Text objects (nodes)\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*huhFfhzTx8o8xAIPqXKwAQ.png\" alt=\"\" width=\"700\" height=\"259\"\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption class=\"qx fe qy na nb qz ra be b bf z dt\"\u003eRetrieved Images\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*LEQOVC_m2CJGTzzEuSWXDw.png\" alt=\"\" width=\"700\" height=\"335\"\u003e\u003c/figure\u003e\u003cp\u003eObserve that the \u003ccode class=\"cw qo qp qq qf b\"\u003enode\u003c/code\u003e object displayed shows the \u003ccode class=\"cw qo qp qq qf b\"\u003eId\u003c/code\u003e of the data chunk , its similarity score and the source text of the chunk that was matched (for images we get the filepath instead of text).\u003c/p\u003e\u003ch1\u003e5. Reasoning and Response Generation\u003c/h1\u003e\u003cp\u003eThe final step leverages GPT4V to reason about the correlations between the input query and the augmented data. Below is the prompt template :\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"392a\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003eqa_tmpl_str = (\n    \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n Given the provided information, including relevant images and retrieved context from the video, \\\n accurately and precisely answer the query without any additional prior knowledge.\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"Please ensure honesty and responsibility, refraining from any racist or sexist remarks.\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"---------------------\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"Context: {context_str}\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"Metadata for video: {metadata_str} \\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"---------------------\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"Query: {query_str}\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"Answer: \"\u003c/span\u003e\n\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n)\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe \u003ccode class=\"cw qo qp qq qf b\"\u003eOpenAIMultiModal\u003c/code\u003e class from LlamaIndex enables us to incorporate image data directly into our prompt object. Thus, in the final step, we enhance the query and contextual elements within the template to produce the response as follows:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"902c\" class=\"qi oo gt qf b bf qj qk l ql qm\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.multi_modal_llms.openai \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAIMultiModal\n\nopenai_mm_llm = OpenAIMultiModal(\n    model=\u003cspan class=\"hljs-string\"\u003e\"gpt-4-vision-preview\"\u003c/span\u003e, api_key=OPENAI_API_TOKEN, max_new_tokens=\u003cspan class=\"hljs-number\"\u003e1500\u003c/span\u003e\n)\n\n\nresponse_1 = openai_mm_llm.complete(\n    prompt=qa_tmpl_str.\u003cspan class=\"hljs-built_in\"\u003eformat\u003c/span\u003e(\n        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n    ),\n    image_documents=image_documents,\n)\n\npprint(response_1.text)\u003c/span\u003e\u003c/pre\u003e\u003cblockquote\u003e\u003cp id=\"e779\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cem class=\"gt\"\u003eThe generated response captures the context pretty well and structures the answer correctly :\u003c/em\u003e\u003c/p\u003e\u003cp id=\"f18e\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003eThe video “A pretty reason why Gaussian + Gaussian = Gaussian” by 3Blue1Brown delves into the Gaussian function or normal distribution, highlighting several critical aspects:\u003c/p\u003e\u003cp id=\"e128\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eCentral Limit Theorem:\u003c/strong\u003e It starts with the central limit theorem, illustrating how the sum of multiple random variable copies tends toward a normal distribution, improving with more variables.\u003c/p\u003e\u003cp id=\"2c09\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eConvolution of Random Variables:\u003c/strong\u003e Explains the addition of two random variables as their distributions’ convolution, focusing on visualizing this through diagonal slices.\u003c/p\u003e\u003cp id=\"eed4\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eGaussian Function:\u003c/strong\u003e Details the Gaussian function, emphasizing the normalization factor for a valid probability distribution, and describes the distribution’s spread and center with standard deviation (σ) and mean (μ).\u003c/p\u003e\u003cp id=\"da5d\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eConvolution of Two Gaussians:\u003c/strong\u003e Discusses adding two normally distributed variables, equivalent to convolving two Gaussian functions, and visualizes this using the graph’s rotational symmetry.\u003c/p\u003e\u003cp id=\"df0b\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eRotational Symmetry and Slices:\u003c/strong\u003e Shows the rotational symmetry of e^(-x²) * e^(-y²) around the origin, a unique Gaussian function property. It explains computing the area under diagonal slices, equating to the functions’ convolution.\u003c/p\u003e\u003cp id=\"36bd\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eResulting Distribution:\u003c/strong\u003e Demonstrates the convolution of two Gaussian functions yielding another Gaussian, a notable exception in convolutions usually resulting in a different function type.\u003c/p\u003e\u003cp id=\"3053\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eStandard Deviation of the Result:\u003c/strong\u003e Concludes that convolving two normal distributions with mean 0 and standard deviation (σ) produces a normal distribution with a standard deviation of sqrt(2) * σ.\u003c/p\u003e\u003cp id=\"1def\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003e\u003cstrong\u003eImplications for the Central Limit Theorem:\u003c/strong\u003e Highlights the convolution of two Gaussians’ role in the central limit theorem, positioning the Gaussian distribution as a distribution space fixed point.\u003c/p\u003e\u003cp id=\"f19c\" class=\"no np qv nq b nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol gm bj\"\u003eThe author uses visual examples and explanations throughout to clarify the mathematical concepts related to the Gaussian function and its significance in probability and statistics.\u003c/p\u003e\u003c/blockquote\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eThe Multimodal RAG architecture offers a powerful and efficient solution for processing and analyzing video content. By leveraging the capabilities of OpenAI’s GPT4V and LanceDB, this approach not only simplifies the video analysis process but also enhances its accuracy and relevance. Whether for content creation, security surveillance, or educational purposes, the potential applications of this technology are vast and varied. As we continue to explore and refine these tools, the future of video analysis looks promising, with AI-driven solutions leading the way towards more insightful and actionable interpretations of video data.\u003c/p\u003e\u003cp\u003eStay tuned for upcoming projects !\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/4ae00650dab860c90f1a8f797b33c1026438ba2f-1024x1024.png","publishedDate":"2024-02-17","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-bf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-03-05","slug":"llamaindex-newsletter-2024-03-05","title":"LlamaIndex Newsletter 2024-03-05"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-a195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265-webp","_type":"reference"}},"publishedDate":"2024-02-27","slug":"querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f","title":"Querying a network of knowledge with llama-index-networks"}],"slug":{"_type":"slug","current":"multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"ef5b9803-4ad2-4734-b693-b3c94a9ec36e","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"generative-ai-tools"},"title":"Generative Ai Tools"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d785a7ff-7dcb-4d46-bb89-e980be5427da","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"vector-database"},"title":"Vector Database"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"412f77ae-efba-466b-abc9-221fc36d252a","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"large-language-models"},"title":"Large Language Models"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"0cc0ccfb-0d08-4719-860e-fbcc11f095cf","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"open-source"},"title":"Open Source"}],"title":"MultiModal RAG for Advanced Video Processing with LlamaIndex \u0026 LanceDB"},"publishedDate":"Invalid Date"},"params":{"slug":"multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"multimodal-rag-for-advanced-video-processing-with-llamaindex-lancedb-33be4804822e"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>