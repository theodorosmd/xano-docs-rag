<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/b06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/b06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="448.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fb06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/ethan-p">Ethan P</a> <!-- -->•<!-- --> <!-- -->2024-01-26</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/evaluation"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Evaluation</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/github"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Github</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ci-cd-pipeline"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Ci Cd Pipeline</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p><em class="ol">In this technical walkthrough, we’ll highlight the functionality of Tonic Validate and its integration with LlamaIndex. Sign up for a free account </em><a href="https://www.tonic.ai/validate" rel="noopener ugc nofollow" target="_blank"><em class="ol">here</em></a><em class="ol"> before you start.</em></p><h1>Introduction</h1><p>As enterprise adoption of generative AI technologies continues, companies are turning to Retrieval Augmented Generation (RAG) systems to extend the application of large-language models (LLMs) to their private data (e.g., a chatbot that can answer questions based on internal technical documentation). Traditionally in software engineering, companies have placed a high emphasis on implementing continuous integration tests to ensure systems remain performant when updates are made. More recently, these same principles have been applied to machine learning models in production.</p><p>However, as a young technology, RAG currently lacks best practices for integration tests to ensure breaking changes aren’t introduced to the production system. In this article, we will demonstrate how you can use Tonic Validate’s RAG performance monitoring capabilities, LlamaIndex, and GitHub Actions to create novel integration tests that alert you to changes in RAG system performance. To make things easy, Tonic Validate is available natively within LlamaIndex’s core library — you can read more about that <a href="https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1>What is Tonic Validate?</h1><p>Tonic Validate is a RAG benchmarking and evaluation platform that monitors performance of RAG systems in production. It provides comprehensive metrics for measuring the performance of each component in your RAG system, visualizations for comparing performance across time as the system changes, and workflows for creating benchmark question-answer sets and reviewing LLM responses. Tonic Validate shines a light on how your RAG system is truly performing, enabling continuous performance monitoring of your production RAG systems. You can <a href="https://www.tonic.ai/validate" rel="noopener ugc nofollow" target="_blank">learn more</a> and <a href="https://validate.tonic.ai/" rel="noopener ugc nofollow" target="_blank">sign up for a free account</a>.</p><h1>Setting up LlamaIndex</h1><p>To get started, let’s create an example RAG system for us to test. In this case, LlamaIndex provides a tool called <code class="cw pq pr ps pt b">create-llama</code> which can generate a full-stack RAG application for us. To install it, we need to make sure we have Node.JS installed and run the following command:</p><pre><span id="3b21" class="qc oo gt pt b bf qd qe l qf qg">npx create-llama@latest</span></pre><p>This command will take you through a series of prompts. Here are the options to select for each prompt:</p><pre><span id="9596" class="qc oo gt pt b bf qd qe l qf qg">What is your project named? » llama-validate-demo
Which template would you like to use? » Chat without streaming
Which framework would you like to use? » FastAPI (Python)
Would you like to install dependencies automatically? » No
Which model would you like to use? » gpt-4–1106-preview
Which data source would you like to use? » Use an example PDF
Would you like to use a vector database? » No, just store the data in the file system</span></pre><p>Once these options are selected, your project should be created in a folder called <code class="cw pq pr ps pt b">llama-validate-demo</code>. For this demo, we are going to replace the example PDF <code class="cw pq pr ps pt b">create-llama</code> provides with our own larger dataset. The dataset consists of a collection of essays from Paul Graham’s blog. This should more closely replicate a real world scenario where a company runs RAG on a larger internal dataset. To add the essays, download them from <a href="https://github.com/TonicAI/llama-validate-demo/blob/main/data.zip" rel="noopener ugc nofollow" target="_blank">our Github</a> and unzip them inside the root folder of your created project. Make sure the unzipped folder is named <code class="cw pq pr ps pt b">data</code>. Be sure to delete any existing files in the folder before copying the new dataset.</p><p>After you have the essays in the right directory, you can set up your OpenAI API key by setting it as an environment variable called <code class="cw pq pr ps pt b">OPENAI_API_KEY</code>. You can do this either via setting the environment variable system wide or by creating a <code class="cw pq pr ps pt b">.env</code> file in the root folder of your <code class="cw pq pr ps pt b">create-llama</code> project. Then you can run the following commands in the root folder for your <code class="cw pq pr ps pt b">create-llama</code> project:</p><pre><span id="c038" class="qc oo gt pt b bf qd qe l qf qg">poetry install
poetry shell
python app/engine/generate.py</span></pre><p>This will install the dependencies and generate the RAG embeddings for the Paul Graham essays. After this, you can run the chatbot with:</p><pre><span id="c0ce" class="qc oo gt pt b bf qd qe l qf qg">python main.py</span></pre><p>To test the chatbot, you can send a request via curl:</p><pre><span id="afef" class="qc oo gt pt b bf qd qe l qf qg">curl - location <span class="hljs-string">'localhost:8000/api/chat'</span> \
 - header <span class="hljs-string">'Content-Type: application/json'</span> \
 - data <span class="hljs-string">'{ "messages": [{ "role": "user", "content": "In the early days, how were the Airbnb founders financing their startup?" }] }'</span></span></pre><p>LlamaIndex will then return a response:</p><pre><span id="4c5c" class="qc oo gt pt b bf qd qe l qf qg">{
    <span class="hljs-string">"result"</span>: {
        <span class="hljs-string">"role"</span>: <span class="hljs-string">"assistant"</span>,
        <span class="hljs-string">"content"</span>: <span class="hljs-string">"In the early days, the Airbnb founders financed their startup by creating and selling themed breakfast cereals. They created limited-edition cereal boxes, such as \"Obama O's\" and \"Cap'n McCain's,\" during the 2008 U.S. presidential election, which became a collectible and helped them raise funds for their company. This creative approach to funding allowed them to sustain the business in its initial phase before securing more traditional forms of investment."</span>
    }
}</span></pre><p>Finally, in <code class="cw pq pr ps pt b">llama-validate-demo/app/api/routers/chat.py</code> we want to replace the <code class="cw pq pr ps pt b">return _Result</code> line at the end of the chat function with the following.</p><pre><span id="39ce" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-keyword">return</span> _Result(
    result=_Message(
        role=<span class="hljs-title class_">MessageRole</span>.<span class="hljs-variable constant_">ASSISTANT</span>,
        content=response.response,
        context=[x.text <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> response.source_nodes]
    )
)</span></pre><p>This allows the LlamaIndex API to return the RAG context that was used to answer the question asked. Now, we can move on to setting up Tonic Validate!</p><h1>Setting up Tonic Validate</h1><p>To set up Tonic Validate, first install it via poetry:</p><pre><span id="9a05" class="qc oo gt pt b bf qd qe l qf qg">poetry add tonic-validate</span></pre><p>Now, we can create our tests for Tonic Validate. To get started, create a file inside <code class="cw pq pr ps pt b">llama-validate-demo/tests</code> called <code class="cw pq pr ps pt b">validate_test.py</code>. We will also need to create a list of test questions and answers which you can find <a href="https://github.com/TonicAI/llama-validate-demo/blob/main/tests/qa_pairs.json" rel="noopener ugc nofollow" target="_blank">here</a>. Alternatively, you can also use the Tonic Validate UI to create the test set and call it via the SDK — we’ll be adding a feature to help generate these benchmarks using synthetic data to make this process even easier. Download the <code class="cw pq pr ps pt b">qa_pairs.json</code> file from the link and paste it into <code class="cw pq pr ps pt b">llama-validate-demo/tests</code>. Once we have both of these files, we can add the following code into <code class="cw pq pr ps pt b">validate_test.py</code>.</p><pre><span id="2bc6" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> tonic_validate <span class="hljs-keyword">import</span> ValidateApi
<span class="hljs-keyword">from</span> tonic_validate.metrics <span class="hljs-keyword">import</span> AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric
<span class="hljs-keyword">from</span> llama_index.evaluation <span class="hljs-keyword">import</span> TonicValidateEvaluator
<span class="hljs-keyword">import</span> requests

<span class="hljs-keyword">from</span> dotenv <span class="hljs-keyword">import</span> load_dotenv

load_dotenv()

<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_llm_response</span>(<span class="hljs-params">prompt</span>):
    url = <span class="hljs-string">"http://localhost:8000/api/chat"</span>

    payload = json.dumps({
        <span class="hljs-string">"messages"</span>: [
            {
                <span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>,
                <span class="hljs-string">"content"</span>: prompt
            }
        ]
    })
    headers = { <span class="hljs-string">'Content-Type'</span>: <span class="hljs-string">'application/json'</span> }
    response = requests.request(<span class="hljs-string">"POST"</span>, url, headers=headers, data=payload).json()
    result = response[<span class="hljs-string">'result'</span>]
    <span class="hljs-keyword">return</span> result[<span class="hljs-string">'content'</span>], result[<span class="hljs-string">'context'</span>]</span></pre><p>This code sets up the dependency imports and also specifies a <code class="cw pq pr ps pt b">get_llm_response</code> function which sends a request to the LlamaIndex API server we set up earlier to get a response. Now, let’s create a function that gets the list of questions to ask LlamaIndex for our testing.</p><pre><span id="d879" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_q_and_a</span>():
    <span class="hljs-comment"># Load qa_pairs.json</span>
    qa_pairs = json.load(<span class="hljs-built_in">open</span>(<span class="hljs-string">'./tests/qa_pairs.json'</span>))
    <span class="hljs-keyword">return</span> ([x[<span class="hljs-string">'question'</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> qa_pairs], [x[<span class="hljs-string">'answer'</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> qa_pairs])</span></pre><p>This function gets the question-answer pairs from our json file. The questions are what we will ask the RAG system and the answers are the correct answers for those questions. For instance, if the question was “What is the capital of France?” then the answer would be “Paris”.</p><p>Next, we can add the code that queries LlamaIndex:</p><pre><span id="100e" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_responses</span>(<span class="hljs-params">questions</span>):
    llm_answers = []
    context_lists = []
    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> questions:
        llm_answer, llm_context_list = get_llm_response(item)
        llm_answers.append(llm_answer)
        context_lists.append(llm_context_list)
    <span class="hljs-keyword">return</span> (llm_answers, context_lists)</span></pre><p>This code iterates over the questions, queries LlamaIndex, and logs each response into an array. We have two arrays. One is the actual answer from LlamaIndex. The other is a list of the snippets of text (called the context list) that LlamaIndex provided to help the LLM answer the question.</p><p>Now we have a list of LLM responses generated from a list of test questions. Let’s score them:</p><pre><span id="ff5c" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-keyword">def</span> <span class="hljs-title function_">score_run</span>(<span class="hljs-params">questions, context_lists, reference_answers, llm_answers</span>):
    metrics = [
        AnswerSimilarityMetric(),
        RetrievalPrecisionMetric(),
        AugmentationPrecisionMetric(),
        AnswerConsistencyMetric()
    ]
    scorer = TonicValidateEvaluator(metrics, model_evaluator=<span class="hljs-string">"gpt-4-1106-preview"</span>)
    run = scorer.evaluate_run(
        questions, llm_answers, context_lists, reference_answers
    )
    <span class="hljs-keyword">return</span> run, metrics</span></pre><p>We first need to define the metrics in Tonic Validate that we want to use. You can find a list of available metrics and their definitions <a href="https://github.com/TonicAI/tonic_validate?tab=readme-ov-file#tonic-validate-metrics" rel="noopener ugc nofollow" target="_blank">here</a>. After we create the metrics, we can take advantage of Tonic Validate’s integration with LlamaIndex. Since Tonic Validate is built into LlamaIndex’s evaluation framework as an evaluator, all we need to do is create a <code class="cw pq pr ps pt b">TonicValidateEvaluator</code>, which scores the LlamaIndex responses across the chosen metrics. Then we return the results along with the metrics.</p><p>Finally, we can create our test function for pytest which evaluates LlamaIndex.</p><pre><span id="9875" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_llama_index</span>():
    questions, reference_answers = get_q_and_a()
    llm_answers, context_lists = get_responses(questions)
    run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)
    <span class="hljs-comment"># Upload results to web ui</span>
    validate_api = ValidateApi()
    <span class="hljs-comment"># Get project id from env</span>
    project_id = os.getenv(<span class="hljs-string">"PROJECT_ID"</span>)
    validate_api.upload_run(project_id, run)</span></pre><p>This runs all the code we’ve written to get the scores and then sends them to Tonic Validate’s API to visualize in the UI. In order to send the metrics for each run to the UI, you need to sign up for a free account, which you can do <a href="https://validate.tonic.ai/" rel="noopener ugc nofollow" target="_blank">here</a>. I highly recommend utilizing the UI to make visualizing and monitoring performance changes a breeze. Once you sign up, you will be taken through a short onboarding process where you create an API key and a project. The API key should be stored in an environment variable called <code class="cw pq pr ps pt b">TONIC_VALIDATE_API_KEY</code> and the project ID in an environment variable called <code class="cw pq pr ps pt b">PROJECT_ID</code>.</p><p>Once you have set up your account and configured your environment variables, you can run the test via the following commands:</p><pre><span id="0add" class="qc oo gt pt b bf qd qe l qf qg">poetry shell
pytest</span></pre><p>You can also make the test fail if the metrics score too low. This would be a pertinent step to add in if you want to avoid introducing breaking changes to a production RAG system; for example, if you update the model version and the answer similarity score suddenly drop below a certain threshold, you could have the test fail and issue a warning to debug the issue.</p><pre><span id="3d49" class="qc oo gt pt b bf qd qe l qf qg"><span class="hljs-comment"># Check none of the metrics scored too low    </span>
<span class="hljs-keyword">for</span> metric <span class="hljs-keyword">in</span> metrics:
    <span class="hljs-keyword">if</span> metric.name == AnswerSimilarityMetric.name:
        <span class="hljs-keyword">assert</span> run.overall_scores[metric.name] &amp;gt;= <span class="hljs-number">3.5</span>
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">assert</span> run.overall_scores[metric.name] &amp;gt;= <span class="hljs-number">0.7</span></span></pre><h1>Setting up GitHub Actions</h1><p>With LlamaIndex and Tonic Validate configured, we have the ability to connect data to an LLM and measure the accuracy of LLM responses. You can push this setup into production and have a functional chatbot. As is common in modern software development practices, you will likely continue to fix bugs, make improvements, and add new data or features to your RAG system. Before pushing to production, QA testing is in place to catch any changes to your code that may introduce unintended effects. For example, adding a new dataset or updating an LLM to a new version could lead to changes in the quality of responses. One approach, the one that we recommend, for adding QA testing for your RAG system is to use GitHub Actions to establish an integration test using Tonic Validate that checks the LLM response quality of your RAG system, allowing you to catch and rectify any performance degradation before it is pushed into production.</p><p>To set up Tonic Validate to run in GitHub Actions, we can create a folder <code class="cw pq pr ps pt b">llama-validate-demo/.github/workflows</code> with a file called <code class="cw pq pr ps pt b">python-app.yml</code>. In this file, we will include the following code configuration that defines the integration test workflow:</p><pre><span id="6437" class="qc oo gt pt b bf qd qe l qf qg"># This workflow will install Python dependencies and run tests with LlamaIndex

name: Python application

on:
  push:
    branches: [ <span class="hljs-string">"main"</span> ]
  pull_request:
    branches: [ <span class="hljs-string">"main"</span> ]

permissions:
  contents: read

jobs:
  build:

    runs-on: ubuntu-latest
    environment: Actions

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python <span class="hljs-number">3.11</span>
      uses: actions/setup-python@v3
      with:
        python-version: <span class="hljs-string">"3.11"</span>
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install poetry
        poetry config virtualenvs.create <span class="hljs-literal">false</span>
        poetry install --no-root --no-dev --no-directory
    - name: Set PYTHONPATH
      run: echo <span class="hljs-string">"PYTHONPATH=$GITHUB_WORKSPACE"</span> &amp;gt;&amp;gt; $GITHUB_ENV
    - name: Set up vector index
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python app/engine/generate.py
    - name: Start up test server
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        MODEL: gpt<span class="hljs-number">-4</span><span class="hljs-number">-1106</span>-preview
      run: |
        python main.py &amp;amp;
        sleep <span class="hljs-number">10</span>
    - name: Test with pytest
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        TONIC_VALIDATE_API_KEY: ${{ secrets.TONIC_VALIDATE_API_KEY }}
        PROJECT_ID: ${{ secrets.PROJECT_ID }}
      run: |
        pytest</span></pre><p>This configures GitHub to run the tests defined with Tonic Validate upon every commit. The GitHub Actions configuration downloads the repo, sets up the dependencies, generates the embeddings, and then starts up the test server and runs the test.</p><p>After this file is set up, we just need to set our secrets in GitHub. In GitHub, go to <code class="cw pq pr ps pt b">Settings &gt; Secrets and variables &gt; Actions</code> for your repo and create a secret called <code class="cw pq pr ps pt b">OPENAI_API_KEY</code>, <code class="cw pq pr ps pt b">TONIC_VALIDATE_API_KEY</code>, and <code class="cw pq pr ps pt b">PROJECT_ID</code>. These values will all be the same as the values you set earlier. Now your GitHub actions set up is complete and you can proactively monitor changes to your RAG system during development and before going into production.</p><p>Try pushing some commits to it and watch it run! To view the results, go to <a href="https://validate.tonic.ai/" rel="noopener ugc nofollow" target="_blank">Tonic Validate’s web app</a> and navigate to your project. You should see a view like this that shows recent metrics and their evolution over time:</p><figure><img src="/blog/images/0*Vyqc1eqUdU2rtH8A" alt="" width="700" height="434"></figure><p>Now you and your team can track your RAG system’s performance over time to make sure there aren’t any dips in performance! Thank you for reading and make sure to check out Tonic Validate!</p><p><em class="ol">For more information on Tonic Validate, visit our </em><a href="https://www.tonic.ai/validate" rel="noopener ugc nofollow" target="_blank"><em class="ol">website</em></a><em class="ol"> and sign up for a </em><a href="https://validate.tonic.ai/signup" rel="noopener ugc nofollow" target="_blank"><em class="ol">free account</em></a><em class="ol"> today. You can also visit our GitHub </em><a href="https://github.com/TonicAI/llama-validate-demo" rel="noopener ugc nofollow" target="_blank"><em class="ol">page</em></a><em class="ol"> to view all of the code used in this post and the rest of our SDK. Our LlamaIndex integration is available </em><a href="https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/TonicValidateEvaluators.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="ol">here</em></a><em class="ol">.</em></p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F05b9a3db3bdc76f63517ca88b44a45a344d5fd14-2700x1550.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F05b9a3db3bdc76f63517ca88b44a45a344d5fd14-2700x1550.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F05b9a3db3bdc76f63517ca88b44a45a344d5fd14-2700x1550.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications">Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-11</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"04cb7cd4-57be-494f-b5ca-ce1093aa8ee3","_rev":"05dtDS0H5iRVsxYMarYvFZ","_type":"blogPost","_updatedAt":"2025-05-21T20:36:15Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:51:07Z","_id":"2ff930ea-5f71-436b-8cc2-568b102892a8","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Ethan P","slug":{"_type":"slug","current":"ethan-p"}}],"featured":false,"htmlContent":"\u003cp\u003e\u003cem class=\"ol\"\u003eIn this technical walkthrough, we’ll highlight the functionality of Tonic Validate and its integration with LlamaIndex. Sign up for a free account \u003c/em\u003e\u003ca href=\"https://www.tonic.ai/validate\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem class=\"ol\"\u003ehere\u003c/em\u003e\u003c/a\u003e\u003cem class=\"ol\"\u003e before you start.\u003c/em\u003e\u003c/p\u003e\u003ch1\u003eIntroduction\u003c/h1\u003e\u003cp\u003eAs enterprise adoption of generative AI technologies continues, companies are turning to Retrieval Augmented Generation (RAG) systems to extend the application of large-language models (LLMs) to their private data (e.g., a chatbot that can answer questions based on internal technical documentation). Traditionally in software engineering, companies have placed a high emphasis on implementing continuous integration tests to ensure systems remain performant when updates are made. More recently, these same principles have been applied to machine learning models in production.\u003c/p\u003e\u003cp\u003eHowever, as a young technology, RAG currently lacks best practices for integration tests to ensure breaking changes aren’t introduced to the production system. In this article, we will demonstrate how you can use Tonic Validate’s RAG performance monitoring capabilities, LlamaIndex, and GitHub Actions to create novel integration tests that alert you to changes in RAG system performance. To make things easy, Tonic Validate is available natively within LlamaIndex’s core library — you can read more about that \u003ca href=\"https://www.tonic.ai/blog/tonic-ai-and-llamaindex-join-forces-to-help-developers-build-more-performant-rag-systems\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003ch1\u003eWhat is Tonic Validate?\u003c/h1\u003e\u003cp\u003eTonic Validate is a RAG benchmarking and evaluation platform that monitors performance of RAG systems in production. It provides comprehensive metrics for measuring the performance of each component in your RAG system, visualizations for comparing performance across time as the system changes, and workflows for creating benchmark question-answer sets and reviewing LLM responses. Tonic Validate shines a light on how your RAG system is truly performing, enabling continuous performance monitoring of your production RAG systems. You can \u003ca href=\"https://www.tonic.ai/validate\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003elearn more\u003c/a\u003e and \u003ca href=\"https://validate.tonic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esign up for a free account\u003c/a\u003e.\u003c/p\u003e\u003ch1\u003eSetting up LlamaIndex\u003c/h1\u003e\u003cp\u003eTo get started, let’s create an example RAG system for us to test. In this case, LlamaIndex provides a tool called \u003ccode class=\"cw pq pr ps pt b\"\u003ecreate-llama\u003c/code\u003e which can generate a full-stack RAG application for us. To install it, we need to make sure we have Node.JS installed and run the following command:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"3b21\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003enpx create-llama@latest\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis command will take you through a series of prompts. Here are the options to select for each prompt:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9596\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003eWhat is your project named? » llama-validate-demo\nWhich template would you like to use? » Chat without streaming\nWhich framework would you like to use? » FastAPI (Python)\nWould you like to install dependencies automatically? » No\nWhich model would you like to use? » gpt-4–1106-preview\nWhich data source would you like to use? » Use an example PDF\nWould you like to use a vector database? » No, just store the data in the file system\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOnce these options are selected, your project should be created in a folder called \u003ccode class=\"cw pq pr ps pt b\"\u003ellama-validate-demo\u003c/code\u003e. For this demo, we are going to replace the example PDF \u003ccode class=\"cw pq pr ps pt b\"\u003ecreate-llama\u003c/code\u003e provides with our own larger dataset. The dataset consists of a collection of essays from Paul Graham’s blog. This should more closely replicate a real world scenario where a company runs RAG on a larger internal dataset. To add the essays, download them from \u003ca href=\"https://github.com/TonicAI/llama-validate-demo/blob/main/data.zip\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eour Github\u003c/a\u003e and unzip them inside the root folder of your created project. Make sure the unzipped folder is named \u003ccode class=\"cw pq pr ps pt b\"\u003edata\u003c/code\u003e. Be sure to delete any existing files in the folder before copying the new dataset.\u003c/p\u003e\u003cp\u003eAfter you have the essays in the right directory, you can set up your OpenAI API key by setting it as an environment variable called \u003ccode class=\"cw pq pr ps pt b\"\u003eOPENAI_API_KEY\u003c/code\u003e. You can do this either via setting the environment variable system wide or by creating a \u003ccode class=\"cw pq pr ps pt b\"\u003e.env\u003c/code\u003e file in the root folder of your \u003ccode class=\"cw pq pr ps pt b\"\u003ecreate-llama\u003c/code\u003e project. Then you can run the following commands in the root folder for your \u003ccode class=\"cw pq pr ps pt b\"\u003ecreate-llama\u003c/code\u003e project:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c038\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003epoetry install\npoetry shell\npython app/engine/generate.py\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis will install the dependencies and generate the RAG embeddings for the Paul Graham essays. After this, you can run the chatbot with:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c0ce\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003epython main.py\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eTo test the chatbot, you can send a request via curl:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"afef\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003ecurl - location \u003cspan class=\"hljs-string\"\u003e'localhost:8000/api/chat'\u003c/span\u003e \\\n - header \u003cspan class=\"hljs-string\"\u003e'Content-Type: application/json'\u003c/span\u003e \\\n - data \u003cspan class=\"hljs-string\"\u003e'{ \"messages\": [{ \"role\": \"user\", \"content\": \"In the early days, how were the Airbnb founders financing their startup?\" }] }'\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eLlamaIndex will then return a response:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4c5c\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e{\n    \u003cspan class=\"hljs-string\"\u003e\"result\"\u003c/span\u003e: {\n        \u003cspan class=\"hljs-string\"\u003e\"role\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"assistant\"\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e\"content\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"In the early days, the Airbnb founders financed their startup by creating and selling themed breakfast cereals. They created limited-edition cereal boxes, such as \\\"Obama O's\\\" and \\\"Cap'n McCain's,\\\" during the 2008 U.S. presidential election, which became a collectible and helped them raise funds for their company. This creative approach to funding allowed them to sustain the business in its initial phase before securing more traditional forms of investment.\"\u003c/span\u003e\n    }\n}\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eFinally, in \u003ccode class=\"cw pq pr ps pt b\"\u003ellama-validate-demo/app/api/routers/chat.py\u003c/code\u003e we want to replace the \u003ccode class=\"cw pq pr ps pt b\"\u003ereturn _Result\u003c/code\u003e line at the end of the chat function with the following.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"39ce\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e _Result(\n    result=_Message(\n        role=\u003cspan class=\"hljs-title class_\"\u003eMessageRole\u003c/span\u003e.\u003cspan class=\"hljs-variable constant_\"\u003eASSISTANT\u003c/span\u003e,\n        content=response.response,\n        context=[x.text \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e response.source_nodes]\n    )\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis allows the LlamaIndex API to return the RAG context that was used to answer the question asked. Now, we can move on to setting up Tonic Validate!\u003c/p\u003e\u003ch1\u003eSetting up Tonic Validate\u003c/h1\u003e\u003cp\u003eTo set up Tonic Validate, first install it via poetry:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9a05\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003epoetry add tonic-validate\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow, we can create our tests for Tonic Validate. To get started, create a file inside \u003ccode class=\"cw pq pr ps pt b\"\u003ellama-validate-demo/tests\u003c/code\u003e called \u003ccode class=\"cw pq pr ps pt b\"\u003evalidate_test.py\u003c/code\u003e. We will also need to create a list of test questions and answers which you can find \u003ca href=\"https://github.com/TonicAI/llama-validate-demo/blob/main/tests/qa_pairs.json\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e. Alternatively, you can also use the Tonic Validate UI to create the test set and call it via the SDK — we’ll be adding a feature to help generate these benchmarks using synthetic data to make this process even easier. Download the \u003ccode class=\"cw pq pr ps pt b\"\u003eqa_pairs.json\u003c/code\u003e file from the link and paste it into \u003ccode class=\"cw pq pr ps pt b\"\u003ellama-validate-demo/tests\u003c/code\u003e. Once we have both of these files, we can add the following code into \u003ccode class=\"cw pq pr ps pt b\"\u003evalidate_test.py\u003c/code\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"2bc6\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tonic_validate \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ValidateApi\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e tonic_validate.metrics \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e AnswerSimilarityMetric, RetrievalPrecisionMetric, AugmentationPrecisionMetric, AnswerConsistencyMetric\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.evaluation \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e TonicValidateEvaluator\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e requests\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e dotenv \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_dotenv\n\nload_dotenv()\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_llm_response\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eprompt\u003c/span\u003e):\n    url = \u003cspan class=\"hljs-string\"\u003e\"http://localhost:8000/api/chat\"\u003c/span\u003e\n\n    payload = json.dumps({\n        \u003cspan class=\"hljs-string\"\u003e\"messages\"\u003c/span\u003e: [\n            {\n                \u003cspan class=\"hljs-string\"\u003e\"role\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"user\"\u003c/span\u003e,\n                \u003cspan class=\"hljs-string\"\u003e\"content\"\u003c/span\u003e: prompt\n            }\n        ]\n    })\n    headers = { \u003cspan class=\"hljs-string\"\u003e'Content-Type'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'application/json'\u003c/span\u003e }\n    response = requests.request(\u003cspan class=\"hljs-string\"\u003e\"POST\"\u003c/span\u003e, url, headers=headers, data=payload).json()\n    result = response[\u003cspan class=\"hljs-string\"\u003e'result'\u003c/span\u003e]\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e result[\u003cspan class=\"hljs-string\"\u003e'content'\u003c/span\u003e], result[\u003cspan class=\"hljs-string\"\u003e'context'\u003c/span\u003e]\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis code sets up the dependency imports and also specifies a \u003ccode class=\"cw pq pr ps pt b\"\u003eget_llm_response\u003c/code\u003e function which sends a request to the LlamaIndex API server we set up earlier to get a response. Now, let’s create a function that gets the list of questions to ask LlamaIndex for our testing.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d879\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_q_and_a\u003c/span\u003e():\n    \u003cspan class=\"hljs-comment\"\u003e# Load qa_pairs.json\u003c/span\u003e\n    qa_pairs = json.load(\u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e'./tests/qa_pairs.json'\u003c/span\u003e))\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e ([x[\u003cspan class=\"hljs-string\"\u003e'question'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e qa_pairs], [x[\u003cspan class=\"hljs-string\"\u003e'answer'\u003c/span\u003e] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e x \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e qa_pairs])\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis function gets the question-answer pairs from our json file. The questions are what we will ask the RAG system and the answers are the correct answers for those questions. For instance, if the question was “What is the capital of France?” then the answer would be “Paris”.\u003c/p\u003e\u003cp\u003eNext, we can add the code that queries LlamaIndex:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"100e\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_responses\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003equestions\u003c/span\u003e):\n    llm_answers = []\n    context_lists = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e item \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e questions:\n        llm_answer, llm_context_list = get_llm_response(item)\n        llm_answers.append(llm_answer)\n        context_lists.append(llm_context_list)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e (llm_answers, context_lists)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis code iterates over the questions, queries LlamaIndex, and logs each response into an array. We have two arrays. One is the actual answer from LlamaIndex. The other is a list of the snippets of text (called the context list) that LlamaIndex provided to help the LLM answer the question.\u003c/p\u003e\u003cp\u003eNow we have a list of LLM responses generated from a list of test questions. Let’s score them:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ff5c\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003escore_run\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003equestions, context_lists, reference_answers, llm_answers\u003c/span\u003e):\n    metrics = [\n        AnswerSimilarityMetric(),\n        RetrievalPrecisionMetric(),\n        AugmentationPrecisionMetric(),\n        AnswerConsistencyMetric()\n    ]\n    scorer = TonicValidateEvaluator(metrics, model_evaluator=\u003cspan class=\"hljs-string\"\u003e\"gpt-4-1106-preview\"\u003c/span\u003e)\n    run = scorer.evaluate_run(\n        questions, llm_answers, context_lists, reference_answers\n    )\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e run, metrics\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe first need to define the metrics in Tonic Validate that we want to use. You can find a list of available metrics and their definitions \u003ca href=\"https://github.com/TonicAI/tonic_validate?tab=readme-ov-file#tonic-validate-metrics\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e. After we create the metrics, we can take advantage of Tonic Validate’s integration with LlamaIndex. Since Tonic Validate is built into LlamaIndex’s evaluation framework as an evaluator, all we need to do is create a \u003ccode class=\"cw pq pr ps pt b\"\u003eTonicValidateEvaluator\u003c/code\u003e, which scores the LlamaIndex responses across the chosen metrics. Then we return the results along with the metrics.\u003c/p\u003e\u003cp\u003eFinally, we can create our test function for pytest which evaluates LlamaIndex.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9875\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003etest_llama_index\u003c/span\u003e():\n    questions, reference_answers = get_q_and_a()\n    llm_answers, context_lists = get_responses(questions)\n    run, metrics = score_run(questions, context_lists, reference_answers, llm_answers)\n    \u003cspan class=\"hljs-comment\"\u003e# Upload results to web ui\u003c/span\u003e\n    validate_api = ValidateApi()\n    \u003cspan class=\"hljs-comment\"\u003e# Get project id from env\u003c/span\u003e\n    project_id = os.getenv(\u003cspan class=\"hljs-string\"\u003e\"PROJECT_ID\"\u003c/span\u003e)\n    validate_api.upload_run(project_id, run)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis runs all the code we’ve written to get the scores and then sends them to Tonic Validate’s API to visualize in the UI. In order to send the metrics for each run to the UI, you need to sign up for a free account, which you can do \u003ca href=\"https://validate.tonic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e. I highly recommend utilizing the UI to make visualizing and monitoring performance changes a breeze. Once you sign up, you will be taken through a short onboarding process where you create an API key and a project. The API key should be stored in an environment variable called \u003ccode class=\"cw pq pr ps pt b\"\u003eTONIC_VALIDATE_API_KEY\u003c/code\u003e and the project ID in an environment variable called \u003ccode class=\"cw pq pr ps pt b\"\u003ePROJECT_ID\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eOnce you have set up your account and configured your environment variables, you can run the test via the following commands:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0add\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003epoetry shell\npytest\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eYou can also make the test fail if the metrics score too low. This would be a pertinent step to add in if you want to avoid introducing breaking changes to a production RAG system; for example, if you update the model version and the answer similarity score suddenly drop below a certain threshold, you could have the test fail and issue a warning to debug the issue.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"3d49\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Check none of the metrics scored too low    \u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e metric \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e metrics:\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e metric.name == AnswerSimilarityMetric.name:\n        \u003cspan class=\"hljs-keyword\"\u003eassert\u003c/span\u003e run.overall_scores[metric.name] \u0026amp;gt;= \u003cspan class=\"hljs-number\"\u003e3.5\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003eassert\u003c/span\u003e run.overall_scores[metric.name] \u0026amp;gt;= \u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eSetting up GitHub Actions\u003c/h1\u003e\u003cp\u003eWith LlamaIndex and Tonic Validate configured, we have the ability to connect data to an LLM and measure the accuracy of LLM responses. You can push this setup into production and have a functional chatbot. As is common in modern software development practices, you will likely continue to fix bugs, make improvements, and add new data or features to your RAG system. Before pushing to production, QA testing is in place to catch any changes to your code that may introduce unintended effects. For example, adding a new dataset or updating an LLM to a new version could lead to changes in the quality of responses. One approach, the one that we recommend, for adding QA testing for your RAG system is to use GitHub Actions to establish an integration test using Tonic Validate that checks the LLM response quality of your RAG system, allowing you to catch and rectify any performance degradation before it is pushed into production.\u003c/p\u003e\u003cp\u003eTo set up Tonic Validate to run in GitHub Actions, we can create a folder \u003ccode class=\"cw pq pr ps pt b\"\u003ellama-validate-demo/.github/workflows\u003c/code\u003e with a file called \u003ccode class=\"cw pq pr ps pt b\"\u003epython-app.yml\u003c/code\u003e. In this file, we will include the following code configuration that defines the integration test workflow:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6437\" class=\"qc oo gt pt b bf qd qe l qf qg\"\u003e# This workflow will install Python dependencies and run tests with LlamaIndex\n\nname: Python application\n\non:\n  push:\n    branches: [ \u003cspan class=\"hljs-string\"\u003e\"main\"\u003c/span\u003e ]\n  pull_request:\n    branches: [ \u003cspan class=\"hljs-string\"\u003e\"main\"\u003c/span\u003e ]\n\npermissions:\n  contents: read\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    environment: Actions\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python \u003cspan class=\"hljs-number\"\u003e3.11\u003c/span\u003e\n      uses: actions/setup-python@v3\n      with:\n        python-version: \u003cspan class=\"hljs-string\"\u003e\"3.11\"\u003c/span\u003e\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install poetry\n        poetry config virtualenvs.create \u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e\n        poetry install --no-root --no-dev --no-directory\n    - name: Set PYTHONPATH\n      run: echo \u003cspan class=\"hljs-string\"\u003e\"PYTHONPATH=$GITHUB_WORKSPACE\"\u003c/span\u003e \u0026amp;gt;\u0026amp;gt; $GITHUB_ENV\n    - name: Set up vector index\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n      run: |\n        python app/engine/generate.py\n    - name: Start up test server\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        MODEL: gpt\u003cspan class=\"hljs-number\"\u003e-4\u003c/span\u003e\u003cspan class=\"hljs-number\"\u003e-1106\u003c/span\u003e-preview\n      run: |\n        python main.py \u0026amp;amp;\n        sleep \u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e\n    - name: Test with pytest\n      env:\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        TONIC_VALIDATE_API_KEY: ${{ secrets.TONIC_VALIDATE_API_KEY }}\n        PROJECT_ID: ${{ secrets.PROJECT_ID }}\n      run: |\n        pytest\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis configures GitHub to run the tests defined with Tonic Validate upon every commit. The GitHub Actions configuration downloads the repo, sets up the dependencies, generates the embeddings, and then starts up the test server and runs the test.\u003c/p\u003e\u003cp\u003eAfter this file is set up, we just need to set our secrets in GitHub. In GitHub, go to \u003ccode class=\"cw pq pr ps pt b\"\u003eSettings \u0026gt; Secrets and variables \u0026gt; Actions\u003c/code\u003e for your repo and create a secret called \u003ccode class=\"cw pq pr ps pt b\"\u003eOPENAI_API_KEY\u003c/code\u003e, \u003ccode class=\"cw pq pr ps pt b\"\u003eTONIC_VALIDATE_API_KEY\u003c/code\u003e, and \u003ccode class=\"cw pq pr ps pt b\"\u003ePROJECT_ID\u003c/code\u003e. These values will all be the same as the values you set earlier. Now your GitHub actions set up is complete and you can proactively monitor changes to your RAG system during development and before going into production.\u003c/p\u003e\u003cp\u003eTry pushing some commits to it and watch it run! To view the results, go to \u003ca href=\"https://validate.tonic.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTonic Validate’s web app\u003c/a\u003e and navigate to your project. You should see a view like this that shows recent metrics and their evolution over time:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/0*Vyqc1eqUdU2rtH8A\" alt=\"\" width=\"700\" height=\"434\"\u003e\u003c/figure\u003e\u003cp\u003eNow you and your team can track your RAG system’s performance over time to make sure there aren’t any dips in performance! Thank you for reading and make sure to check out Tonic Validate!\u003c/p\u003e\u003cp\u003e\u003cem class=\"ol\"\u003eFor more information on Tonic Validate, visit our \u003c/em\u003e\u003ca href=\"https://www.tonic.ai/validate\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem class=\"ol\"\u003ewebsite\u003c/em\u003e\u003c/a\u003e\u003cem class=\"ol\"\u003e and sign up for a \u003c/em\u003e\u003ca href=\"https://validate.tonic.ai/signup\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem class=\"ol\"\u003efree account\u003c/em\u003e\u003c/a\u003e\u003cem class=\"ol\"\u003e today. You can also visit our GitHub \u003c/em\u003e\u003ca href=\"https://github.com/TonicAI/llama-validate-demo\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem class=\"ol\"\u003epage\u003c/em\u003e\u003c/a\u003e\u003cem class=\"ol\"\u003e to view all of the code used in this post and the rest of our SDK. Our LlamaIndex integration is available \u003c/em\u003e\u003ca href=\"https://github.com/run-llama/llama_index/blob/main/docs/examples/evaluation/TonicValidateEvaluators.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cem class=\"ol\"\u003ehere\u003c/em\u003e\u003c/a\u003e\u003cem class=\"ol\"\u003e.\u003c/em\u003e\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-b06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/b06a05a3bcc74ecb4e604d300e649ba58fa54291-1600x897.png","publishedDate":"2024-01-26","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-05b9a3db3bdc76f63517ca88b44a45a344d5fd14-2700x1550-jpg","_type":"reference"}},"publishedDate":"2024-07-11","slug":"arize-ai-and-llamaindex-roll-out-joint-platform-for-evaluating-llm-applications","title":"Arize AI and LlamaIndex Roll Out Joint Platform for Evaluating LLM Applications"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"}],"slug":{"_type":"slug","current":"tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"b3e76bd9-2198-4a5d-85aa-5f2033b04955","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"evaluation"},"title":"Evaluation"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"510d3ffc-ff86-4405-9a4e-00771e43c8f3","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"github"},"title":"Github"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"6ed43e9f-c084-40fb-bc41-a7b5ddf99a4c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ci-cd-pipeline"},"title":"Ci Cd Pipeline"}],"title":"Tonic Validate x LlamaIndex: Implementing integration tests for LlamaIndex"},"publishedDate":"Invalid Date"},"params":{"slug":"tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"tonic-validate-x-llamaindex-implementing-integration-tests-for-llamaindex-43db50b76ed9"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>