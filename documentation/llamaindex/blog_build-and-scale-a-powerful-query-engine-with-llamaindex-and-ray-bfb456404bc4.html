<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Build and Scale a Powerful Query Engine with LlamaIndex and Ray — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Build and Scale a Powerful Query Engine with LlamaIndex and Ray — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Build and Scale a Powerful Query Engine with LlamaIndex and Ray — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Build and Scale a Powerful Query Engine with LlamaIndex and Ray — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="278.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2023-06-27</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Build and Scale a Powerful Query Engine with LlamaIndex and Ray</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/nlp"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">NLP</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/distributed-systems"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Distributed Systems</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/large-language-models"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Large Language Models</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/parallel-computing"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Parallel Computing</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>Co-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at Anyscale)</p><p>(<strong>note: </strong>this is cross-posted from the original blog post on Anyscale’s website. <a href="https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray" rel="noopener ugc nofollow" target="_blank">Check it out here</a>!)</p><p>In this blog, we showcase how you can use LlamaIndex and Ray to build a query engine to answer questions and generate insights about Ray itself, given its documentation and blog posts.</p><p>We’ll give a quick introduction of LlamaIndex + Ray, and then walk through a step-by-step tutorial on building and deploying this query engine. We make use of both Ray Datasets to parallelize building indices as well as Ray Serve to build deployments.</p><iframe width="560" height="315" src="https://www.youtube.com/embed/Vd_8lS1iDBg?si=QKPkWLLWz46zeiCf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><h1>Introduction</h1><p>Large Language Models (LLMs) offer the promise of allowing users to extract complex insights from their unstructured text data. Retrieval-augmented generation pipelines have emerged as a common pattern for developing LLM applications allowing users to effectively perform semantic search over a collection of documents.</p><figure><figcaption class="pu fe pv nz oa pw px be b bf z dt"><em class="py">Example of retrieval augmented generation. Relevant context is pulled from a set of documents and included in the LLM input prompt.</em></figcaption><img src="/blog/images/1*euY0oGTyi5vnt2aqJ9hFjw.png" alt="" width="700" height="282"></figure><p>However, when productionizing these applications over many different data sources, there are a few challenges:</p><ol><li>Tooling for indexing data from many different data sources</li><li>Handling complex queries over different data sources</li><li>Scaling indexing to thousands or millions of documents</li><li>Deploying a scalable LLM application into production</li></ol><p>Here, we showcase how <a href="https://gpt-index.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a> and <a href="https://docs.ray.io/en/latest/" rel="noopener ugc nofollow" target="_blank">Ray</a> are the perfect setup for this task.</p><p>LlamaIndex is a data framework for building LLM applications, and solves Challenges #1 and #2. It also provides a comprehensive toolkit allowing users to connect their private data with a language model. It offers a variety of tools to help users first ingest and index their data — convert different formats of unstructured and structured data into a format that the language model can use, and query their private data.</p><p>Ray is a powerful framework for scalable AI that solves Challenges #3 and #4. We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud.</p><p>More specifically, we showcase a very relevant use case — highlighting Ray features that are present in both the documentation as well as the Ray blog posts!</p><h1>Data Ingestion and Embedding Pipeline</h1><p>We use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below.</p><p>Code for this part of the blog is <a href="https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py" rel="noopener ugc nofollow" target="_blank">available here</a>.</p><figure><figcaption class="pu fe pv nz oa pw px be b bf z dt"><em class="py">Sequential pipeline with “ingest”, “parse” and “embed” stages. Files are processed sequentially resulting in poor hardware utilization and long computation time.</em></figcaption><img src="/blog/images/1*uQpJXp_A-1-AOwz3LMyl3Q.png" alt="" width="700" height="320"></figure><figure><figcaption class="pu fe pv nz oa pw px be b bf z dt"><em class="py">Parallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized.</em></figcaption><img src="/blog/images/1*im0zUrKp8ABSRrZlic0L8Q.png" alt="" width="700" height="325"></figure><h1>Load Data</h1><p>We start by ingesting these two sources of data. We first fetch both data sources and download the HTML files.</p><p>We then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API’s, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by <a href="https://github.com/Unstructured-IO/unstructured" rel="noopener ugc nofollow" target="_blank">Unstructured</a>.</p><pre><span id="ff6e" class="ql or gt qi b bf qm qn l qo qp"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span>
<span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path

<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> download_loader
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> Document

<span class="hljs-comment"># Step 1: Logic for loading and parsing the files into llama_index documents.</span>
UnstructuredReader = download_loader(<span class="hljs-string">"UnstructuredReader"</span>)
loader = UnstructuredReader()

<span class="hljs-keyword">def</span> <span class="hljs-title function_">load_and_parse_files</span>(<span class="hljs-params">file_row: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, Path]</span>) -&amp;gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, Document]:
    documents = []
    file = file_row[<span class="hljs-string">"path"</span>]
    <span class="hljs-keyword">if</span> file.is_dir():
        <span class="hljs-keyword">return</span> []
    <span class="hljs-comment"># Skip all non-html files like png, jpg, etc.</span>
    <span class="hljs-keyword">if</span> file.suffix.lower() == <span class="hljs-string">".html"</span>:
        loaded_doc = loader.load_data(file=file, split_documents=<span class="hljs-literal">False</span>)
        loaded_doc[<span class="hljs-number">0</span>].extra_info = {<span class="hljs-string">"path"</span>: <span class="hljs-built_in">str</span>(file)}
        documents.extend(loaded_doc)
    <span class="hljs-keyword">return</span> [{<span class="hljs-string">"doc"</span>: doc} <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> documents]</span></pre><p>Unstructured offers a robust suite of parsing tools on top of various files. It is able to help sanitize HTML documents by stripping out information like tags and formatting the text accordingly.</p><h2>Scaling Data Ingest</h2><figure><img src="/blog/images/1*prVxsm5aR-a5IQItiiWZXQ.png" alt="" width="700" height="325"></figure><p>Since we have many HTML documents to process, loading/processing each one serially is inefficient and slow. This is an opportunity to use Ray and distribute execution of the `load_and_parse_files` method across multiple CPUs or GPUs.</p><pre><span id="7248" class="ql or gt qi b bf qm qn l qo qp">import ray

<span class="hljs-comment"># Get the paths for the locally downloaded documentation.</span>
all_docs_gen = <span class="hljs-title class_">Path</span>(<span class="hljs-string">"./docs.ray.io/"</span>).rglob(<span class="hljs-string">"*"</span>)
all_docs = [{<span class="hljs-string">"path"</span>: doc.resolve()} <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> all_docs_gen]

<span class="hljs-comment"># Create the Ray Dataset pipeline</span>
ds = ray.data.from_items(all_docs)

<span class="hljs-comment"># Use `flat_map` since there is a 1:N relationship.</span>
<span class="hljs-comment"># Each filepath returns multiple documents.</span>
loaded_docs = ds.flat_map(load_and_parse_files)</span></pre><h1>Parse Files</h1><p>Now that we’ve loaded the documents, the next step is to parse them into Node objects — a “Node” object represents a more granular chunk of text, derived from the source documents. Node objects can be used in the input prompt as context; by setting a small enough chunk size, we can make sure that inserting Node objects do not overflow the context limits.</p><p>We define a function called `convert_documents_into_nodes` which converts documents into nodes using a simple text splitting strategy.</p><pre><span id="4064" class="ql or gt qi b bf qm qn l qo qp"><span class="hljs-comment"># Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.</span>
<span class="hljs-keyword">from</span> llama_index.node_parser <span class="hljs-keyword">import</span> SimpleNodeParser
<span class="hljs-keyword">from</span> llama_index.data_structs <span class="hljs-keyword">import</span> Node

<span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_documents_into_nodes</span>(<span class="hljs-params">documents: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, Document]</span>) -&amp;gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, Node]:
    parser = SimpleNodeParser()
    document = documents[<span class="hljs-string">"doc"</span>]
    nodes = parser.get_nodes_from_documents([document]) 
    <span class="hljs-keyword">return</span> [{<span class="hljs-string">"node"</span>: node} <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> nodes]</span></pre><h2>Run Parsing in Parallel</h2><figure><img src="/blog/images/1*W3LNUEzK6QyH52shEAr4yQ.png" alt="" width="700" height="325"></figure><p>Since we have many documents, processing each document into nodes serially is inefficient and slow. We use Ray `flat_map` method to process documents into nodes in parallel:</p><pre><span id="b29b" class="ql or gt qi b bf qm qn l qo qp">
# Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.
nodes = loaded_docs.flat_map(convert_documents_into_nodes)</span></pre><h1>Generate Embeddings</h1><figure><img src="/blog/images/1*bJMxNgwzfg_ThixXNhOwww.png" alt="" width="700" height="325"></figure><p>We then generate embeddings for each Node using a Hugging Face Sentence Transformers model. We can do this with the help of LangChain’s embedding abstraction.</p><p>Similar to document loading/parsing, embedding generation can similarly be parallelized with Ray. We wrap these embedding operations into a helper class, called `EmbedNodes`, to take advantage of Ray abstractions.</p><pre><span id="32a9" class="ql or gt qi b bf qm qn l qo qp"><span class="hljs-comment"># Step 3: Embed each node using a local embedding model.</span>
<span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings

<span class="hljs-keyword">class</span> <span class="hljs-title class_">EmbedNodes</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
        self.embedding_model = HuggingFaceEmbeddings(
            <span class="hljs-comment"># Use all-mpnet-base-v2 Sentence_transformer.</span>
            <span class="hljs-comment"># This is the default embedding model for LlamaIndex/Langchain.</span>
            model_name=<span class="hljs-string">"sentence-transformers/all-mpnet-base-v2"</span>, 
            model_kwargs={<span class="hljs-string">"device"</span>: <span class="hljs-string">"cuda"</span>},
            <span class="hljs-comment"># Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.</span>
            <span class="hljs-comment"># Remove the "device": "cuda" to use CPU instead.</span>
            encode_kwargs={<span class="hljs-string">"device"</span>: <span class="hljs-string">"cuda"</span>, <span class="hljs-string">"batch_size"</span>: <span class="hljs-number">100</span>}
            )

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, node_batch: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">List</span>[Node]]</span>) -&amp;gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">List</span>[Node]]:
        nodes = node_batch[<span class="hljs-string">"node"</span>]
        text = [node.text <span class="hljs-keyword">for</span> node <span class="hljs-keyword">in</span> nodes]
        embeddings = self.embedding_model.embed_documents(text)
        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(nodes) == <span class="hljs-built_in">len</span>(embeddings)

        <span class="hljs-keyword">for</span> node, embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(nodes, embeddings):
            node.embedding = embedding
        <span class="hljs-keyword">return</span> {<span class="hljs-string">"embedded_nodes"</span>: nodes}</span></pre><p>Afterwards, generating an embedding for each node is as simple as calling the following operation in Ray:</p><pre><span id="b2a1" class="ql or gt qi b bf qm qn l qo qp"><span class="hljs-comment"># Use `map_batches` to specify a batch size to maximize GPU utilization.</span>
<span class="hljs-comment"># We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. </span>

<span class="hljs-comment"># This state can be reused for multiple batches.</span>
embedded_nodes = nodes.map_batches(
    EmbedNodes, 
    batch_size=<span class="hljs-number">100</span>, 
    <span class="hljs-comment"># Use 1 GPU per actor.</span>
    num_gpus=<span class="hljs-number">1</span>,
    <span class="hljs-comment"># There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.</span>
    compute=ActorPoolStrategy(size=<span class="hljs-number">4</span>))

<span class="hljs-comment"># Step 5: Trigger execution and collect all the embedded nodes.</span>
ray_docs_nodes = []
<span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> embedded_nodes.iter_rows():
    node = row[<span class="hljs-string">"embedded_nodes"</span>]
    <span class="hljs-keyword">assert</span> node.embedding <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
    ray_docs_nodes.append(node)</span></pre><h1>Data Indexing</h1><figure><img src="/blog/images/1*WvvbV91UFrPTXViKiF_TBA.png" alt="" width="700" height="325"></figure><p>The next step is to store these nodes within an “index” in LlamaIndex. An index is a core abstraction in LlamaIndex to “structure” your data in a certain way — this structure can then be used for downstream LLM retrieval + querying. An index can interface with a storage or vector store abstraction.</p><p>The most commonly used index abstraction within LlamaIndex is our vector index, where each node is stored along with an embedding. In this example, we use a simple in-memory vector store, but you can also choose to specify any one of LlamaIndex’s 10+ vector store integrations as the storage provider (e.g. Pinecone, Weaviate, Chroma).</p><p>We build two vector indices: one over the documentation nodes, and another over the blog post nodes and persist them to disk. Code is <a href="https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131" rel="noopener ugc nofollow" target="_blank">available here</a>.</p><pre><span id="569b" class="ql or gt qi b bf qm qn l qo qp">from llama_index import GPTVectorStoreIndex

# Store Ray Documentation embeddings
ray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)
ray_docs_index.storage_context.persist(persist_dir="/tmp/ray_docs_index")

# Store Anyscale blog post embeddings
ray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)
ray_blogs_index.storage_context.persist(persist_dir="/tmp/ray_blogs_index")</span></pre><p><strong>That’s it in terms of building a data pipeline using LlamaIndex + Ray Data</strong>!</p><p>Your data is now ready to be used within your LLM application. Check out our next section for how to use advanced LlamaIndex query capabilities on top of your data.</p><h1>Data Querying</h1><figure><img src="/blog/images/1*yp1AZoi-B6ZT2O7eqjIfcQ.png" alt="" width="700" height="189"></figure><p>LlamaIndex provides both simple and advanced query capabilities on top of your data + indices. The central abstraction within LlamaIndex is called a “query engine.” A query engine takes in a natural language query input and returns a natural language “output”. Each index has a “default” corresponding query engine. For instance, the default query engine for a vector index first performs top-k retrieval over the vector store to fetch the most relevant documents.</p><p>These query engines can be easily derived from each index:</p><pre><span id="0f91" class="ql or gt qi b bf qm qn l qo qp">ray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)

ray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)</span></pre><p>LlamaIndex also provides more advanced query engines for multi-document use cases — for instance, we may want to ask how a given feature in Ray is highlighted in both the documentation and blog. `SubQuestionQueryEngine` can take in other query engines as input. Given an existing question, it can decide to break down the question into simpler questions over any subset of query engines; it will execute the simpler questions and combine results at the top-level.</p><p>This abstraction is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.</p><p>For instance, given the following question “What is Ray?”, we can break this into sub-questions “What is Ray according to the documentation”, and “What is Ray according to the blog posts” over the document query engine and blog query engine respectively.</p><pre><span id="86bd" class="ql or gt qi b bf qm qn l qo qp"># Define a sub-question query engine, that can use the individual query engines as tools.
        query_engine_tools = [
            QueryEngineTool(
                query_engine=self.ray_docs_engine,
                metadata=ToolMetadata(name="ray_docs_engine", description="Provides information about the Ray documentation")
            ),
            QueryEngineTool(
                query_engine=self.ray_blogs_engine, 
                metadata=ToolMetadata(name="ray_blogs_engine", description="Provides information about Ray blog posts")
            ),
        ]

sub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)</span></pre><p>Have a look at <a href="https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56" rel="noopener ugc nofollow" target="_blank">deploy_app.py</a> to review the full implementation.</p><h1>Deploying with Ray Serve</h1><figure><img src="/blog/images/1*j0ZjPhwy7L6nyb9krfZMrA.png" alt="" width="700" height="179"></figure><p>We’ve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.</p><p>To do this, you just need to do the following steps:</p><ol><li>Define an outer class that can “wrap” a query engine, and expose a “query” endpoint</li><li>Add a `@ray.serve.deployment` decorator on this class</li><li>Deploy the Ray Serve application</li></ol><p>It will look something like the following:</p><pre><span id="1d2e" class="ql or gt qi b bf qm qn l qo qp"><span class="hljs-keyword">from</span> ray <span class="hljs-keyword">import</span> serve

<span class="hljs-meta">@serve.deployment</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">QADeployment</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
 self.query_engine = ...

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">self, query: <span class="hljs-built_in">str</span></span>):
            response =  self.query_engine.query(query)
            source_nodes = response.source_nodes
            source_str = <span class="hljs-string">""</span>
            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(source_nodes)):
                node = source_nodes[i]
                source_str += <span class="hljs-string">f"Sub-question <span class="hljs-subst">{i+<span class="hljs-number">1</span>}</span>:\n"</span>
                source_str += node.node.text
                source_str += <span class="hljs-string">"\n\n"</span>
            <span class="hljs-keyword">return</span> <span class="hljs-string">f"Response: <span class="hljs-subst">{<span class="hljs-built_in">str</span>(response)}</span> \n\n\n <span class="hljs-subst">{source_str}</span>\n"</span>

    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, request: Request</span>):
        query = request.query_params[<span class="hljs-string">"query"</span>]
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">str</span>(self.query(query))

<span class="hljs-comment"># Deploy the Ray Serve application.</span>
deployment = QADeployment.bind()</span></pre><p>Have a look at the <a href="https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py" rel="noopener ugc nofollow" target="_blank">deploy_app.py</a> for full implementation.</p><h1>Example Queries</h1><p>Once we’ve deployed the application, we can query it with questions about Ray.</p><p>We can query just one of the data sources:</p><pre><span id="3c55" class="ql or gt qi b bf qm qn l qo qp">Q: "What is Ray Serve?"

Ray Serve is a system for deploying and managing applications on a Ray
cluster. It provides APIs for deploying applications, managing replicas, and
making requests to applications. It also provides a command line interface
(CLI) for managing applications and a dashboard for monitoring applications.</span></pre><p>But, we can also provide complex queries that require synthesis across both the documentation and the blog posts. These complex queries are easily handled by the subquestion-query engine that we defined.</p><pre><span id="5b3a" class="ql or gt qi b bf qm qn l qo qp">Q: "Compare and contrast how the Ray docs and the Ray blogs present Ray Serve"

Response: 
The Ray docs and the Ray blogs both present Ray Serve as a web interface
that provides metrics, charts, and other features to help Ray users
understand and debug Ray applications. However, the Ray docs provide more
detailed information, such as a Quick Start guide, user guide, production
guide, performance tuning guide, development workflow guide, API reference,
experimental Java API, and experimental gRPC support. Additionally, the Ray
docs provide a guide for migrating from 1.x to 2.x. On the other hand, the
Ray blogs provide a Quick Start guide, a User Guide, and Advanced Guides to
help users get started and understand the features of Ray Serve.
Additionally, the Ray blogs provide examples and use cases to help users
understand how to use Ray Serve in their own projects.

---

Sub-question 1

Sub question: How does the Ray docs present Ray Serve

Response: 
The Ray docs present Ray Serve as a web interface that provides metrics,
charts, and other features to help Ray users understand and debug Ray
applications. It provides a Quick Start guide, user guide, production guide,
performance tuning guide, and development workflow guide. It also provides
an API reference, experimental Java API, and experimental gRPC support.
Finally, it provides a guide for migrating from 1.x to 2.x.

---

Sub-question 2

Sub question: How does the Ray blogs present Ray Serve

Response: 
The Ray blog presents Ray Serve as a framework for distributed applications
that enables users to handle HTTP requests, scale and allocate resources,
compose models, and more. It provides a Quick Start guide, a User Guide, and
Advanced Guides to help users get started and understand the features of Ray
Serve. Additionally, it provides examples and use cases to help users
understand how to use Ray Serve in their own projects.</span></pre><h1>Conclusion</h1><p>In this example, we showed how you can build a scalable data pipeline and a powerful query engine using LlamaIndex + Ray. We also demonstrated how to deploy LlamaIndex applications using Ray Serve. This allows you to effortlessly ask questions and synthesize insights about Ray across disparate data sources!</p><p>We used LlamaIndex — a data framework for building LLM applications — to load, parse, embed and index the data. We ensured efficient and fast parallel execution by using Ray. Then, we used LlamaIndex querying capabilities to perform semantic search over a single document, or combine results across multiple documents. Finally, we used Ray Serve to package the application for production use.</p><p>Implementation in open source, code is available on GitHub: <a href="https://github.com/amogkam/llama_index_ray" rel="noopener ugc nofollow" target="_blank">LlamaIndex-Ray-app</a></p><h1>What’s next?</h1><p>Visit LlamaIndex <a href="https://www.llamaindex.ai/" rel="noopener ugc nofollow" target="_blank">site</a> and <a href="https://gpt-index.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">docs</a> to learn more about this data framework for building LLM applications.</p><p>Visit <a href="https://docs.ray.io/en/latest/ray-overview/use-cases.html#llms-and-gen-ai" rel="noopener ugc nofollow" target="_blank">Ray docs</a> to learn more about how to build and deploy scalable LLM apps.</p><p>Join our communities!</p><ul><li><a href="https://forms.gle/9TSdDYUgxYs8SA9e8" rel="noopener ugc nofollow" target="_blank">Join Ray community</a> on Slack and Ray #LLM channel.</li><li>You can also join the LlamaIndex <a href="https://discord.gg/UB58qbeq" rel="noopener ugc nofollow" target="_blank">community on discord</a>.</li></ul><p>We have our <a href="https://raysummit.anyscale.com/" rel="noopener ugc nofollow" target="_blank">Ray Summit 2023</a> early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-05">LlamaIndex Newsletter 2024-03-05</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f">Querying a network of knowledge with llama-index-networks</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-27</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"f16d0dcb-47c5-4a73-927f-f128d48595e3","_rev":"Ys5IzmCaJ2UnW2RAX7UyZx","_type":"blogPost","_updatedAt":"2025-05-21T20:40:31Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"htmlContent":"\u003cp\u003eCo-authors: Jerry Liu (CEO at LlamaIndex), Amog Kamsetty (Software Engineer at Anyscale)\u003c/p\u003e\u003cp\u003e(\u003cstrong\u003enote: \u003c/strong\u003ethis is cross-posted from the original blog post on Anyscale’s website. \u003ca href=\"https://www.anyscale.com/blog/build-and-scale-a-powerful-query-engine-with-llamaindex-ray\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCheck it out here\u003c/a\u003e!)\u003c/p\u003e\u003cp\u003eIn this blog, we showcase how you can use LlamaIndex and Ray to build a query engine to answer questions and generate insights about Ray itself, given its documentation and blog posts.\u003c/p\u003e\u003cp\u003eWe’ll give a quick introduction of LlamaIndex + Ray, and then walk through a step-by-step tutorial on building and deploying this query engine. We make use of both Ray Datasets to parallelize building indices as well as Ray Serve to build deployments.\u003c/p\u003e\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vd_8lS1iDBg?si=QKPkWLLWz46zeiCf\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen\u003e\u003c/iframe\u003e\u003ch1\u003eIntroduction\u003c/h1\u003e\u003cp\u003eLarge Language Models (LLMs) offer the promise of allowing users to extract complex insights from their unstructured text data. Retrieval-augmented generation pipelines have emerged as a common pattern for developing LLM applications allowing users to effectively perform semantic search over a collection of documents.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pu fe pv nz oa pw px be b bf z dt\"\u003e\u003cem class=\"py\"\u003eExample of retrieval augmented generation. Relevant context is pulled from a set of documents and included in the LLM input prompt.\u003c/em\u003e\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*euY0oGTyi5vnt2aqJ9hFjw.png\" alt=\"\" width=\"700\" height=\"282\"\u003e\u003c/figure\u003e\u003cp\u003eHowever, when productionizing these applications over many different data sources, there are a few challenges:\u003c/p\u003e\u003col\u003e\u003cli\u003eTooling for indexing data from many different data sources\u003c/li\u003e\u003cli\u003eHandling complex queries over different data sources\u003c/li\u003e\u003cli\u003eScaling indexing to thousands or millions of documents\u003c/li\u003e\u003cli\u003eDeploying a scalable LLM application into production\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eHere, we showcase how \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex\u003c/a\u003e and \u003ca href=\"https://docs.ray.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRay\u003c/a\u003e are the perfect setup for this task.\u003c/p\u003e\u003cp\u003eLlamaIndex is a data framework for building LLM applications, and solves Challenges #1 and #2. It also provides a comprehensive toolkit allowing users to connect their private data with a language model. It offers a variety of tools to help users first ingest and index their data — convert different formats of unstructured and structured data into a format that the language model can use, and query their private data.\u003c/p\u003e\u003cp\u003eRay is a powerful framework for scalable AI that solves Challenges #3 and #4. We can use it to dramatically accelerate ingest, inference, pretraining, and also effortlessly deploy and scale the query capabilities of LlamaIndex into the cloud.\u003c/p\u003e\u003cp\u003eMore specifically, we showcase a very relevant use case — highlighting Ray features that are present in both the documentation as well as the Ray blog posts!\u003c/p\u003e\u003ch1\u003eData Ingestion and Embedding Pipeline\u003c/h1\u003e\u003cp\u003eWe use LlamaIndex + Ray to ingest, parse, embed and store Ray docs and blog posts in a parallel fashion. For the most part, these steps are duplicated across the two data sources, so we show the steps for just the documentation below.\u003c/p\u003e\u003cp\u003eCode for this part of the blog is \u003ca href=\"https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eavailable here\u003c/a\u003e.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pu fe pv nz oa pw px be b bf z dt\"\u003e\u003cem class=\"py\"\u003eSequential pipeline with “ingest”, “parse” and “embed” stages. Files are processed sequentially resulting in poor hardware utilization and long computation time.\u003c/em\u003e\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*uQpJXp_A-1-AOwz3LMyl3Q.png\" alt=\"\" width=\"700\" height=\"320\"\u003e\u003c/figure\u003e\u003cfigure\u003e\u003cfigcaption class=\"pu fe pv nz oa pw px be b bf z dt\"\u003e\u003cem class=\"py\"\u003eParallel pipeline. Thanks to Ray we can process multiple input files simultaneously. Parallel processing has much better performance, because hardware is better utilized.\u003c/em\u003e\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*im0zUrKp8ABSRrZlic0L8Q.png\" alt=\"\" width=\"700\" height=\"325\"\u003e\u003c/figure\u003e\u003ch1\u003eLoad Data\u003c/h1\u003e\u003cp\u003eWe start by ingesting these two sources of data. We first fetch both data sources and download the HTML files.\u003c/p\u003e\u003cp\u003eWe then need to load and parse these files. We can do this with the help of LlamaHub, our community-driven repository of 100+ data loaders from various API’s, file formats (.pdf, .html, .docx), and databases. We use an HTML data loader offered by \u003ca href=\"https://github.com/Unstructured-IO/unstructured\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUnstructured\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ff6e\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e typing \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e, \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pathlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Path\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e download_loader\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Document\n\n\u003cspan class=\"hljs-comment\"\u003e# Step 1: Logic for loading and parsing the files into llama_index documents.\u003c/span\u003e\nUnstructuredReader = download_loader(\u003cspan class=\"hljs-string\"\u003e\"UnstructuredReader\"\u003c/span\u003e)\nloader = UnstructuredReader()\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eload_and_parse_files\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003efile_row: \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, Path]\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, Document]:\n    documents = []\n    file = file_row[\u003cspan class=\"hljs-string\"\u003e\"path\"\u003c/span\u003e]\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e file.is_dir():\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e []\n    \u003cspan class=\"hljs-comment\"\u003e# Skip all non-html files like png, jpg, etc.\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e file.suffix.lower() == \u003cspan class=\"hljs-string\"\u003e\".html\"\u003c/span\u003e:\n        loaded_doc = loader.load_data(file=file, split_documents=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e)\n        loaded_doc[\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e].extra_info = {\u003cspan class=\"hljs-string\"\u003e\"path\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(file)}\n        documents.extend(loaded_doc)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [{\u003cspan class=\"hljs-string\"\u003e\"doc\"\u003c/span\u003e: doc} \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e doc \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e documents]\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eUnstructured offers a robust suite of parsing tools on top of various files. It is able to help sanitize HTML documents by stripping out information like tags and formatting the text accordingly.\u003c/p\u003e\u003ch2\u003eScaling Data Ingest\u003c/h2\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*prVxsm5aR-a5IQItiiWZXQ.png\" alt=\"\" width=\"700\" height=\"325\"\u003e\u003c/figure\u003e\u003cp\u003eSince we have many HTML documents to process, loading/processing each one serially is inefficient and slow. This is an opportunity to use Ray and distribute execution of the `load_and_parse_files` method across multiple CPUs or GPUs.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"7248\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003eimport ray\n\n\u003cspan class=\"hljs-comment\"\u003e# Get the paths for the locally downloaded documentation.\u003c/span\u003e\nall_docs_gen = \u003cspan class=\"hljs-title class_\"\u003ePath\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"./docs.ray.io/\"\u003c/span\u003e).rglob(\u003cspan class=\"hljs-string\"\u003e\"*\"\u003c/span\u003e)\nall_docs = [{\u003cspan class=\"hljs-string\"\u003e\"path\"\u003c/span\u003e: doc.resolve()} \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e doc \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e all_docs_gen]\n\n\u003cspan class=\"hljs-comment\"\u003e# Create the Ray Dataset pipeline\u003c/span\u003e\nds = ray.data.from_items(all_docs)\n\n\u003cspan class=\"hljs-comment\"\u003e# Use `flat_map` since there is a 1:N relationship.\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# Each filepath returns multiple documents.\u003c/span\u003e\nloaded_docs = ds.flat_map(load_and_parse_files)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eParse Files\u003c/h1\u003e\u003cp\u003eNow that we’ve loaded the documents, the next step is to parse them into Node objects — a “Node” object represents a more granular chunk of text, derived from the source documents. Node objects can be used in the input prompt as context; by setting a small enough chunk size, we can make sure that inserting Node objects do not overflow the context limits.\u003c/p\u003e\u003cp\u003eWe define a function called `convert_documents_into_nodes` which converts documents into nodes using a simple text splitting strategy.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4064\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Step 2: Convert the loaded documents into llama_index Nodes. This will split the documents into chunks.\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.node_parser \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SimpleNodeParser\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.data_structs \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Node\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003econvert_documents_into_nodes\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edocuments: \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, Document]\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, Node]:\n    parser = SimpleNodeParser()\n    document = documents[\u003cspan class=\"hljs-string\"\u003e\"doc\"\u003c/span\u003e]\n    nodes = parser.get_nodes_from_documents([document]) \n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e [{\u003cspan class=\"hljs-string\"\u003e\"node\"\u003c/span\u003e: node} \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e node \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e nodes]\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eRun Parsing in Parallel\u003c/h2\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*W3LNUEzK6QyH52shEAr4yQ.png\" alt=\"\" width=\"700\" height=\"325\"\u003e\u003c/figure\u003e\u003cp\u003eSince we have many documents, processing each document into nodes serially is inefficient and slow. We use Ray `flat_map` method to process documents into nodes in parallel:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b29b\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e\n# Use `flat_map` since there is a 1:N relationship. Each document returns multiple nodes.\nnodes = loaded_docs.flat_map(convert_documents_into_nodes)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eGenerate Embeddings\u003c/h1\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*bJMxNgwzfg_ThixXNhOwww.png\" alt=\"\" width=\"700\" height=\"325\"\u003e\u003c/figure\u003e\u003cp\u003eWe then generate embeddings for each Node using a Hugging Face Sentence Transformers model. We can do this with the help of LangChain’s embedding abstraction.\u003c/p\u003e\u003cp\u003eSimilar to document loading/parsing, embedding generation can similarly be parallelized with Ray. We wrap these embedding operations into a helper class, called `EmbedNodes`, to take advantage of Ray abstractions.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"32a9\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Step 3: Embed each node using a local embedding model.\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e langchain.embeddings.huggingface \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e HuggingFaceEmbeddings\n\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eEmbedNodes\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n        self.embedding_model = HuggingFaceEmbeddings(\n            \u003cspan class=\"hljs-comment\"\u003e# Use all-mpnet-base-v2 Sentence_transformer.\u003c/span\u003e\n            \u003cspan class=\"hljs-comment\"\u003e# This is the default embedding model for LlamaIndex/Langchain.\u003c/span\u003e\n            model_name=\u003cspan class=\"hljs-string\"\u003e\"sentence-transformers/all-mpnet-base-v2\"\u003c/span\u003e, \n            model_kwargs={\u003cspan class=\"hljs-string\"\u003e\"device\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e},\n            \u003cspan class=\"hljs-comment\"\u003e# Use GPU for embedding and specify a large enough batch size to maximize GPU utilization.\u003c/span\u003e\n            \u003cspan class=\"hljs-comment\"\u003e# Remove the \"device\": \"cuda\" to use CPU instead.\u003c/span\u003e\n            encode_kwargs={\u003cspan class=\"hljs-string\"\u003e\"device\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"cuda\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"batch_size\"\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e}\n            )\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, node_batch: \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[Node]]\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eDict\u003c/span\u003e[\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e, \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[Node]]:\n        nodes = node_batch[\u003cspan class=\"hljs-string\"\u003e\"node\"\u003c/span\u003e]\n        text = [node.text \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e node \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e nodes]\n        embeddings = self.embedding_model.embed_documents(text)\n        \u003cspan class=\"hljs-keyword\"\u003eassert\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(nodes) == \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(embeddings)\n\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e node, embedding \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003ezip\u003c/span\u003e(nodes, embeddings):\n            node.embedding = embedding\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\u003cspan class=\"hljs-string\"\u003e\"embedded_nodes\"\u003c/span\u003e: nodes}\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAfterwards, generating an embedding for each node is as simple as calling the following operation in Ray:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b2a1\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Use `map_batches` to specify a batch size to maximize GPU utilization.\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# We define `EmbedNodes` as a class instead of a function so we only initialize the embedding model once. \u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# This state can be reused for multiple batches.\u003c/span\u003e\nembedded_nodes = nodes.map_batches(\n    EmbedNodes, \n    batch_size=\u003cspan class=\"hljs-number\"\u003e100\u003c/span\u003e, \n    \u003cspan class=\"hljs-comment\"\u003e# Use 1 GPU per actor.\u003c/span\u003e\n    num_gpus=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e,\n    \u003cspan class=\"hljs-comment\"\u003e# There are 4 GPUs in the cluster. Each actor uses 1 GPU. So we want 4 total actors.\u003c/span\u003e\n    compute=ActorPoolStrategy(size=\u003cspan class=\"hljs-number\"\u003e4\u003c/span\u003e))\n\n\u003cspan class=\"hljs-comment\"\u003e# Step 5: Trigger execution and collect all the embedded nodes.\u003c/span\u003e\nray_docs_nodes = []\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e row \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e embedded_nodes.iter_rows():\n    node = row[\u003cspan class=\"hljs-string\"\u003e\"embedded_nodes\"\u003c/span\u003e]\n    \u003cspan class=\"hljs-keyword\"\u003eassert\u003c/span\u003e node.embedding \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e\n    ray_docs_nodes.append(node)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eData Indexing\u003c/h1\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*WvvbV91UFrPTXViKiF_TBA.png\" alt=\"\" width=\"700\" height=\"325\"\u003e\u003c/figure\u003e\u003cp\u003eThe next step is to store these nodes within an “index” in LlamaIndex. An index is a core abstraction in LlamaIndex to “structure” your data in a certain way — this structure can then be used for downstream LLM retrieval + querying. An index can interface with a storage or vector store abstraction.\u003c/p\u003e\u003cp\u003eThe most commonly used index abstraction within LlamaIndex is our vector index, where each node is stored along with an embedding. In this example, we use a simple in-memory vector store, but you can also choose to specify any one of LlamaIndex’s 10+ vector store integrations as the storage provider (e.g. Pinecone, Weaviate, Chroma).\u003c/p\u003e\u003cp\u003eWe build two vector indices: one over the documentation nodes, and another over the blog post nodes and persist them to disk. Code is \u003ca href=\"https://github.com/amogkam/llama_index_ray/blob/main/create_vector_index.py#L102:L131\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eavailable here\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"569b\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003efrom llama_index import GPTVectorStoreIndex\n\n# Store Ray Documentation embeddings\nray_docs_index = GPTVectorStoreIndex(nodes=ray_docs_nodes)\nray_docs_index.storage_context.persist(persist_dir=\"/tmp/ray_docs_index\")\n\n# Store Anyscale blog post embeddings\nray_blogs_index = GPTVectorStoreIndex(nodes=ray_blogs_nodes)\nray_blogs_index.storage_context.persist(persist_dir=\"/tmp/ray_blogs_index\")\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eThat’s it in terms of building a data pipeline using LlamaIndex + Ray Data\u003c/strong\u003e!\u003c/p\u003e\u003cp\u003eYour data is now ready to be used within your LLM application. Check out our next section for how to use advanced LlamaIndex query capabilities on top of your data.\u003c/p\u003e\u003ch1\u003eData Querying\u003c/h1\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*yp1AZoi-B6ZT2O7eqjIfcQ.png\" alt=\"\" width=\"700\" height=\"189\"\u003e\u003c/figure\u003e\u003cp\u003eLlamaIndex provides both simple and advanced query capabilities on top of your data + indices. The central abstraction within LlamaIndex is called a “query engine.” A query engine takes in a natural language query input and returns a natural language “output”. Each index has a “default” corresponding query engine. For instance, the default query engine for a vector index first performs top-k retrieval over the vector store to fetch the most relevant documents.\u003c/p\u003e\u003cp\u003eThese query engines can be easily derived from each index:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0f91\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003eray_docs_engine = ray_docs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\n\nray_blogs_engine = ray_blogs_index.as_query_engine(similarity_top_k=5, service_context=service_context)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eLlamaIndex also provides more advanced query engines for multi-document use cases — for instance, we may want to ask how a given feature in Ray is highlighted in both the documentation and blog. `SubQuestionQueryEngine` can take in other query engines as input. Given an existing question, it can decide to break down the question into simpler questions over any subset of query engines; it will execute the simpler questions and combine results at the top-level.\u003c/p\u003e\u003cp\u003eThis abstraction is quite powerful; it can perform semantic search over one document, or combine results across multiple documents.\u003c/p\u003e\u003cp\u003eFor instance, given the following question “What is Ray?”, we can break this into sub-questions “What is Ray according to the documentation”, and “What is Ray according to the blog posts” over the document query engine and blog query engine respectively.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"86bd\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e# Define a sub-question query engine, that can use the individual query engines as tools.\n        query_engine_tools = [\n            QueryEngineTool(\n                query_engine=self.ray_docs_engine,\n                metadata=ToolMetadata(name=\"ray_docs_engine\", description=\"Provides information about the Ray documentation\")\n            ),\n            QueryEngineTool(\n                query_engine=self.ray_blogs_engine, \n                metadata=ToolMetadata(name=\"ray_blogs_engine\", description=\"Provides information about Ray blog posts\")\n            ),\n        ]\n\nsub_query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools, service_context=service_context, use_async=False)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eHave a look at \u003ca href=\"https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py#L22:L56\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edeploy_app.py\u003c/a\u003e to review the full implementation.\u003c/p\u003e\u003ch1\u003eDeploying with Ray Serve\u003c/h1\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*j0ZjPhwy7L6nyb9krfZMrA.png\" alt=\"\" width=\"700\" height=\"179\"\u003e\u003c/figure\u003e\u003cp\u003eWe’ve now created an incredibly powerful query module over your data. As a next step, what if we could seamlessly deploy this function to production and serve users? Ray Serve makes this incredibly easy to do. Ray Serve is a scalable compute layer for serving ML models and LLMs that enables serving individual models or creating composite model pipelines where you can independently deploy, update, and scale individual components.\u003c/p\u003e\u003cp\u003eTo do this, you just need to do the following steps:\u003c/p\u003e\u003col\u003e\u003cli\u003eDefine an outer class that can “wrap” a query engine, and expose a “query” endpoint\u003c/li\u003e\u003cli\u003eAdd a `@ray.serve.deployment` decorator on this class\u003c/li\u003e\u003cli\u003eDeploy the Ray Serve application\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eIt will look something like the following:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"1d2e\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e ray \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e serve\n\n\u003cspan class=\"hljs-meta\"\u003e@serve.deployment\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eQADeployment\u003c/span\u003e:\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself\u003c/span\u003e):\n self.query_engine = ...\n\n    \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003equery\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, query: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e\u003c/span\u003e):\n            response =  self.query_engine.query(query)\n            source_nodes = response.source_nodes\n            source_str = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e i \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(source_nodes)):\n                node = source_nodes[i]\n                source_str += \u003cspan class=\"hljs-string\"\u003ef\"Sub-question \u003cspan class=\"hljs-subst\"\u003e{i+\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e}\u003c/span\u003e:\\n\"\u003c/span\u003e\n                source_str += node.node.text\n                source_str += \u003cspan class=\"hljs-string\"\u003e\"\\n\\n\"\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003ef\"Response: \u003cspan class=\"hljs-subst\"\u003e{\u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(response)}\u003c/span\u003e \\n\\n\\n \u003cspan class=\"hljs-subst\"\u003e{source_str}\u003c/span\u003e\\n\"\u003c/span\u003e\n\n    \u003cspan class=\"hljs-keyword\"\u003easync\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__call__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, request: Request\u003c/span\u003e):\n        query = request.query_params[\u003cspan class=\"hljs-string\"\u003e\"query\"\u003c/span\u003e]\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(self.query(query))\n\n\u003cspan class=\"hljs-comment\"\u003e# Deploy the Ray Serve application.\u003c/span\u003e\ndeployment = QADeployment.bind()\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eHave a look at the \u003ca href=\"https://github.com/amogkam/llama_index_ray/blob/main/deploy_app.py\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edeploy_app.py\u003c/a\u003e for full implementation.\u003c/p\u003e\u003ch1\u003eExample Queries\u003c/h1\u003e\u003cp\u003eOnce we’ve deployed the application, we can query it with questions about Ray.\u003c/p\u003e\u003cp\u003eWe can query just one of the data sources:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"3c55\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003eQ: \"What is Ray Serve?\"\n\nRay Serve is a system for deploying and managing applications on a Ray\ncluster. It provides APIs for deploying applications, managing replicas, and\nmaking requests to applications. It also provides a command line interface\n(CLI) for managing applications and a dashboard for monitoring applications.\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eBut, we can also provide complex queries that require synthesis across both the documentation and the blog posts. These complex queries are easily handled by the subquestion-query engine that we defined.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5b3a\" class=\"ql or gt qi b bf qm qn l qo qp\"\u003eQ: \"Compare and contrast how the Ray docs and the Ray blogs present Ray Serve\"\n\nResponse: \nThe Ray docs and the Ray blogs both present Ray Serve as a web interface\nthat provides metrics, charts, and other features to help Ray users\nunderstand and debug Ray applications. However, the Ray docs provide more\ndetailed information, such as a Quick Start guide, user guide, production\nguide, performance tuning guide, development workflow guide, API reference,\nexperimental Java API, and experimental gRPC support. Additionally, the Ray\ndocs provide a guide for migrating from 1.x to 2.x. On the other hand, the\nRay blogs provide a Quick Start guide, a User Guide, and Advanced Guides to\nhelp users get started and understand the features of Ray Serve.\nAdditionally, the Ray blogs provide examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\n\n---\n\nSub-question 1\n\nSub question: How does the Ray docs present Ray Serve\n\nResponse: \nThe Ray docs present Ray Serve as a web interface that provides metrics,\ncharts, and other features to help Ray users understand and debug Ray\napplications. It provides a Quick Start guide, user guide, production guide,\nperformance tuning guide, and development workflow guide. It also provides\nan API reference, experimental Java API, and experimental gRPC support.\nFinally, it provides a guide for migrating from 1.x to 2.x.\n\n---\n\nSub-question 2\n\nSub question: How does the Ray blogs present Ray Serve\n\nResponse: \nThe Ray blog presents Ray Serve as a framework for distributed applications\nthat enables users to handle HTTP requests, scale and allocate resources,\ncompose models, and more. It provides a Quick Start guide, a User Guide, and\nAdvanced Guides to help users get started and understand the features of Ray\nServe. Additionally, it provides examples and use cases to help users\nunderstand how to use Ray Serve in their own projects.\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eIn this example, we showed how you can build a scalable data pipeline and a powerful query engine using LlamaIndex + Ray. We also demonstrated how to deploy LlamaIndex applications using Ray Serve. This allows you to effortlessly ask questions and synthesize insights about Ray across disparate data sources!\u003c/p\u003e\u003cp\u003eWe used LlamaIndex — a data framework for building LLM applications — to load, parse, embed and index the data. We ensured efficient and fast parallel execution by using Ray. Then, we used LlamaIndex querying capabilities to perform semantic search over a single document, or combine results across multiple documents. Finally, we used Ray Serve to package the application for production use.\u003c/p\u003e\u003cp\u003eImplementation in open source, code is available on GitHub: \u003ca href=\"https://github.com/amogkam/llama_index_ray\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex-Ray-app\u003c/a\u003e\u003c/p\u003e\u003ch1\u003eWhat’s next?\u003c/h1\u003e\u003cp\u003eVisit LlamaIndex \u003ca href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esite\u003c/a\u003e and \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edocs\u003c/a\u003e to learn more about this data framework for building LLM applications.\u003c/p\u003e\u003cp\u003eVisit \u003ca href=\"https://docs.ray.io/en/latest/ray-overview/use-cases.html#llms-and-gen-ai\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRay docs\u003c/a\u003e to learn more about how to build and deploy scalable LLM apps.\u003c/p\u003e\u003cp\u003eJoin our communities!\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJoin Ray community\u003c/a\u003e on Slack and Ray #LLM channel.\u003c/li\u003e\u003cli\u003eYou can also join the LlamaIndex \u003ca href=\"https://discord.gg/UB58qbeq\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecommunity on discord\u003c/a\u003e.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe have our \u003ca href=\"https://raysummit.anyscale.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRay Summit 2023\u003c/a\u003e early-bird registration open until 6/30. Secure your spot, save some money, savor the community camaraderie at the summit.\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/958a7e3655d67819e61eab2b7606fca78e37aec7-1200x557.png","publishedDate":"2023-06-27","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-bf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-03-05","slug":"llamaindex-newsletter-2024-03-05","title":"LlamaIndex Newsletter 2024-03-05"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-a195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265-webp","_type":"reference"}},"publishedDate":"2024-02-27","slug":"querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f","title":"Querying a network of knowledge with llama-index-networks"}],"slug":{"_type":"slug","current":"build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"},"tags":[{"_createdAt":"2024-02-22T20:19:13Z","_id":"78713226-8bff-400f-bbfe-fd8a3d90be1d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"nlp"},"title":"NLP"},{"_createdAt":"2024-02-22T20:19:14Z","_id":"8ed7cab7-64a5-4c6c-8bb7-509232fd8f3f","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:14Z","slug":{"_type":"slug","current":"distributed-systems"},"title":"Distributed Systems"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"412f77ae-efba-466b-abc9-221fc36d252a","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"large-language-models"},"title":"Large Language Models"},{"_createdAt":"2024-02-22T20:19:14Z","_id":"a3196d23-125f-4d89-b3bf-4812f66491f5","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:14Z","slug":{"_type":"slug","current":"parallel-computing"},"title":"Parallel Computing"}],"title":"Build and Scale a Powerful Query Engine with LlamaIndex and Ray"},"publishedDate":"Invalid Date"},"params":{"slug":"build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"build-and-scale-a-powerful-query-engine-with-llamaindex-and-ray-bfb456404bc4"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>