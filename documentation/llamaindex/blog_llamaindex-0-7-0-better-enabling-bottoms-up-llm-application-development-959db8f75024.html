<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/89a392fad09ab8e8294bab827a369cd213426275-1970x1412.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/89a392fad09ab8e8294bab827a369cd213426275-1970x1412.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="706" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89a392fad09ab8e8294bab827a369cd213426275-1970x1412.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89a392fad09ab8e8294bab827a369cd213426275-1970x1412.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89a392fad09ab8e8294bab827a369cd213426275-1970x1412.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2023-07-04</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/machine-learning"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Machine Learning</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/nlp"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">NLP</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>A few months ago, we launched LlamaIndex 0.6.0, which included a massive rewrite of our codebase to make our library more modular, customizable, and accessible to both beginner and advanced users:</p><ul><li>We created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines).</li><li>We created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system.</li></ul><p>Today, we’re excited to launch LlamaIndex 0.7.0. Our latest release continues the theme of improving modularity/customizability at the lower level to enable <strong>bottoms-up development of LLM applications over your data.</strong> You now have even more control over using key abstractions: the LLM, our response synthesizer, and our Document and Node objects.</p><ul><li>We’ve created <strong>standalone LLM abstractions</strong> (OpenAI, HuggingFace, PaLM).</li><li>We’ve made our <strong>response synthesis module an independent module</strong> you can use completely independently of the rest of our abstractions — get rid of the prompt boilerplate of trying to figure out how to fit context within a context window.</li><li>We’ve added <strong>extensive metadata management capabilities</strong> to our Document/Node objects — now you have complete control over context you decide to inject into your documents.</li></ul><p>Below, we describe each section more in detail. We also outline a full list of breaking changes at the bottom.</p><h1>Standalone LLM Abstractions</h1><p>We’ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM. These abstractions can be used on their own, or as part of an existing LlamaIndex system (query engines, retrievers).</p><h2>High-level Motivation</h2><p>We did this for multiple reasons:</p><ul><li>Cleaner abstractions in the codebase. Before, our <code class="cw qm qn qo qp b">LLMPredictor</code> class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize.</li><li>Slightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use “text-davinci-003”, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it’s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain’s LLMs if you wish.</li><li>Conducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It’s reflective of our bigger push in 0.7.0 to let users compose their own workflows.</li></ul><h2><strong>Using on their own</strong></h2><p>Our LLM abstractions support both <code class="cw qm qn qo qp b">complete</code> and <code class="cw qm qn qo qp b">chat</code> endpoints. The main difference is that <code class="cw qm qn qo qp b">complete</code> is designed to take in a simple string input, and output a <code class="cw qm qn qo qp b">CompletionResponse</code> (containing text output + additional fields). <code class="cw qm qn qo qp b">chat</code> takes in a <code class="cw qm qn qo qp b">ChatMessage</code> and outputs a <code class="cw qm qn qo qp b">ChatResponse</code> (containing a chat message + additional fields).</p><p>These LLM endpoints also natively support streaming via <code class="cw qm qn qo qp b">stream_complete</code> and <code class="cw qm qn qo qp b">stream_chat</code>.</p><p>Here’s on how you can use the LLM abstractions on their own:</p><pre><span id="127a" class="qy ov gt qp b bf qz ra l rb rc"><span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI

<span class="hljs-comment"># using complete endpoint</span>
resp = OpenAI().complete(<span class="hljs-string">'Paul Graham is '</span>)
<span class="hljs-built_in">print</span>(resp)
<span class="hljs-comment"># get raw object</span>
resp_raw = resp.raw
<span class="hljs-comment"># using chat endpoint</span>
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> ChatMessage, OpenAI
messages = [
    ChatMessage(role=<span class="hljs-string">'system'</span>, content=<span class="hljs-string">'You are a pirate with a colorful personality'</span>),
    ChatMessage(role=<span class="hljs-string">'user'</span>, content=<span class="hljs-string">'What is your name'</span>)
]
resp = OpenAI().chat(messages)
<span class="hljs-built_in">print</span>(resp)
<span class="hljs-comment"># get raw object</span>
resp_raw = resp.raw
<span class="hljs-comment"># using streaming endpoint</span>
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI
llm = OpenAI()
resp = llm.stream_complete(<span class="hljs-string">'Paul Graham is '</span>)
<span class="hljs-keyword">for</span> delta <span class="hljs-keyword">in</span> resp:
    <span class="hljs-built_in">print</span>(delta, end=<span class="hljs-string">''</span>)</span></pre><p>Here’s how you can use the LLM abstractions as part of an overall LlamaIndex system.</p><pre><span id="289e" class="qy ov gt qp b bf qz ra l rb rc"><span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-keyword">from</span> llama_index.indices.service_context <span class="hljs-keyword">import</span> ServiceContext
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> VectorStoreIndex

llm = OpenAI(model=<span class="hljs-string">'gpt-3.5-turbo'</span>, temperature=<span class="hljs-number">0</span>)
service_context = ServiceContext.from_defaults(llm=llm)
index = VectorStoreIndex.from_documents(docs, service_context=service_context)
response = index.as_query_engine().query(<span class="hljs-string">"&amp;lt;question&amp;gt;"</span>)</span></pre><p>Note: Our top-level <code class="cw qm qn qo qp b">LLMPredictor</code> still exists but is less user-facing (and we might deprecate in the future). Also, you can still use LangChain LLMs through our <code class="cw qm qn qo qp b">LangChainLLM</code> class.</p><h2><strong>Resources</strong></h2><p>All of our notebooks have by default been updated to use our native OpenAI LLM integration. Here’s some resources to show both the LLM abstraction on its own as well as how it can be used in the overall system:</p><ul><li><a href="https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/openai.ipynb" rel="noopener ugc nofollow" target="_blank">OpenAI LLM</a></li><li><a href="https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/llm_predictor.ipynb" rel="noopener ugc nofollow" target="_blank">Using LLM in LLMPredictor</a></li><li><a href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-changing-the-underlying-llm" rel="noopener ugc nofollow" target="_blank">Changing LLM within Index/Query Engine</a></li><li><a href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced" rel="noopener ugc nofollow" target="_blank">Defining a custom LLM Model</a></li></ul><h1>Standalone Response Synthesis Modules</h1><h2><strong>Context</strong></h2><p>In any RAG system, there is retrieval and there is synthesis. The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM.</p><p>Fundamentally, the synthesis module needs to synthesize a response over <strong>any</strong> context list, regardless of how long that context list is. This is essentially “boilerplate” that an LLM developer / <a href="https://www.latent.space/p/ai-engineer" rel="noopener ugc nofollow" target="_blank">“AI engineer”</a> must write.</p><p>We had this as an internal abstraction in LlamaIndex before (as a <code class="cw qm qn qo qp b">ResponseSynthesizer</code>), but the external-facing UX was unfriendly to users. The actual piece that gathered responses (the <code class="cw qm qn qo qp b">ResponseBuilder</code> ) was hard to customize, and the <code class="cw qm qn qo qp b">ResponseSynthesizer</code> itself was adding an extra unnecessary layer.</p><p>Now we have a set of standalone modules that you can easily import. Previously, when you set the <code class="cw qm qn qo qp b">response_mode</code> in the query engine, these were being setup for you. Now they are more directly available and user-facing.</p><p>Here’s a list of all the new <code class="cw qm qn qo qp b">Response Synthesiszer</code> modules available from <code class="cw qm qn qo qp b">llama_index.response_synthesizer</code>:</p><ul><li><code class="cw qm qn qo qp b">Refine</code> - Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk.</li><li><code class="cw qm qn qo qp b">Accumulate</code> - Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses</li><li><code class="cw qm qn qo qp b">Compact</code> - The same as <code class="cw qm qn qo qp b">Refine</code>, but puts as much text as possible into each LLM call</li><li><code class="cw qm qn qo qp b">CompactAndAccumulate</code> - The same as <code class="cw qm qn qo qp b">Accumulate</code>, but puts as much text as possible</li><li><code class="cw qm qn qo qp b">TreeSummarize</code> - Create a bottom-up summary from the provided text chunks, and return the root summary</li><li><code class="cw qm qn qo qp b">SimpleSummarize</code> - Combine and truncate all text chunks, and summarize in a single LLM call</li></ul><h2><strong>Usage</strong></h2><p>As detailed above, you can directly set a response synthesizer in a query engine, or let the <code class="cw qm qn qo qp b">response_mode</code> fetch the relevant response synthesizer.</p><p>Furthermore though, you can directly call and use these synthesizers as low level modules. Here’s a small example:</p><pre><span id="950d" class="qy ov gt qp b bf qz ra l rb rc"><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> ServiceContext
<span class="hljs-keyword">from</span> llama_index.response_synthesizers <span class="hljs-keyword">import</span> CompactAndRefine

<span class="hljs-comment"># you can also configure the text_qa_template, refine_template, </span>
<span class="hljs-comment"># and streaming toggle from here</span>
response_synthesizer = CompactAndRefine(
  service_context=service_context.from_defaults()
)
response = response_synthesizer.get_response(
 <span class="hljs-string">"What skills does Bob have?"</span>,
  text_chunks=[<span class="hljs-string">" ..."</span>]  <span class="hljs-comment"># here would be text, hopefully about Bob's skills</span>
)</span></pre><h2>Resources</h2><p>Here are some additional notebooks showing how to use <code class="cw qm qn qo qp b">get_response_synthesizer</code> :</p><ul><li><a href="https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#low-level-api" rel="noopener ugc nofollow" target="_blank">Low-level API Usage Pattern</a></li><li><a href="https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html#plugin-retriever-into-query-engine" rel="noopener ugc nofollow" target="_blank">Custom Retrievers</a></li></ul><h1>Metadata Management Capabilities</h1><p>If you want to have good performance in any LLM application over your data (including a RAG pipeline), you need to make sure that your documents actually contain relevant context for the query. One way to do this is to add proper metadata, both at the document-level and after the documents have been parsed into text chunks (into Nodes).</p><p>We allow you to define metadata fields within a Document, customize the ID, and also customize the metadata text/format for LLM usage and embedding usage.</p><p><strong>Defining Metadata Fields</strong></p><pre><span id="95fa" class="qy ov gt qp b bf qz ra l rb rc">document = Document(
    text='text', 
    metadata={
        'filename': '<span class="hljs-symbol">&amp;lt;</span>doc_file_name<span class="hljs-symbol">&amp;gt;</span>', 
        'category': '<span class="hljs-symbol">&amp;lt;</span>category<span class="hljs-symbol">&amp;gt;</span>'
    }
)</span></pre><p><strong>Customizing the ID</strong></p><p>The ID of each document can be set multiple ways</p><ul><li>Within the constructor: <code class="cw qm qn qo qp b">document = Document(text="text", doc_id_="id")</code></li><li>After constructing the object: <code class="cw qm qn qo qp b">document.doc_id = "id"</code></li><li>Automatically using the <code class="cw qm qn qo qp b">SimpleDirectoryReader</code> : <code class="cw qm qn qo qp b">SimpleDirectoryReader(filename_as_id=True).load_data()</code></li></ul><p><strong>Customizing the Metadata Text for LLMs and Embeddings</strong></p><p>As seen above, you can set metadata containing useful information. By default, all the metadata will be seen by the embedding model and the LLM. However, sometimes you may want to only include data to bias embeddings, or only include data as extra information for the LLM!</p><p>With the new <code class="cw qm qn qo qp b">Document</code> objects, you can configure what each metadata field is used for:</p><pre><span id="62bd" class="qy ov gt qp b bf qz ra l rb rc">document = Document(
    text='text', 
    metadata={
        'filename': '<span class="hljs-symbol">&amp;lt;</span>doc_file_name<span class="hljs-symbol">&amp;gt;</span>', 
        'category': '<span class="hljs-symbol">&amp;lt;</span>category<span class="hljs-symbol">&amp;gt;</span>'
    },
    excluded_llm_metadata_keys=['filename', 'category'],
    excluded_embed_metadata_keys=['filename']
)</span></pre><p><strong>Customizing the Metadata Format Template</strong></p><p>When the metadata is inserted into the text, it follows a very specific format. This format is configurable at multiple levels:</p><pre><span id="21c6" class="qy ov gt qp b bf qz ra l rb rc">from llama_index.schema import MetadataMode

document = Document(
  text='text',
  metadata={"key": "val"},
  metadata_seperator="::",
    metadata_template="{key}=<span class="hljs-symbol">&amp;gt;</span>{value}",
    text_template="Metadata: {metadata_str}\\n-----\\nContent: {content}"
)
# available modes are ALL, NONE, LLM, and EMBED
print(document.get_content(metadata_mode=MetadataMode.ALL))
# output:
# Metadata: key=<span class="hljs-symbol">&amp;gt;</span>val
# -----
# text</span></pre><p>Please check out this guide for more <a href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_documents.html" rel="noopener ugc nofollow" target="_blank">details</a>!</p><h1>Full List of Breaking Changes</h1><h2>Response Synthesis + Node Postprocessors</h2><p>The <code class="cw qm qn qo qp b">ResponseSynthesizer</code> object class has been removed, and replaced with <code class="cw qm qn qo qp b">get_response_synthesizer</code> . In addition to this, node post processors are now handled by the query engine directly, and the old <code class="cw qm qn qo qp b">SentenceEmbeddingOptimizer</code> has been switched to become a node post processor instance itself.</p><p>Here is an example of the required migration to use all moved features.</p><p><strong>Old</strong></p><pre><span id="0837" class="qy ov gt qp b bf qz ra l rb rc"><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    VectorStoreIndex,
    ResponseSynthesizer,
)
<span class="hljs-keyword">from</span> llama_index.indices.postprocessor <span class="hljs-keyword">import</span> SimilarityPostprocessor
<span class="hljs-keyword">from</span> llama_index.optimizers <span class="hljs-keyword">import</span> SentenceEmbeddingOptimizer
<span class="hljs-keyword">from</span> llama_index.query_engine <span class="hljs-keyword">import</span> RetrieverQueryEngine

documents = ...
<span class="hljs-comment"># build index</span>
index = VectorStoreIndex.from_documents(documents)
<span class="hljs-comment"># configure retriever</span>
retriever = index.as_retriever(
   similarity_top_k=<span class="hljs-number">3</span>
)
<span class="hljs-comment"># configure response synthesizer</span>
response_synthesizer = ResponseSynthesizer.from_args(
   response_mode=<span class="hljs-string">"tree_summarize"</span>,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=<span class="hljs-number">0.7</span>),
        SentenceEmbeddingOptimizer(percentile_cutoff=<span class="hljs-number">0.5</span>)
    ]
)
<span class="hljs-comment"># assemble query engine</span>
query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer,
)</span></pre><p><strong>New</strong></p><pre><span id="d66c" class="qy ov gt qp b bf qz ra l rb rc">from llama_index import (
    VectorStoreIndex,
    get_response_synthesizer,
)
from llama_index.indices.postprocessor import (
    SimilarityPostprocessor,
    SentenceEmbeddingOptimizer
)

documents = ...
# build index
index = VectorStoreIndex.from_documents(documents)
# configure response synthesizer
response_synthesizer = get_response_synthesizer(
   response_mode="tree_summarize",
)
# assemble query engine
query_engine = index.as_query_engine(
  similarity_top_k=3,
    response_synthesizer=response_synthesizer,
    node_postprocessors=[
        SimilarityPostprocessor(similarity_cutoff=0.7),
        SentenceEmbeddingOptimizer(percentile_cutoff=0.5)
    ]
)</span></pre><h2>LLM Predictor</h2><p>While introducing a new LLM abstraction, we cleaned up the LLM Predictor and removed several deprecated functionalities:</p><ol><li>Remove <code class="cw qm qn qo qp b">ChatGPTLLMPredictor</code> and <code class="cw qm qn qo qp b">HuggingFaceLLMPredictor</code> (use <code class="cw qm qn qo qp b">OpenAI</code> and <code class="cw qm qn qo qp b">HuggingFaceLLM</code> instead, see <a href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/llms_migration_guide.html" rel="noopener ugc nofollow" target="_blank">migration guide</a>)</li><li>Remove support for setting <code class="cw qm qn qo qp b">cache</code> via <code class="cw qm qn qo qp b">LLMPredictor</code> constructor.</li><li>Removed <code class="cw qm qn qo qp b">llama_index.token_counter.token_counter</code> module (see <a href="https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html" rel="noopener ugc nofollow" target="_blank">migration guide</a>).</li></ol><p>Now, the LLM Predictor class is mostly a lightweight wrapper on top of the <code class="cw qm qn qo qp b">LLM</code> abstraction that handles:</p><ul><li>conversion of prompts to the string or chat message input format expected by the LLM</li><li>logging of prompts and responses to a callback manager</li></ul><p>We advice users to configure the <code class="cw qm qn qo qp b">llm</code> argument in <code class="cw qm qn qo qp b">ServiceContext</code> directly (instead of creating LLM Predictor).</p><h2>Chat Engine</h2><p>We updated the <code class="cw qm qn qo qp b">BaseChatEngine</code> interface to take in a <code class="cw qm qn qo qp b">List[ChatMessage]]</code> for the <code class="cw qm qn qo qp b">chat_history</code> instead of tuple of strings. This makes the data model consistent with the input/output of the <code class="cw qm qn qo qp b">LLM</code> , also more flexibility to specify consecutive messages with the same role.</p><p><strong>Old</strong></p><pre><span id="5a50" class="qy ov gt qp b bf qz ra l rb rc">engine = SimpleChatEngine.<span class="hljs-built_in">from_defaults</span>(
	chat_history=[(<span class="hljs-string">"human message"</span>, <span class="hljs-string">"assistant message"</span>)],
)
response = engine.<span class="hljs-built_in">chat</span>(<span class="hljs-string">"new human message"</span>)</span></pre><p><strong>New</strong></p><pre><span id="bdfc" class="qy ov gt qp b bf qz ra l rb rc">engine = SimpleChatEngine.from_defaults(
    service_context=mock_service_context,
    chat_history=[
        ChatMessage(role=MessageRole.USER, content="human message"),
        ChatMessage(role=MessageRole.ASSISTANT, content="assistant message"),
    ],
)
response = engine.chat("new human message")</span></pre><p>We also exposed <code class="cw qm qn qo qp b">chat_history</code> state as a property and supported overriding <code class="cw qm qn qo qp b">chat_history</code> in <code class="cw qm qn qo qp b">chat</code> and <code class="cw qm qn qo qp b">achat</code> endpoints.</p><h2>Prompt Helper</h2><p>We removed some previously deprecated arguments: <code class="cw qm qn qo qp b">max_input_size</code>, <code class="cw qm qn qo qp b">embedding_limit</code>, <code class="cw qm qn qo qp b">max_chunk_overlap</code></p><h1>Conclusion</h1><p>At a high-level, we hope that these changes continue to enable bottoms-up development of LLM applications over your data. We first encourage you to play around with our new modules on their own to get a sense what they do and where they can be used. Once you’re ready to use them in more advanced workflows, then you can figure out how to use our outer components to setup a sophisticated RAG pipeline.</p><p>As always, our <a href="https://github.com/jerryjliu/llama_index" rel="noopener ugc nofollow" target="_blank">repo</a> is here and our <a href="https://gpt-index.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">docs</a> are here. If you have thoughts/comments, don’t hesitate to hop in our <a href="https://discord.gg/dGcwcsnxhU" rel="noopener ugc nofollow" target="_blank">Discord</a>!</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"0915415e-a083-4d20-9716-2ad75dad4d30","_rev":"Ys5IzmCaJ2UnW2RAX7U23A","_type":"blogPost","_updatedAt":"2025-05-21T20:36:21Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"htmlContent":"\u003cp\u003eA few months ago, we launched LlamaIndex 0.6.0, which included a massive rewrite of our codebase to make our library more modular, customizable, and accessible to both beginner and advanced users:\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe created modular storage abstractions (data, indices), and compute abstractions (retrievers, query engines).\u003c/li\u003e\u003cli\u003eWe created a lower-level API where users could use our modules (retrievers, query engines) independently and customize it as part of a larger system.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eToday, we’re excited to launch LlamaIndex 0.7.0. Our latest release continues the theme of improving modularity/customizability at the lower level to enable \u003cstrong\u003ebottoms-up development of LLM applications over your data.\u003c/strong\u003e You now have even more control over using key abstractions: the LLM, our response synthesizer, and our Document and Node objects.\u003c/p\u003e\u003cul\u003e\u003cli\u003eWe’ve created \u003cstrong\u003estandalone LLM abstractions\u003c/strong\u003e (OpenAI, HuggingFace, PaLM).\u003c/li\u003e\u003cli\u003eWe’ve made our \u003cstrong\u003eresponse synthesis module an independent module\u003c/strong\u003e you can use completely independently of the rest of our abstractions — get rid of the prompt boilerplate of trying to figure out how to fit context within a context window.\u003c/li\u003e\u003cli\u003eWe’ve added \u003cstrong\u003eextensive metadata management capabilities\u003c/strong\u003e to our Document/Node objects — now you have complete control over context you decide to inject into your documents.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eBelow, we describe each section more in detail. We also outline a full list of breaking changes at the bottom.\u003c/p\u003e\u003ch1\u003eStandalone LLM Abstractions\u003c/h1\u003e\u003cp\u003eWe’ve created standalone LLM abstractions for OpenAI, HuggingFace, and PaLM. These abstractions can be used on their own, or as part of an existing LlamaIndex system (query engines, retrievers).\u003c/p\u003e\u003ch2\u003eHigh-level Motivation\u003c/h2\u003e\u003cp\u003eWe did this for multiple reasons:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCleaner abstractions in the codebase. Before, our \u003ccode class=\"cw qm qn qo qp b\"\u003eLLMPredictor\u003c/code\u003e class had a ton of leaky abstractions with the underlying LangChain LLM class. This made our LLM abstractions hard to reason about, and hard to customize.\u003c/li\u003e\u003cli\u003eSlightly cleaner dev UX. Before, if you wanted to customize the default LLM (for instance, use “text-davinci-003”, you had to import the correct LangChain class, wrap it in our LLMPredictor, and then pass it to ServiceContext. Now it’s easy to just import our LLM abstraction (which is natively documented with our docs) and plug it into ServiceContext. Of course, you can still use LangChain’s LLMs if you wish.\u003c/li\u003e\u003cli\u003eConducive to bottoms-up development: it makes sense to play around with these LLM modules independently before plugging them in as part of a larger system. It’s reflective of our bigger push in 0.7.0 to let users compose their own workflows.\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eUsing on their own\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eOur LLM abstractions support both \u003ccode class=\"cw qm qn qo qp b\"\u003ecomplete\u003c/code\u003e and \u003ccode class=\"cw qm qn qo qp b\"\u003echat\u003c/code\u003e endpoints. The main difference is that \u003ccode class=\"cw qm qn qo qp b\"\u003ecomplete\u003c/code\u003e is designed to take in a simple string input, and output a \u003ccode class=\"cw qm qn qo qp b\"\u003eCompletionResponse\u003c/code\u003e (containing text output + additional fields). \u003ccode class=\"cw qm qn qo qp b\"\u003echat\u003c/code\u003e takes in a \u003ccode class=\"cw qm qn qo qp b\"\u003eChatMessage\u003c/code\u003e and outputs a \u003ccode class=\"cw qm qn qo qp b\"\u003eChatResponse\u003c/code\u003e (containing a chat message + additional fields).\u003c/p\u003e\u003cp\u003eThese LLM endpoints also natively support streaming via \u003ccode class=\"cw qm qn qo qp b\"\u003estream_complete\u003c/code\u003e and \u003ccode class=\"cw qm qn qo qp b\"\u003estream_chat\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eHere’s on how you can use the LLM abstractions on their own:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"127a\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\n\u003cspan class=\"hljs-comment\"\u003e# using complete endpoint\u003c/span\u003e\nresp = OpenAI().complete(\u003cspan class=\"hljs-string\"\u003e'Paul Graham is '\u003c/span\u003e)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(resp)\n\u003cspan class=\"hljs-comment\"\u003e# get raw object\u003c/span\u003e\nresp_raw = resp.raw\n\u003cspan class=\"hljs-comment\"\u003e# using chat endpoint\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ChatMessage, OpenAI\nmessages = [\n    ChatMessage(role=\u003cspan class=\"hljs-string\"\u003e'system'\u003c/span\u003e, content=\u003cspan class=\"hljs-string\"\u003e'You are a pirate with a colorful personality'\u003c/span\u003e),\n    ChatMessage(role=\u003cspan class=\"hljs-string\"\u003e'user'\u003c/span\u003e, content=\u003cspan class=\"hljs-string\"\u003e'What is your name'\u003c/span\u003e)\n]\nresp = OpenAI().chat(messages)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(resp)\n\u003cspan class=\"hljs-comment\"\u003e# get raw object\u003c/span\u003e\nresp_raw = resp.raw\n\u003cspan class=\"hljs-comment\"\u003e# using streaming endpoint\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\nllm = OpenAI()\nresp = llm.stream_complete(\u003cspan class=\"hljs-string\"\u003e'Paul Graham is '\u003c/span\u003e)\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e delta \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e resp:\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(delta, end=\u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eHere’s how you can use the LLM abstractions as part of an overall LlamaIndex system.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"289e\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.indices.service_context \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ServiceContext\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e VectorStoreIndex\n\nllm = OpenAI(model=\u003cspan class=\"hljs-string\"\u003e'gpt-3.5-turbo'\u003c/span\u003e, temperature=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e)\nservice_context = ServiceContext.from_defaults(llm=llm)\nindex = VectorStoreIndex.from_documents(docs, service_context=service_context)\nresponse = index.as_query_engine().query(\u003cspan class=\"hljs-string\"\u003e\"\u0026amp;lt;question\u0026amp;gt;\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNote: Our top-level \u003ccode class=\"cw qm qn qo qp b\"\u003eLLMPredictor\u003c/code\u003e still exists but is less user-facing (and we might deprecate in the future). Also, you can still use LangChain LLMs through our \u003ccode class=\"cw qm qn qo qp b\"\u003eLangChainLLM\u003c/code\u003e class.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eResources\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAll of our notebooks have by default been updated to use our native OpenAI LLM integration. Here’s some resources to show both the LLM abstraction on its own as well as how it can be used in the overall system:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/openai.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI LLM\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/llm/llm_predictor.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUsing LLM in LLMPredictor\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-changing-the-underlying-llm\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eChanging LLM within Index/Query Engine\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_llms.html#example-using-a-custom-llm-model-advanced\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDefining a custom LLM Model\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eStandalone Response Synthesis Modules\u003c/h1\u003e\u003ch2\u003e\u003cstrong\u003eContext\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn any RAG system, there is retrieval and there is synthesis. The responsibility of the synthesis component is to take in incoming context as input, and synthesize a response using the LLM.\u003c/p\u003e\u003cp\u003eFundamentally, the synthesis module needs to synthesize a response over \u003cstrong\u003eany\u003c/strong\u003e context list, regardless of how long that context list is. This is essentially “boilerplate” that an LLM developer / \u003ca href=\"https://www.latent.space/p/ai-engineer\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e“AI engineer”\u003c/a\u003e must write.\u003c/p\u003e\u003cp\u003eWe had this as an internal abstraction in LlamaIndex before (as a \u003ccode class=\"cw qm qn qo qp b\"\u003eResponseSynthesizer\u003c/code\u003e), but the external-facing UX was unfriendly to users. The actual piece that gathered responses (the \u003ccode class=\"cw qm qn qo qp b\"\u003eResponseBuilder\u003c/code\u003e ) was hard to customize, and the \u003ccode class=\"cw qm qn qo qp b\"\u003eResponseSynthesizer\u003c/code\u003e itself was adding an extra unnecessary layer.\u003c/p\u003e\u003cp\u003eNow we have a set of standalone modules that you can easily import. Previously, when you set the \u003ccode class=\"cw qm qn qo qp b\"\u003eresponse_mode\u003c/code\u003e in the query engine, these were being setup for you. Now they are more directly available and user-facing.\u003c/p\u003e\u003cp\u003eHere’s a list of all the new \u003ccode class=\"cw qm qn qo qp b\"\u003eResponse Synthesiszer\u003c/code\u003e modules available from \u003ccode class=\"cw qm qn qo qp b\"\u003ellama_index.response_synthesizer\u003c/code\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw qm qn qo qp b\"\u003eRefine\u003c/code\u003e - Query an LLM, sending each text chunk individually. After the first LLM call, the existing answer is also sent to the LLM for updating and refinement using the next text chunk.\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qm qn qo qp b\"\u003eAccumulate\u003c/code\u003e - Query an LLM with the same prompt across multiple text chunks, and return a formatted list of responses\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qm qn qo qp b\"\u003eCompact\u003c/code\u003e - The same as \u003ccode class=\"cw qm qn qo qp b\"\u003eRefine\u003c/code\u003e, but puts as much text as possible into each LLM call\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qm qn qo qp b\"\u003eCompactAndAccumulate\u003c/code\u003e - The same as \u003ccode class=\"cw qm qn qo qp b\"\u003eAccumulate\u003c/code\u003e, but puts as much text as possible\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qm qn qo qp b\"\u003eTreeSummarize\u003c/code\u003e - Create a bottom-up summary from the provided text chunks, and return the root summary\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qm qn qo qp b\"\u003eSimpleSummarize\u003c/code\u003e - Combine and truncate all text chunks, and summarize in a single LLM call\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003e\u003cstrong\u003eUsage\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eAs detailed above, you can directly set a response synthesizer in a query engine, or let the \u003ccode class=\"cw qm qn qo qp b\"\u003eresponse_mode\u003c/code\u003e fetch the relevant response synthesizer.\u003c/p\u003e\u003cp\u003eFurthermore though, you can directly call and use these synthesizers as low level modules. Here’s a small example:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"950d\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ServiceContext\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.response_synthesizers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e CompactAndRefine\n\n\u003cspan class=\"hljs-comment\"\u003e# you can also configure the text_qa_template, refine_template, \u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# and streaming toggle from here\u003c/span\u003e\nresponse_synthesizer = CompactAndRefine(\n  service_context=service_context.from_defaults()\n)\nresponse = response_synthesizer.get_response(\n \u003cspan class=\"hljs-string\"\u003e\"What skills does Bob have?\"\u003c/span\u003e,\n  text_chunks=[\u003cspan class=\"hljs-string\"\u003e\" ...\"\u003c/span\u003e]  \u003cspan class=\"hljs-comment\"\u003e# here would be text, hopefully about Bob's skills\u003c/span\u003e\n)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eResources\u003c/h2\u003e\u003cp\u003eHere are some additional notebooks showing how to use \u003ccode class=\"cw qm qn qo qp b\"\u003eget_response_synthesizer\u003c/code\u003e :\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://gpt-index.readthedocs.io/en/latest/guides/primer/usage_pattern.html#low-level-api\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLow-level API Usage Pattern\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html#plugin-retriever-into-query-engine\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCustom Retrievers\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eMetadata Management Capabilities\u003c/h1\u003e\u003cp\u003eIf you want to have good performance in any LLM application over your data (including a RAG pipeline), you need to make sure that your documents actually contain relevant context for the query. One way to do this is to add proper metadata, both at the document-level and after the documents have been parsed into text chunks (into Nodes).\u003c/p\u003e\u003cp\u003eWe allow you to define metadata fields within a Document, customize the ID, and also customize the metadata text/format for LLM usage and embedding usage.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eDefining Metadata Fields\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"95fa\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003edocument = Document(\n    text='text', \n    metadata={\n        'filename': '\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003edoc_file_name\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e', \n        'category': '\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003ecategory\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e'\n    }\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eCustomizing the ID\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe ID of each document can be set multiple ways\u003c/p\u003e\u003cul\u003e\u003cli\u003eWithin the constructor: \u003ccode class=\"cw qm qn qo qp b\"\u003edocument = Document(text=\"text\", doc_id_=\"id\")\u003c/code\u003e\u003c/li\u003e\u003cli\u003eAfter constructing the object: \u003ccode class=\"cw qm qn qo qp b\"\u003edocument.doc_id = \"id\"\u003c/code\u003e\u003c/li\u003e\u003cli\u003eAutomatically using the \u003ccode class=\"cw qm qn qo qp b\"\u003eSimpleDirectoryReader\u003c/code\u003e : \u003ccode class=\"cw qm qn qo qp b\"\u003eSimpleDirectoryReader(filename_as_id=True).load_data()\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eCustomizing the Metadata Text for LLMs and Embeddings\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eAs seen above, you can set metadata containing useful information. By default, all the metadata will be seen by the embedding model and the LLM. However, sometimes you may want to only include data to bias embeddings, or only include data as extra information for the LLM!\u003c/p\u003e\u003cp\u003eWith the new \u003ccode class=\"cw qm qn qo qp b\"\u003eDocument\u003c/code\u003e objects, you can configure what each metadata field is used for:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"62bd\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003edocument = Document(\n    text='text', \n    metadata={\n        'filename': '\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003edoc_file_name\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e', \n        'category': '\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003ecategory\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e'\n    },\n    excluded_llm_metadata_keys=['filename', 'category'],\n    excluded_embed_metadata_keys=['filename']\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eCustomizing the Metadata Format Template\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhen the metadata is inserted into the text, it follows a very specific format. This format is configurable at multiple levels:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"21c6\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003efrom llama_index.schema import MetadataMode\n\ndocument = Document(\n  text='text',\n  metadata={\"key\": \"val\"},\n  metadata_seperator=\"::\",\n    metadata_template=\"{key}=\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e{value}\",\n    text_template=\"Metadata: {metadata_str}\\\\n-----\\\\nContent: {content}\"\n)\n# available modes are ALL, NONE, LLM, and EMBED\nprint(document.get_content(metadata_mode=MetadataMode.ALL))\n# output:\n# Metadata: key=\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003eval\n# -----\n# text\u003c/span\u003e\u003c/pre\u003e\u003cp\u003ePlease check out this guide for more \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/custom_documents.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edetails\u003c/a\u003e!\u003c/p\u003e\u003ch1\u003eFull List of Breaking Changes\u003c/h1\u003e\u003ch2\u003eResponse Synthesis + Node Postprocessors\u003c/h2\u003e\u003cp\u003eThe \u003ccode class=\"cw qm qn qo qp b\"\u003eResponseSynthesizer\u003c/code\u003e object class has been removed, and replaced with \u003ccode class=\"cw qm qn qo qp b\"\u003eget_response_synthesizer\u003c/code\u003e . In addition to this, node post processors are now handled by the query engine directly, and the old \u003ccode class=\"cw qm qn qo qp b\"\u003eSentenceEmbeddingOptimizer\u003c/code\u003e has been switched to become a node post processor instance itself.\u003c/p\u003e\u003cp\u003eHere is an example of the required migration to use all moved features.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOld\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0837\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    VectorStoreIndex,\n    ResponseSynthesizer,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.indices.postprocessor \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SimilarityPostprocessor\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.optimizers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SentenceEmbeddingOptimizer\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.query_engine \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e RetrieverQueryEngine\n\ndocuments = ...\n\u003cspan class=\"hljs-comment\"\u003e# build index\u003c/span\u003e\nindex = VectorStoreIndex.from_documents(documents)\n\u003cspan class=\"hljs-comment\"\u003e# configure retriever\u003c/span\u003e\nretriever = index.as_retriever(\n   similarity_top_k=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e\n)\n\u003cspan class=\"hljs-comment\"\u003e# configure response synthesizer\u003c/span\u003e\nresponse_synthesizer = ResponseSynthesizer.from_args(\n   response_mode=\u003cspan class=\"hljs-string\"\u003e\"tree_summarize\"\u003c/span\u003e,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=\u003cspan class=\"hljs-number\"\u003e0.7\u003c/span\u003e),\n        SentenceEmbeddingOptimizer(percentile_cutoff=\u003cspan class=\"hljs-number\"\u003e0.5\u003c/span\u003e)\n    ]\n)\n\u003cspan class=\"hljs-comment\"\u003e# assemble query engine\u003c/span\u003e\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eNew\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d66c\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003efrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.indices.postprocessor import (\n    SimilarityPostprocessor,\n    SentenceEmbeddingOptimizer\n)\n\ndocuments = ...\n# build index\nindex = VectorStoreIndex.from_documents(documents)\n# configure response synthesizer\nresponse_synthesizer = get_response_synthesizer(\n   response_mode=\"tree_summarize\",\n)\n# assemble query engine\nquery_engine = index.as_query_engine(\n  similarity_top_k=3,\n    response_synthesizer=response_synthesizer,\n    node_postprocessors=[\n        SimilarityPostprocessor(similarity_cutoff=0.7),\n        SentenceEmbeddingOptimizer(percentile_cutoff=0.5)\n    ]\n)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eLLM Predictor\u003c/h2\u003e\u003cp\u003eWhile introducing a new LLM abstraction, we cleaned up the LLM Predictor and removed several deprecated functionalities:\u003c/p\u003e\u003col\u003e\u003cli\u003eRemove \u003ccode class=\"cw qm qn qo qp b\"\u003eChatGPTLLMPredictor\u003c/code\u003e and \u003ccode class=\"cw qm qn qo qp b\"\u003eHuggingFaceLLMPredictor\u003c/code\u003e (use \u003ccode class=\"cw qm qn qo qp b\"\u003eOpenAI\u003c/code\u003e and \u003ccode class=\"cw qm qn qo qp b\"\u003eHuggingFaceLLM\u003c/code\u003e instead, see \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/how_to/customization/llms_migration_guide.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emigration guide\u003c/a\u003e)\u003c/li\u003e\u003cli\u003eRemove support for setting \u003ccode class=\"cw qm qn qo qp b\"\u003ecache\u003c/code\u003e via \u003ccode class=\"cw qm qn qo qp b\"\u003eLLMPredictor\u003c/code\u003e constructor.\u003c/li\u003e\u003cli\u003eRemoved \u003ccode class=\"cw qm qn qo qp b\"\u003ellama_index.token_counter.token_counter\u003c/code\u003e module (see \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/how_to/callbacks/token_counting_migration.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emigration guide\u003c/a\u003e).\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eNow, the LLM Predictor class is mostly a lightweight wrapper on top of the \u003ccode class=\"cw qm qn qo qp b\"\u003eLLM\u003c/code\u003e abstraction that handles:\u003c/p\u003e\u003cul\u003e\u003cli\u003econversion of prompts to the string or chat message input format expected by the LLM\u003c/li\u003e\u003cli\u003elogging of prompts and responses to a callback manager\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe advice users to configure the \u003ccode class=\"cw qm qn qo qp b\"\u003ellm\u003c/code\u003e argument in \u003ccode class=\"cw qm qn qo qp b\"\u003eServiceContext\u003c/code\u003e directly (instead of creating LLM Predictor).\u003c/p\u003e\u003ch2\u003eChat Engine\u003c/h2\u003e\u003cp\u003eWe updated the \u003ccode class=\"cw qm qn qo qp b\"\u003eBaseChatEngine\u003c/code\u003e interface to take in a \u003ccode class=\"cw qm qn qo qp b\"\u003eList[ChatMessage]]\u003c/code\u003e for the \u003ccode class=\"cw qm qn qo qp b\"\u003echat_history\u003c/code\u003e instead of tuple of strings. This makes the data model consistent with the input/output of the \u003ccode class=\"cw qm qn qo qp b\"\u003eLLM\u003c/code\u003e , also more flexibility to specify consecutive messages with the same role.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eOld\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5a50\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003eengine = SimpleChatEngine.\u003cspan class=\"hljs-built_in\"\u003efrom_defaults\u003c/span\u003e(\n\tchat_history=[(\u003cspan class=\"hljs-string\"\u003e\"human message\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"assistant message\"\u003c/span\u003e)],\n)\nresponse = engine.\u003cspan class=\"hljs-built_in\"\u003echat\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"new human message\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eNew\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"bdfc\" class=\"qy ov gt qp b bf qz ra l rb rc\"\u003eengine = SimpleChatEngine.from_defaults(\n    service_context=mock_service_context,\n    chat_history=[\n        ChatMessage(role=MessageRole.USER, content=\"human message\"),\n        ChatMessage(role=MessageRole.ASSISTANT, content=\"assistant message\"),\n    ],\n)\nresponse = engine.chat(\"new human message\")\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe also exposed \u003ccode class=\"cw qm qn qo qp b\"\u003echat_history\u003c/code\u003e state as a property and supported overriding \u003ccode class=\"cw qm qn qo qp b\"\u003echat_history\u003c/code\u003e in \u003ccode class=\"cw qm qn qo qp b\"\u003echat\u003c/code\u003e and \u003ccode class=\"cw qm qn qo qp b\"\u003eachat\u003c/code\u003e endpoints.\u003c/p\u003e\u003ch2\u003ePrompt Helper\u003c/h2\u003e\u003cp\u003eWe removed some previously deprecated arguments: \u003ccode class=\"cw qm qn qo qp b\"\u003emax_input_size\u003c/code\u003e, \u003ccode class=\"cw qm qn qo qp b\"\u003eembedding_limit\u003c/code\u003e, \u003ccode class=\"cw qm qn qo qp b\"\u003emax_chunk_overlap\u003c/code\u003e\u003c/p\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eAt a high-level, we hope that these changes continue to enable bottoms-up development of LLM applications over your data. We first encourage you to play around with our new modules on their own to get a sense what they do and where they can be used. Once you’re ready to use them in more advanced workflows, then you can figure out how to use our outer components to setup a sophisticated RAG pipeline.\u003c/p\u003e\u003cp\u003eAs always, our \u003ca href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003erepo\u003c/a\u003e is here and our \u003ca href=\"https://gpt-index.readthedocs.io/en/latest/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edocs\u003c/a\u003e are here. If you have thoughts/comments, don’t hesitate to hop in our \u003ca href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDiscord\u003c/a\u003e!\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-89a392fad09ab8e8294bab827a369cd213426275-1970x1412-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/89a392fad09ab8e8294bab827a369cd213426275-1970x1412.png","publishedDate":"2023-07-04","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"}],"slug":{"_type":"slug","current":"llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"3e4feb2c-4916-495e-a9db-a829131f6c42","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"machine-learning"},"title":"Machine Learning"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"78713226-8bff-400f-bbfe-fd8a3d90be1d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"nlp"},"title":"NLP"}],"title":"LlamaIndex 0.7.0: Better Enabling Bottoms-Up LLM Application Development"},"publishedDate":"Invalid Date"},"params":{"slug":"llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"llamaindex-0-7-0-better-enabling-bottoms-up-llm-application-development-959db8f75024"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>