<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640.jpg"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640.jpg"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="320" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/ravi-theja">Ravi Theja</a> <!-- -->•<!-- --> <!-- -->2023-10-05</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/openai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">OpenAI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/retrieval"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Retrieval</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><h1>Introduction</h1><p>Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the <code class="cw pk pl pm pn b">chunk_size</code>. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex <code class="cw pk pl pm pn b">Response Evaluation</code> comes in handy. In this blog post, we'll guide you through the steps to determine the best <code class="cw pk pl pm pn b">chunk size</code> using LlamaIndex’s <code class="cw pk pl pm pn b">Response Evaluation</code> module. If you're unfamiliar with the <code class="cw pk pl pm pn b">Response</code> Evaluation module, we recommend reviewing its <a href="https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html" rel="noopener ugc nofollow" target="_blank">documentation</a> before proceeding.</p><h1>Why Chunk Size Matters</h1><p>Choosing the right <code class="cw pk pl pm pn b">chunk_size</code> is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:</p><ol><li><strong>Relevance and Granularity</strong>: A small <code class="cw pk pl pm pn b">chunk_size</code>, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the <code class="cw pk pl pm pn b">similarity_top_k</code> setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.</li><li><strong>Response Generation Time</strong>: As the <code class="cw pk pl pm pn b">chunk_size</code> increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.</li></ol><p>In essence, determining the optimal <code class="cw pk pl pm pn b">chunk_size</code> is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.</p><p>For a practical evaluation in choosing the right <code class="cw pk pl pm pn b">chunk_size</code>, you can access and run the following setup on this <a href="https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong>Google Colab Notebook</strong></a>.</p><h1>Setup</h1><p>Before embarking on the experiment, we need to ensure all requisite modules are imported:</p><pre><span id="c77c" class="qk np gt pn b bf ql qm l qn qo"><span class="hljs-keyword">import</span> nest_asyncio

nest_asyncio.apply()

<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    SimpleDirectoryReader,
    VectorStoreIndex,
    ServiceContext,
)
<span class="hljs-keyword">from</span> llama_index.evaluation <span class="hljs-keyword">import</span> (
    DatasetGenerator,
    FaithfulnessEvaluator,
    RelevancyEvaluator
)
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI

<span class="hljs-keyword">import</span> openai
<span class="hljs-keyword">import</span> time
openai.api_key = <span class="hljs-string">'OPENAI-API-KEY'</span></span></pre><h1>Download Data</h1><p>We’ll be using the Uber 10K SEC Filings for 2021 for this experiment.</p><pre><span id="b979" class="qk np gt pn b bf ql qm l qn qo">!mkdir -p 'data/10k/'
!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'</span></pre><h1>Load Data</h1><p>Let’s load our document.</p><pre><span id="e95c" class="qk np gt pn b bf ql qm l qn qo">documents = SimpleDirectoryReader("./data/10k/").load_data()</span></pre><h1>Question Generation</h1><p>To select the right <code class="cw pk pl pm pn b">chunk_size</code>, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various <code class="cw pk pl pm pn b">chunk_sizes</code>. The <code class="cw pk pl pm pn b">DatasetGenerator</code> will help us generate questions from the documents.</p><pre><span id="ad22" class="qk np gt pn b bf ql qm l qn qo">data_generator = DatasetGenerator.from_documents(documents)
eval_questions = data_generator.generate_questions_from_nodes()</span></pre><h1>Setting Up Evaluators</h1><p>We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, <code class="cw pk pl pm pn b">FaithfulnessEvaluator</code> and <code class="cw pk pl pm pn b">RelevancyEvaluator</code>, are initialised with the <code class="cw pk pl pm pn b">service_context</code> .</p><ol><li><strong>Faithfulness Evaluator</strong> — It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.</li><li><strong>Relevancy Evaluator</strong> — It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query.</li></ol><pre><span id="d485" class="qk np gt pn b bf ql qm l qn qo"># We will use GPT-4 for evaluating the responses
gpt4 = OpenAI(temperature=0, model="gpt-4")

# Define service context for GPT-4 for evaluation
service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)

# Define Faithfulness and Relevancy Evaluators which are based on GPT-4
faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)
relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)</span></pre><h1>Response Evaluation For A Chunk Size</h1><p>We evaluate each chunk_size based on 3 metrics.</p><ol><li>Average Response Time.</li><li>Average Faithfulness.</li><li>Average Relevancy.</li></ol><p>Here’s a function, <code class="cw pk pl pm pn b">evaluate_response_time_and_accuracy</code>, that does just that which has:</p><ol><li>VectorIndex Creation.</li><li>Building the Query Engine.</li><li>Metrics Calculation.</li></ol><pre><span id="8a18" class="qk np gt pn b bf ql qm l qn qo"><span class="hljs-comment"># Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size</span>
<span class="hljs-comment"># We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_response_time_and_accuracy</span>(<span class="hljs-params">chunk_size, eval_questions</span>):
    <span class="hljs-string">"""
    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.
    
    Parameters:
    chunk_size (int): The size of data chunks being processed.
    
    Returns:
    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.
    """</span>

    total_response_time = <span class="hljs-number">0</span>
    total_faithfulness = <span class="hljs-number">0</span>
    total_relevancy = <span class="hljs-number">0</span>

    <span class="hljs-comment"># create vector index</span>
    llm = OpenAI(model=<span class="hljs-string">"gpt-3.5-turbo"</span>)
    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)
    vector_index = VectorStoreIndex.from_documents(
        eval_documents, service_context=service_context
    )
    <span class="hljs-comment"># build query engine</span>
    query_engine = vector_index.as_query_engine()
    num_questions = <span class="hljs-built_in">len</span>(eval_questions)

    <span class="hljs-comment"># Iterate over each question in eval_questions to compute metrics.</span>
    <span class="hljs-comment"># While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),</span>
    <span class="hljs-comment"># we're using a loop here to specifically measure response time for different chunk sizes.</span>
    <span class="hljs-keyword">for</span> question <span class="hljs-keyword">in</span> eval_questions:
        start_time = time.time()
        response_vector = query_engine.query(question)
        elapsed_time = time.time() - start_time
        
        faithfulness_result = faithfulness_gpt4.evaluate_response(
            response=response_vector
        ).passing
        
        relevancy_result = relevancy_gpt4.evaluate_response(
            query=question, response=response_vector
        ).passing

        total_response_time += elapsed_time
        total_faithfulness += faithfulness_result
        total_relevancy += relevancy_result

    average_response_time = total_response_time / num_questions
    average_faithfulness = total_faithfulness / num_questions
    average_relevancy = total_relevancy / num_questions

    <span class="hljs-keyword">return</span> average_response_time, average_faithfulness, average_relevancy</span></pre><h1>Testing Across Different Chunk Sizes</h1><p>We’ll evaluate a range of chunk sizes to identify which offers the most promising metrics.</p><pre><span id="262a" class="qk np gt pn b bf ql qm l qn qo">chunk_sizes = [<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">2048</span>]

<span class="hljs-keyword">for</span> chunk_size <span class="hljs-keyword">in</span> chunk_sizes:
  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Chunk size <span class="hljs-subst">{chunk_size}</span> - Average Response time: <span class="hljs-subst">{avg_response_time:<span class="hljs-number">.2</span>f}</span>s, Average Faithfulness: <span class="hljs-subst">{avg_faithfulness:<span class="hljs-number">.2</span>f}</span>, Average Relevancy: <span class="hljs-subst">{avg_relevancy:<span class="hljs-number">.2</span>f}</span>"</span>)</span></pre><h1>Bringing It All Together</h1><p>Let’s compile the processes:</p><pre><span id="ba5d" class="qk np gt pn b bf ql qm l qn qo"><span class="hljs-keyword">import</span> nest_asyncio

nest_asyncio.apply()

<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    SimpleDirectoryReader,
    VectorStoreIndex,
    ServiceContext,
)
<span class="hljs-keyword">from</span> llama_index.evaluation <span class="hljs-keyword">import</span> (
    DatasetGenerator,
    FaithfulnessEvaluator,
    RelevancyEvaluator
)
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI

<span class="hljs-keyword">import</span> openai
<span class="hljs-keyword">import</span> time

openai.api_key = <span class="hljs-string">'OPENAI-API-KEY'</span>

<span class="hljs-comment"># Download Data</span>
!mkdir -p <span class="hljs-string">'data/10k/'</span>
!wget <span class="hljs-string">'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'</span> -O <span class="hljs-string">'data/10k/uber_2021.pdf'</span>

<span class="hljs-comment"># Load Data</span>
reader = SimpleDirectoryReader(<span class="hljs-string">"./data/10k/"</span>)
documents = reader.load_data()

<span class="hljs-comment"># To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.</span>
eval_documents = documents[:<span class="hljs-number">20</span>]
data_generator = DatasetGenerator.from_documents()
eval_questions = data_generator.generate_questions_from_nodes(num = <span class="hljs-number">20</span>)

<span class="hljs-comment"># We will use GPT-4 for evaluating the responses</span>
gpt4 = OpenAI(temperature=<span class="hljs-number">0</span>, model=<span class="hljs-string">"gpt-4"</span>)

<span class="hljs-comment"># Define service context for GPT-4 for evaluation</span>
service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)

<span class="hljs-comment"># Define Faithfulness and Relevancy Evaluators which are based on GPT-4</span>
faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)
relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)

<span class="hljs-comment"># Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_response_time_and_accuracy</span>(<span class="hljs-params">chunk_size</span>):
    total_response_time = <span class="hljs-number">0</span>
    total_faithfulness = <span class="hljs-number">0</span>
    total_relevancy = <span class="hljs-number">0</span>

    <span class="hljs-comment"># create vector index</span>
    llm = OpenAI(model=<span class="hljs-string">"gpt-3.5-turbo"</span>)
    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)
    vector_index = VectorStoreIndex.from_documents(
        eval_documents, service_context=service_context
    )

    query_engine = vector_index.as_query_engine()
    num_questions = <span class="hljs-built_in">len</span>(eval_questions)

    <span class="hljs-keyword">for</span> question <span class="hljs-keyword">in</span> eval_questions:
        start_time = time.time()
        response_vector = query_engine.query(question)
        elapsed_time = time.time() - start_time
        
        faithfulness_result = faithfulness_gpt4.evaluate_response(
            response=response_vector
        ).passing
        
        relevancy_result = relevancy_gpt4.evaluate_response(
            query=question, response=response_vector
        ).passing

        total_response_time += elapsed_time
        total_faithfulness += faithfulness_result
        total_relevancy += relevancy_result

    average_response_time = total_response_time / num_questions
    average_faithfulness = total_faithfulness / num_questions
    average_relevancy = total_relevancy / num_questions

    <span class="hljs-keyword">return</span> average_response_time, average_faithfulness, average_relevancy

<span class="hljs-comment"># Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.</span>
<span class="hljs-keyword">for</span> chunk_size <span class="hljs-keyword">in</span> [<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">2048</span>]
  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)
  <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Chunk size <span class="hljs-subst">{chunk_size}</span> - Average Response time: <span class="hljs-subst">{avg_time:<span class="hljs-number">.2</span>f}</span>s, Average Faithfulness: <span class="hljs-subst">{avg_faithfulness:<span class="hljs-number">.2</span>f}</span>, Average Relevancy: <span class="hljs-subst">{avg_relevancy:<span class="hljs-number">.2</span>f}</span>"</span>)</span></pre><h1>Result</h1><figure><img src="/blog/images/1*dynabe9lAsB9DBe7XqmxQw.png" alt="" width="662" height="212"></figure><p>The above table illustrates that as the chunk size increases, there is a minor uptick in the Average Response Time. Interestingly, the Average Faithfulness seems to reach its zenith at <code class="cw pk pl pm pn b">chunk_size</code>of 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy.</p><h1>Conclusion</h1><p>Identifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. With LlamaIndex’s <code class="cw pk pl pm pn b">Response Evaluation</code> module, you can experiment with various sizes and base your decisions on concrete data. When building a RAG system, always remember that <code class="cw pk pl pm pn b">chunk_size</code> is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"994ed63a-1bea-41a7-8017-69d47f48cd1a","_rev":"Ys5IzmCaJ2UnW2RAX7UPXF","_type":"blogPost","_updatedAt":"2025-05-21T20:38:48Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:58:55Z","_id":"60575af5-a5c2-40f6-9aab-d5e02da9c000","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Ravi Theja","slug":{"_type":"slug","current":"ravi-theja"}}],"featured":false,"htmlContent":"\u003ch1\u003eIntroduction\u003c/h1\u003e\u003cp\u003eRetrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex \u003ccode class=\"cw pk pl pm pn b\"\u003eResponse Evaluation\u003c/code\u003e comes in handy. In this blog post, we'll guide you through the steps to determine the best \u003ccode class=\"cw pk pl pm pn b\"\u003echunk size\u003c/code\u003e using LlamaIndex’s \u003ccode class=\"cw pk pl pm pn b\"\u003eResponse Evaluation\u003c/code\u003e module. If you're unfamiliar with the \u003ccode class=\"cw pk pl pm pn b\"\u003eResponse\u003c/code\u003e Evaluation module, we recommend reviewing its \u003ca href=\"https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edocumentation\u003c/a\u003e before proceeding.\u003c/p\u003e\u003ch1\u003eWhy Chunk Size Matters\u003c/h1\u003e\u003cp\u003eChoosing the right \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eRelevance and Granularity\u003c/strong\u003e: A small \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the \u003ccode class=\"cw pk pl pm pn b\"\u003esimilarity_top_k\u003c/code\u003e setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eResponse Generation Time\u003c/strong\u003e: As the \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eIn essence, determining the optimal \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use case and dataset.\u003c/p\u003e\u003cp\u003eFor a practical evaluation in choosing the right \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e, you can access and run the following setup on this \u003ca href=\"https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eGoogle Colab Notebook\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\u003ch1\u003eSetup\u003c/h1\u003e\u003cp\u003eBefore embarking on the experiment, we need to ensure all requisite modules are imported:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c77c\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e nest_asyncio\n\nnest_asyncio.apply()\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.evaluation \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e openai\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\nopenai.api_key = \u003cspan class=\"hljs-string\"\u003e'OPENAI-API-KEY'\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eDownload Data\u003c/h1\u003e\u003cp\u003eWe’ll be using the Uber 10K SEC Filings for 2021 for this experiment.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b979\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003e!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eLoad Data\u003c/h1\u003e\u003cp\u003eLet’s load our document.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e95c\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003edocuments = SimpleDirectoryReader(\"./data/10k/\").load_data()\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eQuestion Generation\u003c/h1\u003e\u003cp\u003eTo select the right \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_sizes\u003c/code\u003e. The \u003ccode class=\"cw pk pl pm pn b\"\u003eDatasetGenerator\u003c/code\u003e will help us generate questions from the documents.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ad22\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003edata_generator = DatasetGenerator.from_documents(documents)\neval_questions = data_generator.generate_questions_from_nodes()\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eSetting Up Evaluators\u003c/h1\u003e\u003cp\u003eWe are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, \u003ccode class=\"cw pk pl pm pn b\"\u003eFaithfulnessEvaluator\u003c/code\u003e and \u003ccode class=\"cw pk pl pm pn b\"\u003eRelevancyEvaluator\u003c/code\u003e, are initialised with the \u003ccode class=\"cw pk pl pm pn b\"\u003eservice_context\u003c/code\u003e .\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eFaithfulness Evaluator\u003c/strong\u003e — It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eRelevancy Evaluator\u003c/strong\u003e — It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query.\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003cspan id=\"d485\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003e# We will use GPT-4 for evaluating the responses\ngpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n\n# Define service context for GPT-4 for evaluation\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eResponse Evaluation For A Chunk Size\u003c/h1\u003e\u003cp\u003eWe evaluate each chunk_size based on 3 metrics.\u003c/p\u003e\u003col\u003e\u003cli\u003eAverage Response Time.\u003c/li\u003e\u003cli\u003eAverage Faithfulness.\u003c/li\u003e\u003cli\u003eAverage Relevancy.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eHere’s a function, \u003ccode class=\"cw pk pl pm pn b\"\u003eevaluate_response_time_and_accuracy\u003c/code\u003e, that does just that which has:\u003c/p\u003e\u003col\u003e\u003cli\u003eVectorIndex Creation.\u003c/li\u003e\u003cli\u003eBuilding the Query Engine.\u003c/li\u003e\u003cli\u003eMetrics Calculation.\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003cspan id=\"8a18\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eevaluate_response_time_and_accuracy\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003echunk_size, eval_questions\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"\n    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n    \n    Parameters:\n    chunk_size (int): The size of data chunks being processed.\n    \n    Returns:\n    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n    \"\"\"\u003c/span\u003e\n\n    total_response_time = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    total_faithfulness = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    total_relevancy = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# create vector index\u003c/span\u003e\n    llm = OpenAI(model=\u003cspan class=\"hljs-string\"\u003e\"gpt-3.5-turbo\"\u003c/span\u003e)\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n    \u003cspan class=\"hljs-comment\"\u003e# build query engine\u003c/span\u003e\n    query_engine = vector_index.as_query_engine()\n    num_questions = \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(eval_questions)\n\n    \u003cspan class=\"hljs-comment\"\u003e# Iterate over each question in eval_questions to compute metrics.\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# we're using a loop here to specifically measure response time for different chunk sizes.\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e question \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e average_response_time, average_faithfulness, average_relevancy\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eTesting Across Different Chunk Sizes\u003c/h1\u003e\u003cp\u003eWe’ll evaluate a range of chunk sizes to identify which offers the most promising metrics.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"262a\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003echunk_sizes = [\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2048\u003c/span\u003e]\n\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e chunk_size \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e chunk_sizes:\n  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, eval_questions)\n  \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Chunk size \u003cspan class=\"hljs-subst\"\u003e{chunk_size}\u003c/span\u003e - Average Response time: \u003cspan class=\"hljs-subst\"\u003e{avg_response_time:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003es, Average Faithfulness: \u003cspan class=\"hljs-subst\"\u003e{avg_faithfulness:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003e, Average Relevancy: \u003cspan class=\"hljs-subst\"\u003e{avg_relevancy:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eBringing It All Together\u003c/h1\u003e\u003cp\u003eLet’s compile the processes:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ba5d\" class=\"qk np gt pn b bf ql qm l qn qo\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e nest_asyncio\n\nnest_asyncio.apply()\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    SimpleDirectoryReader,\n    VectorStoreIndex,\n    ServiceContext,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.evaluation \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    DatasetGenerator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e openai\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e time\n\nopenai.api_key = \u003cspan class=\"hljs-string\"\u003e'OPENAI-API-KEY'\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# Download Data\u003c/span\u003e\n!mkdir -p \u003cspan class=\"hljs-string\"\u003e'data/10k/'\u003c/span\u003e\n!wget \u003cspan class=\"hljs-string\"\u003e'https://raw.githubusercontent.com/jerryjliu/llama_index/main/docs/examples/data/10k/uber_2021.pdf'\u003c/span\u003e -O \u003cspan class=\"hljs-string\"\u003e'data/10k/uber_2021.pdf'\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# Load Data\u003c/span\u003e\nreader = SimpleDirectoryReader(\u003cspan class=\"hljs-string\"\u003e\"./data/10k/\"\u003c/span\u003e)\ndocuments = reader.load_data()\n\n\u003cspan class=\"hljs-comment\"\u003e# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\u003c/span\u003e\neval_documents = documents[:\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e]\ndata_generator = DatasetGenerator.from_documents()\neval_questions = data_generator.generate_questions_from_nodes(num = \u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# We will use GPT-4 for evaluating the responses\u003c/span\u003e\ngpt4 = OpenAI(temperature=\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, model=\u003cspan class=\"hljs-string\"\u003e\"gpt-4\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Define service context for GPT-4 for evaluation\u003c/span\u003e\nservice_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n\n\u003cspan class=\"hljs-comment\"\u003e# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\u003c/span\u003e\nfaithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\nrelevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n\n\u003cspan class=\"hljs-comment\"\u003e# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eevaluate_response_time_and_accuracy\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003echunk_size\u003c/span\u003e):\n    total_response_time = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    total_faithfulness = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n    total_relevancy = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\n    \u003cspan class=\"hljs-comment\"\u003e# create vector index\u003c/span\u003e\n    llm = OpenAI(model=\u003cspan class=\"hljs-string\"\u003e\"gpt-3.5-turbo\"\u003c/span\u003e)\n    service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n    vector_index = VectorStoreIndex.from_documents(\n        eval_documents, service_context=service_context\n    )\n\n    query_engine = vector_index.as_query_engine()\n    num_questions = \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(eval_questions)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e question \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e eval_questions:\n        start_time = time.time()\n        response_vector = query_engine.query(question)\n        elapsed_time = time.time() - start_time\n        \n        faithfulness_result = faithfulness_gpt4.evaluate_response(\n            response=response_vector\n        ).passing\n        \n        relevancy_result = relevancy_gpt4.evaluate_response(\n            query=question, response=response_vector\n        ).passing\n\n        total_response_time += elapsed_time\n        total_faithfulness += faithfulness_result\n        total_relevancy += relevancy_result\n\n    average_response_time = total_response_time / num_questions\n    average_faithfulness = total_faithfulness / num_questions\n    average_relevancy = total_relevancy / num_questions\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e average_response_time, average_faithfulness, average_relevancy\n\n\u003cspan class=\"hljs-comment\"\u003e# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e chunk_size \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e [\u003cspan class=\"hljs-number\"\u003e128\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e512\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e1024\u003c/span\u003e, \u003cspan class=\"hljs-number\"\u003e2048\u003c/span\u003e]\n  avg_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size)\n  \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Chunk size \u003cspan class=\"hljs-subst\"\u003e{chunk_size}\u003c/span\u003e - Average Response time: \u003cspan class=\"hljs-subst\"\u003e{avg_time:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003es, Average Faithfulness: \u003cspan class=\"hljs-subst\"\u003e{avg_faithfulness:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003e, Average Relevancy: \u003cspan class=\"hljs-subst\"\u003e{avg_relevancy:\u003cspan class=\"hljs-number\"\u003e.2\u003c/span\u003ef}\u003c/span\u003e\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eResult\u003c/h1\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*dynabe9lAsB9DBe7XqmxQw.png\" alt=\"\" width=\"662\" height=\"212\"\u003e\u003c/figure\u003e\u003cp\u003eThe above table illustrates that as the chunk size increases, there is a minor uptick in the Average Response Time. Interestingly, the Average Faithfulness seems to reach its zenith at \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003eof 1024, whereas Average Relevancy shows a consistent improvement with larger chunk sizes, also peaking at 1024. This suggests that a chunk size of 1024 might strike an optimal balance between response time and the quality of the responses, measured in terms of faithfulness and relevancy.\u003c/p\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eIdentifying the best chunk size for a RAG system is as much about intuition as it is empirical evidence. With LlamaIndex’s \u003ccode class=\"cw pk pl pm pn b\"\u003eResponse Evaluation\u003c/code\u003e module, you can experiment with various sizes and base your decisions on concrete data. When building a RAG system, always remember that \u003ccode class=\"cw pk pl pm pn b\"\u003echunk_size\u003c/code\u003e is a pivotal parameter. Invest the time to meticulously evaluate and adjust your chunk size for unmatched results.\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640-jpg","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/9d4643fc433bf8b5e162fff289c4a2b7767cf3b2-1024x640.jpg","publishedDate":"2023-10-05","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"}],"slug":{"_type":"slug","current":"evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"e171aa9d-bc85-4645-8a08-eabe04c530c7","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"openai"},"title":"OpenAI"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"e1120853-8c14-4c8c-88e6-c0af59d38017","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"retrieval"},"title":"Retrieval"}],"title":"Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"},"publishedDate":"Invalid Date"},"params":{"slug":"evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>