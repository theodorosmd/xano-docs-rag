<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Introducing Query Pipelines — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Introducing Query Pipelines — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Introducing Query Pipelines — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/dab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Introducing Query Pipelines — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/dab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="946" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2024-01-08</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Introducing Query Pipelines</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/retrieval-augmented"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Retrieval Augmented</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>Today we introduce <strong>Query Pipelines, </strong>a new declarative API within LlamaIndex that allows you to <strong>concisely orchestrate simple-to-advanced query workflows over your data for different use cases</strong> (RAG, structured data extraction, and more).</p><p>At the core of all this is our<code class="cw ny nz oa ob b">QueryPipeline</code> abstraction. It can take in many LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It can create a computational graph over these modules (e.g. a sequential chain or a DAG). It has callback support and native support with our <a href="https://docs.llamaindex.ai/en/latest/module_guides/observability/observability.html" rel="noopener ugc nofollow" target="_blank">observability partners</a>.</p><p>The end goal is that it’s even easier to build LLM workflows over your data. Check out our comprehensive <a href="https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html" rel="noopener ugc nofollow" target="_blank">introduction guide</a>, as well as our <a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html" rel="noopener ugc nofollow" target="_blank">docs page</a> for more details.</p><figure><figcaption class="or fe os od oe ot ou be b bf z dt">Example `QueryPipeline` setup for an advanced RAG pipeline</figcaption><img src="/blog/images/1*Xh4gGrK_MurM6Hou-qjszA.png" alt="" width="700" height="418"></figure><h1>Context</h1><p>Over the past year AI engineers have developed customized, complex orchestration flows with LLMs to solve a variety of different use cases. Over time some common patterns developed. At a top-level, paradigms emerged to query a user’s data — this includes RAG (in a narrow definition) to query unstructured data, and text-to-SQL to query structured data. Other paradigms emerged around use cases like structured data extraction (e.g. prompt the LLM to output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and agents that could interact with external services (combine prompt chaining</p><p><strong>There is a lot of query orchestration in RAG. </strong>Even within RAG itself there can be a lot of work to build an advanced RAG pipeline optimized for performance. Starting from the user query, we may want to run query understanding/transformations (re-writing, routing). We also may want to run multi-stage retrieval algorithms — e.g. top-k lookup + reranking. We may also want to use prompts + LLMs to do response synthesis in different ways. Here’s a great blog on advanced RAG <a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6" rel="noopener ugc nofollow" target="_blank">components</a>.</p><figure><figcaption class="or fe os od oe ot ou be b bf z dt"><a href="https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6" rel="noopener ugc nofollow" target="_blank">Source: “Advanced RAG Techniques: an Illustrated Overview” by Ivan Ilin</a></figcaption><img src="/blog/images/1*pMzip79Hk1qFjNbhvw59Lw.png" alt="" width="700" height="378"></figure><p><strong>RAG has become more modular: </strong>Instead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the <a href="https://arxiv.org/pdf/2312.10997.pdf" rel="noopener ugc nofollow" target="_blank">RAG Survey paper by Gao et al.</a></p><p>This leads to creative new patterns like <a href="https://github.com/stanfordnlp/dspy" rel="noopener ugc nofollow" target="_blank">DSP</a>, <a href="https://arxiv.org/abs/2305.14283" rel="noopener ugc nofollow" target="_blank">Rewrite-Retrieve-Read</a>, or <a href="https://arxiv.org/abs/2305.15294" rel="noopener ugc nofollow" target="_blank">interleaving retrieval+generation multiple times</a>.</p><h2>Previous State of LlamaIndex</h2><p>LlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes letting users setup<a rel="noopener ugc nofollow" target="_blank" href="/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b"> different RAG pipelines</a>, and has been at the forefront of establishing advanced RAG patterns.</p><p>We’ve also exposed low-level modules such as <a href="https://docs.llamaindex.ai/en/latest/module_guides/models/llms.html" rel="noopener ugc nofollow" target="_blank">LLMs</a>, <a href="https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html#prompts" rel="noopener ugc nofollow" target="_blank">prompts</a>, <a href="https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html" rel="noopener ugc nofollow" target="_blank">embeddings</a>, <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html" rel="noopener ugc nofollow" target="_blank">postprocessors</a> and easy subclassability of core components like <a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers.html" rel="noopener ugc nofollow" target="_blank">retrievers</a> and <a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html" rel="noopener ugc nofollow" target="_blank">query engines</a> so that users can define their own workflows.</p><p>But up until now, we didn’t explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively.</p><h1>Query Pipeline</h1><p>As a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity.</p><p>You can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code.</p><p>It has the following benefits:</p><ul><li><strong>Express common query workflows with fewer lines of code/boilerplate: </strong>Stop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module!</li><li><strong>Greater readability: </strong>Reduced boilerplate leads to greater readability.</li><li><strong>End-to-end observability: </strong>Get callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations.</li><li><strong>[In the future] Easy Serializability: </strong>A declarative interface allows the core components to be serialized/redeployed on other systems much more easily.</li><li><strong>[In the future] Caching: </strong>This interface also allows us to build a caching layer under the hood, allowing input re-use.</li></ul><figure><figcaption class="or fe os od oe ot ou be b bf z dt">Visualization of our advanced RAG QueryPipeline using `networkx` and `pyvis`</figcaption><img src="/blog/images/1*sMhFv97QePYOj2G3jURhZw.png" alt="" width="700" height="471"></figure><h1>Usage</h1><p>The QueryPipeline allows you to a DAG-based query workflow using LlamaIndex modules. There are two main ways to use it:</p><ul><li>As a sequential chain (easiest/most concise)</li><li>As a full DAG (more expressive)</li></ul><p>See our <a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html" rel="noopener ugc nofollow" target="_blank">usage pattern guide</a> for more details.</p><h2>Sequential Chain</h2><p>Some simple pipelines are purely linear in nature — the output of the previous module directly goes into the input of the next module.</p><p>Some examples:</p><ul><li>Prompt → LLM → Output parsing</li><li>Retriever →Response synthesizer</li></ul><p>Here’s the most basic example, chaining a prompt with LLM. Simply initialize <code class="cw ny nz oa ob b">QueryPipeline</code> with the <code class="cw ny nz oa ob b">chain</code> parameter.</p><pre><span id="7749" class="ra ow gt ob b bf rb rc l rd re"># try chaining basic prompts
prompt_str = "Please generate related movies to {movie_name}"
prompt_tmpl = PromptTemplate(prompt_str)
llm = OpenAI(model="gpt-3.5-turbo")

p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)</span></pre><h2>Setting up a DAG for an Advanced RAG Workflow</h2><p>Generally setting up a query workflow will require using our lower-level functions to build a DAG.</p><p>For instance, to build an “advanced RAG” consisting of query rewriting/retrieval/reranking/synthesis, you’d do something like the following.</p><pre><span id="74ea" class="ra ow gt ob b bf rb rc l rd re"><span class="hljs-keyword">from</span> llama_index.postprocessor <span class="hljs-keyword">import</span> CohereRerank
<span class="hljs-keyword">from</span> llama_index.response_synthesizers <span class="hljs-keyword">import</span> TreeSummarize
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> ServiceContext

<span class="hljs-comment"># define modules</span>
prompt_str = <span class="hljs-string">"Please generate a question about Paul Graham's life regarding the following topic {topic}"</span>
prompt_tmpl = PromptTemplate(prompt_str)
llm = OpenAI(model=<span class="hljs-string">"gpt-3.5-turbo"</span>)
retriever = index.as_retriever(similarity_top_k=<span class="hljs-number">3</span>)
reranker = CohereRerank()
summarizer = TreeSummarize(
    service_context=ServiceContext.from_defaults(llm=llm)
)

<span class="hljs-comment"># define query pipeline</span>
p = QueryPipeline(verbose=<span class="hljs-literal">True</span>)
p.add_modules(
    {
        <span class="hljs-string">"llm"</span>: llm,
        <span class="hljs-string">"prompt_tmpl"</span>: prompt_tmpl,
        <span class="hljs-string">"retriever"</span>: retriever,
        <span class="hljs-string">"summarizer"</span>: summarizer,
        <span class="hljs-string">"reranker"</span>: reranker,
    }
)
<span class="hljs-comment"># add edges </span>
p.add_link(<span class="hljs-string">"prompt_tmpl"</span>, <span class="hljs-string">"llm"</span>)
p.add_link(<span class="hljs-string">"llm"</span>, <span class="hljs-string">"retriever"</span>)
p.add_link(<span class="hljs-string">"retriever"</span>, <span class="hljs-string">"reranker"</span>, dest_key=<span class="hljs-string">"nodes"</span>)
p.add_link(<span class="hljs-string">"llm"</span>, <span class="hljs-string">"reranker"</span>, dest_key=<span class="hljs-string">"query_str"</span>)
p.add_link(<span class="hljs-string">"reranker"</span>, <span class="hljs-string">"summarizer"</span>, dest_key=<span class="hljs-string">"nodes"</span>)
p.add_link(<span class="hljs-string">"llm"</span>, <span class="hljs-string">"summarizer"</span>, dest_key=<span class="hljs-string">"query_str"</span>)</span></pre><p>In this code block we 1) add modules, and then 2) define relationships between modules. Note that by <code class="cw ny nz oa ob b">source_key</code> and <code class="cw ny nz oa ob b">dest_key</code> are <strong>optional </strong>and are only required if first module has more than one output / the second module has more than one input respectively.</p><h2><strong>Running the Pipeline</strong></h2><p>If the pipeline has one “root” node and one output node, use <code class="cw ny nz oa ob b">run</code> . Using the previous example,</p><pre><span id="5b3e" class="ra ow gt ob b bf rb rc l rd re">output = p.run(topic="YC")
# output type is Response
type(output)</span></pre><p>If the pipeline has multiple root nodes and/or multiple output nodes, use <code class="cw ny nz oa ob b">run_multi</code> .</p><pre><span id="6bee" class="ra ow gt ob b bf rb rc l rd re">output_dict = p.<span class="hljs-title function_">run_multi</span>({<span class="hljs-string">"llm"</span>: {<span class="hljs-string">"topic"</span>: <span class="hljs-string">"YC"</span>}})
<span class="hljs-title function_">print</span>(output_dict)</span></pre><h2>Defining a Custom Query Component</h2><p>It’s super easy to subclass <code class="cw ny nz oa ob b">CustomQueryComponent</code> so you can plug it into the QueryPipeline.</p><p>Check out <a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html#defining-a-custom-query-component" rel="noopener ugc nofollow" target="_blank">our walkthrough</a> for more details.</p><h2>Supported Modules</h2><p>Currently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own!</p><ol><li>LLMs (both completion and chat) ( <code class="cw ny nz oa ob b">LLM</code> )</li><li>Prompts ( <code class="cw ny nz oa ob b">PromptTemplate</code> )</li><li>Query Engines ( <code class="cw ny nz oa ob b">BaseQueryEngine</code> )</li><li>Query Transforms ( <code class="cw ny nz oa ob b">BaseQueryTransform</code> )</li><li>Retrievers ( <code class="cw ny nz oa ob b">BaseRetriever</code> )</li><li>Output Parsers ( <code class="cw ny nz oa ob b">BaseOutputParser</code> )</li><li>Postprocessors/Rerankers ( <code class="cw ny nz oa ob b">BaseNodePostprocessor</code>)</li><li>Response Synthesizers ( <code class="cw ny nz oa ob b">BaseSynthesizer</code> )</li><li>Other <code class="cw ny nz oa ob b">QueryPipeline</code>objects</li><li>Custom components ( <code class="cw ny nz oa ob b">CustomQueryComponent</code> )</li></ol><p>Check out the <a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html" rel="noopener ugc nofollow" target="_blank">module usage guide</a> for more details.</p><h1>Walkthrough Example</h1><p>Make sure to check out our <a href="https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html" rel="noopener ugc nofollow" target="_blank">Introduction to Query Pipelines guide</a> for full details. We go over all the steps above with concrete examples!</p><p>The notebook guide also logs traces through <a href="https://github.com/Arize-ai/phoenix" rel="noopener ugc nofollow" target="_blank">Arize Phoenix</a>. You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider.</p><figure><img src="/blog/images/1*_5dEy2YMGz8kfpJHYhQoGA.png" alt="" width="700" height="423"></figure><h1>Related Work</h1><p>The idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include <a href="https://docs.haystack.deepset.ai/docs/pipelines" rel="noopener ugc nofollow" target="_blank">Haystack</a> as well as the <a href="https://python.langchain.com/docs/expression_language/" rel="noopener ugc nofollow" target="_blank">LangChain Expression Language</a>. Other related works include pipelines that are setup in the no-code/low-code setting such as <a href="https://github.com/logspace-ai/langflow" rel="noopener ugc nofollow" target="_blank">Langflow</a> / <a href="https://flowiseai.com/" rel="noopener ugc nofollow" target="_blank">Flowise</a>.</p><p>Our main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There’s a lot of optimizations/guides to be done here!</p><h1>FAQ</h1><p><strong>What’s the difference between a </strong><code class="cw ny nz oa ob b"><strong>QueryPipeline</strong></code><strong> and </strong><code class="cw ny nz oa ob b"><a href="https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html" rel="noopener ugc nofollow" target="_blank"><strong>IngestionPipeline</strong></a></code><strong> ?</strong></p><p>Great question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there’s potentially some shared abstractions we’ll develop for both!</p><h1>Conclusion + Resources</h1><p>That’s it! As mentioned above we’ll be adding a lot more resources and guides soon. In the meantime check out our current guides:</p><ul><li><a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html" rel="noopener ugc nofollow" target="_blank">Query Pipelines Guide</a></li><li><a href="https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html" rel="noopener ugc nofollow" target="_blank">Query Pipelines Walkthrough</a></li><li><a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html" rel="noopener ugc nofollow" target="_blank">Query Pipeline Usage Pattern</a></li><li><a href="https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html" rel="noopener ugc nofollow" target="_blank">Query Pipelines Module Usage Guide</a></li></ul></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"e18ddc13-1535-44c6-8a64-2425850d22ec","_rev":"Ys5IzmCaJ2UnW2RAX7Ujr3","_type":"blogPost","_updatedAt":"2025-05-21T20:40:14Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"htmlContent":"\u003cp\u003eToday we introduce \u003cstrong\u003eQuery Pipelines, \u003c/strong\u003ea new declarative API within LlamaIndex that allows you to \u003cstrong\u003econcisely orchestrate simple-to-advanced query workflows over your data for different use cases\u003c/strong\u003e (RAG, structured data extraction, and more).\u003c/p\u003e\u003cp\u003eAt the core of all this is our\u003ccode class=\"cw ny nz oa ob b\"\u003eQueryPipeline\u003c/code\u003e abstraction. It can take in many LlamaIndex modules (LLMs, prompts, query engines, retrievers, itself). It can create a computational graph over these modules (e.g. a sequential chain or a DAG). It has callback support and native support with our \u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/observability/observability.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eobservability partners\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eThe end goal is that it’s even easier to build LLM workflows over your data. Check out our comprehensive \u003ca href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eintroduction guide\u003c/a\u003e, as well as our \u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edocs page\u003c/a\u003e for more details.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"or fe os od oe ot ou be b bf z dt\"\u003eExample `QueryPipeline` setup for an advanced RAG pipeline\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*Xh4gGrK_MurM6Hou-qjszA.png\" alt=\"\" width=\"700\" height=\"418\"\u003e\u003c/figure\u003e\u003ch1\u003eContext\u003c/h1\u003e\u003cp\u003eOver the past year AI engineers have developed customized, complex orchestration flows with LLMs to solve a variety of different use cases. Over time some common patterns developed. At a top-level, paradigms emerged to query a user’s data — this includes RAG (in a narrow definition) to query unstructured data, and text-to-SQL to query structured data. Other paradigms emerged around use cases like structured data extraction (e.g. prompt the LLM to output JSON, and parse it), prompt chaining (e.g. chain-of-thought), and agents that could interact with external services (combine prompt chaining\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThere is a lot of query orchestration in RAG. \u003c/strong\u003eEven within RAG itself there can be a lot of work to build an advanced RAG pipeline optimized for performance. Starting from the user query, we may want to run query understanding/transformations (re-writing, routing). We also may want to run multi-stage retrieval algorithms — e.g. top-k lookup + reranking. We may also want to use prompts + LLMs to do response synthesis in different ways. Here’s a great blog on advanced RAG \u003ca href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ecomponents\u003c/a\u003e.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"or fe os od oe ot ou be b bf z dt\"\u003e\u003ca href=\"https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSource: “Advanced RAG Techniques: an Illustrated Overview” by Ivan Ilin\u003c/a\u003e\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*pMzip79Hk1qFjNbhvw59Lw.png\" alt=\"\" width=\"700\" height=\"378\"\u003e\u003c/figure\u003e\u003cp\u003e\u003cstrong\u003eRAG has become more modular: \u003c/strong\u003eInstead of a single way to do retrieval/RAG, developers are encouraged to pick and choose the best modules for their use cases. This sentiment is echoed in the \u003ca href=\"https://arxiv.org/pdf/2312.10997.pdf\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRAG Survey paper by Gao et al.\u003c/a\u003e\u003c/p\u003e\u003cp\u003eThis leads to creative new patterns like \u003ca href=\"https://github.com/stanfordnlp/dspy\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDSP\u003c/a\u003e, \u003ca href=\"https://arxiv.org/abs/2305.14283\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRewrite-Retrieve-Read\u003c/a\u003e, or \u003ca href=\"https://arxiv.org/abs/2305.15294\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003einterleaving retrieval+generation multiple times\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003ePrevious State of LlamaIndex\u003c/h2\u003e\u003cp\u003eLlamaIndex itself has hundreds of RAG guides and 16+ Llama Pack recipes letting users setup\u003ca rel=\"noopener ugc nofollow\" target=\"_blank\" href=\"/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b\"\u003e different RAG pipelines\u003c/a\u003e, and has been at the forefront of establishing advanced RAG patterns.\u003c/p\u003e\u003cp\u003eWe’ve also exposed low-level modules such as \u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/models/llms.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLLMs\u003c/a\u003e, \u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/prompts.html#prompts\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eprompts\u003c/a\u003e, \u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eembeddings\u003c/a\u003e, \u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003epostprocessors\u003c/a\u003e and easy subclassability of core components like \u003ca href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eretrievers\u003c/a\u003e and \u003ca href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/custom_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003equery engines\u003c/a\u003e so that users can define their own workflows.\u003c/p\u003e\u003cp\u003eBut up until now, we didn’t explicitly have an orchestration abstraction. Users were responsible for figuring out their own workflows by reading the API guides of each module, converting outputs to the right inputs, and using the modules imperatively.\u003c/p\u003e\u003ch1\u003eQuery Pipeline\u003c/h1\u003e\u003cp\u003eAs a result, our QueryPipeline provides a declarative query orchestration abstraction. You can use it to compose both sequential chains and directed acyclic graphs (DAGs) of arbitrary complexity.\u003c/p\u003e\u003cp\u003eYou can already compose these workflows imperatively with LlamaIndex modules, but the QueryPipeline allows you to do it efficiently with fewer lines of code.\u003c/p\u003e\u003cp\u003eIt has the following benefits:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eExpress common query workflows with fewer lines of code/boilerplate: \u003c/strong\u003eStop writing converter logic between outputs/inputs, and figuring out the exact typing of arguments for each module!\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eGreater readability: \u003c/strong\u003eReduced boilerplate leads to greater readability.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eEnd-to-end observability: \u003c/strong\u003eGet callback integration across the entire pipeline (even for arbitrarily nested DAGs), so you stop fiddling around with our observability integrations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003e[In the future] Easy Serializability: \u003c/strong\u003eA declarative interface allows the core components to be serialized/redeployed on other systems much more easily.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003e[In the future] Caching: \u003c/strong\u003eThis interface also allows us to build a caching layer under the hood, allowing input re-use.\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cfigcaption class=\"or fe os od oe ot ou be b bf z dt\"\u003eVisualization of our advanced RAG QueryPipeline using `networkx` and `pyvis`\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*sMhFv97QePYOj2G3jURhZw.png\" alt=\"\" width=\"700\" height=\"471\"\u003e\u003c/figure\u003e\u003ch1\u003eUsage\u003c/h1\u003e\u003cp\u003eThe QueryPipeline allows you to a DAG-based query workflow using LlamaIndex modules. There are two main ways to use it:\u003c/p\u003e\u003cul\u003e\u003cli\u003eAs a sequential chain (easiest/most concise)\u003c/li\u003e\u003cli\u003eAs a full DAG (more expressive)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSee our \u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eusage pattern guide\u003c/a\u003e for more details.\u003c/p\u003e\u003ch2\u003eSequential Chain\u003c/h2\u003e\u003cp\u003eSome simple pipelines are purely linear in nature — the output of the previous module directly goes into the input of the next module.\u003c/p\u003e\u003cp\u003eSome examples:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePrompt → LLM → Output parsing\u003c/li\u003e\u003cli\u003eRetriever →Response synthesizer\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eHere’s the most basic example, chaining a prompt with LLM. Simply initialize \u003ccode class=\"cw ny nz oa ob b\"\u003eQueryPipeline\u003c/code\u003e with the \u003ccode class=\"cw ny nz oa ob b\"\u003echain\u003c/code\u003e parameter.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"7749\" class=\"ra ow gt ob b bf rb rc l rd re\"\u003e# try chaining basic prompts\nprompt_str = \"Please generate related movies to {movie_name}\"\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\"gpt-3.5-turbo\")\n\np = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eSetting up a DAG for an Advanced RAG Workflow\u003c/h2\u003e\u003cp\u003eGenerally setting up a query workflow will require using our lower-level functions to build a DAG.\u003c/p\u003e\u003cp\u003eFor instance, to build an “advanced RAG” consisting of query rewriting/retrieval/reranking/synthesis, you’d do something like the following.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"74ea\" class=\"ra ow gt ob b bf rb rc l rd re\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.postprocessor \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e CohereRerank\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.response_synthesizers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e TreeSummarize\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ServiceContext\n\n\u003cspan class=\"hljs-comment\"\u003e# define modules\u003c/span\u003e\nprompt_str = \u003cspan class=\"hljs-string\"\u003e\"Please generate a question about Paul Graham's life regarding the following topic {topic}\"\u003c/span\u003e\nprompt_tmpl = PromptTemplate(prompt_str)\nllm = OpenAI(model=\u003cspan class=\"hljs-string\"\u003e\"gpt-3.5-turbo\"\u003c/span\u003e)\nretriever = index.as_retriever(similarity_top_k=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e)\nreranker = CohereRerank()\nsummarizer = TreeSummarize(\n    service_context=ServiceContext.from_defaults(llm=llm)\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# define query pipeline\u003c/span\u003e\np = QueryPipeline(verbose=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\np.add_modules(\n    {\n        \u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e: llm,\n        \u003cspan class=\"hljs-string\"\u003e\"prompt_tmpl\"\u003c/span\u003e: prompt_tmpl,\n        \u003cspan class=\"hljs-string\"\u003e\"retriever\"\u003c/span\u003e: retriever,\n        \u003cspan class=\"hljs-string\"\u003e\"summarizer\"\u003c/span\u003e: summarizer,\n        \u003cspan class=\"hljs-string\"\u003e\"reranker\"\u003c/span\u003e: reranker,\n    }\n)\n\u003cspan class=\"hljs-comment\"\u003e# add edges \u003c/span\u003e\np.add_link(\u003cspan class=\"hljs-string\"\u003e\"prompt_tmpl\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e)\np.add_link(\u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"retriever\"\u003c/span\u003e)\np.add_link(\u003cspan class=\"hljs-string\"\u003e\"retriever\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"reranker\"\u003c/span\u003e, dest_key=\u003cspan class=\"hljs-string\"\u003e\"nodes\"\u003c/span\u003e)\np.add_link(\u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"reranker\"\u003c/span\u003e, dest_key=\u003cspan class=\"hljs-string\"\u003e\"query_str\"\u003c/span\u003e)\np.add_link(\u003cspan class=\"hljs-string\"\u003e\"reranker\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"summarizer\"\u003c/span\u003e, dest_key=\u003cspan class=\"hljs-string\"\u003e\"nodes\"\u003c/span\u003e)\np.add_link(\u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"summarizer\"\u003c/span\u003e, dest_key=\u003cspan class=\"hljs-string\"\u003e\"query_str\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eIn this code block we 1) add modules, and then 2) define relationships between modules. Note that by \u003ccode class=\"cw ny nz oa ob b\"\u003esource_key\u003c/code\u003e and \u003ccode class=\"cw ny nz oa ob b\"\u003edest_key\u003c/code\u003e are \u003cstrong\u003eoptional \u003c/strong\u003eand are only required if first module has more than one output / the second module has more than one input respectively.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eRunning the Pipeline\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIf the pipeline has one “root” node and one output node, use \u003ccode class=\"cw ny nz oa ob b\"\u003erun\u003c/code\u003e . Using the previous example,\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5b3e\" class=\"ra ow gt ob b bf rb rc l rd re\"\u003eoutput = p.run(topic=\"YC\")\n# output type is Response\ntype(output)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eIf the pipeline has multiple root nodes and/or multiple output nodes, use \u003ccode class=\"cw ny nz oa ob b\"\u003erun_multi\u003c/code\u003e .\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6bee\" class=\"ra ow gt ob b bf rb rc l rd re\"\u003eoutput_dict = p.\u003cspan class=\"hljs-title function_\"\u003erun_multi\u003c/span\u003e({\u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e\"topic\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"YC\"\u003c/span\u003e}})\n\u003cspan class=\"hljs-title function_\"\u003eprint\u003c/span\u003e(output_dict)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eDefining a Custom Query Component\u003c/h2\u003e\u003cp\u003eIt’s super easy to subclass \u003ccode class=\"cw ny nz oa ob b\"\u003eCustomQueryComponent\u003c/code\u003e so you can plug it into the QueryPipeline.\u003c/p\u003e\u003cp\u003eCheck out \u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html#defining-a-custom-query-component\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eour walkthrough\u003c/a\u003e for more details.\u003c/p\u003e\u003ch2\u003eSupported Modules\u003c/h2\u003e\u003cp\u003eCurrently the following LlamaIndex modules are supported within a QueryPipeline. Remember, you can define your own!\u003c/p\u003e\u003col\u003e\u003cli\u003eLLMs (both completion and chat) ( \u003ccode class=\"cw ny nz oa ob b\"\u003eLLM\u003c/code\u003e )\u003c/li\u003e\u003cli\u003ePrompts ( \u003ccode class=\"cw ny nz oa ob b\"\u003ePromptTemplate\u003c/code\u003e )\u003c/li\u003e\u003cli\u003eQuery Engines ( \u003ccode class=\"cw ny nz oa ob b\"\u003eBaseQueryEngine\u003c/code\u003e )\u003c/li\u003e\u003cli\u003eQuery Transforms ( \u003ccode class=\"cw ny nz oa ob b\"\u003eBaseQueryTransform\u003c/code\u003e )\u003c/li\u003e\u003cli\u003eRetrievers ( \u003ccode class=\"cw ny nz oa ob b\"\u003eBaseRetriever\u003c/code\u003e )\u003c/li\u003e\u003cli\u003eOutput Parsers ( \u003ccode class=\"cw ny nz oa ob b\"\u003eBaseOutputParser\u003c/code\u003e )\u003c/li\u003e\u003cli\u003ePostprocessors/Rerankers ( \u003ccode class=\"cw ny nz oa ob b\"\u003eBaseNodePostprocessor\u003c/code\u003e)\u003c/li\u003e\u003cli\u003eResponse Synthesizers ( \u003ccode class=\"cw ny nz oa ob b\"\u003eBaseSynthesizer\u003c/code\u003e )\u003c/li\u003e\u003cli\u003eOther \u003ccode class=\"cw ny nz oa ob b\"\u003eQueryPipeline\u003c/code\u003eobjects\u003c/li\u003e\u003cli\u003eCustom components ( \u003ccode class=\"cw ny nz oa ob b\"\u003eCustomQueryComponent\u003c/code\u003e )\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eCheck out the \u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emodule usage guide\u003c/a\u003e for more details.\u003c/p\u003e\u003ch1\u003eWalkthrough Example\u003c/h1\u003e\u003cp\u003eMake sure to check out our \u003ca href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eIntroduction to Query Pipelines guide\u003c/a\u003e for full details. We go over all the steps above with concrete examples!\u003c/p\u003e\u003cp\u003eThe notebook guide also logs traces through \u003ca href=\"https://github.com/Arize-ai/phoenix\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eArize Phoenix\u003c/a\u003e. You can see the full run of each QueryPipeline in the Phoenix dashboard. Our full callback support throughout every component in a QueryComponent allows you to easily integrate with any observability provider.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*_5dEy2YMGz8kfpJHYhQoGA.png\" alt=\"\" width=\"700\" height=\"423\"\u003e\u003c/figure\u003e\u003ch1\u003eRelated Work\u003c/h1\u003e\u003cp\u003eThe idea of a declarative syntax for building LLM-powered pipelines is not new. Related works include \u003ca href=\"https://docs.haystack.deepset.ai/docs/pipelines\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHaystack\u003c/a\u003e as well as the \u003ca href=\"https://python.langchain.com/docs/expression_language/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLangChain Expression Language\u003c/a\u003e. Other related works include pipelines that are setup in the no-code/low-code setting such as \u003ca href=\"https://github.com/logspace-ai/langflow\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLangflow\u003c/a\u003e / \u003ca href=\"https://flowiseai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eFlowise\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur main goal here was highlighted above: provide a convenient dev UX to define common query workflows over your data. There’s a lot of optimizations/guides to be done here!\u003c/p\u003e\u003ch1\u003eFAQ\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eWhat’s the difference between a \u003c/strong\u003e\u003ccode class=\"cw ny nz oa ob b\"\u003e\u003cstrong\u003eQueryPipeline\u003c/strong\u003e\u003c/code\u003e\u003cstrong\u003e and \u003c/strong\u003e\u003ccode class=\"cw ny nz oa ob b\"\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eIngestionPipeline\u003c/strong\u003e\u003c/a\u003e\u003c/code\u003e\u003cstrong\u003e ?\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eGreat question. Currently the IngestionPipeline operates during the data ingestion stage, and the QueryPipeline operates during the query stage. That said, there’s potentially some shared abstractions we’ll develop for both!\u003c/p\u003e\u003ch1\u003eConclusion + Resources\u003c/h1\u003e\u003cp\u003eThat’s it! As mentioned above we’ll be adding a lot more resources and guides soon. In the meantime check out our current guides:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eQuery Pipelines Guide\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eQuery Pipelines Walkthrough\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eQuery Pipeline Usage Pattern\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/module_usage.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eQuery Pipelines Module Usage Guide\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","image":{"_type":"image","asset":{"_ref":"image-dab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/dab9490c1e6bef7aad51f30986810b5393b5a09c-3176x1892.png","publishedDate":"2024-01-08","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"}],"slug":{"_type":"slug","current":"introducing-query-pipelines-025dc2bb0537"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"ab1559f4-5cbd-47f4-ac89-7b293fc14f4b","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"retrieval-augmented"},"title":"Retrieval Augmented"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"}],"title":"Introducing Query Pipelines"},"publishedDate":"Invalid Date"},"params":{"slug":"introducing-query-pipelines-025dc2bb0537"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"introducing-query-pipelines-025dc2bb0537"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>