<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Easily Finetune Llama 2 for Your Text-to-SQL Applications — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Easily Finetune Llama 2 for Your Text-to-SQL Applications — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Easily Finetune Llama 2 for Your Text-to-SQL Applications — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Easily Finetune Llama 2 for Your Text-to-SQL Applications — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="512" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2023-08-17</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Easily Finetune Llama 2 for Your Text-to-SQL Applications</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/fine-tuning"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Fine Tuning</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/large-language-models"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Large Language Models</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/nlp"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">NLP</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p><a href="https://ai.meta.com/llama/" rel="noopener ugc nofollow" target="_blank">Llama 2</a> is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Hugging Face Open LLM Leaderboard</a>. Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.</p><h1>Context: Llama-2–7B is Not Good at Text-to-SQL</h1><p>A downside of the smallest Llama 2 model (7B parameters), however, is that it’s not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:</p><pre><span id="08e7" class="py os gt pv b bf pz qa l qb qc">You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. 

You must output the SQL query that answers the question.

### Input:
{input}

### Context:
{context}

### Response:</span></pre><p>Here we plugged in a sample entry from the <a href="https://huggingface.co/datasets/b-mc2/sql-create-context" rel="noopener ugc nofollow" target="_blank">sql-create-context dataset</a>.</p><pre><span id="72dd" class="py os gt pv b bf pz qa l qb qc">input: In 1981 which team picked overall 148?
context: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)</span></pre><p>Meanwhile, here is the generated output vs. correct output:</p><pre><span id="416a" class="py os gt pv b bf pz qa l qb qc">Generated output: SELECT * FROM `table_name_8` WHERE <span class="hljs-string">'1980'</span> = YEAR AND TEAM = <span class="hljs-string">"Boston Celtics"</span> ORDER BY OVERALL_PICK DESC LIMIT <span class="hljs-number">1</span>;

Correct output: SELECT team FROM table_name_8 WHERE year = <span class="hljs-number">1981</span> AND overall_pick = <span class="hljs-string">"148"</span></span></pre><p>This is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.</p><p>This is exactly where fine-tuning comes in — given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g. <a href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">how LoRA works</a>).</p><p>Once the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on “in-context learning” and “retrieval-augmentation” use cases — freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.</p><h1>Tutorial Overview</h1><p>In this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of <a href="https://github.com/jerryjliu/llama_index" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a>.</p><p>Here is the stack that we use:</p><ul><li><code class="cw qg qh qi pv b"><a href="https://huggingface.co/datasets/b-mc2/sql-create-context" rel="noopener ugc nofollow" target="_blank">b-mc2/sql-create-context</a></code><a href="https://huggingface.co/datasets/b-mc2/sql-create-context" rel="noopener ugc nofollow" target="_blank"> from Hugging Face datasets</a> as the training dataset</li><li><a href="https://github.com/openlm-research/open_llama" rel="noopener ugc nofollow" target="_blank">OpenLLaMa</a> <code class="cw qg qh qi pv b">open_llama_7b_v2</code> as the base model</li><li><a href="https://github.com/huggingface/peft" rel="noopener ugc nofollow" target="_blank">PEFT for efficient finetuning</a></li><li><a href="https://modal.com/" rel="noopener ugc nofollow" target="_blank">Modal</a> for handling all cloud compute/orchestration for finetuning. And also for the excellent reference <a href="https://github.com/modal-labs/doppel-bot" rel="noopener ugc nofollow" target="_blank">doppel-bot repo</a>.</li><li><a href="https://www.llamaindex.ai/" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a> for text-to-SQL inference against any SQL database.</li></ul><p>Special mention to the awesome <a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="noopener ugc nofollow" target="_blank">Llama 2 tutorial from Anyscale that helped to inspire this project</a>.</p><p>All of our materials can be found in our Github repo: <a href="https://github.com/run-llama/modal_finetune_sql" rel="noopener ugc nofollow" target="_blank">https://github.com/run-llama/modal_finetune_sql</a> (again emphasizing that this is adapted from <a href="https://github.com/modal-labs/doppel-bot" rel="noopener ugc nofollow" target="_blank">doppel-bot</a>). Also, the full tutorial can be found in our <a href="https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter notebook guide</a>. Make sure to check it out!</p><p>As mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don’t cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.</p><p>There are also certainly higher-level APIs that we could’ve used (e.g. OpenAI, Lamini) in order to achieve this task. There’s plenty of room for followup tutorials to cover these topics!</p><h2>Step 1: Loading Training Data for Finetuning LLaMa</h2><p>The first step here is to open up the <a href="https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter notebook</a>. The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.</p><p>Our code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don’t contain Python blocks of their own.</p><p>First we use Modal to load in the <code class="cw qg qh qi pv b">b-mc2/sql-create-context</code> dataset. This is a simple task that just loads in the dataset and formats it into a <code class="cw qg qh qi pv b">.jsonl</code> file.</p><pre><span id="9cb1" class="py os gt pv b bf pz qa l qb qc">modal run src.load_data_sql --data-dir "data_sql"</span></pre><p>As we can see, under the hood the task is quite straightforward:</p><pre><span id="47a3" class="py os gt pv b bf pz qa l qb qc"><span class="hljs-comment"># Modal stubs allow our function to run remotely</span>
<span class="hljs-meta">@stub.function(<span class="hljs-params">
    retries=Retries(<span class="hljs-params">
        max_retries=<span class="hljs-number">3</span>,
        initial_delay=<span class="hljs-number">5.0</span>,
        backoff_coefficient=<span class="hljs-number">2.0</span>,
    </span>),
    timeout=<span class="hljs-number">60</span> * <span class="hljs-number">60</span> * <span class="hljs-number">2</span>,
    network_file_systems={VOL_MOUNT_PATH.as_posix(<span class="hljs-params"></span>): output_vol},
    cloud=<span class="hljs-string">"gcp"</span>,
</span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data_sql</span>(<span class="hljs-params">data_dir: <span class="hljs-built_in">str</span> = <span class="hljs-string">"data_sql"</span></span>):
    <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

    dataset = load_dataset(<span class="hljs-string">"b-mc2/sql-create-context"</span>)

    dataset_splits = {<span class="hljs-string">"train"</span>: dataset[<span class="hljs-string">"train"</span>]}
    out_path = get_data_path(data_dir)

    out_path.parent.mkdir(parents=<span class="hljs-literal">True</span>, exist_ok=<span class="hljs-literal">True</span>)

    <span class="hljs-keyword">for</span> key, ds <span class="hljs-keyword">in</span> dataset_splits.items():
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(out_path, <span class="hljs-string">"w"</span>) <span class="hljs-keyword">as</span> f:
            <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ds:
                newitem = {
                    <span class="hljs-string">"input"</span>: item[<span class="hljs-string">"question"</span>],
                    <span class="hljs-string">"context"</span>: item[<span class="hljs-string">"context"</span>],
                    <span class="hljs-string">"output"</span>: item[<span class="hljs-string">"answer"</span>],
                }
                f.write(json.dumps(newitem) + <span class="hljs-string">"\n"</span>)</span></pre><h2>Step 2: Run Finetuning Script</h2><p>The next step is to run our finetuning script on the parsed dataset.</p><pre><span id="6964" class="py os gt pv b bf pz qa l qb qc">modal run src.finetune_sql --data-dir "data_sql" --model-dir "model_sql"</span></pre><p>The finetuning script performs the following steps.</p><p><strong>Splits the dataset into training and validation splits</strong></p><pre><span id="596b" class="py os gt pv b bf pz qa l qb qc">train_val = data["train"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)
train_data = train_val["train"].shuffle().map(generate_and_tokenize_prompt)
val_data = train_val["test"].shuffle().map(generate_and_tokenize_prompt)</span></pre><p><strong>Formats each split into tuples of (input prompt, label): </strong>The input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt — this allows the model to train on next-token prediction.</p><pre><span id="22f3" class="py os gt pv b bf pz qa l qb qc"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_and_tokenize_prompt</span>(<span class="hljs-params">data_point</span>):
  full_prompt = generate_prompt_sql(
      data_point[<span class="hljs-string">"input"</span>],
      data_point[<span class="hljs-string">"context"</span>],
      data_point[<span class="hljs-string">"output"</span>],
  )
  tokenized_full_prompt = tokenize(full_prompt)
  <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> train_on_inputs:
      <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">"not implemented yet"</span>)
  <span class="hljs-keyword">return</span> tokenized_full_prompt</span></pre><p>The input prompt is the exact same as what was given at the top of this blog.</p><p>When the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not specified).</p><h2>Step 3: Evaluation</h2><p>The model has been finetuned and can be served from the cloud. We can run some basic evaluations using sample data from sql-create-context to compare the performance of the finetuned model vs. the baseline Llama 2 model.</p><pre><span id="a768" class="py os gt pv b bf pz qa l qb qc">modal run src.eval_sql::main</span></pre><p>The results demonstrate a massive improvement for the finetuned model:</p><pre><span id="9cf8" class="py os gt pv b bf pz qa l qb qc">Input <span class="hljs-number">1</span>: {<span class="hljs-string">'input'</span>: <span class="hljs-string">'Which region (year) has Abigail at number 7, Sophia at number 1 and Aaliyah at number 5?'</span>, <span class="hljs-string">'context'</span>: <span class="hljs-string">'CREATE TABLE table_name_12 (region__year_ VARCHAR, no_5 VARCHAR, no_7 VARCHAR, no_1 VARCHAR)'</span>, <span class="hljs-string">'output'</span>: <span class="hljs-string">'SELECT region__year_ FROM table_name_12 WHERE no_7 = "abigail" AND no_1 = "sophia" AND
no_5 = "aaliyah"'</span>}
Output <span class="hljs-number">1</span> (finetuned model): SELECT region__year_ FROM table_name_12 WHERE no_7 = <span class="hljs-string">"abigail"</span> AND no_1 = <span class="hljs-string">"aaliyah"</span> AND no_5 = <span class="hljs-string">"sophia"</span>
Output <span class="hljs-number">1</span> (base model): SELECT * FROM table_name_12 WHERE region__year = <span class="hljs-string">'2018'</span> AND no_5 = <span class="hljs-string">'Abigail'</span> AND no_7 = <span class="hljs-string">'Sophia'</span> AND no_1 = <span class="hljs-string">'Aaliyah'</span>;


Input <span class="hljs-number">2</span>: {<span class="hljs-string">'input'</span>: <span class="hljs-string">'Name the result/games for 54741'</span>, <span class="hljs-string">'context'</span>: <span class="hljs-string">'CREATE TABLE table_21436373_11 (result_games VARCHAR, attendance VARCHAR)'</span>, <span class="hljs-string">'output'</span>: <span class="hljs-string">'SELECT result_games FROM table_21436373_11 WHERE attendance = 54741'</span>}
Output <span class="hljs-number">2</span> (finetuned model): SELECT result_games FROM table_21436373_11 WHERE attendance = <span class="hljs-string">"54741"</span>
Output <span class="hljs-number">2</span> (base model): SELECT * FROM table_21436373_11 WHERE result_games = <span class="hljs-string">'name'</span> AND attendance &amp;gt; <span class="hljs-number">0</span>;</span></pre><p>Whereas the base model produces wrongly formatted outputs, or incorrect SQL statements,</p><p>the finetuned model is able to produce outputs that are much closer to that of the expected output.</p><h2>Step 4: Integrating the Finetuned Model with LlamaIndex</h2><p>We can now use this model in LlamaIndex for text-to-SQL over any database.</p><p>We first define a test SQL database that we can then use to test the inference capabilities of the model.</p><p>We create a toy <code class="cw qg qh qi pv b">city_stats</code> table that contains city name, population, and country information, and populate it with a few sample cities.</p><pre><span id="f910" class="py os gt pv b bf pz qa l qb qc">db_file = <span class="hljs-string">"cities.db"</span>
engine = <span class="hljs-title function_">create_engine</span>(f<span class="hljs-string">"sqlite:///{db_file}"</span>)
metadata_obj = <span class="hljs-title class_">MetaData</span>()
# create city <span class="hljs-variable constant_">SQL</span> table
table_name = <span class="hljs-string">"city_stats"</span>
city_stats_table = <span class="hljs-title class_">Table</span>(
    table_name,
    metadata_obj,
    <span class="hljs-title class_">Column</span>(<span class="hljs-string">"city_name"</span>, <span class="hljs-title class_">String</span>(<span class="hljs-number">16</span>), primary_key=<span class="hljs-title class_">True</span>),
    <span class="hljs-title class_">Column</span>(<span class="hljs-string">"population"</span>, <span class="hljs-title class_">Integer</span>),
    <span class="hljs-title class_">Column</span>(<span class="hljs-string">"country"</span>, <span class="hljs-title class_">String</span>(<span class="hljs-number">16</span>), nullable=<span class="hljs-title class_">False</span>),
)
metadata_obj.<span class="hljs-title function_">create_all</span>(engine)</span></pre><p>This is stored in a <code class="cw qg qh qi pv b">cities.db</code> file.</p><p>We can then use Modal to load both the finetuned model and this database file into the <code class="cw qg qh qi pv b">NLSQLTableQueryEngine</code> in LlamaIndex - this query engine allows users easily start performing text-to-SQL over a given database.</p><pre><span id="4eb3" class="py os gt pv b bf pz qa l qb qc">modal run src.inference_sql_llamaindex::main --query <span class="hljs-string">"Which city has the highest population?"</span> --sqlite-file-path <span class="hljs-string">"nbs/cities.db"</span> --model-dir <span class="hljs-string">"model_sql"</span> --use-finetuned-model True</span></pre><p>We get a response like the following:</p><pre><span id="ef94" class="py os gt pv b bf pz qa l qb qc"><span class="hljs-variable constant_">SQL</span> <span class="hljs-title class_">Query</span>: <span class="hljs-variable constant_">SELECT</span> <span class="hljs-title function_">MAX</span>(population) <span class="hljs-variable constant_">FROM</span> city_stats <span class="hljs-variable constant_">WHERE</span> country = <span class="hljs-string">"United States"</span>
<span class="hljs-title class_">Response</span>: [(<span class="hljs-number">2679000</span>,)]</span></pre><h1>Conclusion</h1><p>And that’s basically it! This tutorial provides a very high-level way for you to get started finetuning a Llama 2 model on generating SQL statements, and showcases end-to-end how you can plug it into your text-to-SQL workflows with LlamaIndex.</p><p><strong>Resources</strong></p><p>For the sake of completeness we’re linking all of our resources again here.</p><p>Tutorial repo: <a href="https://github.com/run-llama/modal_finetune_sql" rel="noopener ugc nofollow" target="_blank">https://github.com/run-llama/modal_finetune_sql</a> (adapted from <a href="https://modal.com/" rel="noopener ugc nofollow" target="_blank">doppel-bot</a>).</p><p><a href="https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter notebook guide</a>.</p><p>Stack:</p><ul><li><code class="cw qg qh qi pv b">[b-mc2/sql-create-context</code> from Hugging Face datasets](<a href="https://huggingface.co/datasets/b-mc2/sql-create-context" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/datasets/b-mc2/sql-create-context</a>)</li><li><a href="https://github.com/openlm-research/open_llama" rel="noopener ugc nofollow" target="_blank">OpenLLaMa</a></li><li><a href="https://github.com/huggingface/peft" rel="noopener ugc nofollow" target="_blank">PEFT</a></li><li><a href="https://modal.com/" rel="noopener ugc nofollow" target="_blank">Modal</a> (+ <a href="https://github.com/modal-labs/doppel-bot" rel="noopener ugc nofollow" target="_blank">doppel-bot repo</a>).</li><li><a href="https://www.llamaindex.ai/" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a></li></ul><p>Special mention: <a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" rel="noopener ugc nofollow" target="_blank">Llama 2 tutorial from Anyscale</a>.</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-05">LlamaIndex Newsletter 2024-03-05</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f">Querying a network of knowledge with llama-index-networks</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-27</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"efc2f294-dddb-413d-96d8-61581f7bf73a","_rev":"05dtDS0H5iRVsxYMarZrsD","_type":"blogPost","_updatedAt":"2025-05-21T20:40:28Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"htmlContent":"\u003cp\u003e\u003ca href=\"https://ai.meta.com/llama/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlama 2\u003c/a\u003e is a huge milestone in the advancement of open-source LLMs. The biggest model and its finetuned variants sit at the top of the \u003ca href=\"https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eHugging Face Open LLM Leaderboard\u003c/a\u003e. Multiple benchmarks show that it is approaching GPT-3.5 (or in some cases even surpassing it) in terms of performance. All of this means that open-source LLMs are an increasingly viable and reliable option for use in complex LLM applications, from RAG systems to agents.\u003c/p\u003e\u003ch1\u003eContext: Llama-2–7B is Not Good at Text-to-SQL\u003c/h1\u003e\u003cp\u003eA downside of the smallest Llama 2 model (7B parameters), however, is that it’s not very good at generating SQL, making it impractical for structured analytics use cases. As an example, we tried prompting Llama 2 to generate the correct SQL statement given the following prompt template:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"08e7\" class=\"py os gt pv b bf pz qa l qb qc\"\u003eYou are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \n\nYou must output the SQL query that answers the question.\n\n### Input:\n{input}\n\n### Context:\n{context}\n\n### Response:\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eHere we plugged in a sample entry from the \u003ca href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esql-create-context dataset\u003c/a\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"72dd\" class=\"py os gt pv b bf pz qa l qb qc\"\u003einput: In 1981 which team picked overall 148?\ncontext: CREATE TABLE table_name_8 (team VARCHAR, year VARCHAR, overall_pick VARCHAR)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eMeanwhile, here is the generated output vs. correct output:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"416a\" class=\"py os gt pv b bf pz qa l qb qc\"\u003eGenerated output: SELECT * FROM `table_name_8` WHERE \u003cspan class=\"hljs-string\"\u003e'1980'\u003c/span\u003e = YEAR AND TEAM = \u003cspan class=\"hljs-string\"\u003e\"Boston Celtics\"\u003c/span\u003e ORDER BY OVERALL_PICK DESC LIMIT \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e;\n\nCorrect output: SELECT team FROM table_name_8 WHERE year = \u003cspan class=\"hljs-number\"\u003e1981\u003c/span\u003e AND overall_pick = \u003cspan class=\"hljs-string\"\u003e\"148\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis is clearly not ideal. Unlike ChatGPT and GPT-4, Llama 2 does not reliably produce well-formatted and correct SQL outputs.\u003c/p\u003e\u003cp\u003eThis is exactly where fine-tuning comes in — given a proper corpus of text-to-SQL data, we can teach Llama 2 to be better at generating SQL outputs from natural language. At a high-level, fine-tuning involves modifying the weights of the model in some capacity. There are different ways to finetune models, from updating all parameters of the network, to a subset of the parameters, to only finetuning additional parameters (e.g. \u003ca href=\"https://arxiv.org/abs/2106.09685\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehow LoRA works\u003c/a\u003e).\u003c/p\u003e\u003cp\u003eOnce the model is finetuned, it can still be plugged into a downstream LLM application. That is exactly what this tutorial aims to show. It is a step more involved than our existing tutorials which have primarily focused on “in-context learning” and “retrieval-augmentation” use cases — freezing the model itself but focusing on the orchestration of data into the input prompt. Finetuning can have a high learning curve and also require a lot of compute. This tutorial makes it as easy as possible to get started.\u003c/p\u003e\u003ch1\u003eTutorial Overview\u003c/h1\u003e\u003cp\u003eIn this tutorial, we show you how you can finetune Llama 2 on a text-to-SQL dataset, and then use it for structured analytics against any SQL database using the capabilities of \u003ca href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHere is the stack that we use:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw qg qh qi pv b\"\u003e\u003ca href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eb-mc2/sql-create-context\u003c/a\u003e\u003c/code\u003e\u003ca href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e from Hugging Face datasets\u003c/a\u003e as the training dataset\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/openlm-research/open_llama\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenLLaMa\u003c/a\u003e \u003ccode class=\"cw qg qh qi pv b\"\u003eopen_llama_7b_v2\u003c/code\u003e as the base model\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/huggingface/peft\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePEFT for efficient finetuning\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://modal.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eModal\u003c/a\u003e for handling all cloud compute/orchestration for finetuning. And also for the excellent reference \u003ca href=\"https://github.com/modal-labs/doppel-bot\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edoppel-bot repo\u003c/a\u003e.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex\u003c/a\u003e for text-to-SQL inference against any SQL database.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSpecial mention to the awesome \u003ca href=\"https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlama 2 tutorial from Anyscale that helped to inspire this project\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAll of our materials can be found in our Github repo: \u003ca href=\"https://github.com/run-llama/modal_finetune_sql\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/run-llama/modal_finetune_sql\u003c/a\u003e (again emphasizing that this is adapted from \u003ca href=\"https://github.com/modal-labs/doppel-bot\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edoppel-bot\u003c/a\u003e). Also, the full tutorial can be found in our \u003ca href=\"https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJupyter notebook guide\u003c/a\u003e. Make sure to check it out!\u003c/p\u003e\u003cp\u003eAs mentioned above, performing finetuning does require quite a few steps. Our goal is to make this as straightforward as possible to follow and use out of the box. We don’t cover all the nitty gritty detailsof Modal, PEFT, the finetuning procedure itself, etc. but we do give a rough overview.\u003c/p\u003e\u003cp\u003eThere are also certainly higher-level APIs that we could’ve used (e.g. OpenAI, Lamini) in order to achieve this task. There’s plenty of room for followup tutorials to cover these topics!\u003c/p\u003e\u003ch2\u003eStep 1: Loading Training Data for Finetuning LLaMa\u003c/h2\u003e\u003cp\u003eThe first step here is to open up the \u003ca href=\"https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJupyter notebook\u003c/a\u003e. The notebook is organized into a series of runnable scripts that each perform the steps needed to load data.\u003c/p\u003e\u003cp\u003eOur code uses Modal for every step of the orchestration, and Modal is best used on top of the Python scripts themselves. That is why a lot of these cells don’t contain Python blocks of their own.\u003c/p\u003e\u003cp\u003eFirst we use Modal to load in the \u003ccode class=\"cw qg qh qi pv b\"\u003eb-mc2/sql-create-context\u003c/code\u003e dataset. This is a simple task that just loads in the dataset and formats it into a \u003ccode class=\"cw qg qh qi pv b\"\u003e.jsonl\u003c/code\u003e file.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9cb1\" class=\"py os gt pv b bf pz qa l qb qc\"\u003emodal run src.load_data_sql --data-dir \"data_sql\"\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAs we can see, under the hood the task is quite straightforward:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"47a3\" class=\"py os gt pv b bf pz qa l qb qc\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Modal stubs allow our function to run remotely\u003c/span\u003e\n\u003cspan class=\"hljs-meta\"\u003e@stub.function(\u003cspan class=\"hljs-params\"\u003e\n    retries=Retries(\u003cspan class=\"hljs-params\"\u003e\n        max_retries=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e,\n        initial_delay=\u003cspan class=\"hljs-number\"\u003e5.0\u003c/span\u003e,\n        backoff_coefficient=\u003cspan class=\"hljs-number\"\u003e2.0\u003c/span\u003e,\n    \u003c/span\u003e),\n    timeout=\u003cspan class=\"hljs-number\"\u003e60\u003c/span\u003e * \u003cspan class=\"hljs-number\"\u003e60\u003c/span\u003e * \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e,\n    network_file_systems={VOL_MOUNT_PATH.as_posix(\u003cspan class=\"hljs-params\"\u003e\u003c/span\u003e): output_vol},\n    cloud=\u003cspan class=\"hljs-string\"\u003e\"gcp\"\u003c/span\u003e,\n\u003c/span\u003e)\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eload_data_sql\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edata_dir: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e = \u003cspan class=\"hljs-string\"\u003e\"data_sql\"\u003c/span\u003e\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e datasets \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e load_dataset\n\n    dataset = load_dataset(\u003cspan class=\"hljs-string\"\u003e\"b-mc2/sql-create-context\"\u003c/span\u003e)\n\n    dataset_splits = {\u003cspan class=\"hljs-string\"\u003e\"train\"\u003c/span\u003e: dataset[\u003cspan class=\"hljs-string\"\u003e\"train\"\u003c/span\u003e]}\n    out_path = get_data_path(data_dir)\n\n    out_path.parent.mkdir(parents=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e, exist_ok=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e key, ds \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e dataset_splits.items():\n        \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eopen\u003c/span\u003e(out_path, \u003cspan class=\"hljs-string\"\u003e\"w\"\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e f:\n            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e item \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e ds:\n                newitem = {\n                    \u003cspan class=\"hljs-string\"\u003e\"input\"\u003c/span\u003e: item[\u003cspan class=\"hljs-string\"\u003e\"question\"\u003c/span\u003e],\n                    \u003cspan class=\"hljs-string\"\u003e\"context\"\u003c/span\u003e: item[\u003cspan class=\"hljs-string\"\u003e\"context\"\u003c/span\u003e],\n                    \u003cspan class=\"hljs-string\"\u003e\"output\"\u003c/span\u003e: item[\u003cspan class=\"hljs-string\"\u003e\"answer\"\u003c/span\u003e],\n                }\n                f.write(json.dumps(newitem) + \u003cspan class=\"hljs-string\"\u003e\"\\n\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eStep 2: Run Finetuning Script\u003c/h2\u003e\u003cp\u003eThe next step is to run our finetuning script on the parsed dataset.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6964\" class=\"py os gt pv b bf pz qa l qb qc\"\u003emodal run src.finetune_sql --data-dir \"data_sql\" --model-dir \"model_sql\"\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe finetuning script performs the following steps.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eSplits the dataset into training and validation splits\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"596b\" class=\"py os gt pv b bf pz qa l qb qc\"\u003etrain_val = data[\"train\"].train_test_split(test_size=val_set_size, shuffle=True, seed=42)\ntrain_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\nval_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eFormats each split into tuples of (input prompt, label): \u003c/strong\u003eThe input query and context are formatted into the same input prompt. The input prompt is then tokenized, and the labels are set to the exact same as the input prompt — this allows the model to train on next-token prediction.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"22f3\" class=\"py os gt pv b bf pz qa l qb qc\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003egenerate_and_tokenize_prompt\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003edata_point\u003c/span\u003e):\n  full_prompt = generate_prompt_sql(\n      data_point[\u003cspan class=\"hljs-string\"\u003e\"input\"\u003c/span\u003e],\n      data_point[\u003cspan class=\"hljs-string\"\u003e\"context\"\u003c/span\u003e],\n      data_point[\u003cspan class=\"hljs-string\"\u003e\"output\"\u003c/span\u003e],\n  )\n  tokenized_full_prompt = tokenize(full_prompt)\n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e train_on_inputs:\n      \u003cspan class=\"hljs-keyword\"\u003eraise\u003c/span\u003e NotImplementedError(\u003cspan class=\"hljs-string\"\u003e\"not implemented yet\"\u003c/span\u003e)\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e tokenized_full_prompt\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe input prompt is the exact same as what was given at the top of this blog.\u003c/p\u003e\u003cp\u003eWhen the finetuning script is run, the model is saved in the remote cloud directory specified by model_dir (which is set to a default value if not specified).\u003c/p\u003e\u003ch2\u003eStep 3: Evaluation\u003c/h2\u003e\u003cp\u003eThe model has been finetuned and can be served from the cloud. We can run some basic evaluations using sample data from sql-create-context to compare the performance of the finetuned model vs. the baseline Llama 2 model.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a768\" class=\"py os gt pv b bf pz qa l qb qc\"\u003emodal run src.eval_sql::main\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe results demonstrate a massive improvement for the finetuned model:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9cf8\" class=\"py os gt pv b bf pz qa l qb qc\"\u003eInput \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e'input'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Which region (year) has Abigail at number 7, Sophia at number 1 and Aaliyah at number 5?'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'context'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'CREATE TABLE table_name_12 (region__year_ VARCHAR, no_5 VARCHAR, no_7 VARCHAR, no_1 VARCHAR)'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'output'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'SELECT region__year_ FROM table_name_12 WHERE no_7 = \"abigail\" AND no_1 = \"sophia\" AND\nno_5 = \"aaliyah\"'\u003c/span\u003e}\nOutput \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e (finetuned model): SELECT region__year_ FROM table_name_12 WHERE no_7 = \u003cspan class=\"hljs-string\"\u003e\"abigail\"\u003c/span\u003e AND no_1 = \u003cspan class=\"hljs-string\"\u003e\"aaliyah\"\u003c/span\u003e AND no_5 = \u003cspan class=\"hljs-string\"\u003e\"sophia\"\u003c/span\u003e\nOutput \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e (base model): SELECT * FROM table_name_12 WHERE region__year = \u003cspan class=\"hljs-string\"\u003e'2018'\u003c/span\u003e AND no_5 = \u003cspan class=\"hljs-string\"\u003e'Abigail'\u003c/span\u003e AND no_7 = \u003cspan class=\"hljs-string\"\u003e'Sophia'\u003c/span\u003e AND no_1 = \u003cspan class=\"hljs-string\"\u003e'Aaliyah'\u003c/span\u003e;\n\n\nInput \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e'input'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'Name the result/games for 54741'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'context'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'CREATE TABLE table_21436373_11 (result_games VARCHAR, attendance VARCHAR)'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'output'\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e'SELECT result_games FROM table_21436373_11 WHERE attendance = 54741'\u003c/span\u003e}\nOutput \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e (finetuned model): SELECT result_games FROM table_21436373_11 WHERE attendance = \u003cspan class=\"hljs-string\"\u003e\"54741\"\u003c/span\u003e\nOutput \u003cspan class=\"hljs-number\"\u003e2\u003c/span\u003e (base model): SELECT * FROM table_21436373_11 WHERE result_games = \u003cspan class=\"hljs-string\"\u003e'name'\u003c/span\u003e AND attendance \u0026amp;gt; \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e;\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWhereas the base model produces wrongly formatted outputs, or incorrect SQL statements,\u003c/p\u003e\u003cp\u003ethe finetuned model is able to produce outputs that are much closer to that of the expected output.\u003c/p\u003e\u003ch2\u003eStep 4: Integrating the Finetuned Model with LlamaIndex\u003c/h2\u003e\u003cp\u003eWe can now use this model in LlamaIndex for text-to-SQL over any database.\u003c/p\u003e\u003cp\u003eWe first define a test SQL database that we can then use to test the inference capabilities of the model.\u003c/p\u003e\u003cp\u003eWe create a toy \u003ccode class=\"cw qg qh qi pv b\"\u003ecity_stats\u003c/code\u003e table that contains city name, population, and country information, and populate it with a few sample cities.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"f910\" class=\"py os gt pv b bf pz qa l qb qc\"\u003edb_file = \u003cspan class=\"hljs-string\"\u003e\"cities.db\"\u003c/span\u003e\nengine = \u003cspan class=\"hljs-title function_\"\u003ecreate_engine\u003c/span\u003e(f\u003cspan class=\"hljs-string\"\u003e\"sqlite:///{db_file}\"\u003c/span\u003e)\nmetadata_obj = \u003cspan class=\"hljs-title class_\"\u003eMetaData\u003c/span\u003e()\n# create city \u003cspan class=\"hljs-variable constant_\"\u003eSQL\u003c/span\u003e table\ntable_name = \u003cspan class=\"hljs-string\"\u003e\"city_stats\"\u003c/span\u003e\ncity_stats_table = \u003cspan class=\"hljs-title class_\"\u003eTable\u003c/span\u003e(\n    table_name,\n    metadata_obj,\n    \u003cspan class=\"hljs-title class_\"\u003eColumn\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"city_name\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eString\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e), primary_key=\u003cspan class=\"hljs-title class_\"\u003eTrue\u003c/span\u003e),\n    \u003cspan class=\"hljs-title class_\"\u003eColumn\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"population\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eInteger\u003c/span\u003e),\n    \u003cspan class=\"hljs-title class_\"\u003eColumn\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"country\"\u003c/span\u003e, \u003cspan class=\"hljs-title class_\"\u003eString\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e16\u003c/span\u003e), nullable=\u003cspan class=\"hljs-title class_\"\u003eFalse\u003c/span\u003e),\n)\nmetadata_obj.\u003cspan class=\"hljs-title function_\"\u003ecreate_all\u003c/span\u003e(engine)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis is stored in a \u003ccode class=\"cw qg qh qi pv b\"\u003ecities.db\u003c/code\u003e file.\u003c/p\u003e\u003cp\u003eWe can then use Modal to load both the finetuned model and this database file into the \u003ccode class=\"cw qg qh qi pv b\"\u003eNLSQLTableQueryEngine\u003c/code\u003e in LlamaIndex - this query engine allows users easily start performing text-to-SQL over a given database.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4eb3\" class=\"py os gt pv b bf pz qa l qb qc\"\u003emodal run src.inference_sql_llamaindex::main --query \u003cspan class=\"hljs-string\"\u003e\"Which city has the highest population?\"\u003c/span\u003e --sqlite-file-path \u003cspan class=\"hljs-string\"\u003e\"nbs/cities.db\"\u003c/span\u003e --model-dir \u003cspan class=\"hljs-string\"\u003e\"model_sql\"\u003c/span\u003e --use-finetuned-model True\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe get a response like the following:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ef94\" class=\"py os gt pv b bf pz qa l qb qc\"\u003e\u003cspan class=\"hljs-variable constant_\"\u003eSQL\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eQuery\u003c/span\u003e: \u003cspan class=\"hljs-variable constant_\"\u003eSELECT\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eMAX\u003c/span\u003e(population) \u003cspan class=\"hljs-variable constant_\"\u003eFROM\u003c/span\u003e city_stats \u003cspan class=\"hljs-variable constant_\"\u003eWHERE\u003c/span\u003e country = \u003cspan class=\"hljs-string\"\u003e\"United States\"\u003c/span\u003e\n\u003cspan class=\"hljs-title class_\"\u003eResponse\u003c/span\u003e: [(\u003cspan class=\"hljs-number\"\u003e2679000\u003c/span\u003e,)]\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eAnd that’s basically it! This tutorial provides a very high-level way for you to get started finetuning a Llama 2 model on generating SQL statements, and showcases end-to-end how you can plug it into your text-to-SQL workflows with LlamaIndex.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResources\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eFor the sake of completeness we’re linking all of our resources again here.\u003c/p\u003e\u003cp\u003eTutorial repo: \u003ca href=\"https://github.com/run-llama/modal_finetune_sql\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/run-llama/modal_finetune_sql\u003c/a\u003e (adapted from \u003ca href=\"https://modal.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edoppel-bot\u003c/a\u003e).\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/run-llama/modal_finetune_sql/blob/main/tutorial.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eJupyter notebook guide\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eStack:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw qg qh qi pv b\"\u003e[b-mc2/sql-create-context\u003c/code\u003e from Hugging Face datasets](\u003ca href=\"https://huggingface.co/datasets/b-mc2/sql-create-context\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://huggingface.co/datasets/b-mc2/sql-create-context\u003c/a\u003e)\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/openlm-research/open_llama\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenLLaMa\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://github.com/huggingface/peft\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePEFT\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://modal.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eModal\u003c/a\u003e (+ \u003ca href=\"https://github.com/modal-labs/doppel-bot\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edoppel-bot repo\u003c/a\u003e).\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.llamaindex.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eSpecial mention: \u003ca href=\"https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlama 2 tutorial from Anyscale\u003c/a\u003e.\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/95f1268f640e36dd907dce8d0b366765bb436ee8-1024x1024.png","publishedDate":"2023-08-17","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-bf9b74d4436b1204f7567421bf0421e9319655a6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-03-05","slug":"llamaindex-newsletter-2024-03-05","title":"LlamaIndex Newsletter 2024-03-05"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-a195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265-webp","_type":"reference"}},"publishedDate":"2024-02-27","slug":"querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f","title":"Querying a network of knowledge with llama-index-networks"}],"slug":{"_type":"slug","current":"easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"397e326e-8d93-43fd-b559-cf89ee05f65c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"fine-tuning"},"title":"Fine Tuning"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"412f77ae-efba-466b-abc9-221fc36d252a","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"large-language-models"},"title":"Large Language Models"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"78713226-8bff-400f-bbfe-fd8a3d90be1d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"nlp"},"title":"NLP"}],"title":"Easily Finetune Llama 2 for Your Text-to-SQL Applications"},"publishedDate":"Invalid Date"},"params":{"slug":"easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"easily-finetune-llama-2-for-your-text-to-sql-applications-ecd53640e10d"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>