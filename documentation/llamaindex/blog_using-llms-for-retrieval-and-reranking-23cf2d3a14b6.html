<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Using LLM’s for Retrieval and Reranking — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Using LLM’s for Retrieval and Reranking — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Using LLM’s for Retrieval and Reranking — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Using LLM’s for Retrieval and Reranking — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="467" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2023-05-17</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Using LLM’s for Retrieval and Reranking</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/artificial-intelligence"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Artificial Intelligence</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/machine-learning"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Machine Learning</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/large-language-models"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Large Language Models</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/nlp"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">NLP</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><h1><strong>Summary</strong></h1><p>This blog post outlines some of the core abstractions we have created in <a href="https://github.com/jerryjliu/llama_index" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a> around LLM-powered retrieval and reranking, which helps to create enhancements to document retrieval beyond naive top-k embedding-based lookup.</p><p>LLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost. We show how using embedding-based retrieval as a first-stage pass, and second-stage retrieval as a reranking step can help provide a happy medium. We provide results over the Great Gatsby and the Lyft SEC 10-k.</p><figure><figcaption class="pq fe pr pc pd ps pt be b bf z dt">Two-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based reranking</figcaption><img src="/blog/images/1*xGWz2V-s5sQ6fWDM05auzA.png" alt="" width="700" height="200"></figure><h1><strong>Introduction and Background</strong></h1><p>There has been a wave of “Build a chatbot over your data” applications in the past few months, made possible with frameworks like <a href="https://github.com/jerryjliu/llama_index" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a> and <a href="https://github.com/hwchase17/langchain" rel="noopener ugc nofollow" target="_blank">LangChain</a>. A lot of these applications use a standard stack for retrieval augmented generation (RAG):</p><ul><li>Use a vector store to store unstructured documents (knowledge corpus)</li><li>Given a query, use a <strong>retrieval model </strong>to retrieve relevant documents from the corpus, and a <strong>synthesis model</strong> to generate a response.</li><li>The <strong>retrieval model </strong>fetches<strong> </strong>the top-k documents by embedding similarity to the query.</li></ul><p>In this stack, the retrieval model is not a novel idea; the concept of top-k embedding-based semantic search has been around for at least a decade, and doesn’t involve the LLM at all.</p><p>There are a lot of benefits to embedding-based retrieval:</p><ul><li>It’s very fast to compute dot products. Doesn’t require any model calls during query-time.</li><li>Even if not perfect, embeddings can encode the semantics of the document and query reasonably well. There’s a class of queries where embedding-based retrieval returns very relevant results.</li></ul><p>Yet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant context to the query, which in turn degrades the quality of the overall RAG system, regardless of the quality of the LLM.</p><p>This is also not a new problem: one approach to resolve this in existing IR and recommendation systems is to create a <strong>two stage process</strong>. The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision. Then the second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to “rerank” the existing retrieved candidates.</p><p>Covering the downsides of embedding-based retrieval is worth an entire series of blog posts. This blog post is an initial exploration of an alternative retrieval method and how it can (potentially) augment embedding-based retrieval methods.</p><h1><strong>LLM Retrieval and Reranking</strong></h1><p>Over the past week, we’ve developed a variety of initial abstractions around the concept of “LLM-based” retrieval and reranking. At a high-level, this approach uses the LLM to decide which document(s) / text chunk(s) are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric.</p><figure><figcaption class="pq fe pr pc pd ps pt be b bf z dt">Simple diagram of how LLM-based retrieval works</figcaption><img src="/blog/images/1*_y-GDOgTtQFS48D152B7MQ.png" alt="" width="700" height="676"></figure><p>An example prompt would look like the following:</p>

<pre><code>A list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.
  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well
  as the relevance score. The relevance score is a number from 1–10 based on how relevant you think the document is to the question.
  Do not include any documents that are not relevant to the question.
  Example format:
  Document 1:
  &lt;summary of document 1&gt;
  Document 2:
  &lt;summary of document 2&gt;
  …
  Document 10:
  &lt;summary of document 10&gt;
  Question: &lt;question&gt;
  Answer:
  Doc: 9, Relevance: 7
  Doc: 3, Relevance: 4
  Doc: 7, Relevance: 3
  Let's try this now:
  {context_str}
  Question: {query_str}
  Answer:</code></pre>






<p>The prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document:</p><ul><li>You can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk.</li><li>You can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new <a href="https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec" rel="noopener">document summary index</a>, but you can also choose to do it yourself.</li></ul><p>Given a collection of documents, we can then create document “batches” and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches.</p><p>You can use our abstractions in two forms: as a standalone retriever module (<code class="cw qm qn qo qe b">ListIndexLLMRetriever</code>) or a reranker module (<code class="cw qm qn qo qe b">LLMRerank</code>). The remainder of this blog primarily focuses on the reranker module given the speed/cost.</p><h2><strong>LLM Retriever</strong><code class="cw qm qn qo qe b">(ListIndexLLMRetriever)</code></h2><p>This module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index.</p><pre><span id="6811" class="qh nb gt qe b bf qi qj l qk ql"><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> GPTListIndex
<span class="hljs-keyword">from</span> llama_index.indices.<span class="hljs-built_in">list</span>.retrievers <span class="hljs-keyword">import</span> ListIndexLLMRetriever
index = GPTListIndex.from_documents(documents, service_context=service_context)
<span class="hljs-comment"># high - level API</span>
query_str = <span class="hljs-string">"What did the author do during his time in college?"</span>
retriever = index.as_retriever(retriever_mode=<span class="hljs-string">"llm"</span>)
nodes = retriever.retrieve(query_str)
<span class="hljs-comment"># lower-level API</span>
retriever = ListIndexLLMRetriever()
response_synthesizer = ResponseSynthesizer.from_args()
query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)
response = query_engine.query(query_str)</span></pre><p><strong>Use Case: </strong>This could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes.</p><h2><strong>LLM Reranker (LLMRerank)</strong></h2><p>This module is defined as part of our <code class="cw qm qn qo qe b">NodePostprocessor</code> abstraction, which is defined for second-stage processing after an initial retrieval pass.</p><p>The postprocessor can be used on its own or as part of a <code class="cw qm qn qo qe b">RetrieverQueryEngine</code> call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index.</p><pre><span id="0ef3" class="qh nb gt qe b bf qi qj l qk ql"><span class="hljs-keyword">from</span> llama_index.indices.query.schema <span class="hljs-keyword">import</span> QueryBundle
query_bundle = QueryBundle(query_str)
<span class="hljs-comment"># configure retriever</span>
retriever = VectorIndexRetriever(
index=index,
similarity_top_k=vector_top_k,
)
retrieved_nodes = retriever.retrieve(query_bundle)
<span class="hljs-comment"># configure reranker</span>
reranker = LLMRerank(choice_batch_size=<span class="hljs-number">5</span>, top_n=reranker_top_n, service_context=service_context)
retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)</span></pre><h2><strong>Limitations/Caveats</strong></h2><p>There are certain limitations and caveats to LLM-based retrieval, especially with this initial version.</p><ul><li>LLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete.</li><li>Using third-party LLM API’s costs money.</li><li>The current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents.</li></ul><p>Using the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful.</p><h1><strong>Initial Experimental Results</strong></h1><p>Let’s take a look at how well LLM reranking works!</p><p>We show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don’t showcase as many results given that it tends to run a lot slower than either of the first two approaches).</p><p>We analyze results over two very different sources of data: the Great Gatsby and the 2021 Lyft SEC 10-k. We only analyze results over the “retrieval” portion and not synthesis to better isolate the performance of different retrieval methods.</p><p>The results are presented in a qualitative fashion. A next step would definitely be more comprehensive evaluation over an entire dataset!</p><h2><strong>The Great Gatsby</strong></h2><p>In our first example, we load in the Great Gatsby as a <code class="cw qm qn qo qe b">Document</code> object, and build a vector index over it (with chunk size set to 512).</p><pre><span id="78ed" class="qh nb gt qe b bf qi qj l qk ql"># LLM Predictor (gpt-3.5-turbo) + service context
llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo"))
service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)
# load documents
documents = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()
index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)</span></pre><p>We then define a <code class="cw qm qn qo qe b">get_retrieved_nodes</code> function — this function can either do just embedding-based retrieval over the index, or embedding-based retrieval + reranking.</p><pre><span id="d46b" class="qh nb gt qe b bf qi qj l qk ql"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_retrieved_nodes</span>(<span class="hljs-params">
    query_str, vector_top_k=<span class="hljs-number">10</span>, reranker_top_n=<span class="hljs-number">3</span>, with_reranker=<span class="hljs-literal">False</span>
</span>):
  query_bundle = QueryBundle(query_str)
  <span class="hljs-comment"># configure retriever</span>
  retriever = VectorIndexRetriever(
    index=index,
    similarity_top_k=vector_top_k,
  )
  retrieved_nodes = retriever.retrieve(query_bundle)
  <span class="hljs-keyword">if</span> with_reranker:
    <span class="hljs-comment"># configure reranker</span>
    reranker = LLMRerank(choice_batch_size=<span class="hljs-number">5</span>, top_n=reranker_top_n, service_context=service_context)
    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)
  <span class="hljs-keyword">return</span> retrieved_nodes</span></pre><p>We then ask some questions. With embedding-based retrieval we set k=3. With two-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based reranking.</p><p><strong>Question: ”Who was driving the car that hit Myrtle?”</strong></p><p>(For those of you who are not familiar with the Great Gatsby, the narrator finds out later on from Gatsby that Daisy was actually the one driving the car, but Gatsby takes the blame for her).</p><p>The top retrieved contexts are shown in the images below. We see that in embedding-based retrieval, the top two texts contain semantics of the car crash but give no details as to who was actually responsible. Only the third text contains the proper answer.</p><figure><figcaption class="pq fe pr pc pd ps pt be b bf z dt">Retrieved context using top-k embedding lookup (baseline)</figcaption><img src="/blog/images/1*uIPFf5S38najwUSjJSojxQ.png" alt="" width="700" height="869"></figure><p>In contrast, the two-stage approach returns just one relevant context, and it contains the correct answer.</p><figure><figcaption class="pq fe pr pc pd ps pt be b bf z dt">Retrieved context using two-stage pipeline (embedding lookup then rerank)</figcaption><img src="/blog/images/1*DMQrDV30TpMVlj7K5uDJSQ.png" alt="" width="700" height="325"></figure><h2><strong>2021 Lyft SEC 10-K</strong></h2><p>We want to ask some questions over the 2021 Lyft SEC 10-K, specifically about the COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a ctrl-f for “COVID-19” returns 127 matches.</p><p>We use a similar setup as the Gatsby example above. The main differences are that we set the chunk size to 128 instead of 512, we set k=5 for the embedding retrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage approach.</p><p>We then ask the following questions and analyze the results.</p><p><strong>Question: “What initiatives are the company focusing on independently of COVID-19?”</strong></p><p>Results for the baseline are shown in the image above. We see that results corresponding to indices 0, 1, 3, 4, are about measures directly in response to Covid-19, even though the question was specifically about company initiatives that were independent of the COVID-19 pandemic.</p><figure><figcaption class="pq fe pr pc pd ps pt be b bf z dt">Retrieved context using top-k embedding lookup (baseline)</figcaption><img src="/blog/images/1*PHdLuisZw-o4Ceys1EOJIA.png" alt="" width="700" height="350"></figure><p>We get more relevant results in approach 2, by widening the top-k to 40 and then using an LLM to filter for the top-5 contexts. The independent company initiatives include “expansion of Light Vehicles” (1), “incremental investments in brand/marketing” (2), international expansion (3), and accounting for misc. risks such as natural disasters and operational risks in terms of financial performance (4).</p><figure><figcaption class="pq fe pr pc pd ps pt be b bf z dt">Retrieved context using two-stage pipeline (embedding lookup then rerank)</figcaption><img src="/blog/images/1*OjEeiUvX4TKPPFqTlEJSrg.png" alt="" width="700" height="389"></figure><h1><strong>Conclusion</strong></h1><p>That’s it for now! We’ve added some initial functionality to help support LLM-augmented retrieval pipelines, but of course there’s a ton of future steps that we couldn’t quite get to. Some questions we’d love to explore:</p><ul><li>How our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.)</li><li>What the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance.</li><li>Exploring different prompts and text summarization methods to help determine document relevance</li><li>Exploring if there’s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?)</li></ul><h2>Resources</h2><p>You can play around with the notebooks yourself!</p><p><a href="https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-Gatsby.ipynb" rel="noopener ugc nofollow" target="_blank">Great Gatsby Notebook</a></p><p><a href="https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb" rel="noopener ugc nofollow" target="_blank">2021 Lyft 10-K Notebook</a></p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f">Querying a network of knowledge with llama-index-networks</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-27</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824">LlamaIndex Newsletter 2024–02–27</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-27</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3">Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-24</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b">Introducing LlamaCloud and LlamaParse</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-20</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:48:20Z","_id":"9fa5fdab-9cca-4651-92aa-e3a3d64e0463","_rev":"TLgH6AcXrxoqw75SBDhjhB","_type":"blogPost","_updatedAt":"2025-05-21T20:38:59Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"htmlContent":"\u003ch1\u003e\u003cstrong\u003eSummary\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThis blog post outlines some of the core abstractions we have created in \u003ca href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex\u003c/a\u003e around LLM-powered retrieval and reranking, which helps to create enhancements to document retrieval beyond naive top-k embedding-based lookup.\u003c/p\u003e\u003cp\u003eLLM-powered retrieval can return more relevant documents than embedding-based retrieval, with the tradeoff being much higher latency and cost. We show how using embedding-based retrieval as a first-stage pass, and second-stage retrieval as a reranking step can help provide a happy medium. We provide results over the Great Gatsby and the Lyft SEC 10-k.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pq fe pr pc pd ps pt be b bf z dt\"\u003eTwo-stage retrieval pipeline: 1) Top-k embedding retrieval, then 2) LLM-based reranking\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*xGWz2V-s5sQ6fWDM05auzA.png\" alt=\"\" width=\"700\" height=\"200\"\u003e\u003c/figure\u003e\u003ch1\u003e\u003cstrong\u003eIntroduction and Background\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThere has been a wave of “Build a chatbot over your data” applications in the past few months, made possible with frameworks like \u003ca href=\"https://github.com/jerryjliu/llama_index\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaIndex\u003c/a\u003e and \u003ca href=\"https://github.com/hwchase17/langchain\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLangChain\u003c/a\u003e. A lot of these applications use a standard stack for retrieval augmented generation (RAG):\u003c/p\u003e\u003cul\u003e\u003cli\u003eUse a vector store to store unstructured documents (knowledge corpus)\u003c/li\u003e\u003cli\u003eGiven a query, use a \u003cstrong\u003eretrieval model \u003c/strong\u003eto retrieve relevant documents from the corpus, and a \u003cstrong\u003esynthesis model\u003c/strong\u003e to generate a response.\u003c/li\u003e\u003cli\u003eThe \u003cstrong\u003eretrieval model \u003c/strong\u003efetches\u003cstrong\u003e \u003c/strong\u003ethe top-k documents by embedding similarity to the query.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn this stack, the retrieval model is not a novel idea; the concept of top-k embedding-based semantic search has been around for at least a decade, and doesn’t involve the LLM at all.\u003c/p\u003e\u003cp\u003eThere are a lot of benefits to embedding-based retrieval:\u003c/p\u003e\u003cul\u003e\u003cli\u003eIt’s very fast to compute dot products. Doesn’t require any model calls during query-time.\u003c/li\u003e\u003cli\u003eEven if not perfect, embeddings can encode the semantics of the document and query reasonably well. There’s a class of queries where embedding-based retrieval returns very relevant results.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYet for a variety of reasons, embedding-based retrieval can be imprecise and return irrelevant context to the query, which in turn degrades the quality of the overall RAG system, regardless of the quality of the LLM.\u003c/p\u003e\u003cp\u003eThis is also not a new problem: one approach to resolve this in existing IR and recommendation systems is to create a \u003cstrong\u003etwo stage process\u003c/strong\u003e. The first stage uses embedding-based retrieval with a high top-k value to maximize recall while accepting a lower precision. Then the second stage uses a slightly more computationally expensive process that is higher precision and lower recall (for instance with BM25) to “rerank” the existing retrieved candidates.\u003c/p\u003e\u003cp\u003eCovering the downsides of embedding-based retrieval is worth an entire series of blog posts. This blog post is an initial exploration of an alternative retrieval method and how it can (potentially) augment embedding-based retrieval methods.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eLLM Retrieval and Reranking\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eOver the past week, we’ve developed a variety of initial abstractions around the concept of “LLM-based” retrieval and reranking. At a high-level, this approach uses the LLM to decide which document(s) / text chunk(s) are relevant to the given query. The input prompt would consist of a set of candidate documents, and the LLM is tasked with selecting the relevant set of documents as well as scoring their relevance with an internal metric.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pq fe pr pc pd ps pt be b bf z dt\"\u003eSimple diagram of how LLM-based retrieval works\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*_y-GDOgTtQFS48D152B7MQ.png\" alt=\"\" width=\"700\" height=\"676\"\u003e\u003c/figure\u003e\u003cp\u003eAn example prompt would look like the following:\u003c/p\u003e\n\n\u003cpre\u003e\u003ccode\u003eA list of documents is shown below. Each document has a number next to it along with a summary of the document. A question is also provided.\n  Respond with the numbers of the documents you should consult to answer the question, in order of relevance, as well\n  as the relevance score. The relevance score is a number from 1–10 based on how relevant you think the document is to the question.\n  Do not include any documents that are not relevant to the question.\n  Example format:\n  Document 1:\n  \u0026lt;summary of document 1\u0026gt;\n  Document 2:\n  \u0026lt;summary of document 2\u0026gt;\n  …\n  Document 10:\n  \u0026lt;summary of document 10\u0026gt;\n  Question: \u0026lt;question\u0026gt;\n  Answer:\n  Doc: 9, Relevance: 7\n  Doc: 3, Relevance: 4\n  Doc: 7, Relevance: 3\n  Let's try this now:\n  {context_str}\n  Question: {query_str}\n  Answer:\u003c/code\u003e\u003c/pre\u003e\n\n\n\n\n\n\n\u003cp\u003eThe prompt format implies that the text for each document should be relatively concise. There are two ways of feeding in the text to the prompt corresponding to each document:\u003c/p\u003e\u003cul\u003e\u003cli\u003eYou can directly feed in the raw text corresponding to the document. This works well if the document corresponds to a bite-sized text chunk.\u003c/li\u003e\u003cli\u003eYou can feed in a condensed summary for each document. This would be preferred if the document itself corresponds to a long-piece of text. We do this under the hood with our new \u003ca href=\"https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec\" rel=\"noopener\"\u003edocument summary index\u003c/a\u003e, but you can also choose to do it yourself.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eGiven a collection of documents, we can then create document “batches” and send each batch into the LLM input prompt. The output of each batch would be the set of relevant documents + relevance scores within that batch. The final retrieval response would aggregate relevant documents from all batches.\u003c/p\u003e\u003cp\u003eYou can use our abstractions in two forms: as a standalone retriever module (\u003ccode class=\"cw qm qn qo qe b\"\u003eListIndexLLMRetriever\u003c/code\u003e) or a reranker module (\u003ccode class=\"cw qm qn qo qe b\"\u003eLLMRerank\u003c/code\u003e). The remainder of this blog primarily focuses on the reranker module given the speed/cost.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eLLM Retriever\u003c/strong\u003e\u003ccode class=\"cw qm qn qo qe b\"\u003e(ListIndexLLMRetriever)\u003c/code\u003e\u003c/h2\u003e\u003cp\u003eThis module is defined over a list index, which simply stores a set of nodes as a flat list. You can build the list index over a set of documents and then use the LLM retriever to retrieve the relevant documents from the index.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6811\" class=\"qh nb gt qe b bf qi qj l qk ql\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e GPTListIndex\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.indices.\u003cspan class=\"hljs-built_in\"\u003elist\u003c/span\u003e.retrievers \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ListIndexLLMRetriever\nindex = GPTListIndex.from_documents(documents, service_context=service_context)\n\u003cspan class=\"hljs-comment\"\u003e# high - level API\u003c/span\u003e\nquery_str = \u003cspan class=\"hljs-string\"\u003e\"What did the author do during his time in college?\"\u003c/span\u003e\nretriever = index.as_retriever(retriever_mode=\u003cspan class=\"hljs-string\"\u003e\"llm\"\u003c/span\u003e)\nnodes = retriever.retrieve(query_str)\n\u003cspan class=\"hljs-comment\"\u003e# lower-level API\u003c/span\u003e\nretriever = ListIndexLLMRetriever()\nresponse_synthesizer = ResponseSynthesizer.from_args()\nquery_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer)\nresponse = query_engine.query(query_str)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eUse Case: \u003c/strong\u003eThis could potentially be used in place of our vector store index. You use the LLM instead of embedding-based lookup to select the nodes.\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eLLM Reranker (LLMRerank)\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThis module is defined as part of our \u003ccode class=\"cw qm qn qo qe b\"\u003eNodePostprocessor\u003c/code\u003e abstraction, which is defined for second-stage processing after an initial retrieval pass.\u003c/p\u003e\u003cp\u003eThe postprocessor can be used on its own or as part of a \u003ccode class=\"cw qm qn qo qe b\"\u003eRetrieverQueryEngine\u003c/code\u003e call. In the below example we show how to use the postprocessor as an independent module after an initial retriever call from a vector index.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0ef3\" class=\"qh nb gt qe b bf qi qj l qk ql\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.indices.query.schema \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e QueryBundle\nquery_bundle = QueryBundle(query_str)\n\u003cspan class=\"hljs-comment\"\u003e# configure retriever\u003c/span\u003e\nretriever = VectorIndexRetriever(\nindex=index,\nsimilarity_top_k=vector_top_k,\n)\nretrieved_nodes = retriever.retrieve(query_bundle)\n\u003cspan class=\"hljs-comment\"\u003e# configure reranker\u003c/span\u003e\nreranker = LLMRerank(choice_batch_size=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, top_n=reranker_top_n, service_context=service_context)\nretrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003e\u003cstrong\u003eLimitations/Caveats\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eThere are certain limitations and caveats to LLM-based retrieval, especially with this initial version.\u003c/p\u003e\u003cul\u003e\u003cli\u003eLLM-based retrieval is orders of magnitude slower than embedding-based retrieval. Embedding search over thousands or even millions of embeddings can take less than a second. Each LLM prompt of 4000 tokens to OpenAI can take minutes to complete.\u003c/li\u003e\u003cli\u003eUsing third-party LLM API’s costs money.\u003c/li\u003e\u003cli\u003eThe current method of batching documents may not be optimal, because it relies on an assumption that document batches can be scored independently of each other. This lacks a global view of the ranking for all documents.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eUsing the LLM to retrieve and rank every node in the document corpus can be prohibitively expensive. This is why using the LLM as a second-stage reranking step, after a first-stage embedding pass, can be helpful.\u003c/p\u003e\u003ch1\u003e\u003cstrong\u003eInitial Experimental Results\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eLet’s take a look at how well LLM reranking works!\u003c/p\u003e\u003cp\u003eWe show some comparisons between naive top-k embedding-based retrieval as well as the two-stage retrieval pipeline with a first-stage embedding-retrieval filter and second-stage LLM reranking. We also showcase some results of pure LLM-based retrieval (though we don’t showcase as many results given that it tends to run a lot slower than either of the first two approaches).\u003c/p\u003e\u003cp\u003eWe analyze results over two very different sources of data: the Great Gatsby and the 2021 Lyft SEC 10-k. We only analyze results over the “retrieval” portion and not synthesis to better isolate the performance of different retrieval methods.\u003c/p\u003e\u003cp\u003eThe results are presented in a qualitative fashion. A next step would definitely be more comprehensive evaluation over an entire dataset!\u003c/p\u003e\u003ch2\u003e\u003cstrong\u003eThe Great Gatsby\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eIn our first example, we load in the Great Gatsby as a \u003ccode class=\"cw qm qn qo qe b\"\u003eDocument\u003c/code\u003e object, and build a vector index over it (with chunk size set to 512).\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"78ed\" class=\"qh nb gt qe b bf qi qj l qk ql\"\u003e# LLM Predictor (gpt-3.5-turbo) + service context\nllm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\nservice_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n# load documents\ndocuments = SimpleDirectoryReader('../../../examples/gatsby/data').load_data()\nindex = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe then define a \u003ccode class=\"cw qm qn qo qe b\"\u003eget_retrieved_nodes\u003c/code\u003e function — this function can either do just embedding-based retrieval over the index, or embedding-based retrieval + reranking.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d46b\" class=\"qh nb gt qe b bf qi qj l qk ql\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eget_retrieved_nodes\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n    query_str, vector_top_k=\u003cspan class=\"hljs-number\"\u003e10\u003c/span\u003e, reranker_top_n=\u003cspan class=\"hljs-number\"\u003e3\u003c/span\u003e, with_reranker=\u003cspan class=\"hljs-literal\"\u003eFalse\u003c/span\u003e\n\u003c/span\u003e):\n  query_bundle = QueryBundle(query_str)\n  \u003cspan class=\"hljs-comment\"\u003e# configure retriever\u003c/span\u003e\n  retriever = VectorIndexRetriever(\n    index=index,\n    similarity_top_k=vector_top_k,\n  )\n  retrieved_nodes = retriever.retrieve(query_bundle)\n  \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e with_reranker:\n    \u003cspan class=\"hljs-comment\"\u003e# configure reranker\u003c/span\u003e\n    reranker = LLMRerank(choice_batch_size=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e, top_n=reranker_top_n, service_context=service_context)\n    retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n  \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e retrieved_nodes\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe then ask some questions. With embedding-based retrieval we set k=3. With two-stage retrieval we set k=10 for embedding retrieval and n=3 for LLM-based reranking.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQuestion: ”Who was driving the car that hit Myrtle?”\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e(For those of you who are not familiar with the Great Gatsby, the narrator finds out later on from Gatsby that Daisy was actually the one driving the car, but Gatsby takes the blame for her).\u003c/p\u003e\u003cp\u003eThe top retrieved contexts are shown in the images below. We see that in embedding-based retrieval, the top two texts contain semantics of the car crash but give no details as to who was actually responsible. Only the third text contains the proper answer.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pq fe pr pc pd ps pt be b bf z dt\"\u003eRetrieved context using top-k embedding lookup (baseline)\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*uIPFf5S38najwUSjJSojxQ.png\" alt=\"\" width=\"700\" height=\"869\"\u003e\u003c/figure\u003e\u003cp\u003eIn contrast, the two-stage approach returns just one relevant context, and it contains the correct answer.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pq fe pr pc pd ps pt be b bf z dt\"\u003eRetrieved context using two-stage pipeline (embedding lookup then rerank)\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*DMQrDV30TpMVlj7K5uDJSQ.png\" alt=\"\" width=\"700\" height=\"325\"\u003e\u003c/figure\u003e\u003ch2\u003e\u003cstrong\u003e2021 Lyft SEC 10-K\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003eWe want to ask some questions over the 2021 Lyft SEC 10-K, specifically about the COVID-19 impacts and responses. The Lyft SEC 10-K is 238 pages long, and a ctrl-f for “COVID-19” returns 127 matches.\u003c/p\u003e\u003cp\u003eWe use a similar setup as the Gatsby example above. The main differences are that we set the chunk size to 128 instead of 512, we set k=5 for the embedding retrieval baseline, and an embedding k=40 and reranker n=5 for the two-stage approach.\u003c/p\u003e\u003cp\u003eWe then ask the following questions and analyze the results.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eQuestion: “What initiatives are the company focusing on independently of COVID-19?”\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eResults for the baseline are shown in the image above. We see that results corresponding to indices 0, 1, 3, 4, are about measures directly in response to Covid-19, even though the question was specifically about company initiatives that were independent of the COVID-19 pandemic.\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pq fe pr pc pd ps pt be b bf z dt\"\u003eRetrieved context using top-k embedding lookup (baseline)\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*PHdLuisZw-o4Ceys1EOJIA.png\" alt=\"\" width=\"700\" height=\"350\"\u003e\u003c/figure\u003e\u003cp\u003eWe get more relevant results in approach 2, by widening the top-k to 40 and then using an LLM to filter for the top-5 contexts. The independent company initiatives include “expansion of Light Vehicles” (1), “incremental investments in brand/marketing” (2), international expansion (3), and accounting for misc. risks such as natural disasters and operational risks in terms of financial performance (4).\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"pq fe pr pc pd ps pt be b bf z dt\"\u003eRetrieved context using two-stage pipeline (embedding lookup then rerank)\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*OjEeiUvX4TKPPFqTlEJSrg.png\" alt=\"\" width=\"700\" height=\"389\"\u003e\u003c/figure\u003e\u003ch1\u003e\u003cstrong\u003eConclusion\u003c/strong\u003e\u003c/h1\u003e\u003cp\u003eThat’s it for now! We’ve added some initial functionality to help support LLM-augmented retrieval pipelines, but of course there’s a ton of future steps that we couldn’t quite get to. Some questions we’d love to explore:\u003c/p\u003e\u003cul\u003e\u003cli\u003eHow our LLM reranking implementation compares to other reranking methods (e.g. BM25, Cohere Rerank, etc.)\u003c/li\u003e\u003cli\u003eWhat the optimal values of embedding top-k and reranking top-n are for the two stage pipeline, accounting for latency, cost, and performance.\u003c/li\u003e\u003cli\u003eExploring different prompts and text summarization methods to help determine document relevance\u003c/li\u003e\u003cli\u003eExploring if there’s a class of applications where LLM-based retrieval on its own would suffice, without embedding-based filtering (maybe over smaller document collections?)\u003c/li\u003e\u003c/ul\u003e\u003ch2\u003eResources\u003c/h2\u003e\u003cp\u003eYou can play around with the notebooks yourself!\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGreat Gatsby Notebook\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/jerryjliu/llama_index/blob/main/docs/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e2021 Lyft 10-K Notebook\u003c/a\u003e\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/5c48f23d866b935957145fbec2e851e8d9ed4f62-3272x934.png","publishedDate":"2023-05-17","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-a195d5cbe68a6c2cb0847c985ead93111909f0bf-3378x3265-webp","_type":"reference"}},"publishedDate":"2024-02-27","slug":"querying-a-network-of-knowledge-with-llama-index-networks-d784b4c3006f","title":"Querying a network of knowledge with llama-index-networks"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-02-27","slug":"llamaindex-newsletter-2024-02-27-4b9102a0f824","title":"LlamaIndex Newsletter 2024–02–27"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646-png","_type":"reference"}},"publishedDate":"2024-02-24","slug":"bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3","title":"Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863-png","_type":"reference"}},"publishedDate":"2024-02-20","slug":"introducing-llamacloud-and-llamaparse-af8cedf9006b","title":"Introducing LlamaCloud and LlamaParse"}],"slug":{"_type":"slug","current":"using-llms-for-retrieval-and-reranking-23cf2d3a14b6"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"3d5b5d70-cd50-435c-86e7-3ba4c4c44d61","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"artificial-intelligence"},"title":"Artificial Intelligence"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"3e4feb2c-4916-495e-a9db-a829131f6c42","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"machine-learning"},"title":"Machine Learning"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"412f77ae-efba-466b-abc9-221fc36d252a","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"large-language-models"},"title":"Large Language Models"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"78713226-8bff-400f-bbfe-fd8a3d90be1d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"nlp"},"title":"NLP"}],"title":"Using LLM’s for Retrieval and Reranking"},"publishedDate":"Invalid Date"},"params":{"slug":"using-llms-for-retrieval-and-reranking-23cf2d3a14b6"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"using-llms-for-retrieval-and-reranking-23cf2d3a14b6"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>