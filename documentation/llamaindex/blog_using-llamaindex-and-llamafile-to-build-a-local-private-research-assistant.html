<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Using LlamaIndex and llamafile to build a local, private research assistant — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Using LlamaIndex and llamafile to build a local, private research assistant — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Using LlamaIndex and llamafile to build a local, private research assistant — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/6c60685882d9559573134b867ae725524173d545-1282x798.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Using LlamaIndex and llamafile to build a local, private research assistant — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/6c60685882d9559573134b867ae725524173d545-1282x798.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="399" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6c60685882d9559573134b867ae725524173d545-1282x798.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6c60685882d9559573134b867ae725524173d545-1282x798.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6c60685882d9559573134b867ae725524173d545-1282x798.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/kate-silverstein">Kate Silverstein</a> <!-- -->•<!-- --> <!-- -->2024-05-14</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Using LlamaIndex and llamafile to build a local, private research assistant</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamafile"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">llamafile</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>This is a guest post from our friends at Mozilla about <a href="https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-democratizing-ai/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Llamafile</a></em></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://github.com/Mozilla-Ocho/llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">llamafile</a>, an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from <a href="https://huggingface.co/models?sort=trending&amp;search=llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">HuggingFace</a> then run the file. That&#x27;s it. <strong>On most computers, you won&#x27;t need to install anything.</strong></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are a few reasons why you might want to run an LLM on your laptop, including:</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">1. Privacy: Running locally means you won&#x27;t have to share your data with third parties.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2. High availability: Run your LLM-based app without an internet connection.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">3. Bring your own model: You can easily test many different open-source LLMs (anything available on HuggingFace) and see which one works best for your task.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">4. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-based system without paying for API calls.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this blog post, we&#x27;ll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we&#x27;ll show how to use LlamaIndex with your llamafile as the LLM &amp; embedding backend for a local RAG-based research assistant. You won&#x27;t have to sign up for any cloud service or send your data to any third party--everything will just run on your laptop.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Note: You can also get all of the example code below as a Jupyter notebook from our <a href="https://github.com/Mozilla-Ocho/llamafile-llamaindex-examples" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">GitHub repo</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Download and run a llamafile</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">First, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There&#x27;s nothing to install or configure (with a few caveats, discussed <a href="https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gotchas" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>).</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Each llamafile bundles 1) model weights &amp; metadata in gguf format + 2) a copy of <a href="https://github.com/ggerganov/llama.cpp" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">`llama.cpp`</a> specially compiled using [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT-like browser interface, a CLI, and an OpenAI-compatible REST API for chat models.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are only 2 steps to setting up a llamafile:</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">1. Download a llamafile</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2. Make the llamafile executable</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We&#x27;ll go through each step in detail below.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA"><br/>Step 1: Download a llamafile</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are many llamafiles available on the <a href="https://huggingface.co/models?sort=trending&amp;search=llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">HuggingFace model hub</a> (just search for &#x27;llamafile&#x27;) but for the purpose of this walkthrough, we&#x27;ll use <a href="https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">TinyLlama-1.1B</a> (0.67 GB, <a href="https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">model info</a>). To download the model, you can either click this download link: <a href="https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile?download=true" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">TinyLlama-1.1B</a> or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection.</p><pre><code>wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile </code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This model is small and won&#x27;t be very good at actually answering questions but, since it&#x27;s a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it&#x27;s good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like <a href="https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Mistral-7B-Instruct</a> (5.15 GB, <a href="https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">model info</a>).</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA"><br/>Step 2: Make the llamafile executable</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">If you didn&#x27;t download the llamafile from the command line, figure out where your browser stored your downloaded llamafile.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Now, open your computer&#x27;s terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile`</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>If you&#x27;re using macOS, Linux, or BSD</strong>, you&#x27;ll need to grant permission for your computer to execute this new file. (You only need to do this once.):</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>If you&#x27;re on Windows, instead just rename the file by adding &quot;.exe&quot; on the end</strong> e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`</p><pre><code>chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile</code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Kick the tires</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Now, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded:</p><pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version

llamafile v0.7.0</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don&#x27;t work as expected, please <a href="https://github.com/Mozilla-Ocho/llamafile/issues" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">post an issue on the llamafile issue tracker</a>.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The easiest way to use your llamafile is via its built-in chat interface. In a terminal, run</p><pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Your browser should open automatically and display a chat interface. (If it doesn&#x27;t, just open your browser and point it at http://localhost:8080). When you&#x27;re done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you&#x27;re running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In the rest of this walkthrough, we&#x27;ll be using the llamafile&#x27;s built-in inference server instead of the browser interface. The llamafile&#x27;s server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available <a href="https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>. To start the llamafile in server mode, run:</p><pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding</code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Summary: Download and run a llamafile</h3><pre><code><span class="hljs-comment"># 1. Download the llamafile-ized model</span>
wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile

<span class="hljs-comment"># 2. Make it executable (you only need to do this once)</span>
<span class="hljs-built_in">chmod</span> +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

<span class="hljs-comment"># 3. Run in server mode</span>
./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding</code></pre><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Build a research assistant using LlamaIndex and llamafile</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Now, we&#x27;ll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose <a href="https://en.wikipedia.org/wiki/Homing_pigeon" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">homing pigeons</a>. We&#x27;ll show how to prepare your data, index into a vector store, then query it.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">One of the nice things about running an LLM locally is privacy. You can mix both &quot;public data&quot; like Wikipedia pages and &quot;private data&quot; without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won&#x27;t have to worry about leaking data. Below, we&#x27;ll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">To get started, download our example data:</p><pre><code><span class="hljs-built_in">mkdir</span> data

<span class="hljs-comment"># Download &#x27;The Homing Pigeon&#x27; manual from Project Gutenberg</span>
wget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt

<span class="hljs-comment"># Download some notes on homing pigeons</span>
wget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Next, we&#x27;ll need to install LlamaIndex and a few of its integrations:</p><pre><code><span class="hljs-comment"># Install llama-index</span>
pip install llama-index-core
<span class="hljs-comment"># Install llamafile integrations and SimpleWebPageReader</span>
pip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web</code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Start your llamafile server and configure LlamaIndex</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this example, we&#x27;ll use the same llamafile to both produce the embeddings that will get indexed in our vector store and as the LLM that will answer queries later on. (However, there is no reason you can&#x27;t use one llamafile for the embeddings and separate llamafile for the LLM functionality--you would just need to start the llamafile servers on different ports.)</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">To start the llamafile server, open a terminal and run:</p><pre><code>./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Now, we&#x27;ll configure LlamaIndex to use this llamafile:</p><pre><code><span class="hljs-comment"># Configure LlamaIndex</span>
<span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> Settings
<span class="hljs-keyword">from</span> llama_index.embeddings.llamafile <span class="hljs-keyword">import</span> LlamafileEmbedding
<span class="hljs-keyword">from</span> llama_index.llms.llamafile <span class="hljs-keyword">import</span> Llamafile
<span class="hljs-keyword">from</span> llama_index.core.node_parser <span class="hljs-keyword">import</span> SentenceSplitter

Settings.embed_model = LlamafileEmbedding(base_url=<span class="hljs-string">&quot;http://localhost:8080&quot;</span>)

Settings.llm = Llamafile(
	base_url=<span class="hljs-string">&quot;http://localhost:8080&quot;</span>,
	temperature=<span class="hljs-number">0</span>,
	seed=<span class="hljs-number">0</span>
)

<span class="hljs-comment"># Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don&#x27;t take up the model&#x27;s entire</span>
<span class="hljs-comment"># context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both</span>
<span class="hljs-comment"># the system prompt and the user&#x27;s actual question.</span>
Settings.transformations = [
	SentenceSplitter(
    	chunk_size=<span class="hljs-number">256</span>,
    	chunk_overlap=<span class="hljs-number">5</span>
	)
]</code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Prepare your data and build a vector store</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Now, we&#x27;ll load our data and index it.</p><pre><code><span class="hljs-comment"># Load local data</span>
<span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> SimpleDirectoryReader
local_doc_reader = SimpleDirectoryReader(input_dir=<span class="hljs-string">&#x27;./data&#x27;</span>)
docs = local_doc_reader.load_data(show_progress=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># We&#x27;ll load some Wikipedia pages as well</span>
<span class="hljs-keyword">from</span> llama_index.readers.web <span class="hljs-keyword">import</span> SimpleWebPageReader
urls = [
	<span class="hljs-string">&#x27;https://en.wikipedia.org/wiki/Homing_pigeon&#x27;</span>,
	<span class="hljs-string">&#x27;https://en.wikipedia.org/wiki/Magnetoreception&#x27;</span>,
]
web_reader = SimpleWebPageReader(html_to_text=<span class="hljs-literal">True</span>)
docs.extend(web_reader.load_data(urls))

<span class="hljs-comment"># Build the index</span>
<span class="hljs-keyword">from</span> llama_index.core <span class="hljs-keyword">import</span> VectorStoreIndex

index = VectorStoreIndex.from_documents(
	docs,
	show_progress=<span class="hljs-literal">True</span>,
)

<span class="hljs-comment"># Save the index</span>
index.storage_context.persist(persist_dir=<span class="hljs-string">&quot;./storage&quot;</span>)</code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Query your research assistant</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Finally, we&#x27;re ready to ask some questions about homing pigeons.</p><pre><code>query_engine = index.as_query_engine()
<span class="hljs-built_in">print</span>(query_engine.query(<span class="hljs-string">&quot;What were homing pigeons used for?&quot;</span>))</code></pre><pre><code>	Homing pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection.</code></pre><pre><code><span class="hljs-built_in">print</span>(query_engine.query(<span class="hljs-string">&quot;When were homing pigeons first used?&quot;</span>))</code></pre><pre><code>The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries.</code></pre><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Conclusion</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this post, we&#x27;ve shown how to download and set up an LLM running locally via llamafile. Then, we showed how to use this LLM with LlamaIndex to build a simple RAG-based research assistant for learning about homing pigeons. Your assistant ran 100% locally: you didn&#x27;t have to pay for API calls or send data to a third party.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">As a next step, you could try running the examples above with a better model like <a href="https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Mistral-7B-Instruct</a>. You could also try building a research assistant for different topic like &quot;semiconductors&quot; or &quot;how to bake bread&quot;.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">To find out more about llamafile, check out the project on <a href="https://github.com/Mozilla-Ocho/llamafile" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">GitHub</a>, read this <a href="https://justine.lol/oneliners/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">blog post</a> on bash one-liners using LLMs, or say hi to the community on <a href="https://discord.com/invite/teDuGYVTB2" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Discord</a>.</p></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-05-14T17:21:12Z","_id":"ad6fc301-a654-4c22-bda0-b320e6ed9dc3","_rev":"05dtDS0H5iRVsxYMarZYRT","_type":"blogPost","_updatedAt":"2025-05-21T20:39:21Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-05-14T17:22:51Z","_id":"445b1847-9e48-4ad9-b264-b028f31b3672","_rev":"D8alISzEH7lhjnNbD82QJg","_type":"people","_updatedAt":"2024-05-14T17:22:51Z","name":"Kate Silverstein","slug":{"_type":"slug","current":"kate-silverstein"}}],"featured":false,"image":{"_type":"image","asset":{"_ref":"image-6c60685882d9559573134b867ae725524173d545-1282x798-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/6c60685882d9559573134b867ae725524173d545-1282x798.png","publishedDate":"2024-05-14","relatedPosts":[],"slug":{"_type":"slug","current":"using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"},"tags":[{"_createdAt":"2024-05-14T17:23:12Z","_id":"5ae75b65-d4b8-4b64-badf-0f85c5e605b5","_rev":"D8alISzEH7lhjnNbD82R9T","_type":"blogTag","_updatedAt":"2024-05-14T17:23:12Z","slug":{"_type":"slug","current":"llamafile"},"title":"llamafile"}],"text":[{"_key":"387f26de4368","_type":"block","children":[{"_key":"bbf25f40af780","_type":"span","marks":["em"],"text":"This is a guest post from our friends at Mozilla about "},{"_key":"2dd38f90a284","_type":"span","marks":["00b3bcdda4d4","em"],"text":"Llamafile"}],"markDefs":[{"_key":"00b3bcdda4d4","_type":"link","href":"https://future.mozilla.org/news/llamafile-four-months-of-progress-towards-democratizing-ai/"}],"style":"normal"},{"_key":"a9dc5cbc9e61","_type":"block","children":[{"_key":"5526927b5506","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"b0f5623cd65d","_type":"block","children":[{"_key":"5bccbb00b551","_type":"span","marks":["bbe31c41821d"],"text":"llamafile"},{"_key":"bbf25f40af781","_type":"span","marks":[],"text":", an open source project from Mozilla, is one of the simplest ways to run a large language model (LLM) on your laptop. All you have to do is download a llamafile from "},{"_key":"bbf25f40af782","_type":"span","marks":["9a1eb449cc31"],"text":"HuggingFace"},{"_key":"bbf25f40af783","_type":"span","marks":[],"text":" then run the file. That's it. "},{"_key":"cd3832e1af76","_type":"span","marks":["strong"],"text":"On most computers, you won't need to install anything."}],"markDefs":[{"_key":"bbe31c41821d","_type":"link","href":"https://github.com/Mozilla-Ocho/llamafile"},{"_key":"9a1eb449cc31","_type":"link","href":"https://huggingface.co/models?sort=trending\u0026search=llamafile"}],"style":"normal"},{"_key":"8bc6ed14dac9","_type":"block","children":[{"_key":"8a4e9694f59a0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"7a7f0683bc63","_type":"block","children":[{"_key":"80b12bdc68880","_type":"span","marks":[],"text":"There are a few reasons why you might want to run an LLM on your laptop, including:"}],"markDefs":[],"style":"normal"},{"_key":"0eb178841c66","_type":"block","children":[{"_key":"f87a566c4e0e0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"21521bc88030","_type":"block","children":[{"_key":"9729754a340e0","_type":"span","marks":[],"text":"1. Privacy: Running locally means you won't have to share your data with third parties."}],"markDefs":[],"style":"normal"},{"_key":"523f9d6ea5a0","_type":"block","children":[{"_key":"cbd613e0c5ba0","_type":"span","marks":[],"text":"2. High availability: Run your LLM-based app without an internet connection."}],"markDefs":[],"style":"normal"},{"_key":"2c4c08d14b3a","_type":"block","children":[{"_key":"597c20bff0e00","_type":"span","marks":[],"text":"3. Bring your own model: You can easily test many different open-source LLMs (anything available on HuggingFace) and see which one works best for your task."}],"markDefs":[],"style":"normal"},{"_key":"bf7fe30a1dc6","_type":"block","children":[{"_key":"f5c5764897d00","_type":"span","marks":[],"text":"4. Free debugging/testing: Local LLMs allow you to test many parts of an LLM-based system without paying for API calls."}],"markDefs":[],"style":"normal"},{"_key":"e9c35e6e5cfe","_type":"block","children":[{"_key":"b1605a477b030","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"2fd6048a0981","_type":"block","children":[{"_key":"0e79de0543600","_type":"span","marks":[],"text":"In this blog post, we'll show how to set up a llamafile and use it to run a local LLM on your computer. Then, we'll show how to use LlamaIndex with your llamafile as the LLM \u0026 embedding backend for a local RAG-based research assistant. You won't have to sign up for any cloud service or send your data to any third party--everything will just run on your laptop."}],"markDefs":[],"style":"normal"},{"_key":"c50e7bf5207f","_type":"block","children":[{"_key":"852279ac813e0","_type":"span","marks":[],"text":"Note: You can also get all of the example code below as a Jupyter notebook from our "},{"_key":"852279ac813e1","_type":"span","marks":["0b638c1a965a"],"text":"GitHub repo"},{"_key":"852279ac813e2","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"0b638c1a965a","_type":"link","href":"https://github.com/Mozilla-Ocho/llamafile-llamaindex-examples"}],"style":"normal"},{"_key":"4af39732a627","_type":"block","children":[{"_key":"6f7c347e29b40","_type":"span","marks":[],"text":"Download and run a llamafile"}],"markDefs":[],"style":"h2"},{"_key":"86d68bcbc9d4","_type":"block","children":[{"_key":"d871fc3015210","_type":"span","marks":[],"text":"First, what is a llamafile? A llamafile is an executable LLM that you can run on your own computer. It contains the weights for a given open source LLM, as well as everything needed to actually run that model on your computer. There's nothing to install or configure (with a few caveats, discussed "},{"_key":"d871fc3015211","_type":"span","marks":["6a340025ff99"],"text":"here"},{"_key":"d871fc3015212","_type":"span","marks":[],"text":")."}],"markDefs":[{"_key":"6a340025ff99","_type":"link","href":"https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#gotchas"}],"style":"normal"},{"_key":"edc6ebddb39a","_type":"block","children":[{"_key":"091662d66b4c0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"97be277304c4","_type":"block","children":[{"_key":"4bf0847094f80","_type":"span","marks":[],"text":"Each llamafile bundles 1) model weights \u0026 metadata in gguf format + 2) a copy of "},{"_key":"4bf0847094f81","_type":"span","marks":["7bf0214c729f"],"text":"`llama.cpp`"},{"_key":"4bf0847094f82","_type":"span","marks":[],"text":" specially compiled using [Cosmopolitan Libc](https://github.com/jart/cosmopolitan). This allows the models to run on most computers without additional installation. llamafiles also come with a ChatGPT-like browser interface, a CLI, and an OpenAI-compatible REST API for chat models."}],"markDefs":[{"_key":"7bf0214c729f","_type":"link","href":"https://github.com/ggerganov/llama.cpp"}],"style":"normal"},{"_key":"f594c34abefd","_type":"block","children":[{"_key":"324107f76c6f0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"e3f9ad0b69e5","_type":"block","children":[{"_key":"7f908c2dec190","_type":"span","marks":[],"text":"There are only 2 steps to setting up a llamafile:"}],"markDefs":[],"style":"normal"},{"_key":"ae168969c7d5","_type":"block","children":[{"_key":"221c816d2970","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"7ca00745a881","_type":"block","children":[{"_key":"c32287a1ab9f0","_type":"span","marks":[],"text":"1. Download a llamafile"}],"markDefs":[],"style":"normal"},{"_key":"5fe86dea6043","_type":"block","children":[{"_key":"e5e3c6b828140","_type":"span","marks":[],"text":"2. Make the llamafile executable"}],"markDefs":[],"style":"normal"},{"_key":"11ceda2db069","_type":"block","children":[{"_key":"caca650f77c80","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"5e3c82326fbd","_type":"block","children":[{"_key":"53908f2f66d50","_type":"span","marks":[],"text":"We'll go through each step in detail below."}],"markDefs":[],"style":"normal"},{"_key":"e52b325a64cb","_type":"block","children":[{"_key":"52e15624d58e","_type":"span","marks":[],"text":"\nStep 1: Download a llamafile"}],"markDefs":[],"style":"h3"},{"_key":"5e8417264599","_type":"block","children":[{"_key":"c7b92aa5819f0","_type":"span","marks":[],"text":"There are many llamafiles available on the "},{"_key":"c7b92aa5819f1","_type":"span","marks":["839b6b36953f"],"text":"HuggingFace model hub"},{"_key":"c7b92aa5819f2","_type":"span","marks":[],"text":" (just search for 'llamafile') but for the purpose of this walkthrough, we'll use "},{"_key":"c7b92aa5819f3","_type":"span","marks":["a7f6a0ac0280"],"text":"TinyLlama-1.1B"},{"_key":"c7b92aa5819f4","_type":"span","marks":[],"text":" (0.67 GB, "},{"_key":"c7b92aa5819f5","_type":"span","marks":["43233fcb212d"],"text":"model info"},{"_key":"c7b92aa5819f6","_type":"span","marks":[],"text":"). To download the model, you can either click this download link: "},{"_key":"c7b92aa5819f7","_type":"span","marks":["48a08f6ecc00"],"text":"TinyLlama-1.1B"},{"_key":"c7b92aa5819f8","_type":"span","marks":[],"text":" or open a terminal and use something like `wget`. The download should take 5-10 minutes depending on the quality of your internet connection."}],"markDefs":[{"_key":"839b6b36953f","_type":"link","href":"https://huggingface.co/models?sort=trending\u0026search=llamafile"},{"_key":"a7f6a0ac0280","_type":"link","href":"https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile"},{"_key":"43233fcb212d","_type":"link","href":"https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile"},{"_key":"48a08f6ecc00","_type":"link","href":"https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile?download=true"}],"style":"normal"},{"_key":"3494faf99ce1","_type":"codeBlock","code":"wget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile ","language":"text"},{"_key":"233692614434","_type":"block","children":[{"_key":"eff28ecc1fcd0","_type":"span","marks":[],"text":"This model is small and won't be very good at actually answering questions but, since it's a relatively quick download and its inference speed will allow you to index your vector store in just a few minutes, it's good enough for the examples below. For a higher-quality LLM, you may want to use a larger model like "},{"_key":"eff28ecc1fcd1","_type":"span","marks":["da66e2756b86"],"text":"Mistral-7B-Instruct"},{"_key":"eff28ecc1fcd2","_type":"span","marks":[],"text":" (5.15 GB, "},{"_key":"eff28ecc1fcd3","_type":"span","marks":["239fdae02bfa"],"text":"model info"},{"_key":"eff28ecc1fcd4","_type":"span","marks":[],"text":")."}],"markDefs":[{"_key":"da66e2756b86","_type":"link","href":"https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true"},{"_key":"239fdae02bfa","_type":"link","href":"https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile"}],"style":"normal"},{"_key":"473864e96051","_type":"block","children":[{"_key":"1f8f3de59b850","_type":"span","marks":[],"text":"\nStep 2: Make the llamafile executable"}],"markDefs":[],"style":"h3"},{"_key":"a4b61f10083f","_type":"block","children":[{"_key":"a12301d87bc20","_type":"span","marks":[],"text":"If you didn't download the llamafile from the command line, figure out where your browser stored your downloaded llamafile."}],"markDefs":[],"style":"normal"},{"_key":"c12060ffad4c","_type":"block","children":[{"_key":"86a287bccef90","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"5ec8737d02f1","_type":"block","children":[{"_key":"7220e3583c330","_type":"span","marks":[],"text":"Now, open your computer's terminal and, if necessary, go to the directory where your llamafile is stored: `cd path/to/downloaded/llamafile`"}],"markDefs":[],"style":"normal"},{"_key":"f2e3dd7eeab1","_type":"block","children":[{"_key":"964e64f814920","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"369851453929","_type":"block","children":[{"_key":"e28537a205730","_type":"span","marks":["strong"],"text":"If you're using macOS, Linux, or BSD"},{"_key":"57105b08f833","_type":"span","marks":[],"text":", you'll need to grant permission for your computer to execute this new file. (You only need to do this once.):"}],"markDefs":[],"style":"normal"},{"_key":"bb045a98ef13","_type":"block","children":[{"_key":"0c2e048a66490","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"9750ff5ec8c8","_type":"block","children":[{"_key":"7a5877edaa750","_type":"span","marks":["strong"],"text":"If you're on Windows, instead just rename the file by adding \".exe\" on the end"},{"_key":"aba58a44a8a2","_type":"span","marks":[],"text":" e.g. rename `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile` to `TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile.exe`"}],"markDefs":[],"style":"normal"},{"_key":"c1e83a80cf2f","_type":"codeBlock","code":"chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile","language":"text"},{"_key":"4af1d86457c3","_type":"block","children":[{"_key":"0494580d4dde0","_type":"span","marks":[],"text":"Kick the tires"}],"markDefs":[],"style":"h3"},{"_key":"f27ef03a5f0c","_type":"block","children":[{"_key":"b9d979b637630","_type":"span","marks":[],"text":"Now, your llamafile should be ready to go. First, you can check which version of the llamafile library was used to build the llamafile binary you should downloaded:"}],"markDefs":[],"style":"normal"},{"_key":"476391fc5b64","_type":"codeBlock","code":"./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --version\n\nllamafile v0.7.0","language":"text"},{"_key":"3343fa89c67e","_type":"block","children":[{"_key":"ddd05d4404840","_type":"span","marks":[],"text":"This post was written using a model built with `llamafile v0.7.0`. If your llamafile displays a different version and some of the steps below don't work as expected, please "},{"_key":"fa1e4532e2a9","_type":"span","marks":["a0a7f0e943db"],"text":"post an issue on the llamafile issue tracker"},{"_key":"8c0bd7763b60","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"a0a7f0e943db","_type":"link","href":"https://github.com/Mozilla-Ocho/llamafile/issues"}],"style":"normal"},{"_key":"28609a3d81c5","_type":"block","children":[{"_key":"b3acfd7f9ad8","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"3286d57c4191","_type":"block","children":[{"_key":"b437d773beb00","_type":"span","marks":[],"text":"The easiest way to use your llamafile is via its built-in chat interface. In a terminal, run"}],"markDefs":[],"style":"normal"},{"_key":"da4819cb21de","_type":"codeBlock","code":"./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile","language":"sh"},{"_key":"6519d7de2ac0","_type":"block","children":[{"_key":"5566795fe9c20","_type":"span","marks":[],"text":"Your browser should open automatically and display a chat interface. (If it doesn't, just open your browser and point it at http://localhost:8080). When you're done chatting, return to your terminal and hit `Control-C` to shut down llamafile. If you're running these commands inside a notebook, just interrupt the notebook kernel to stop the llamafile."}],"markDefs":[],"style":"normal"},{"_key":"eff9184df728","_type":"block","children":[{"_key":"7fd9b1d124bf0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"81594c6551c9","_type":"block","children":[{"_key":"c11854419a2a0","_type":"span","marks":[],"text":"In the rest of this walkthrough, we'll be using the llamafile's built-in inference server instead of the browser interface. The llamafile's server provides a REST API for interacting with the TinyLlama LLM via HTTP. Full server API documentation is available "},{"_key":"c11854419a2a1","_type":"span","marks":["d8c77ae4827d"],"text":"here"},{"_key":"c11854419a2a2","_type":"span","marks":[],"text":". To start the llamafile in server mode, run:"}],"markDefs":[{"_key":"d8c77ae4827d","_type":"link","href":"https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints"}],"style":"normal"},{"_key":"cd88b5d8c53e","_type":"codeBlock","code":"./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding","language":"sh"},{"_key":"124ab3d8fc94","_type":"block","children":[{"_key":"ad67b0cbcea10","_type":"span","marks":[],"text":"Summary: Download and run a llamafile"}],"markDefs":[],"style":"h3"},{"_key":"ec261d1f78f5","_type":"codeBlock","code":"# 1. Download the llamafile-ized model\nwget https://huggingface.co/Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile/resolve/main/TinyLlama-1.1B-Chat-v1.0.F16.llamafile\n\n# 2. Make it executable (you only need to do this once)\nchmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n\n# 3. Run in server mode\n./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding","language":"sh"},{"_key":"361adfae623d","_type":"block","children":[{"_key":"e4e00f92b2040","_type":"span","marks":[],"text":"Build a research assistant using LlamaIndex and llamafile"}],"markDefs":[],"style":"h2"},{"_key":"10dec5517b71","_type":"block","children":[{"_key":"446e71c197580","_type":"span","marks":[],"text":"Now, we'll show how to use LlamaIndex with your llamafile to build a research assistant to help you learn about some topic of interest--for this post, we chose "},{"_key":"446e71c197581","_type":"span","marks":["d4a8281e61f2"],"text":"homing pigeons"},{"_key":"446e71c197582","_type":"span","marks":[],"text":". We'll show how to prepare your data, index into a vector store, then query it."}],"markDefs":[{"_key":"d4a8281e61f2","_type":"link","href":"https://en.wikipedia.org/wiki/Homing_pigeon"}],"style":"normal"},{"_key":"e5b3700f9420","_type":"block","children":[{"_key":"31014305a9b60","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"fdf2e662064b","_type":"block","children":[{"_key":"d02f6cb94a150","_type":"span","marks":[],"text":"One of the nice things about running an LLM locally is privacy. You can mix both \"public data\" like Wikipedia pages and \"private data\" without worrying about sharing your data with a third party. Private data could include e.g. your private notes on a topic or PDFs of classified content. As long as you use a local LLM (and a local vector store), you won't have to worry about leaking data. Below, we'll show how to combine both types of data. Our vector store will include Wikipedia pages, an Army manual on caring for homing pigeons, and some brief notes we took while we were reading about this topic."}],"markDefs":[],"style":"normal"},{"_key":"fcc70aa3c6e7","_type":"block","children":[{"_key":"a062da3238540","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"aed82ef4e065","_type":"block","children":[{"_key":"4d715f9b60da0","_type":"span","marks":[],"text":"To get started, download our example data:"}],"markDefs":[],"style":"normal"},{"_key":"8a635121706f","_type":"codeBlock","code":"mkdir data\n\n# Download 'The Homing Pigeon' manual from Project Gutenberg\nwget https://www.gutenberg.org/cache/epub/55084/pg55084.txt -O data/The_Homing_Pigeon.txt\n\n# Download some notes on homing pigeons\nwget https://gist.githubusercontent.com/k8si/edf5a7ca2cc3bef7dd3d3e2ca42812de/raw/24955ee9df819e21975b1dd817938c1bfe955634/homing_pigeon_notes.md -O data/homing_pigeon_notes.md","language":"sh"},{"_key":"6a08b6a1931e","_type":"block","children":[{"_key":"59623e8dfe000","_type":"span","marks":[],"text":"Next, we'll need to install LlamaIndex and a few of its integrations:"}],"markDefs":[],"style":"normal"},{"_key":"248ade4ef900","_type":"codeBlock","code":"# Install llama-index\npip install llama-index-core\n# Install llamafile integrations and SimpleWebPageReader\npip install llama-index-embeddings-llamafile llama-index-llms-llamafile llama-index-readers-web","language":"sh"},{"_key":"b386c67e33de","_type":"block","children":[{"_key":"6c686f7489f6","_type":"span","marks":[],"text":"Start your llamafile server and configure LlamaIndex"}],"markDefs":[],"style":"h3"},{"_key":"1dd0eddb4268","_type":"block","children":[{"_key":"29ab365443e40","_type":"span","marks":[],"text":"In this example, we'll use the same llamafile to both produce the embeddings that will get indexed in our vector store and as the LLM that will answer queries later on. (However, there is no reason you can't use one llamafile for the embeddings and separate llamafile for the LLM functionality--you would just need to start the llamafile servers on different ports.)"}],"markDefs":[],"style":"normal"},{"_key":"462784bfe1d5","_type":"block","children":[{"_key":"48407a494f900","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"98e758c4c0d2","_type":"block","children":[{"_key":"c5b74a8d19910","_type":"span","marks":[],"text":"To start the llamafile server, open a terminal and run:"}],"markDefs":[],"style":"normal"},{"_key":"177d0e33b9b7","_type":"codeBlock","code":"./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding --port 8080","language":"sh"},{"_key":"1b720664f07a","_type":"block","children":[{"_key":"224bae51fcc00","_type":"span","marks":[],"text":"Now, we'll configure LlamaIndex to use this llamafile:"}],"markDefs":[],"style":"normal"},{"_key":"cbbf71224b2a","_type":"codeBlock","code":"# Configure LlamaIndex\nfrom llama_index.core import Settings\nfrom llama_index.embeddings.llamafile import LlamafileEmbedding\nfrom llama_index.llms.llamafile import Llamafile\nfrom llama_index.core.node_parser import SentenceSplitter\n\nSettings.embed_model = LlamafileEmbedding(base_url=\"http://localhost:8080\")\n\nSettings.llm = Llamafile(\n\tbase_url=\"http://localhost:8080\",\n\ttemperature=0,\n\tseed=0\n)\n\n# Also set up a sentence splitter to ensure texts are broken into semantically-meaningful chunks (sentences) that don't take up the model's entire\n# context window (2048 tokens). Since these chunks will be added to LLM prompts as part of the RAG process, we want to leave plenty of space for both\n# the system prompt and the user's actual question.\nSettings.transformations = [\n\tSentenceSplitter(\n    \tchunk_size=256,\n    \tchunk_overlap=5\n\t)\n]","language":"python"},{"_key":"3210d3702dfa","_type":"block","children":[{"_key":"c25868cba4390","_type":"span","marks":[],"text":"Prepare your data and build a vector store"}],"markDefs":[],"style":"h3"},{"_key":"70d9714be44b","_type":"block","children":[{"_key":"79486d7aa6170","_type":"span","marks":[],"text":"Now, we'll load our data and index it."}],"markDefs":[],"style":"normal"},{"_key":"b157668e05f1","_type":"codeBlock","code":"# Load local data\nfrom llama_index.core import SimpleDirectoryReader\nlocal_doc_reader = SimpleDirectoryReader(input_dir='./data')\ndocs = local_doc_reader.load_data(show_progress=True)\n\n# We'll load some Wikipedia pages as well\nfrom llama_index.readers.web import SimpleWebPageReader\nurls = [\n\t'https://en.wikipedia.org/wiki/Homing_pigeon',\n\t'https://en.wikipedia.org/wiki/Magnetoreception',\n]\nweb_reader = SimpleWebPageReader(html_to_text=True)\ndocs.extend(web_reader.load_data(urls))\n\n# Build the index\nfrom llama_index.core import VectorStoreIndex\n\nindex = VectorStoreIndex.from_documents(\n\tdocs,\n\tshow_progress=True,\n)\n\n# Save the index\nindex.storage_context.persist(persist_dir=\"./storage\")","language":"python"},{"_key":"fa41c6f0f79a","_type":"block","children":[{"_key":"e89001449d9d0","_type":"span","marks":[],"text":"Query your research assistant"}],"markDefs":[],"style":"h3"},{"_key":"07f46c1c4334","_type":"block","children":[{"_key":"10086be7691d0","_type":"span","marks":[],"text":"Finally, we're ready to ask some questions about homing pigeons."}],"markDefs":[],"style":"normal"},{"_key":"12ab0cd45c50","_type":"codeBlock","code":"query_engine = index.as_query_engine()\nprint(query_engine.query(\"What were homing pigeons used for?\"))","language":"python"},{"_key":"9202e52aa50f","_type":"codeBlock","code":"\tHoming pigeons were used for a variety of purposes, including military reconnaissance, communication, and transportation. They were also used for scientific research, such as studying the behavior of birds in flight and their migration patterns. In addition, they were used for religious ceremonies and as a symbol of devotion and loyalty. Overall, homing pigeons played an important role in the history of aviation and were a symbol of the human desire for communication and connection.","language":"text"},{"_key":"5b91b9cae5bf","_type":"codeBlock","code":"print(query_engine.query(\"When were homing pigeons first used?\"))","language":"python"},{"_key":"04cb14e0e76a","_type":"codeBlock","code":"The context information provided in the given context is that homing pigeons were first used in the 19th century. However, prior knowledge would suggest that homing pigeons have been used for navigation and communication for centuries.","language":"text"},{"_key":"8abbf130c80e","_type":"block","children":[{"_key":"a864c86806850","_type":"span","marks":[],"text":"Conclusion"}],"markDefs":[],"style":"h2"},{"_key":"e84714836d85","_type":"block","children":[{"_key":"57500b86829c0","_type":"span","marks":[],"text":"In this post, we've shown how to download and set up an LLM running locally via llamafile. Then, we showed how to use this LLM with LlamaIndex to build a simple RAG-based research assistant for learning about homing pigeons. Your assistant ran 100% locally: you didn't have to pay for API calls or send data to a third party."}],"markDefs":[],"style":"normal"},{"_key":"2b0b5cc04e77","_type":"block","children":[{"_key":"99e45d454da50","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"fb68ee1a83ca","_type":"block","children":[{"_key":"5fd1f704124d0","_type":"span","marks":[],"text":"As a next step, you could try running the examples above with a better model like "},{"_key":"5fd1f704124d1","_type":"span","marks":["a5322915278d"],"text":"Mistral-7B-Instruct"},{"_key":"5fd1f704124d2","_type":"span","marks":[],"text":". You could also try building a research assistant for different topic like \"semiconductors\" or \"how to bake bread\"."}],"markDefs":[{"_key":"a5322915278d","_type":"link","href":"https://huggingface.co/Mozilla/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.Q4_0.llamafile?download=true"}],"style":"normal"},{"_key":"d77dcf0f28df","_type":"block","children":[{"_key":"729e7589adc10","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"30557b522f5d","_type":"block","children":[{"_key":"5429b8a0e96e0","_type":"span","marks":[],"text":"To find out more about llamafile, check out the project on "},{"_key":"5429b8a0e96e1","_type":"span","marks":["84ece9acc30f"],"text":"GitHub"},{"_key":"5429b8a0e96e2","_type":"span","marks":[],"text":", read this "},{"_key":"5429b8a0e96e3","_type":"span","marks":["bd42ce306f0d"],"text":"blog post"},{"_key":"5429b8a0e96e4","_type":"span","marks":[],"text":" on bash one-liners using LLMs, or say hi to the community on "},{"_key":"5429b8a0e96e5","_type":"span","marks":["d74f3490f4f7"],"text":"Discord"},{"_key":"5429b8a0e96e6","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"84ece9acc30f","_type":"link","href":"https://github.com/Mozilla-Ocho/llamafile"},{"_key":"bd42ce306f0d","_type":"link","href":"https://justine.lol/oneliners/"},{"_key":"d74f3490f4f7","_type":"link","href":"https://discord.com/invite/teDuGYVTB2"}],"style":"normal"}],"title":"Using LlamaIndex and llamafile to build a local, private research assistant"},"publishedDate":"Invalid Date"},"params":{"slug":"using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"using-llamaindex-and-llamafile-to-build-a-local-private-research-assistant"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>