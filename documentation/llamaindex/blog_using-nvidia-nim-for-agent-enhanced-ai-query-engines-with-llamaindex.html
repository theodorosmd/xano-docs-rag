<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Using NVIDIA NIM for Agent-Enhanced AI Query Engines with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Using NVIDIA NIM for Agent-Enhanced AI Query Engines with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Using NVIDIA NIM for Agent-Enhanced AI Query Engines with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/5131c90793639ba82a7980d4b2bd944685200871-896x1012.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Using NVIDIA NIM for Agent-Enhanced AI Query Engines with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/5131c90793639ba82a7980d4b2bd944685200871-896x1012.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="506" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5131c90793639ba82a7980d4b2bd944685200871-896x1012.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5131c90793639ba82a7980d4b2bd944685200871-896x1012.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F5131c90793639ba82a7980d4b2bd944685200871-896x1012.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/nvidia">NVIDIA</a> <!-- -->•<!-- --> <!-- -->2024-11-08</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Using NVIDIA NIM for Agent-Enhanced AI Query Engines with LlamaIndex</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/workflows"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Workflows</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/agents"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Agents</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Agents in generative AI applications provide powerful techniques for coordinating an AI system and augmenting it with additional capabilities. <a href="https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">NVIDIA NIM™</a>, a set of microservices for deploying AI models, supports agents using models that are trained for agentic behavior. NIM microservices integrate with many frameworks like <a href="https://docs.llamaindex.ai/en/stable/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">LlamaIndex</a> and <a href="https://python.langchain.com/docs/introduction/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">LangChain</a> to support using NIM for agentic applications. In this example, we’ll show you how to use them with <a href="https://docs.llamaindex.ai/en/stable/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">LlamaIndex</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">NVIDIA NIM for Agents</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">An agent is an entity that uses the capabilities of an LLM to perform a set of actions, or use tools, within a system state to achieve a desired outcome. They often involve reasoning, planning, executing, and responding to changes in the environment.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">NIM microservices provide a route to deploying production-ready, best-performance agents anywhere. They are part of <a href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">NVIDIA AI Enterprise</a>, a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI model inferencing across the cloud, data center and workstations with industry-standard APIs.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">NIM microservices are available to test for free from <a href="https://build.nvidia.com/explore/discover" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">NVIDIA’s API catalog</a> to build agents with LlamaIndex.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Why Use Agents?</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Agents are powered by LLMs to perform complex tasks using data and reasoning to inform their decision-making process. With agents, a goal is set by a human and an agent reasons and makes decisions, choosing what actions are needed to accomplish them. They excel in complex systems that may require splitting out subtasks to other models or tools.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let’s talk about agents in an example use case. Retail chatbots are used to assist customers in their product experience. A retail chatbot equipped with an agent may provide customers with more insight and value than a chatbot without an agent. For instance, an agent could recognize when to use a tool to search through customer reviews to provide information to a user query like, “What is the fit of this clothing?” While information about the fit of the clothing may not be available in descriptions of the product, it may be available in reviews. A chatbot without an agent would be restricted to just looking at the description, whereas an agent-equipped chatbot could reason that it should look at reviews to get the best answer to the customer’s question. With agents, systems like retail chatbots can make better decisions to assist shoppers.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Complex user queries can also be handled by agents; agents can break down these queries into smaller questions and route them to appropriate query engines. This is commonly known as “router agents” and results in more accurate responses. Suppose you are building a chatbot for 10-K reports (annual financial disclosures for public companies in the United States). We’ll take NVIDIA as an example. When a user asks, “How do NVIDIA’s earnings for the third quarter of 2023 compare to now?” An agent could be equipped to break down this query into subqueries like:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu">What were NVIDIA’s earnings?</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">What were NVIDIA’s most recent earnings?</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">What were NVIDIA’s earnings in the third quarter of 2023?</li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Then these sub-questions can be answered and, if applicable, the agent will use the retrieved answers to form a full response.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The agent also can be provided with a set of tools that can help answer some of the sub-questions. For example, there could be a set of tools, where each tool is responsible for answering questions about NVIDIA’s financials for a specific year. By leveraging tools in this way, it can be assured that the agent routes queries about a given quarter to the appropriate fiscal year’s information.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this blog, we’ll cover how to use LlamaIndex with NIM to build a query router for answering questions related to <a href="https://data.sfgov.org/City-Management-and-Ethics/Budget/xdgd-c79v" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">San Francisco city budget data</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Query Routing Agent with LlamaIndex</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The full code for this blog is in this notebook, but snippets are explained in this post as well. In this example, we’ll use an agent enhanced query engine with LlamaIndex. This query engine will first break down a complex query about San Francisco’s budget into subqueries. Because the subqueries are best answered by different source documents, the agent is equipped with tools to reference the correct source document. Once all the relevant queries have been answered, the agent will coalesce the information into a comprehensive response.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let’s get into the technical details of how this is done!</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Getting Started</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Two sets of NVIDIA NIM are used in this example:</p><ol><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://build.nvidia.com/nvidia/nv-embedqa-e5-v5" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">NV-EmbedQA-E5-v5 NIM microservice</a> is used to convert document chunks and the user query to embeddings with LlamaIndex.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://build.nvidia.com/meta/llama-3_1-8b-instruct" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Llama 3.1 8B NIM microservice</a> is used to power the agent with LlamaIndex.</li></ol><p class="Text_text__zPO0D Text_text-size-16__PkjFu">To get started quickly, access the NIM microservices as hosted APIs at <a href="https://build.nvidia.com/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">https://build.nvidia.com</a>. Or, if you’d like, you can host the NIM microservices yourself by downloading them through <a href="https://build.nvidia.com/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">https://build.nvidia.com</a> and following <a href="https://developer.nvidia.com/blog/a-simple-guide-to-deploying-generative-ai-with-nvidia-nim/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">documentation</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Using NIM Microservices with LlamaIndex</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">NIM microservices are easy and flexible to deploy as well as simple to use with LlamaIndex. Once they’re deployed, to use them with LlamaIndex, you will need to install the NVIDIA LlamaIndex packages.</p><pre><code>pip install llama-index-llms-nvidia llama-index-embeddings-nvidia</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Next, declare the NIM you will use. An API key is required if you are using NIM hosted by NVIDIA. It is assumed that your NVDIA API key is stored as an environment variable within the operating system. More documentation on using NIM with LlamaIndex is available <a href="https://docs.llamaindex.ai/en/stable/examples/llm/nvidia/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>.</p><pre><code><span class="hljs-keyword">from</span> llama_index.llms.nvidia <span class="hljs-keyword">import</span> NVIDIA
<span class="hljs-keyword">from</span> llama_index.embeddings.nvidia <span class="hljs-keyword">import</span> NVIDIAEmbedding

Settings.embed_model = NVIDIAEmbedding(model=<span class="hljs-string">&quot;nvidia/nv-embedqa-e5-v5&quot;</span>, api_key=os.environ[<span class="hljs-string">&quot;NVIDIA_API_KEY&quot;</span>],truncate=<span class="hljs-string">&quot;END&quot;</span>)

Settings.llm = NVIDIA(model=<span class="hljs-string">&quot;meta/llama-3.1-8b-instruct&quot;</span>, api_key=os.environ[<span class="hljs-string">&quot;NVIDIA_API_KEY&quot;</span>])</code></pre><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Sub-Question Query Engine</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">A <a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">query engine</a> is a concept in LlamaIndex that takes in natural language and returns a rich response. In this example, we create a sub-question query engine. It takes a single, complex question and breaks it into multiple sub-questions, each of which can be answered by a different tool.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This query engine is declared as a <a href="https://docs.llamaindex.ai/en/stable/module_guides/workflow/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Workflow</a>, which is another concept in LlamaIndex. Workflows are event driven and are used to chain together several events using steps. Workflows use the @step decorator on individual functions to define steps. Each step is responsible for handling certain event types and emitting new events. This decorator also infers input and output types for the step.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are 3 steps in this query engine that connect to one another through the event stream. Let’s break these down.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Step 1: Break down original query</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><code class="SanityPortableText_inlineCode__cI85z">query</code>: takes an original query and splits it into subquestions.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Step 2: Generate subquestions</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><code class="SanityPortableText_inlineCode__cI85z">sub_question</code>: for a given subquestion, generate a response from a ReAct agent equipped with tools.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">If you recall from earlier, the tools are useful for providing the agent with more unique ways to pull from data. ReAct (reasoning and acting) is a common agent implementation that uses prompting to guide an LLM to dynamically create, maintain, and adjust plans through explaining reasoning and actions. If you want to see what the prompt for this example looks like, you can view it in LlamaIndex’s source code <a href="https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/agent/react/templates/system_header_template.md" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>!</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">An answer for each individual subquestion is generated.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Step 2: Combine answers</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><code class="SanityPortableText_inlineCode__cI85z">combine_answers</code> : combine the individual answers into a full response.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Check out the sub-question query engine code below!</p><pre><code><span class="hljs-keyword">class</span> <span class="hljs-title class_">QueryEvent</span>(<span class="hljs-title class_ inherited__">Event</span>):
    question: <span class="hljs-built_in">str</span>


<span class="hljs-keyword">class</span> <span class="hljs-title class_">AnswerEvent</span>(<span class="hljs-title class_ inherited__">Event</span>):
    question: <span class="hljs-built_in">str</span>
    answer: <span class="hljs-built_in">str</span>


<span class="hljs-keyword">class</span> <span class="hljs-title class_">SubQuestionQueryEngine</span>(<span class="hljs-title class_ inherited__">Workflow</span>):
<span class="hljs-meta">    @step</span>
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">self, ctx: Context, ev: StartEvent</span>) -&gt; QueryEvent:
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(ev, <span class="hljs-string">&quot;query&quot;</span>):
            <span class="hljs-keyword">await</span> ctx.<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;original_query&quot;</span>, ev.query)
            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Query is <span class="hljs-subst">{<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&#x27;original_query&#x27;</span>)}</span>&quot;</span>)

        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(ev, <span class="hljs-string">&quot;llm&quot;</span>):
            <span class="hljs-keyword">await</span> ctx.<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;llm&quot;</span>, ev.llm)

        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(ev, <span class="hljs-string">&quot;tools&quot;</span>):
            <span class="hljs-keyword">await</span> ctx.<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;tools&quot;</span>, ev.tools)

        response = (<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;llm&quot;</span>)).complete(
            <span class="hljs-string">f&quot;&quot;&quot;
            Given a user question, and a list of tools, output a list of
            relevant sub-questions, such that the answers to all the
            sub-questions put together will answer the question. Respond
            in pure JSON without any markdown, like this:
            {{
                &quot;sub_questions&quot;: [
                    &quot;What is the population of San Francisco?&quot;,
                    &quot;What is the budget of San Francisco?&quot;,
                    &quot;What is the GDP of San Francisco?&quot;
                ]
            }}
            Here is the user question: <span class="hljs-subst">{<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&#x27;original_query&#x27;</span>)}</span>

            And here is the list of tools: <span class="hljs-subst">{<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&#x27;tools&#x27;</span>)}</span>
            &quot;&quot;&quot;</span>
        )

        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Sub-questions are <span class="hljs-subst">{response}</span>&quot;</span>)

        response_obj = json.loads(<span class="hljs-built_in">str</span>(response))
        sub_questions = response_obj[<span class="hljs-string">&quot;sub_questions&quot;</span>]

        <span class="hljs-keyword">await</span> ctx.<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;sub_question_count&quot;</span>, <span class="hljs-built_in">len</span>(sub_questions))

        <span class="hljs-keyword">for</span> question <span class="hljs-keyword">in</span> sub_questions:
            self.send_event(QueryEvent(question=question))

        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>

<span class="hljs-meta">    @step</span>
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">sub_question</span>(<span class="hljs-params">self, ctx: Context, ev: QueryEvent</span>) -&gt; AnswerEvent:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Sub-question is <span class="hljs-subst">{ev.question}</span>&quot;</span>)

        agent = ReActAgent.from_tools(
            <span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;tools&quot;</span>), llm=<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;llm&quot;</span>), verbose=<span class="hljs-literal">True</span>
        )
        response = agent.chat(ev.question)

        <span class="hljs-keyword">return</span> AnswerEvent(question=ev.question, answer=<span class="hljs-built_in">str</span>(response))

<span class="hljs-meta">    @step</span>
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">combine_answers</span>(<span class="hljs-params">
        self, ctx: Context, ev: AnswerEvent
    </span>) -&gt; StopEvent | <span class="hljs-literal">None</span>:
        ready = ctx.collect_events(
            ev, [AnswerEvent] * <span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;sub_question_count&quot;</span>)
        )
        <span class="hljs-keyword">if</span> ready <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>

        answers = <span class="hljs-string">&quot;\n\n&quot;</span>.join(
            [
                <span class="hljs-string">f&quot;Question: <span class="hljs-subst">{event.question}</span>: \n Answer: <span class="hljs-subst">{event.answer}</span>&quot;</span>
                <span class="hljs-keyword">for</span> event <span class="hljs-keyword">in</span> ready
            ]
        )

        prompt = <span class="hljs-string">f&quot;&quot;&quot;
            You are given an overall question that has been split into sub-questions,
            each of which has been answered. Combine the answers to all the sub-questions
            into a single answer to the original question.

            Original question: <span class="hljs-subst">{<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&#x27;original_query&#x27;</span>)}</span>

            Sub-questions and answers:
            <span class="hljs-subst">{answers}</span>
        &quot;&quot;&quot;</span>

        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Final prompt is <span class="hljs-subst">{prompt}</span>&quot;</span>)

        response = (<span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;llm&quot;</span>)).complete(prompt)

        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Final response is&quot;</span>, response)

        <span class="hljs-keyword">return</span> StopEvent(result=<span class="hljs-built_in">str</span>(response))</code></pre><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Run the Agent Enhanced Subquestion Query Engine</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We’ve skipped over creating the query engine tools, but this is available in the <a href="https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/nvidia_sub_question_query_engine.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">full notebook</a> for reference. In short, each tool is an individual query engine based on a singular, but lengthy (300+ pages) SF budget document.</p><pre><code>engine = SubQuestionQueryEngine(timeout=<span class="hljs-number">120</span>, verbose=<span class="hljs-literal">True</span>)
result = <span class="hljs-keyword">await</span> engine.run(
    llm=Settings.llm,
    tools=query_engine_tools,
    query=<span class="hljs-string">&quot;How has the total amount of San Francisco&#x27;s budget changed from 2016 to 2023?&quot;</span>,
)

<span class="hljs-built_in">print</span>(result)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In the results, we can see that sub questions are generated and the ReAct pattern is used across the sub questions. The full output is truncated for readability, but run the notebook to view the full answer!</p><pre><code>Sub-question is What is the budget of San Francisco in 2016?
&gt; Running step 543e99b5-0b95-40a1-969c-f2ccecbcf405. Step input: What is the budget of San Francisco in 2016?
Thought: The current language of the user is: English. I need to use a tool to help me answer the question.
Action: budget_2016
Action Input: {&#x27;input&#x27;: &#x27;What is the budget of San Francisco in 2016?&#x27;}
Observation: According to the provided information, the budget of San Francisco in 2016-17 is $51,569,787.
&gt; Running step fc16dc8e-de61-4221-9cd9-0e2831a20067. Step input: None
Thought: I can answer without using any more tools. I&#x27;ll use the user&#x27;s language to answer
Answer: The budget of San Francisco in 2016 is $51,569,787.
Step sub_question produced event AnswerEvent
Running step sub_question
Sub-question is What is the budget of San Francisco in 2023?
&gt; Running step 6eca7b14-7748-4daf-a186-8338995605ef. Step input: What is the budget of San Francisco in 2023?
Observation: Error: Could not parse output. Please follow the thought-action-input format. Try again.
&gt; Running step d8fed16d-6654-4b15-bc4d-3749e0d600de. Step input: None
Thought: The current language of the user is: English. I need to use a tool to help me answer the question.
Action: budget_2023
Action Input: {&#x27;input&#x27;: &#x27;What is the budget of San Francisco in 2023?&#x27;}
Observation: The budget of San Francisco in 2023 is $14.6 billion.
&gt; Running step 90f90257-aca3-4458-a7f6-f47ea3e10b48. Step input: None
Thought: I can answer without using any more tools. I&#x27;ll use the user&#x27;s language to answer
Answer: The budget of San Francisco in 2023 is $14.6 billion.
...</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">You can see the final response below.</p><pre><code>The budget of San Francisco in 2016 was $51,569,787, and the budget in 2023 is $14.6 billion. Therefore, the total amount of San Francisco&#x27;s budget has increased significantly from 2016 to 2023, with a change of approximately $14.6 billion - $51,569,787 = $14,548,213,213.</code></pre><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Next Steps</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Check out the <a href="https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/nvidia_sub_question_query_engine.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Jupyter notebook</a> with the full code for LlamaIndex and NVIDIA NIM. You can ask multi-step queries about SF housing data or you can adapt it to work on your own information.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Head to <a href="http://build.nvidia.com" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">build.nvidia.com</a> to get started with NIM microservices in the notebook!</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F89380b5946452462d6c4a91f8109f705dc044c82-1080x1080.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/introducing-the-spreadsheet-agent-in-private-preview">Introducing the Spreadsheet Agent, in private preview</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-06-05</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/rag-is-dead-long-live-agentic-retrieval">RAG is dead, long live agentic retrieval</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-29</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fadd5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/improved-long-and-short-term-memory-for-llamaindex-agents">Improved Long &amp; Short-Term Memory for LlamaIndex Agents</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-05-13</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/bending-without-breaking-optimal-design-patterns-for-effective-agents">Bending without breaking: optimal design patterns for effective agents</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2025-04-25</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-11-08T14:50:52Z","_id":"7fbc47df-25a7-43f7-aab4-d4fd6c7a8729","_rev":"Ys5IzmCaJ2UnW2RAX7UM9H","_type":"blogPost","_updatedAt":"2025-05-21T20:38:24Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-10-24T22:30:24Z","_id":"3fd026ed-4437-4fc2-8b36-b01f89819fa3","_rev":"J9rQi3NETB3aSuoLNTF3k2","_type":"people","_updatedAt":"2024-10-24T22:30:30Z","name":"NVIDIA","slug":{"_type":"slug","current":"nvidia"}}],"featured":false,"image":{"_type":"image","asset":{"_ref":"image-5131c90793639ba82a7980d4b2bd944685200871-896x1012-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/5131c90793639ba82a7980d4b2bd944685200871-896x1012.png","publishedDate":"2024-11-08","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-89380b5946452462d6c4a91f8109f705dc044c82-1080x1080-png","_type":"reference"}},"publishedDate":"2025-06-05","slug":"introducing-the-spreadsheet-agent-in-private-preview","title":"Introducing the Spreadsheet Agent, in private preview"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-9fdb15bafdf8c0921f36c6cd8cdac43c8ca87e27-2232x1562-png","_type":"reference"}},"publishedDate":"2025-05-29","slug":"rag-is-dead-long-live-agentic-retrieval","title":"RAG is dead, long live agentic retrieval"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-add5d90d8aa67ca128a834e2b8fcbb7746c585cd-734x379-png","_type":"reference"}},"publishedDate":"2025-05-13","slug":"improved-long-and-short-term-memory-for-llamaindex-agents","title":"Improved Long \u0026 Short-Term Memory for LlamaIndex Agents"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-dde8a9c277605647a344c6f4cd83d1ad192e1a09-1024x1536-png","_type":"reference"}},"publishedDate":"2025-04-25","slug":"bending-without-breaking-optimal-design-patterns-for-effective-agents","title":"Bending without breaking: optimal design patterns for effective agents"}],"slug":{"_type":"slug","current":"using-nvidia-nim-for-agent-enhanced-ai-query-engines-with-llamaindex"},"tags":[{"_createdAt":"2024-08-01T16:07:34Z","_id":"fb42e2ed-a8a6-4957-a542-104903d0a766","_rev":"M6ETZdzcY6usphylMIwkdv","_type":"blogTag","_updatedAt":"2024-08-01T16:07:34Z","slug":{"_type":"slug","current":"workflows"},"title":"Workflows"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"248fed26-a405-4fa0-ba99-8f1af0df185c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"agents"},"title":"Agents"}],"text":[{"_key":"c05a92d447dc","_type":"block","children":[{"_key":"03ac2d8cbc3d0","_type":"span","marks":[],"text":"Agents in generative AI applications provide powerful techniques for coordinating an AI system and augmenting it with additional capabilities. "},{"_key":"03ac2d8cbc3d1","_type":"span","marks":["5eef82c21ae8"],"text":"NVIDIA NIM™"},{"_key":"03ac2d8cbc3d2","_type":"span","marks":[],"text":", a set of microservices for deploying AI models, supports agents using models that are trained for agentic behavior. NIM microservices integrate with many frameworks like "},{"_key":"03ac2d8cbc3d3","_type":"span","marks":["ac3bc9092b0c"],"text":"LlamaIndex"},{"_key":"03ac2d8cbc3d4","_type":"span","marks":[],"text":" and "},{"_key":"03ac2d8cbc3d5","_type":"span","marks":["0ba1515f4202"],"text":"LangChain"},{"_key":"03ac2d8cbc3d6","_type":"span","marks":[],"text":" to support using NIM for agentic applications. In this example, we’ll show you how to use them with "},{"_key":"03ac2d8cbc3d7","_type":"span","marks":["c28a9a83dfd5"],"text":"LlamaIndex"},{"_key":"03ac2d8cbc3d8","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"5eef82c21ae8","_type":"link","href":"https://www.nvidia.com/en-us/ai/#referrer=ai-subdomain"},{"_key":"ac3bc9092b0c","_type":"link","href":"https://docs.llamaindex.ai/en/stable/"},{"_key":"0ba1515f4202","_type":"link","href":"https://python.langchain.com/docs/introduction/"},{"_key":"c28a9a83dfd5","_type":"link","href":"https://docs.llamaindex.ai/en/stable/"}],"style":"normal"},{"_key":"5b728d52a947","_type":"block","children":[{"_key":"8583e13028f00","_type":"span","marks":[],"text":"NVIDIA NIM for Agents"}],"markDefs":[],"style":"h2"},{"_key":"3f7ca4c913d3","_type":"block","children":[{"_key":"0a8ba31f0f990","_type":"span","marks":[],"text":"An agent is an entity that uses the capabilities of an LLM to perform a set of actions, or use tools, within a system state to achieve a desired outcome. They often involve reasoning, planning, executing, and responding to changes in the environment."}],"markDefs":[],"style":"normal"},{"_key":"c4447fa510e2","_type":"block","children":[{"_key":"1d6d5b2949f60","_type":"span","marks":[],"text":"NIM microservices provide a route to deploying production-ready, best-performance agents anywhere. They are part of "},{"_key":"1d6d5b2949f61","_type":"span","marks":["bb3951e033f4"],"text":"NVIDIA AI Enterprise"},{"_key":"1d6d5b2949f62","_type":"span","marks":[],"text":", a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI model inferencing across the cloud, data center and workstations with industry-standard APIs."}],"markDefs":[{"_key":"bb3951e033f4","_type":"link","href":"https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/"}],"style":"normal"},{"_key":"964b4b5339e5","_type":"block","children":[{"_key":"12786d4ec57b0","_type":"span","marks":[],"text":"NIM microservices are available to test for free from "},{"_key":"12786d4ec57b1","_type":"span","marks":["14e93aa2e64c"],"text":"NVIDIA’s API catalog"},{"_key":"12786d4ec57b2","_type":"span","marks":[],"text":" to build agents with LlamaIndex."}],"markDefs":[{"_key":"14e93aa2e64c","_type":"link","href":"https://build.nvidia.com/explore/discover"}],"style":"normal"},{"_key":"79e4ec9bfa62","_type":"block","children":[{"_key":"3e80560778690","_type":"span","marks":[],"text":"Why Use Agents?"}],"markDefs":[],"style":"h2"},{"_key":"323323cd0dbf","_type":"block","children":[{"_key":"d2fe65a33b650","_type":"span","marks":[],"text":"Agents are powered by LLMs to perform complex tasks using data and reasoning to inform their decision-making process. With agents, a goal is set by a human and an agent reasons and makes decisions, choosing what actions are needed to accomplish them. They excel in complex systems that may require splitting out subtasks to other models or tools."}],"markDefs":[],"style":"normal"},{"_key":"051a5dcb4c90","_type":"block","children":[{"_key":"a78a902648660","_type":"span","marks":[],"text":"Let’s talk about agents in an example use case. Retail chatbots are used to assist customers in their product experience. A retail chatbot equipped with an agent may provide customers with more insight and value than a chatbot without an agent. For instance, an agent could recognize when to use a tool to search through customer reviews to provide information to a user query like, “What is the fit of this clothing?” While information about the fit of the clothing may not be available in descriptions of the product, it may be available in reviews. A chatbot without an agent would be restricted to just looking at the description, whereas an agent-equipped chatbot could reason that it should look at reviews to get the best answer to the customer’s question. With agents, systems like retail chatbots can make better decisions to assist shoppers."}],"markDefs":[],"style":"normal"},{"_key":"867efa334f7f","_type":"block","children":[{"_key":"be7f6ac948ab0","_type":"span","marks":[],"text":"Complex user queries can also be handled by agents; agents can break down these queries into smaller questions and route them to appropriate query engines. This is commonly known as “router agents” and results in more accurate responses. Suppose you are building a chatbot for 10-K reports (annual financial disclosures for public companies in the United States). We’ll take NVIDIA as an example. When a user asks, “How do NVIDIA’s earnings for the third quarter of 2023 compare to now?” An agent could be equipped to break down this query into subqueries like:"}],"markDefs":[],"style":"normal"},{"_key":"cbc882b6eff3","_type":"block","children":[{"_key":"e1531b23c37c0","_type":"span","marks":[],"text":"What were NVIDIA’s earnings?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"8eabda5733c3","_type":"block","children":[{"_key":"3482b6c350980","_type":"span","marks":[],"text":"What were NVIDIA’s most recent earnings?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"b6121380f199","_type":"block","children":[{"_key":"a5a7dae2c9020","_type":"span","marks":[],"text":"What were NVIDIA’s earnings in the third quarter of 2023?"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"3c346d46ab9a","_type":"block","children":[{"_key":"5e8979df25d90","_type":"span","marks":[],"text":"Then these sub-questions can be answered and, if applicable, the agent will use the retrieved answers to form a full response."}],"markDefs":[],"style":"normal"},{"_key":"5fb5f480882d","_type":"block","children":[{"_key":"aa4bce242bd10","_type":"span","marks":[],"text":"The agent also can be provided with a set of tools that can help answer some of the sub-questions. For example, there could be a set of tools, where each tool is responsible for answering questions about NVIDIA’s financials for a specific year. By leveraging tools in this way, it can be assured that the agent routes queries about a given quarter to the appropriate fiscal year’s information."}],"markDefs":[],"style":"normal"},{"_key":"a34e219ee4f8","_type":"block","children":[{"_key":"bb6d889b9c170","_type":"span","marks":[],"text":"In this blog, we’ll cover how to use LlamaIndex with NIM to build a query router for answering questions related to "},{"_key":"bb6d889b9c171","_type":"span","marks":["01618cd12c1c"],"text":"San Francisco city budget data"},{"_key":"bb6d889b9c172","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"01618cd12c1c","_type":"link","href":"https://data.sfgov.org/City-Management-and-Ethics/Budget/xdgd-c79v"}],"style":"normal"},{"_key":"2084ef8cb797","_type":"block","children":[{"_key":"7b0d3604497a0","_type":"span","marks":[],"text":"Query Routing Agent with LlamaIndex"}],"markDefs":[],"style":"h2"},{"_key":"6692dca03666","_type":"block","children":[{"_key":"0e2b001ab5430","_type":"span","marks":[],"text":"The full code for this blog is in this notebook, but snippets are explained in this post as well. In this example, we’ll use an agent enhanced query engine with LlamaIndex. This query engine will first break down a complex query about San Francisco’s budget into subqueries. Because the subqueries are best answered by different source documents, the agent is equipped with tools to reference the correct source document. Once all the relevant queries have been answered, the agent will coalesce the information into a comprehensive response."}],"markDefs":[],"style":"normal"},{"_key":"9a607aa59ce5","_type":"block","children":[{"_key":"d064553a52020","_type":"span","marks":[],"text":"Let’s get into the technical details of how this is done!"}],"markDefs":[],"style":"normal"},{"_key":"e51124afb825","_type":"block","children":[{"_key":"8e9c4a2f37e90","_type":"span","marks":[],"text":"Getting Started"}],"markDefs":[],"style":"h2"},{"_key":"8fa9be89bf3d","_type":"block","children":[{"_key":"7e8198ff18560","_type":"span","marks":[],"text":"Two sets of NVIDIA NIM are used in this example:"}],"markDefs":[],"style":"normal"},{"_key":"1371ad63821b","_type":"block","children":[{"_key":"1a047b21ae910","_type":"span","marks":["11ce59b3bcf2"],"text":"NV-EmbedQA-E5-v5 NIM microservice"},{"_key":"1a047b21ae911","_type":"span","marks":[],"text":" is used to convert document chunks and the user query to embeddings with LlamaIndex."}],"level":1,"listItem":"number","markDefs":[{"_key":"11ce59b3bcf2","_type":"link","href":"https://build.nvidia.com/nvidia/nv-embedqa-e5-v5"}],"style":"normal"},{"_key":"025b96484e96","_type":"block","children":[{"_key":"352dff51cbdf0","_type":"span","marks":["5968687fdab3"],"text":"Llama 3.1 8B NIM microservice"},{"_key":"352dff51cbdf1","_type":"span","marks":[],"text":" is used to power the agent with LlamaIndex."}],"level":1,"listItem":"number","markDefs":[{"_key":"5968687fdab3","_type":"link","href":"https://build.nvidia.com/meta/llama-3_1-8b-instruct"}],"style":"normal"},{"_key":"84c6851c3ee0","_type":"block","children":[{"_key":"6ed4bdf0cc310","_type":"span","marks":[],"text":"To get started quickly, access the NIM microservices as hosted APIs at "},{"_key":"6ed4bdf0cc311","_type":"span","marks":["80e57797541a"],"text":"https://build.nvidia.com"},{"_key":"6ed4bdf0cc312","_type":"span","marks":[],"text":". Or, if you’d like, you can host the NIM microservices yourself by downloading them through "},{"_key":"6ed4bdf0cc313","_type":"span","marks":["6b92b4a6bd1a"],"text":"https://build.nvidia.com"},{"_key":"6ed4bdf0cc314","_type":"span","marks":[],"text":" and following "},{"_key":"6ed4bdf0cc315","_type":"span","marks":["0c7d49486dd5"],"text":"documentation"},{"_key":"6ed4bdf0cc316","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"80e57797541a","_type":"link","href":"https://build.nvidia.com/"},{"_key":"6b92b4a6bd1a","_type":"link","href":"https://build.nvidia.com/"},{"_key":"0c7d49486dd5","_type":"link","href":"https://developer.nvidia.com/blog/a-simple-guide-to-deploying-generative-ai-with-nvidia-nim/"}],"style":"normal"},{"_key":"75a9b0257389","_type":"block","children":[{"_key":"7f65984341340","_type":"span","marks":[],"text":"Using NIM Microservices with LlamaIndex"}],"markDefs":[],"style":"h2"},{"_key":"a3ef2892f826","_type":"block","children":[{"_key":"1677038c364e0","_type":"span","marks":[],"text":"NIM microservices are easy and flexible to deploy as well as simple to use with LlamaIndex. Once they’re deployed, to use them with LlamaIndex, you will need to install the NVIDIA LlamaIndex packages."}],"markDefs":[],"style":"normal"},{"_key":"b133faafd464","_type":"codeBlock","code":"pip install llama-index-llms-nvidia llama-index-embeddings-nvidia","language":"sh"},{"_key":"18ed23755860","_type":"block","children":[{"_key":"defc99ed7c440","_type":"span","marks":[],"text":"Next, declare the NIM you will use. An API key is required if you are using NIM hosted by NVIDIA. It is assumed that your NVDIA API key is stored as an environment variable within the operating system. More documentation on using NIM with LlamaIndex is available "},{"_key":"defc99ed7c441","_type":"span","marks":["8d06bd44325a"],"text":"here"},{"_key":"defc99ed7c442","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"8d06bd44325a","_type":"link","href":"https://docs.llamaindex.ai/en/stable/examples/llm/nvidia/"}],"style":"normal"},{"_key":"d8cd2bf0a112","_type":"codeBlock","code":"from llama_index.llms.nvidia import NVIDIA\nfrom llama_index.embeddings.nvidia import NVIDIAEmbedding\n\nSettings.embed_model = NVIDIAEmbedding(model=\"nvidia/nv-embedqa-e5-v5\", api_key=os.environ[\"NVIDIA_API_KEY\"],truncate=\"END\")\n\nSettings.llm = NVIDIA(model=\"meta/llama-3.1-8b-instruct\", api_key=os.environ[\"NVIDIA_API_KEY\"])","language":"python"},{"_key":"7f8711e23fe2","_type":"block","children":[{"_key":"533ca7da5af20","_type":"span","marks":[],"text":"Sub-Question Query Engine"}],"markDefs":[],"style":"h2"},{"_key":"49507542a3fb","_type":"block","children":[{"_key":"98062fd4bb9e0","_type":"span","marks":[],"text":"A "},{"_key":"98062fd4bb9e1","_type":"span","marks":["d9025feb1152"],"text":"query engine"},{"_key":"98062fd4bb9e2","_type":"span","marks":[],"text":" is a concept in LlamaIndex that takes in natural language and returns a rich response. In this example, we create a sub-question query engine. It takes a single, complex question and breaks it into multiple sub-questions, each of which can be answered by a different tool."}],"markDefs":[{"_key":"d9025feb1152","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/"}],"style":"normal"},{"_key":"a6aa418f588c","_type":"block","children":[{"_key":"dd8b0d235fb00","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"368093717b04","_type":"block","children":[{"_key":"ec13ecbd6ce30","_type":"span","marks":[],"text":"This query engine is declared as a "},{"_key":"ec13ecbd6ce31","_type":"span","marks":["3e52eb5a2e07"],"text":"Workflow"},{"_key":"ec13ecbd6ce32","_type":"span","marks":[],"text":", which is another concept in LlamaIndex. Workflows are event driven and are used to chain together several events using steps. Workflows use the @step decorator on individual functions to define steps. Each step is responsible for handling certain event types and emitting new events. This decorator also infers input and output types for the step."}],"markDefs":[{"_key":"3e52eb5a2e07","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/workflow/"}],"style":"normal"},{"_key":"be54e2ac8e83","_type":"block","children":[{"_key":"93e6f5f1c7fc0","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"e54aa58a6955","_type":"block","children":[{"_key":"ca050aa644cb0","_type":"span","marks":[],"text":"There are 3 steps in this query engine that connect to one another through the event stream. Let’s break these down."}],"markDefs":[],"style":"normal"},{"_key":"fce08a1766c0","_type":"block","children":[{"_key":"3cd08c7edbea0","_type":"span","marks":[],"text":"Step 1: Break down original query"}],"markDefs":[],"style":"h3"},{"_key":"77241700d1fd","_type":"block","children":[{"_key":"c1c5475a40080","_type":"span","marks":["code"],"text":"query"},{"_key":"5e1e800ed8c2","_type":"span","marks":[],"text":": takes an original query and splits it into subquestions."}],"markDefs":[],"style":"normal"},{"_key":"cd9e0a814e6c","_type":"block","children":[{"_key":"5811d18225850","_type":"span","marks":[],"text":"Step 2: Generate subquestions"}],"markDefs":[],"style":"h3"},{"_key":"fefe66a60124","_type":"block","children":[{"_key":"41cdff6170fa0","_type":"span","marks":["code"],"text":"sub_question"},{"_key":"de0950356a5e","_type":"span","marks":[],"text":": for a given subquestion, generate a response from a ReAct agent equipped with tools."}],"markDefs":[],"style":"normal"},{"_key":"cbf4c49c749f","_type":"block","children":[{"_key":"79b712dc3b5f0","_type":"span","marks":[],"text":"If you recall from earlier, the tools are useful for providing the agent with more unique ways to pull from data. ReAct (reasoning and acting) is a common agent implementation that uses prompting to guide an LLM to dynamically create, maintain, and adjust plans through explaining reasoning and actions. If you want to see what the prompt for this example looks like, you can view it in LlamaIndex’s source code "},{"_key":"79b712dc3b5f1","_type":"span","marks":["cb5f93ef3638"],"text":"here"},{"_key":"79b712dc3b5f2","_type":"span","marks":[],"text":"!"}],"markDefs":[{"_key":"cb5f93ef3638","_type":"link","href":"https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/agent/react/templates/system_header_template.md"}],"style":"normal"},{"_key":"2b69a9ea86e2","_type":"block","children":[{"_key":"1bcc9fc4f9330","_type":"span","marks":[],"text":"An answer for each individual subquestion is generated."}],"markDefs":[],"style":"normal"},{"_key":"7c08f8ef75a6","_type":"block","children":[{"_key":"ffee0ce235060","_type":"span","marks":[],"text":"Step 2: Combine answers"}],"markDefs":[],"style":"h3"},{"_key":"a8ec0939fc06","_type":"block","children":[{"_key":"de9bf3c33f140","_type":"span","marks":["code"],"text":"combine_answers"},{"_key":"261082b320ad","_type":"span","marks":[],"text":" : combine the individual answers into a full response."}],"markDefs":[],"style":"normal"},{"_key":"07ef22eca3fc","_type":"block","children":[{"_key":"383de12f65840","_type":"span","marks":[],"text":"Check out the sub-question query engine code below!"}],"markDefs":[],"style":"normal"},{"_key":"3d5959bcbeef","_type":"codeBlock","code":"class QueryEvent(Event):\n    question: str\n\n\nclass AnswerEvent(Event):\n    question: str\n    answer: str\n\n\nclass SubQuestionQueryEngine(Workflow):\n    @step\n    async def query(self, ctx: Context, ev: StartEvent) -\u003e QueryEvent:\n        if hasattr(ev, \"query\"):\n            await ctx.set(\"original_query\", ev.query)\n            print(f\"Query is {await ctx.get('original_query')}\")\n\n        if hasattr(ev, \"llm\"):\n            await ctx.set(\"llm\", ev.llm)\n\n        if hasattr(ev, \"tools\"):\n            await ctx.set(\"tools\", ev.tools)\n\n        response = (await ctx.get(\"llm\")).complete(\n            f\"\"\"\n            Given a user question, and a list of tools, output a list of\n            relevant sub-questions, such that the answers to all the\n            sub-questions put together will answer the question. Respond\n            in pure JSON without any markdown, like this:\n            {{\n                \"sub_questions\": [\n                    \"What is the population of San Francisco?\",\n                    \"What is the budget of San Francisco?\",\n                    \"What is the GDP of San Francisco?\"\n                ]\n            }}\n            Here is the user question: {await ctx.get('original_query')}\n\n            And here is the list of tools: {await ctx.get('tools')}\n            \"\"\"\n        )\n\n        print(f\"Sub-questions are {response}\")\n\n        response_obj = json.loads(str(response))\n        sub_questions = response_obj[\"sub_questions\"]\n\n        await ctx.set(\"sub_question_count\", len(sub_questions))\n\n        for question in sub_questions:\n            self.send_event(QueryEvent(question=question))\n\n        return None\n\n    @step\n    async def sub_question(self, ctx: Context, ev: QueryEvent) -\u003e AnswerEvent:\n        print(f\"Sub-question is {ev.question}\")\n\n        agent = ReActAgent.from_tools(\n            await ctx.get(\"tools\"), llm=await ctx.get(\"llm\"), verbose=True\n        )\n        response = agent.chat(ev.question)\n\n        return AnswerEvent(question=ev.question, answer=str(response))\n\n    @step\n    async def combine_answers(\n        self, ctx: Context, ev: AnswerEvent\n    ) -\u003e StopEvent | None:\n        ready = ctx.collect_events(\n            ev, [AnswerEvent] * await ctx.get(\"sub_question_count\")\n        )\n        if ready is None:\n            return None\n\n        answers = \"\\n\\n\".join(\n            [\n                f\"Question: {event.question}: \\n Answer: {event.answer}\"\n                for event in ready\n            ]\n        )\n\n        prompt = f\"\"\"\n            You are given an overall question that has been split into sub-questions,\n            each of which has been answered. Combine the answers to all the sub-questions\n            into a single answer to the original question.\n\n            Original question: {await ctx.get('original_query')}\n\n            Sub-questions and answers:\n            {answers}\n        \"\"\"\n\n        print(f\"Final prompt is {prompt}\")\n\n        response = (await ctx.get(\"llm\")).complete(prompt)\n\n        print(\"Final response is\", response)\n\n        return StopEvent(result=str(response))","language":"python"},{"_key":"a415c34ef771","_type":"block","children":[{"_key":"8e29deae1d990","_type":"span","marks":[],"text":"Run the Agent Enhanced Subquestion Query Engine"}],"markDefs":[],"style":"h2"},{"_key":"9f0719b5b1ea","_type":"block","children":[{"_key":"a152609a86320","_type":"span","marks":[],"text":"We’ve skipped over creating the query engine tools, but this is available in the "},{"_key":"a152609a86321","_type":"span","marks":["c306749b84f2"],"text":"full notebook"},{"_key":"a152609a86322","_type":"span","marks":[],"text":" for reference. In short, each tool is an individual query engine based on a singular, but lengthy (300+ pages) SF budget document."}],"markDefs":[{"_key":"c306749b84f2","_type":"link","href":"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/nvidia_sub_question_query_engine.ipynb"}],"style":"normal"},{"_key":"3ab5bc7382b6","_type":"codeBlock","code":"engine = SubQuestionQueryEngine(timeout=120, verbose=True)\nresult = await engine.run(\n    llm=Settings.llm,\n    tools=query_engine_tools,\n    query=\"How has the total amount of San Francisco's budget changed from 2016 to 2023?\",\n)\n\nprint(result)","language":"python"},{"_key":"bfbe1bdc4d2f","_type":"block","children":[{"_key":"0e0f2d1d0f650","_type":"span","marks":[],"text":"In the results, we can see that sub questions are generated and the ReAct pattern is used across the sub questions. The full output is truncated for readability, but run the notebook to view the full answer!"}],"markDefs":[],"style":"normal"},{"_key":"b1e6b517a340","_type":"codeBlock","code":"Sub-question is What is the budget of San Francisco in 2016?\n\u003e Running step 543e99b5-0b95-40a1-969c-f2ccecbcf405. Step input: What is the budget of San Francisco in 2016?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: budget_2016\nAction Input: {'input': 'What is the budget of San Francisco in 2016?'}\nObservation: According to the provided information, the budget of San Francisco in 2016-17 is $51,569,787.\n\u003e Running step fc16dc8e-de61-4221-9cd9-0e2831a20067. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The budget of San Francisco in 2016 is $51,569,787.\nStep sub_question produced event AnswerEvent\nRunning step sub_question\nSub-question is What is the budget of San Francisco in 2023?\n\u003e Running step 6eca7b14-7748-4daf-a186-8338995605ef. Step input: What is the budget of San Francisco in 2023?\nObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n\u003e Running step d8fed16d-6654-4b15-bc4d-3749e0d600de. Step input: None\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: budget_2023\nAction Input: {'input': 'What is the budget of San Francisco in 2023?'}\nObservation: The budget of San Francisco in 2023 is $14.6 billion.\n\u003e Running step 90f90257-aca3-4458-a7f6-f47ea3e10b48. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The budget of San Francisco in 2023 is $14.6 billion.\n...","language":"text"},{"_key":"5adcd8b249e1","_type":"block","children":[{"_key":"71b9143fe98b0","_type":"span","marks":[],"text":"You can see the final response below."}],"markDefs":[],"style":"normal"},{"_key":"5f0fb0bc7011","_type":"codeBlock","code":"The budget of San Francisco in 2016 was $51,569,787, and the budget in 2023 is $14.6 billion. Therefore, the total amount of San Francisco's budget has increased significantly from 2016 to 2023, with a change of approximately $14.6 billion - $51,569,787 = $14,548,213,213.","language":"text"},{"_key":"1d53e3f8d4d7","_type":"block","children":[{"_key":"b74bac2b3e850","_type":"span","marks":[],"text":"Next Steps"}],"markDefs":[],"style":"h2"},{"_key":"84a3e1b2dbc6","_type":"block","children":[{"_key":"c8e90dc218980","_type":"span","marks":[],"text":"Check out the "},{"_key":"c8e90dc218981","_type":"span","marks":["b58e94ff14ca"],"text":"Jupyter notebook"},{"_key":"c8e90dc218982","_type":"span","marks":[],"text":" with the full code for LlamaIndex and NVIDIA NIM. You can ask multi-step queries about SF housing data or you can adapt it to work on your own information."}],"markDefs":[{"_key":"b58e94ff14ca","_type":"link","href":"https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/agent/nvidia_sub_question_query_engine.ipynb"}],"style":"normal"},{"_key":"de2b7abb28eb","_type":"block","children":[{"_key":"7b3b27c944590","_type":"span","marks":[],"text":"Head to "},{"_key":"7b3b27c944591","_type":"span","marks":["3f49e9e0d61b"],"text":"build.nvidia.com"},{"_key":"7b3b27c944592","_type":"span","marks":[],"text":" to get started with NIM microservices in the notebook!"}],"markDefs":[{"_key":"3f49e9e0d61b","_type":"link","href":"http://build.nvidia.com"}],"style":"normal"}],"title":"Using NVIDIA NIM for Agent-Enhanced AI Query Engines with LlamaIndex"},"publishedDate":"Invalid Date"},"params":{"slug":"using-nvidia-nim-for-agent-enhanced-ai-query-engines-with-llamaindex"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"using-nvidia-nim-for-agent-enhanced-ai-query-engines-with-llamaindex"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>