<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Towards Long Context RAG  — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Towards Long Context RAG  — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Towards Long Context RAG  — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Towards Long Context RAG  — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="485" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/jerry-liu">Jerry Liu</a> <!-- -->•<!-- --> <!-- -->2024-03-01</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Towards Long Context RAG </h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Google recently released <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Gemini 1.5 Pro with a 1M context window</a>, available to a limited set of developers and enterprise customers. Its performance has caught the imagination of <a href="https://x.com/alliekmiller/status/1760522046251962459?s=20" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">AI Twitter</a>. It <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">achieves 99.7% recall in the “Needle in a Haystack”</a> experiment popularized by <a href="https://x.com/GregKamradt/status/1722386725635580292?s=20" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Greg Kamradt</a>. Early users have shared results feeding dozens of research papers, financial reports at once and report impressive results in terms of its ability to synthesize across vast troves of information.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Naturally, this begs the question - is RAG dead? Some <a href="https://twitter.com/francis_yao_/status/1759962812229800012?s=46&amp;t=pfae6EnnrBq2o8ok0KpVqw" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">folks think so</a>, while others <a href="https://x.com/ptsi/status/1758511307433947625?s=20" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">disagree</a>. Those in the first camp make valid points. Most small data use cases can fit within a 1-10M context window. Tokens will get cheaper and faster to process over time. Having an LLM natively interleave retrieval/generation via attention layers leads to a higher response quality compared to the one-shot retrieval present in naive RAG.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We were fortunate to have a preview of Gemini 1.5 Pro’s capabilities, and through playing around with it developed a thesis for how context-augmented LLM applications will evolve. This blog post clarifies <strong>our mission as a data framework</strong> along with <strong>our view of what long-context LLM architectures will look like.</strong> Our view is that while long-context LLMs will simplify certain parts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG architectures to handle the new use cases that long-context LLMs bring along. No matter what new paradigms emerge, our mission at LlamaIndex is to build tooling towards that future.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Our Mission Goes Beyond RAG</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The goal of LlamaIndex is very simple: <strong>enable developers to build LLM applications over their data.</strong> This mission goes beyond just RAG. To date we have invested a considerable amount of effort in advancing RAG techniques for existing LLMs, and we’ve done so because it’s enabled developers to unlock dozens of new use cases such as QA over semi-structured data, over complex documents, and agentic reasoning in a multi-doc setting.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">But we’re also excited about Gemini Pro, and we will continue to advance LlamaIndex as a production data framework in a long-context LLM future.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>An LLM framework is intrinsically valuable.</strong> As an open-source data framework, LlamaIndex paves the cowpaths towards building any LLM use case from prototype to production. A framework makes it easier to build these use cases versus building from scratch. We enable <em>all</em> developers to build for these use cases, whether it’s setting up the proper architecture using our core abstractions or leveraging the hundreds of integrations in our ecosystem. No matter what the underlying LLM advancements are and whether RAG continues to exist in its current form, we continue to make the framework production-ready, including watertight abstractions, first-class documentation, and consistency.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">We also launched LlamaCloud last week</a>. Our mission for LlamaCloud remains building the data infra enabling any enterprise to make their vast unstructured, semi-structured, and structured data sources production-ready for use with LLMs.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Initial Gemini 1.5 Pro Observations</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">During our initial testing we played around with some PDFs: SEC 10K Filings, ArXiv papers, this monster <a href="https://www.lowellhsproject.com/DocumentCenter/View/236/LHS-Schematic-Design-Binder" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Schematic Design Binder</a>, and more. We will do a lot more deeper analyses once the APIs are available, but in the meantime we share observations below.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Gemini results are impressive and consistent with what we’ve seen in the technical report and on socials:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Gemini has impressive recall of specific details:</strong> We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Gemini has impressive summarization capabilities.</strong> The model can analyze large swaths of information across multiple documents and synthesize answers.</li></ul><figure><img alt="" loading="lazy" width="1250" height="703.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe52833d16338dfb5a8c5c4f1c8d96e2a55277252-2500x1407.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe52833d16338dfb5a8c5c4f1c8d96e2a55277252-2500x1407.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe52833d16338dfb5a8c5c4f1c8d96e2a55277252-2500x1407.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75"/><figcaption>This figure shows a question-response pair from Gemini over the 2019 Uber 10K filing. The question and answer is shown at the top and the source table is shown at the bottom. Gemini is able to return the correct answer.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu"></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are some parts where we noticed Gemini struggles a bit.</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Gemini doesn’t read all tables and charts correctly.</strong> Gemini Pro still has a hard time being able to read figures and complex tables.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Gemini can take a long time.</strong> Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Gemini can hallucinate page numbers.</strong> When asked to give a summary but also with page number citations, Gemini hallucinated the sources.</li></ul><figure><img alt="" loading="lazy" width="1250" height="703.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ac7723fbaeea5d785662feee8e815a3b763562f-2500x1407.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ac7723fbaeea5d785662feee8e815a3b763562f-2500x1407.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ac7723fbaeea5d785662feee8e815a3b763562f-2500x1407.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75"/><figcaption>An example where Gemini 1.5 Pro still hallucinates. The model hallucinates a number when asked about the total number of gross bookings across all segments - the number is visible in the chart and can also be pieced together from the table. </figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Directionally though it’s an exciting glimpse of the future and warrants a bigger discussion on which RAG paradigms will fade and new architectures that will emerge. See below!</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Long Contexts Resolve Some Pain Points, but some Challenges Remain</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Gemini 1.5 Pro is just the first of many long-context LLMs to emerge, which will inevitably change how users are building RAG.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Here are some existing RAG pain points that we believe long-context LLMs will solve:</p><ol><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Developers will worry less about how to precisely tune chunking algorithms.</strong> We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Developers will need to spend less time tuning retrieval and chain-of-thought over single documents</strong>. An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Summarization will be easier.</strong> This is related to the above statement. A lot of summarization strategies over big documents involve “hacks” such as sequential refinement or hierarchical summarization (see our <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">response synthesis modules</a> as a reference guide). This can now be alleviated with a single LLM call.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Personalized memory will be better and easier to build:</strong> A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction).</li></ol><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are, however, some lingering challenges:</p><ol><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge.</strong> 1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many “small” document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Embedding models are lagging behind in context length.</strong> So far the largest context window we’ve seen for embeddings are <a href="https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">32k from together.ai</a>. This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>Cost and Latency.</strong> Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that <a href="https://twitter.com/Francis_YAO_" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Yao Fu</a> brought up is that a <a href="https://x.com/Francis_YAO_/status/1759962812229800012?s=20" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">KV Cache</a> can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><strong>A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies</strong>. We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document.</li></ol><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Towards New RAG Architectures</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Proper usage of long-context LLMs will necessitate new architectures to best take advantage of their capabilities, while working around their remaining constraints. We outline some proposals below.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">1. Small to Big Retrieval over Documents</h3><figure><img alt="" loading="lazy" width="2342" height="893.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ff3928a9f18608566e7d26279bdce0bad8b6f4d1b-4684x1787.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75 1x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ff3928a9f18608566e7d26279bdce0bad8b6f4d1b-4684x1787.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75"/></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">To the extent that long-context LLMs need retrieval augmentation over big knowledge bases (e.g. in the gigabytes), we will need <strong>small-to-big retrieval:</strong> index and retrieve small chunks, but have each chunk link to big chunks that will ultimately be fed to LLMs during synthesis.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This architecture already exists in LlamaIndex in different forms (<a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">sentence window retriever</a> and <a href="https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">recursive retrieval over chunk sizes</a>), but can be scaled up even more for long-context LLMs - embed document summaries, but link to entire documents.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">One reason we want to embed and index smaller chunks is due to the fact that current embedding models are not keeping up with LLMs in terms of context length. Another reason is that there can actually be retrieval benefits in having multiple granular embedding representations compared to a single document-level embedding for a document. If there is a single embedding for a document, then that embedding has the burden of encoding all information throughout the entire document. On the other hand, we’ve found that embedding many smaller chunks and having each small chunk link to a bigger chunk, will lead to better retrieval of the relevant information.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Check out the diagram above for an illustration of two flavors of small-to-big retrieval. One is indexing document summaries and linking them to documents, and the other is indexing smaller chunks within a document and linking them to the document. Of course, you could also do both - a general best practice for improving retrieval is to just try out multiple techniques at once and fuse the results later.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">2. Intelligent Routing for Latency/Cost Tradeoffs</h3><figure><img alt="" loading="lazy" width="1503" height="761" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F691e821921191e0dff4e28028d9bc0f68d3ddd3f-3006x1522.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F691e821921191e0dff4e28028d9bc0f68d3ddd3f-3006x1522.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F691e821921191e0dff4e28028d9bc0f68d3ddd3f-3006x1522.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75"/></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The arrival of long-context LLMs will inevitably raise questions on the amount of context that is suitable for each use case. Injecting LLMs with long context comes with real cost and latency tradeoffs and isn’t suitable for every use case or even every question. Although cost and latency will decrease in the future, we anticipate users will need to think carefully about this tradeoff for the next year or two.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Certain questions that are asking about specific details are well suited for existing RAG techniques of top-k retrieval and synthesis.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">More complex questions require more context from disparate pieces of different documents, and in those settings it is less clear how to correctly answer these questions while optimizing for latency and cost:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Summarization questions require going over entire documents.</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt.</li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We imagine an intelligent routing layer that operates on top of multiple RAG and LLM synthesis pipelines over a knowledge base. Given a question, the router can ideally choose an optimal strategy in terms of cost and latency in terms of retrieving context to answer the question. This ensures that a single interface can handle different types of questions while not becoming prohibitively expensive.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">3. Retrieval Augmented KV Caching</h3><figure><img alt="" loading="lazy" width="1548" height="862" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F1933946f36e8b0ef097c529e043fae39ef7c5d12-3096x1724.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F1933946f36e8b0ef097c529e043fae39ef7c5d12-3096x1724.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F1933946f36e8b0ef097c529e043fae39ef7c5d12-3096x1724.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=3840&amp;q=75"/></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">An optimization that Google and other companies are certainly working on is resolving latency and cost concerns through a <strong>KV Cache.</strong> At a high-level, a KV cache stores activations from pre-existing key and query vectors in an attention layer, preventing the need to recompute activations across the entire text sequence during LLM generation (we found <a href="https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">this</a> to be a nice intro reference to how a KV Cache works).</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Using a KV Cache to cache all document tokens within the context window prevents the need to recompute activations for these tokens on subsequent conversations, bringing down latency and cost significantly.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">But this leads to interesting retrieval strategies on how to best use the cache, particularly for knowledge corpuses that exceed the context length. We imagine a “<strong>retrieval augmented caching</strong>” paradigm emerging, where we want to retrieve the most relevant documents that the user would want to answer, with the expectation that they will continue to use the documents that are in the cache.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This could involve interleaving retrieval strategies with <a href="https://www.notion.so/Long-Context-Window-Blog-Post-e2e3faaac23140eabc0e066ce2783890?pvs=21" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">traditional caching algorithms</a> such as LRU caching. But a difference with existing KV cache architectures is that the position matters, since the cached vector is a function of all tokens leading up to that position, not just the tokens in the document itself. This means that you can’t just swap out a chunk from the KV cache without affecting all cached tokens that occur after it positionally.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In general the API interface for using a KV Cache is up in the air. It’s also up in the air as to whether the nature of the cache itself will evolve or algorithms will evolve to best leverage the cache.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">What’s Next</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We believe the future of LLM applications is bright, and we are excited to be at the forefront of this rapidly evolving field. We invite developers and researchers to join us in exploring the possibilities of long-context LLMs and building the next generation of intelligent applications.</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-03-01T17:06:40Z","_id":"e4f58618-c10e-4f73-a3be-99c1a8055c44","_rev":"Ys5IzmCaJ2UnW2RAX7UkoO","_type":"blogPost","_updatedAt":"2025-05-21T20:40:17Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:59:39Z","_id":"26898661-ce74-4e56-a3bb-21000059ea8d","_rev":"1yZmiycp7gyBYGbmM40Ock","_type":"people","_updatedAt":"2025-05-07T15:41:41Z","image":{"_type":"image","asset":{"_ref":"image-e4426ff6862cbb8bec81b8407730e6e1e9383c8f-2176x2176-jpg","_type":"reference"}},"name":"Jerry Liu","position":"CEO","slug":{"_type":"slug","current":"jerry-liu"}}],"featured":false,"image":{"_type":"image","asset":{"_ref":"image-81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/81db5a16adc1029aaaf5ec9c9ffc30ce8ffb3d74-3368x970.png","publishedDate":"2024-03-01","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"}],"slug":{"_type":"slug","current":"towards-long-context-rag"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"}],"text":[{"_key":"88a083823fee","_type":"block","children":[{"_key":"8afcff0ddd710","_type":"span","marks":[],"text":"Google recently released "},{"_key":"8afcff0ddd711","_type":"span","marks":["0b1a3b219972"],"text":"Gemini 1.5 Pro with a 1M context window"},{"_key":"8afcff0ddd712","_type":"span","marks":[],"text":", available to a limited set of developers and enterprise customers. Its performance has caught the imagination of "},{"_key":"8afcff0ddd713","_type":"span","marks":["e5808773a864"],"text":"AI Twitter"},{"_key":"8afcff0ddd714","_type":"span","marks":[],"text":". It "},{"_key":"8afcff0ddd715","_type":"span","marks":["b202a3df0c48"],"text":"achieves 99.7% recall in the “Needle in a Haystack”"},{"_key":"8afcff0ddd716","_type":"span","marks":[],"text":" experiment popularized by "},{"_key":"8afcff0ddd717","_type":"span","marks":["2ed5eea29856"],"text":"Greg Kamradt"},{"_key":"8afcff0ddd718","_type":"span","marks":[],"text":". Early users have shared results feeding dozens of research papers, financial reports at once and report impressive results in terms of its ability to synthesize across vast troves of information."}],"markDefs":[{"_key":"0b1a3b219972","_type":"link","href":"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15"},{"_key":"e5808773a864","_type":"link","href":"https://x.com/alliekmiller/status/1760522046251962459?s=20"},{"_key":"b202a3df0c48","_type":"link","href":"https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf"},{"_key":"2ed5eea29856","_type":"link","href":"https://x.com/GregKamradt/status/1722386725635580292?s=20"}],"style":"normal"},{"_key":"70ccbdcc44fb","_type":"block","children":[{"_key":"40860af996810","_type":"span","marks":[],"text":"Naturally, this begs the question - is RAG dead? Some "},{"_key":"40860af996811","_type":"span","marks":["516e0c8e71dd"],"text":"folks think so"},{"_key":"40860af996812","_type":"span","marks":[],"text":", while others "},{"_key":"40860af996813","_type":"span","marks":["c140b1647667"],"text":"disagree"},{"_key":"40860af996814","_type":"span","marks":[],"text":". Those in the first camp make valid points. Most small data use cases can fit within a 1-10M context window. Tokens will get cheaper and faster to process over time. Having an LLM natively interleave retrieval/generation via attention layers leads to a higher response quality compared to the one-shot retrieval present in naive RAG."}],"markDefs":[{"_key":"516e0c8e71dd","_type":"link","href":"https://twitter.com/francis_yao_/status/1759962812229800012?s=46\u0026t=pfae6EnnrBq2o8ok0KpVqw"},{"_key":"c140b1647667","_type":"link","href":"https://x.com/ptsi/status/1758511307433947625?s=20"}],"style":"normal"},{"_key":"96198c6dd713","_type":"block","children":[{"_key":"d895fe01af7e0","_type":"span","marks":[],"text":"We were fortunate to have a preview of Gemini 1.5 Pro’s capabilities, and through playing around with it developed a thesis for how context-augmented LLM applications will evolve. This blog post clarifies "},{"_key":"d895fe01af7e1","_type":"span","marks":["strong"],"text":"our mission as a data framework"},{"_key":"d895fe01af7e2","_type":"span","marks":[],"text":" along with "},{"_key":"d895fe01af7e3","_type":"span","marks":["strong"],"text":"our view of what long-context LLM architectures will look like."},{"_key":"d895fe01af7e4","_type":"span","marks":[],"text":" Our view is that while long-context LLMs will simplify certain parts of the RAG pipeline (e.g. chunking), there will need to be evolved RAG architectures to handle the new use cases that long-context LLMs bring along. No matter what new paradigms emerge, our mission at LlamaIndex is to build tooling towards that future."}],"markDefs":[],"style":"normal"},{"_key":"09563cfec658","_type":"block","children":[{"_key":"1f6bb139639d0","_type":"span","marks":[],"text":"Our Mission Goes Beyond RAG"}],"markDefs":[],"style":"h2"},{"_key":"7934fc240fbb","_type":"block","children":[{"_key":"bbd1b70108bf0","_type":"span","marks":[],"text":"The goal of LlamaIndex is very simple: "},{"_key":"bbd1b70108bf1","_type":"span","marks":["strong"],"text":"enable developers to build LLM applications over their data."},{"_key":"bbd1b70108bf2","_type":"span","marks":[],"text":" This mission goes beyond just RAG. To date we have invested a considerable amount of effort in advancing RAG techniques for existing LLMs, and we’ve done so because it’s enabled developers to unlock dozens of new use cases such as QA over semi-structured data, over complex documents, and agentic reasoning in a multi-doc setting."}],"markDefs":[],"style":"normal"},{"_key":"2e4a5d8b9dd0","_type":"block","children":[{"_key":"078253c940b50","_type":"span","marks":[],"text":"But we’re also excited about Gemini Pro, and we will continue to advance LlamaIndex as a production data framework in a long-context LLM future."}],"markDefs":[],"style":"normal"},{"_key":"9481e4a83f23","_type":"block","children":[{"_key":"19f3f68201bd0","_type":"span","marks":["strong"],"text":"An LLM framework is intrinsically valuable."},{"_key":"19f3f68201bd1","_type":"span","marks":[],"text":" As an open-source data framework, LlamaIndex paves the cowpaths towards building any LLM use case from prototype to production. A framework makes it easier to build these use cases versus building from scratch. We enable "},{"_key":"19f3f68201bd2","_type":"span","marks":["em"],"text":"all"},{"_key":"19f3f68201bd3","_type":"span","marks":[],"text":" developers to build for these use cases, whether it’s setting up the proper architecture using our core abstractions or leveraging the hundreds of integrations in our ecosystem. No matter what the underlying LLM advancements are and whether RAG continues to exist in its current form, we continue to make the framework production-ready, including watertight abstractions, first-class documentation, and consistency."}],"markDefs":[],"style":"normal"},{"_key":"72130b93aead","_type":"block","children":[{"_key":"688eb7ffc4900","_type":"span","marks":["f877ae5dc3c1"],"text":"We also launched LlamaCloud last week"},{"_key":"688eb7ffc4901","_type":"span","marks":[],"text":". Our mission for LlamaCloud remains building the data infra enabling any enterprise to make their vast unstructured, semi-structured, and structured data sources production-ready for use with LLMs."}],"markDefs":[{"_key":"f877ae5dc3c1","_type":"link","href":"https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b"}],"style":"normal"},{"_key":"b72d2da79a22","_type":"block","children":[{"_key":"b424824e24130","_type":"span","marks":[],"text":"Initial Gemini 1.5 Pro Observations"}],"markDefs":[],"style":"h2"},{"_key":"c80d92f22375","_type":"block","children":[{"_key":"da2c03fa08fe0","_type":"span","marks":[],"text":"During our initial testing we played around with some PDFs: SEC 10K Filings, ArXiv papers, this monster "},{"_key":"da2c03fa08fe1","_type":"span","marks":["95b2474a6b0e"],"text":"Schematic Design Binder"},{"_key":"da2c03fa08fe2","_type":"span","marks":[],"text":", and more. We will do a lot more deeper analyses once the APIs are available, but in the meantime we share observations below."}],"markDefs":[{"_key":"95b2474a6b0e","_type":"link","href":"https://www.lowellhsproject.com/DocumentCenter/View/236/LHS-Schematic-Design-Binder"}],"style":"normal"},{"_key":"a5938ba52ef1","_type":"block","children":[{"_key":"53035264de460","_type":"span","marks":[],"text":"Gemini results are impressive and consistent with what we’ve seen in the technical report and on socials:"}],"markDefs":[],"style":"normal"},{"_key":"2bac14391932","_type":"block","children":[{"_key":"e325bac2c8c30","_type":"span","marks":["strong"],"text":"Gemini has impressive recall of specific details:"},{"_key":"e325bac2c8c31","_type":"span","marks":[],"text":" We threw in 100k-1M tokens of context, and asked questions over very specific details in these documents (unstructured text and tabular data), and in all cases Gemini was able to recall the details. See above for Gemini comparing table results in the 2019 Uber 10K Filing."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"c89f4f3a512f","_type":"block","children":[{"_key":"f44a148183820","_type":"span","marks":["strong"],"text":"Gemini has impressive summarization capabilities."},{"_key":"f44a148183821","_type":"span","marks":[],"text":" The model can analyze large swaths of information across multiple documents and synthesize answers."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"519cd816478e","_type":"image","asset":{"_ref":"image-e52833d16338dfb5a8c5c4f1c8d96e2a55277252-2500x1407-png","_type":"reference"},"caption":"This figure shows a question-response pair from Gemini over the 2019 Uber 10K filing. The question and answer is shown at the top and the source table is shown at the bottom. Gemini is able to return the correct answer."},{"_key":"4df356f6bedd","_type":"block","children":[{"_key":"0731bd5aceb10","_type":"span","marks":[],"text":""}],"markDefs":[],"style":"normal"},{"_key":"dccf86f380e6","_type":"block","children":[{"_key":"229dc9a8eac30","_type":"span","marks":[],"text":"There are some parts where we noticed Gemini struggles a bit."}],"markDefs":[],"style":"normal"},{"_key":"06ca0c569766","_type":"block","children":[{"_key":"8f370a3748a80","_type":"span","marks":["strong"],"text":"Gemini doesn’t read all tables and charts correctly."},{"_key":"8f370a3748a81","_type":"span","marks":[],"text":" Gemini Pro still has a hard time being able to read figures and complex tables."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"83f2e7cba23d","_type":"block","children":[{"_key":"c32e5216bd950","_type":"span","marks":["strong"],"text":"Gemini can take a long time."},{"_key":"c32e5216bd951","_type":"span","marks":[],"text":" Returning an answer over the Uber 10K Filing (~160k) takes ~20 seconds. Returning an answer over the LHS Schematic Design Binder (~890k) takes ~60+ seconds."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"9d918163b84f","_type":"block","children":[{"_key":"391fa9bcc0310","_type":"span","marks":["strong"],"text":"Gemini can hallucinate page numbers."},{"_key":"391fa9bcc0311","_type":"span","marks":[],"text":" When asked to give a summary but also with page number citations, Gemini hallucinated the sources."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"386233c6bd92","_type":"image","asset":{"_ref":"image-3ac7723fbaeea5d785662feee8e815a3b763562f-2500x1407-png","_type":"reference"},"caption":"An example where Gemini 1.5 Pro still hallucinates. The model hallucinates a number when asked about the total number of gross bookings across all segments - the number is visible in the chart and can also be pieced together from the table. "},{"_key":"e59081f3ef0a","_type":"block","children":[{"_key":"13fc4c21a32d0","_type":"span","marks":[],"text":"Directionally though it’s an exciting glimpse of the future and warrants a bigger discussion on which RAG paradigms will fade and new architectures that will emerge. See below!"}],"markDefs":[],"style":"normal"},{"_key":"2ab5adabbb78","_type":"block","children":[{"_key":"10e1461649c90","_type":"span","marks":[],"text":"Long Contexts Resolve Some Pain Points, but some Challenges Remain"}],"markDefs":[],"style":"h2"},{"_key":"597feb68eb3c","_type":"block","children":[{"_key":"f2e82c5c6df60","_type":"span","marks":[],"text":"Gemini 1.5 Pro is just the first of many long-context LLMs to emerge, which will inevitably change how users are building RAG."}],"markDefs":[],"style":"normal"},{"_key":"a0c08badbac0","_type":"block","children":[{"_key":"2aa6aa6792180","_type":"span","marks":[],"text":"Here are some existing RAG pain points that we believe long-context LLMs will solve:"}],"markDefs":[],"style":"normal"},{"_key":"740143021141","_type":"block","children":[{"_key":"695ca4d94aac0","_type":"span","marks":["strong"],"text":"Developers will worry less about how to precisely tune chunking algorithms."},{"_key":"695ca4d94aac1","_type":"span","marks":[],"text":" We honestly think this will be a huge blessing to LLM developers. Long-context LLMs enable native chunk sizes to be bigger. Assuming per-token cost and latency also go down, developers will no longer have to split hairs deciding how to split their chunks into granular strips through tuning chunking separators, chunk sizes, and careful metadata injection. Long-context LLMs enable chunks to be at the level of entire documents, or at the very least groups of pages."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"5b3955344f98","_type":"block","children":[{"_key":"6da490b23a070","_type":"span","marks":["strong"],"text":"Developers will need to spend less time tuning retrieval and chain-of-thought over single documents"},{"_key":"6da490b23a071","_type":"span","marks":[],"text":". An issue with small-chunk top-k RAG is that while certain questions may be answered over a specific snippet of the document, other questions require deep analysis between sections or between two documents (for instance comparison queries). For these use cases, developers will no longer have to rely on a chain-of-thought agent to do two retrievals against a weak retriever; instead, they can just one-shot prompt the LLM to obtain the answer."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"10caf7a0e8b6","_type":"block","children":[{"_key":"3ae1261f1dc80","_type":"span","marks":["strong"],"text":"Summarization will be easier."},{"_key":"3ae1261f1dc81","_type":"span","marks":[],"text":" This is related to the above statement. A lot of summarization strategies over big documents involve “hacks” such as sequential refinement or hierarchical summarization (see our "},{"_key":"3ae1261f1dc82","_type":"span","marks":["9508373d289b"],"text":"response synthesis modules"},{"_key":"3ae1261f1dc83","_type":"span","marks":[],"text":" as a reference guide). This can now be alleviated with a single LLM call."}],"level":1,"listItem":"number","markDefs":[{"_key":"9508373d289b","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html"}],"style":"normal"},{"_key":"ac97466250b8","_type":"block","children":[{"_key":"4ae13a135e8c0","_type":"span","marks":["strong"],"text":"Personalized memory will be better and easier to build:"},{"_key":"4ae13a135e8c1","_type":"span","marks":[],"text":" A key issue for building conversational assistants is figuring out how to load sufficient conversational context into the prompt window. 4k tokens easily overflows this window for very basic web search agents - if it decides to load in a Wikipedia page for instance, that text will easily overflow the context. 1M-10M context windows will let developers more easily implement conversational memory with fewer compression hacks (e.g. vector search or automatic KG construction)."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"7eca62ad8080","_type":"block","children":[{"_key":"e1cd849326180","_type":"span","marks":[],"text":"There are, however, some lingering challenges:"}],"markDefs":[],"style":"normal"},{"_key":"d3ad2ba2678d","_type":"block","children":[{"_key":"d6b5b8127b3b0","_type":"span","marks":["strong"],"text":"10M tokens is not enough for large document corpuses - kilodoc retrieval is still a challenge."},{"_key":"d6b5b8127b3b1","_type":"span","marks":[],"text":" 1M tokens is around ~7 Uber SEC 10K filings. 10M tokens would be around ~70 filings. 10M tokens is roughly bounded by 40MB of data. While this is enough for many “small” document corpuses, many knowledge corpuses in the enterprise are in the gigabytes or terabytes. To build LLM-powered systems over these knowledge corpuses, developers will still need to build in some way of retrieving this data to augment language models with context."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"d36f0b7416f5","_type":"block","children":[{"_key":"db2c863b913a0","_type":"span","marks":["strong"],"text":"Embedding models are lagging behind in context length."},{"_key":"db2c863b913a1","_type":"span","marks":[],"text":" So far the largest context window we’ve seen for embeddings are "},{"_key":"db2c863b913a2","_type":"span","marks":["a453d762fede"],"text":"32k from together.ai"},{"_key":"db2c863b913a3","_type":"span","marks":[],"text":". This means that even if the chunks used for synthesis with long-context LLMs can be big, any text chunks used for retrieval still need to be a lot smaller."}],"level":1,"listItem":"number","markDefs":[{"_key":"a453d762fede","_type":"link","href":"https://hazyresearch.stanford.edu/blog/2024-01-11-m2-bert-retrieval"}],"style":"normal"},{"_key":"17a413501619","_type":"block","children":[{"_key":"281248bcb35c0","_type":"span","marks":["strong"],"text":"Cost and Latency."},{"_key":"281248bcb35c1","_type":"span","marks":[],"text":" Yes, all cost and latency concerns are alleviated with time. Nevertheless, stuffing a 1M context window takes ~60 seconds and can cost anywhere from $0.50 to $20 with current pricing. An solution to this that "},{"_key":"281248bcb35c2","_type":"span","marks":["e2f9662b1bca"],"text":"Yao Fu"},{"_key":"281248bcb35c3","_type":"span","marks":[],"text":" brought up is that a "},{"_key":"281248bcb35c4","_type":"span","marks":["a689435f72f9"],"text":"KV Cache"},{"_key":"281248bcb35c5","_type":"span","marks":[],"text":" can cache the document activations, so that any subsequent generations can reuse the same cache. Which leads to our next point below."}],"level":1,"listItem":"number","markDefs":[{"_key":"e2f9662b1bca","_type":"link","href":"https://twitter.com/Francis_YAO_"},{"_key":"a689435f72f9","_type":"link","href":"https://x.com/Francis_YAO_/status/1759962812229800012?s=20"}],"style":"normal"},{"_key":"f0fe3a3054c2","_type":"block","children":[{"_key":"3cd21a01cf310","_type":"span","marks":["strong"],"text":"A KV Cache takes up a significant amount of GPU memory, and has sequential dependencies"},{"_key":"3cd21a01cf311","_type":"span","marks":[],"text":". We chatted with Yao and he mentioned that at the moment, caching 1M tokens worth of activations would use up approximately 100GB of GPU memory, or 2 H100s. There are also interesting challenges on how to best manage the cache especially when the underlying corpus is big - since each activation is a function of all tokens leading up to it, replacing any document in the KV cache would affect all activations following the document."}],"level":1,"listItem":"number","markDefs":[],"style":"normal"},{"_key":"1be299195403","_type":"block","children":[{"_key":"f9e09fd3b62d0","_type":"span","marks":[],"text":"Towards New RAG Architectures"}],"markDefs":[],"style":"h2"},{"_key":"e96b37c63f85","_type":"block","children":[{"_key":"4c0c8500fd140","_type":"span","marks":[],"text":"Proper usage of long-context LLMs will necessitate new architectures to best take advantage of their capabilities, while working around their remaining constraints. We outline some proposals below."}],"markDefs":[],"style":"normal"},{"_key":"c91550345594","_type":"block","children":[{"_key":"f0acbd7d2aa40","_type":"span","marks":[],"text":"1. Small to Big Retrieval over Documents"}],"markDefs":[],"style":"h3"},{"_key":"99b38565979a","_type":"image","asset":{"_ref":"image-f3928a9f18608566e7d26279bdce0bad8b6f4d1b-4684x1787-png","_type":"reference"}},{"_key":"10d4c57c1862","_type":"block","children":[{"_key":"a06a8f9732370","_type":"span","marks":[],"text":"To the extent that long-context LLMs need retrieval augmentation over big knowledge bases (e.g. in the gigabytes), we will need "},{"_key":"a06a8f9732371","_type":"span","marks":["strong"],"text":"small-to-big retrieval:"},{"_key":"a06a8f9732372","_type":"span","marks":[],"text":" index and retrieve small chunks, but have each chunk link to big chunks that will ultimately be fed to LLMs during synthesis."}],"markDefs":[],"style":"normal"},{"_key":"4e61525758f4","_type":"block","children":[{"_key":"150fd4219b890","_type":"span","marks":[],"text":"This architecture already exists in LlamaIndex in different forms ("},{"_key":"150fd4219b891","_type":"span","marks":["2259216d106f"],"text":"sentence window retriever"},{"_key":"150fd4219b892","_type":"span","marks":[],"text":" and "},{"_key":"150fd4219b893","_type":"span","marks":["f9e4d3ba9a2e"],"text":"recursive retrieval over chunk sizes"},{"_key":"150fd4219b894","_type":"span","marks":[],"text":"), but can be scaled up even more for long-context LLMs - embed document summaries, but link to entire documents."}],"markDefs":[{"_key":"2259216d106f","_type":"link","href":"https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html"},{"_key":"f9e4d3ba9a2e","_type":"link","href":"https://docs.llamaindex.ai/en/stable/examples/retrievers/recursive_retriever_nodes.html"}],"style":"normal"},{"_key":"116f9397d190","_type":"block","children":[{"_key":"054b3acce3a60","_type":"span","marks":[],"text":"One reason we want to embed and index smaller chunks is due to the fact that current embedding models are not keeping up with LLMs in terms of context length. Another reason is that there can actually be retrieval benefits in having multiple granular embedding representations compared to a single document-level embedding for a document. If there is a single embedding for a document, then that embedding has the burden of encoding all information throughout the entire document. On the other hand, we’ve found that embedding many smaller chunks and having each small chunk link to a bigger chunk, will lead to better retrieval of the relevant information."}],"markDefs":[],"style":"normal"},{"_key":"d165e1e26f68","_type":"block","children":[{"_key":"c8f1681dbed60","_type":"span","marks":[],"text":"Check out the diagram above for an illustration of two flavors of small-to-big retrieval. One is indexing document summaries and linking them to documents, and the other is indexing smaller chunks within a document and linking them to the document. Of course, you could also do both - a general best practice for improving retrieval is to just try out multiple techniques at once and fuse the results later."}],"markDefs":[],"style":"normal"},{"_key":"4aff7a35d915","_type":"block","children":[{"_key":"befd88e2cb2a0","_type":"span","marks":[],"text":"2. Intelligent Routing for Latency/Cost Tradeoffs"}],"markDefs":[],"style":"h3"},{"_key":"fb252014bad6","_type":"image","asset":{"_ref":"image-691e821921191e0dff4e28028d9bc0f68d3ddd3f-3006x1522-png","_type":"reference"}},{"_key":"6f314caeedd0","_type":"block","children":[{"_key":"bbb6ea4db0040","_type":"span","marks":[],"text":"The arrival of long-context LLMs will inevitably raise questions on the amount of context that is suitable for each use case. Injecting LLMs with long context comes with real cost and latency tradeoffs and isn’t suitable for every use case or even every question. Although cost and latency will decrease in the future, we anticipate users will need to think carefully about this tradeoff for the next year or two."}],"markDefs":[],"style":"normal"},{"_key":"7e4baa9f2106","_type":"block","children":[{"_key":"e450380f37360","_type":"span","marks":[],"text":"Certain questions that are asking about specific details are well suited for existing RAG techniques of top-k retrieval and synthesis."}],"markDefs":[],"style":"normal"},{"_key":"54993bae4d66","_type":"block","children":[{"_key":"0c51548523380","_type":"span","marks":[],"text":"More complex questions require more context from disparate pieces of different documents, and in those settings it is less clear how to correctly answer these questions while optimizing for latency and cost:"}],"markDefs":[],"style":"normal"},{"_key":"c95d12362887","_type":"block","children":[{"_key":"fed4a185d15b0","_type":"span","marks":[],"text":"Summarization questions require going over entire documents."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"774a2c9caa03","_type":"block","children":[{"_key":"cb3ac991cda80","_type":"span","marks":[],"text":"Multi-part questions can be solved by doing chain-of-thought and interleaving retrieval and reasoning; they can also be solved by shoving all context into the prompt."}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"fe6f6bb2b640","_type":"block","children":[{"_key":"5b9e88e8f90f0","_type":"span","marks":[],"text":"We imagine an intelligent routing layer that operates on top of multiple RAG and LLM synthesis pipelines over a knowledge base. Given a question, the router can ideally choose an optimal strategy in terms of cost and latency in terms of retrieving context to answer the question. This ensures that a single interface can handle different types of questions while not becoming prohibitively expensive."}],"markDefs":[],"style":"normal"},{"_key":"5a87ba562dc7","_type":"block","children":[{"_key":"5cd20bbdcd3f0","_type":"span","marks":[],"text":"3. Retrieval Augmented KV Caching"}],"markDefs":[],"style":"h3"},{"_key":"0deaf42f4895","_type":"image","asset":{"_ref":"image-1933946f36e8b0ef097c529e043fae39ef7c5d12-3096x1724-png","_type":"reference"}},{"_key":"df7da9f7780b","_type":"block","children":[{"_key":"33128b1736c30","_type":"span","marks":[],"text":"An optimization that Google and other companies are certainly working on is resolving latency and cost concerns through a "},{"_key":"33128b1736c31","_type":"span","marks":["strong"],"text":"KV Cache."},{"_key":"33128b1736c32","_type":"span","marks":[],"text":" At a high-level, a KV cache stores activations from pre-existing key and query vectors in an attention layer, preventing the need to recompute activations across the entire text sequence during LLM generation (we found "},{"_key":"33128b1736c33","_type":"span","marks":["b54f64395197"],"text":"this"},{"_key":"33128b1736c34","_type":"span","marks":[],"text":" to be a nice intro reference to how a KV Cache works)."}],"markDefs":[{"_key":"b54f64395197","_type":"link","href":"https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8"}],"style":"normal"},{"_key":"279201d82188","_type":"block","children":[{"_key":"3526e03612360","_type":"span","marks":[],"text":"Using a KV Cache to cache all document tokens within the context window prevents the need to recompute activations for these tokens on subsequent conversations, bringing down latency and cost significantly."}],"markDefs":[],"style":"normal"},{"_key":"225c516ffa81","_type":"block","children":[{"_key":"cd3f956156ed0","_type":"span","marks":[],"text":"But this leads to interesting retrieval strategies on how to best use the cache, particularly for knowledge corpuses that exceed the context length. We imagine a “"},{"_key":"cd3f956156ed1","_type":"span","marks":["strong"],"text":"retrieval augmented caching"},{"_key":"cd3f956156ed2","_type":"span","marks":[],"text":"” paradigm emerging, where we want to retrieve the most relevant documents that the user would want to answer, with the expectation that they will continue to use the documents that are in the cache."}],"markDefs":[],"style":"normal"},{"_key":"ede0a57d9fd8","_type":"block","children":[{"_key":"03634961d2670","_type":"span","marks":[],"text":"This could involve interleaving retrieval strategies with "},{"_key":"03634961d2671","_type":"span","marks":["e7e055610cf2"],"text":"traditional caching algorithms"},{"_key":"03634961d2672","_type":"span","marks":[],"text":" such as LRU caching. But a difference with existing KV cache architectures is that the position matters, since the cached vector is a function of all tokens leading up to that position, not just the tokens in the document itself. This means that you can’t just swap out a chunk from the KV cache without affecting all cached tokens that occur after it positionally."}],"markDefs":[{"_key":"e7e055610cf2","_type":"link","href":"https://www.notion.so/Long-Context-Window-Blog-Post-e2e3faaac23140eabc0e066ce2783890?pvs=21"}],"style":"normal"},{"_key":"74f9bce2f480","_type":"block","children":[{"_key":"492339c9d6e90","_type":"span","marks":[],"text":"In general the API interface for using a KV Cache is up in the air. It’s also up in the air as to whether the nature of the cache itself will evolve or algorithms will evolve to best leverage the cache."}],"markDefs":[],"style":"normal"},{"_key":"083dd29501fe","_type":"block","children":[{"_key":"55f557c811d70","_type":"span","marks":[],"text":"What’s Next"}],"markDefs":[],"style":"h3"},{"_key":"aa767f760c51","_type":"block","children":[{"_key":"97f0c9df2d830","_type":"span","marks":[],"text":"We believe the future of LLM applications is bright, and we are excited to be at the forefront of this rapidly evolving field. We invite developers and researchers to join us in exploring the possibilities of long-context LLMs and building the next generation of intelligent applications."}],"markDefs":[],"style":"normal"}],"title":"Towards Long Context RAG "},"publishedDate":"Invalid Date"},"params":{"slug":"towards-long-context-rag"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"towards-long-context-rag"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>