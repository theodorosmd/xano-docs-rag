<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Building knowledge graph agents with LlamaIndex Workflows — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Building knowledge graph agents with LlamaIndex Workflows — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Building knowledge graph agents with LlamaIndex Workflows — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/a9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Building knowledge graph agents with LlamaIndex Workflows — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/a9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="258" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/tomaz-bratanic">Tomaz Bratanic</a> <!-- -->•<!-- --> <!-- -->2025-01-15</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Building knowledge graph agents with LlamaIndex Workflows</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/knowledge-graphs"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Knowledge Graphs</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Retrieval-augmented generation (RAG) is here to stay, and for good reason. It’s a powerful framework that blends advanced language models with targeted information retrieval techniques, enabling quicker access to relevant data and producing more accurate, context-aware responses. While RAG applications often focus on <strong>unstructured data</strong>, I’m a big fan of integrating <strong>structured data</strong> into the mix, a vital yet frequently overlooked approach. One of my favorite ways to do this is by leveraging graph databases like <a href="https://neo4j.com/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Neo4j</a>.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Often, the go-to approach for retrieving data from a graph is <strong>text2cypher</strong>, where natural language queries are automatically converted into Cypher statements to query the graph database. This technique relies on a language model (or rule-based system) that interprets user queries, infers their underlying intent, and translates them into valid Cypher queries, enabling RAG applications to retrieve the relevant information from the knowledge graph and produce accurate answers.</p><figure><img alt="" loading="lazy" width="350" height="172.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3dc91aa952bae365e07cb40a463020e009598182-700x345.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3dc91aa952bae365e07cb40a463020e009598182-700x345.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3dc91aa952bae365e07cb40a463020e009598182-700x345.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Generating Cypher using LLMs. Image credits: https://neo4j.com/developer-blog/fine-tuned-text2cypher-2024-model/</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Text2cypher offers remarkable flexibility because it allows users to formulate questions in natural language without having to know the underlying graph schema or Cypher syntax. However, due to the nuances of language interpretation and the need for precise schema-specific details, its accuracy can still be lacking as shown in <a href="https://medium.com/neo4j/benchmarking-using-the-neo4j-text2cypher-2024-dataset-d77be96ab65a" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">this text2cypher article</a>.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The most important results of the benchmark are shown in the following visualization.</p><figure><img alt="" loading="lazy" width="350" height="295.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc75a9d32eb3787339408e3581f7ccda9faaa1dac-700x591.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc75a9d32eb3787339408e3581f7ccda9faaa1dac-700x591.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fc75a9d32eb3787339408e3581f7ccda9faaa1dac-700x591.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Text2cypher benchmark results. Image from https://medium.com/neo4j/benchmarking-using-the-neo4j-text2cypher-2024-dataset-d77be96ab65a</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">From a high-level perspective, the benchmark compares three groups of models:</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Fine-tuned models for text2cypher tasks</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Open foundational models</li><li class="Text_text__zPO0D Text_text-size-16__PkjFu">Closed foundational models</li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The benchmark evaluates their performance on generating correct Cypher queries using two metrics: GoogleBLEU (top chart) and ExactMatch (bottom chart).</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The <em>GoogleBLEU</em> metric measures the degree of overlap (in terms of n-grams) between the generated query and a reference query. Higher scores generally indicate closer alignment to the reference, but they do not necessarily guarantee that the query will run correctly in a database context.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>ExactMatch</em>, on the other hand, is an execution-based metric. It indicates the percentage of generated queries that exactly match the correct query text, implying they produce the same results when executed. This makes ExactMatch a stricter measure of correctness and more directly tied to the actual utility of the query in a real-world setting.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Despite some promising results with fine-tuning, the overall accuracy levels illustrate that text2cypher remains an evolving technology. Some models still struggle to generate fully correct queries in every case, underscoring the need for further improvement in this area.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this post, we’ll experiment with <a href="https://docs.llamaindex.ai/en/stable/module_guides/workflow/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">LlamaIndex Workflows</a> to implement more agentic strategies for text2cypher. Instead of relying on a single-shot query generation, which is typically how most benchmarks are run, we’ll try a multi-step approach that allows for retries or alternative query formulations. By incorporating these extra steps and fallback options, we’re aiming to boost overall accuracy and reduce the instances of flawed Cypher generation.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>The code is available on</em> <em><a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">GitHub</a>. We also have a hosted version of the application available</em> <em><a href="https://text2cypher-llama-agent.up.railway.app/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>. Thanks to</em> <a href="https://github.com/easwee" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><em>Anej Gorkic</em></a> <em>for contributing to the application and helping with debugging :)</em></p><figure><img alt="" loading="lazy" width="360" height="204" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdeec5a7d4af4bb833277ecc97113d8c4e844c35a-720x408.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdeec5a7d4af4bb833277ecc97113d8c4e844c35a-720x408.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fdeec5a7d4af4bb833277ecc97113d8c4e844c35a-720x408.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Hosted web application with all the agents is available on: https://text2cypher-llama-agent.up.railway.app/</figcaption></figure><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">LlamaIndex Workflows</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://docs.llamaindex.ai/en/stable/module_guides/workflow/" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">LlamaIndex Workflows</a> are a practical way to organize multi-step AI processes by connecting different operations through an event-driven system. They help break down complex tasks into smaller, manageable pieces that can communicate with each other in a structured way. Each step in a workflow handles specific events and produces new ones, creating a chain of operations that can accomplish tasks like document processing, question answering, or content generation. The system handles the coordination between steps automatically, making it easier to build and maintain complex AI applications.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Naive text2cypher flow</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The naive text2cypher architecture is a streamlined approach to converting natural language questions into Cypher queries for Neo4j graph databases. It operates through a three-stage workflow: first, it generates a Cypher query from the input question using few-shot learning with similar examples stored in a vector database. The system then executes the generated Cypher query against the graph database. Finally, it processes the database results through a language model to generate a natural language response that directly answers the original question. This architecture maintains a simple yet effective pipeline, leveraging vector similarity search for example fewshot retrieval and LLM for both Cypher query generation and response formatting.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Here is the visualized naive text2cypher workflow.</p><figure><img alt="" loading="lazy" width="359.5" height="193" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F54fea43edd1e72de2f07d9d584ecf5d531bef2ea-719x386.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F54fea43edd1e72de2f07d9d584ecf5d531bef2ea-719x386.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F54fea43edd1e72de2f07d9d584ecf5d531bef2ea-719x386.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Naive text2cypher flow.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">It is worth noting that most Neo4j schema generation methods struggle with <em>multi-labeled nodes</em>. This issue arises not only due to added complexity but also because of the combinatorial explosion of labels, which can overwhelm the prompt. To mitigate this, we exclude the <code class="SanityPortableText_inlineCode__cI85z">Actor</code> and <code class="SanityPortableText_inlineCode__cI85z">Director</code> labels in the schema generation process.</p><pre><code>schema = graph_store.get_schema_str(exclude_types=[<span class="hljs-string">&quot;Actor&quot;</span>, <span class="hljs-string">&quot;Director&quot;</span>])</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The pipeline starts with the <code class="SanityPortableText_inlineCode__cI85z">generate_cypher</code>step.</p><pre><code><span class="hljs-meta">@step</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_cypher</span>(<span class="hljs-params">self, ctx: Context, ev: StartEvent</span>) -&gt; ExecuteCypherEvent:
    question = ev.<span class="hljs-built_in">input</span>
    <span class="hljs-comment"># Cypher query generation using an LLM</span>
    cypher_query = <span class="hljs-keyword">await</span> generate_cypher_step(
        self.llm, question, self.few_shot_retriever
    )
    <span class="hljs-comment"># Streaming event information to the web UI.</span>
    ctx.write_event_to_stream(
        SseEvent(
            label=<span class="hljs-string">&quot;Cypher generation&quot;</span>,
            message=<span class="hljs-string">f&quot;Generated Cypher: <span class="hljs-subst">{cypher_query}</span>&quot;</span>,
        )
    )

    <span class="hljs-comment"># Return for the next step</span>
    <span class="hljs-keyword">return</span> ExecuteCypherEvent(question=question, cypher=cypher_query)</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The generate_cypher step takes a natural language question and transforms it into a Cypher query by utilizing a language model and retrieving similar examples from a vector store. The step also streams the generated Cypher query back to the user interface in real-time, providing immediate feedback on the query generation process. You can inspect the whole code and prompts <a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/naive_text2cypher.py" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Naive text2cypher with retry flow</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This enhanced version of text2cypher with retry builds upon the original architecture by adding a self-correction mechanism. When a generated Cypher query fails to execute, instead of failing outright, the system attempts to fix the query by feeding the error information back to the language model in the <code class="SanityPortableText_inlineCode__cI85z">CorrectCypherEvent</code>step. This makes the system more resilient and capable of handling initial mistakes, similar to how a human might revise their approach after receiving error feedback.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Here is the visualized naive text2cypher with retry workflow.</p><figure><img alt="" loading="lazy" width="360" height="195.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F17544d9a7d2a51a5ee51e4f4429dd6808867e5ef-720x391.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F17544d9a7d2a51a5ee51e4f4429dd6808867e5ef-720x391.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F17544d9a7d2a51a5ee51e4f4429dd6808867e5ef-720x391.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Naive text2cypher with retry flow.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let’s take a look at the <code class="SanityPortableText_inlineCode__cI85z">ExecuteCypherEvent</code> .</p><pre><code><span class="hljs-meta">@step</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">execute_query</span>(<span class="hljs-params">
    self, ctx: Context, ev: ExecuteCypherEvent
</span>) -&gt; SummarizeEvent | CorrectCypherEvent:
    <span class="hljs-comment"># Get global var</span>
    retries = <span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;retries&quot;</span>)
    <span class="hljs-keyword">try</span>:
        database_output = <span class="hljs-built_in">str</span>(graph_store.structured_query(ev.cypher))
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        database_output = <span class="hljs-built_in">str</span>(e)
        <span class="hljs-comment"># Retry</span>
        <span class="hljs-keyword">if</span> retries &lt; self.max_retries:
            <span class="hljs-keyword">await</span> ctx.<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;retries&quot;</span>, retries + <span class="hljs-number">1</span>)
            <span class="hljs-keyword">return</span> CorrectCypherEvent(
                question=ev.question, cypher=ev.cypher, error=database_output
            )

    <span class="hljs-keyword">return</span> SummarizeEvent(
        question=ev.question, cypher=ev.cypher, context=database_output
    )</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The execute function first attempts to run the query, and if successful, passes the results forward for summarization. However, if something goes wrong, it doesn’t give up immediately — instead, it checks if it has any retry attempts left and, if so, sends the query back for correction along with information about what went wrong. This creates a more forgiving system that can learn from its mistakes, much like how we might revise our approach after receiving feedback. You can inspect the whole code and prompts <a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/naive_text2cypher_retry.py" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>.</p><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Naive text2cypher with retry and evaluation flow</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Building on the <strong>naive text2cypher with retry</strong> flow, this enhanced version adds an <strong>evaluation</strong> phase that checks if the query results are sufficient to answer the user’s question. If the results are deemed inadequate, the system sends the query back for correction with information on how to improve it. If the results are acceptable, the flow proceeds to the final summarization step. This extra layer of validation further bolsters the resilience of the pipeline, ensuring that the user ultimately receives the most accurate and complete answer possible.</p><figure><img alt="" loading="lazy" width="354.5" height="224.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa93183fe7953a69d1f1153941e7e7233b8f9f3cd-709x449.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa93183fe7953a69d1f1153941e7e7233b8f9f3cd-709x449.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa93183fe7953a69d1f1153941e7e7233b8f9f3cd-709x449.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Naive text2cypher with retry and evaluation flow.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The additional evaluation step is implemented in the following manner:</p><pre><code><span class="hljs-meta">@step</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_context</span>(<span class="hljs-params">
    self, ctx: Context, ev: EvaluateEvent
</span>) -&gt; SummarizeEvent | CorrectCypherEvent:
    <span class="hljs-comment"># Get global var</span>
    retries = <span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;retries&quot;</span>)
    evaluation = <span class="hljs-keyword">await</span> evaluate_database_output_step(
        self.llm, ev.question, ev.cypher, ev.context
    )
    <span class="hljs-keyword">if</span> retries &lt; self.max_retries <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> evaluation == <span class="hljs-string">&quot;Ok&quot;</span>:
        <span class="hljs-keyword">await</span> ctx.<span class="hljs-built_in">set</span>(<span class="hljs-string">&quot;retries&quot;</span>, retries + <span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> CorrectCypherEvent(
            question=ev.question, cypher=ev.cypher, error=evaluation
        )
    <span class="hljs-keyword">return</span> SummarizeEvent(
        question=ev.question, cypher=ev.cypher, context=ev.context
    )</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The function <code class="SanityPortableText_inlineCode__cI85z">evaluate_check</code>is a simple check that determines whether the query results adequately address the user’s question. If the evaluation indicates the results are insufficient and there are retry attempts remaining, it returns a <code class="SanityPortableText_inlineCode__cI85z">CorrectCypherEvent</code>so the query can be refined. Otherwise, it proceeds with a <code class="SanityPortableText_inlineCode__cI85z">SummarizeEvent</code>,indicating that the results are suitable for final summarization.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>I later realized it would be an excellent idea to capture instances where the flow successfully self-healed by correcting invalid Cypher statements. These examples could then be leveraged as dynamic few-shot prompts for future Cypher generation. This approach would enable the agent to not only self-heal but also continuously self-learn and improve over time. The example code to store these fewshot examples can be found</em> <a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/text2cypher_retry_check.py#L163" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze"><em>here</em></a> <em>and is implemented currently for this flow only (as it gives the best self-healing accuracy).</em></p><pre><code><span class="hljs-meta">@step</span>
<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">summarize_answer</span>(<span class="hljs-params">self, ctx: Context, ev: SummarizeEvent</span>) -&gt; StopEvent:
    retries = <span class="hljs-keyword">await</span> ctx.get(<span class="hljs-string">&quot;retries&quot;</span>)
    <span class="hljs-comment"># If retry was successful:</span>
    <span class="hljs-keyword">if</span> retries &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> check_ok(ev.evaluation):
        <span class="hljs-comment"># print(f&quot;Learned new example: {ev.question}, {ev.cypher}&quot;)</span>
        <span class="hljs-comment"># Store success retries to be used as fewshots!</span>
        store_fewshot_example(ev.question, ev.cypher, self.llm.model)</code></pre><h3 class="Text_text__zPO0D Text_text-size-40__fIyvA">Iterative planner flow</h3><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The final flow is the most complex, and coincidentally, the one I ambitiously designed first. I’ve kept it in the code so you can learn from my exploration.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The iterative planner flow introduces a more sophisticated approach by implementing an <strong>iterative planning system</strong>. Instead of directly generating a Cypher query, it first creates a plan of sub-queries, validates each subquery Cypher statement before execution, and includes an information checking mechanism that can modify the plan if the initial results are insufficient. The system can make up to three iterations of information gathering, each time refining its approach based on previous results. This creates a more thorough question-answering system that can handle complex queries by breaking them down into manageable steps and validating the information at each stage.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Here is the visualized iterative planner workflow.</p><figure><img alt="" loading="lazy" width="360" height="258" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fa9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Iterative planning flow.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Let’s examine the query planner prompt. I was quite ambitious when I began. I expected the LLMs to produce the following response.</p><pre><code><span class="hljs-keyword">class</span> <span class="hljs-title class_">SubqueriesOutput</span>(<span class="hljs-title class_ inherited__">BaseModel</span>):
    <span class="hljs-string">&quot;&quot;&quot;Defines the output format for transforming a question into parallel-optimized retrieval steps.&quot;&quot;&quot;</span>

    plan: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = Field(
        description=(
            <span class="hljs-string">&quot;&quot;&quot;A list of query groups where:
        - Each group (inner list) contains queries that can be executed in parallel
        - Groups are ordered by dependency (earlier groups must be executed before later ones)
        - Each query must be a specific information retrieval request
        - Split into multiple steps only if intermediate results return ≤25 values
        - No reasoning or comparison tasks, only data fetching queries&quot;&quot;&quot;</span>
        )
    )</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The output represents a structured plan for transforming a complex question into sequential and parallel query steps. Each step consists of a group of queries that can be executed in parallel, with later steps depending on the results of earlier ones. Queries are strictly for information retrieval, avoiding reasoning tasks, and are split into smaller steps if needed to manage result size. For example, the following plan begins by listing movies for two actors in parallel, followed by a step that identifies the highest-grossing movie from the results of the first step.</p><pre><code>plan = [
<span class="hljs-comment"># 2 steps in parallel</span>
    [
        <span class="hljs-string">&quot;List all movies made by Tom Hanks in the 2000s.&quot;</span>,
        <span class="hljs-string">&quot;List all movies made by Tom Cruise in the 2000s.&quot;</span>,
    ],
<span class="hljs-comment"># Second step</span>
    [<span class="hljs-string">&quot;Find the highest profiting movie among winner of step 1&quot;</span>],
]</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This idea is undeniably cool. It’s a smart way to break down complex questions into smaller, actionable steps and even leverage parallelism to optimize retrieval. It sounds like the kind of strategy that could really speed things up. But, in practice, expecting LLMs to execute this reliably is a bit ambitious. Parallelism, while efficient in theory, introduces a lot of complexity. Dependencies, intermediate results, and maintaining logical consistency between parallel steps can easily trip up even advanced models. Sequential execution, though less glamorous, is more reliable for now and significantly reduces the cognitive overhead on the model.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Additionally, LLMs often struggle with following structured tool outputs like lists of lists, especially when reasoning about dependencies between steps. Here, I’m curious to see how much prompting alone (without tool outputs) can improve the model’s performance on these tasks</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The code for the iterative planning flow is available <a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/iterative_planner.py" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>.</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Benchmarking</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Creating a benchmark dataset for evaluating text2ypher agents in the LlamaIndex workflow architecture feels like an exciting step forward.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We sought an alternative to traditional one-shot Cypher execution metrics, such as ExactMatch mentioned in the beginning, which often fall short in capturing the full potential of workflows like iterative planning. In these workflows, multiple steps are employed to refine queries and retrieve relevant information, making single-step execution metrics inadequate.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">That’s why we chose to use <em>answer_relevancy</em> from RAGAS — it feels more aligned with what we want to measure. Here, we use an LLM to generate the answer, and then use LLM as a judge to compare it with ground truth. We’ve prepared a custom dataset of about 50 samples, carefully designed to avoid generating overwhelming outputs, meaning database results that are excessively large or detailed. Such outputs can make it difficult for an LLM judge to evaluate relevance effectively, so keeping results short ensures a fair and focused comparison of single- and multi-step workflows.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Here are the results.</p><figure><img alt="" loading="lazy" width="360" height="306.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F971a7d83e63ffde44854b41f68d7811a277eb688-720x613.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F971a7d83e63ffde44854b41f68d7811a277eb688-720x613.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F971a7d83e63ffde44854b41f68d7811a277eb688-720x613.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Benchmark results.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Sonnet 3.5, Deepsek-v3, and GPT-4o emerge as the top three models in terms of answer relevancy, each scoring above 0.80. The <em>NaiveText2CypherRetryCheckFlow</em> tends to produce the highest relevancy overall, while the <em>IterativePlanningFlow</em> consistently ranks lower (dropping as low as 0.163).</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Although o1 model is fairly accurate, it probably isn’t at the top due to multiple timeouts (set at 90 seconds). Deepsek-v3 stands out as especially promising, given its strong scores paired with relatively low latency. Overall, these results underscore the importance not just of raw accuracy, but also of stability and speed in practical deployment scenarios.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Here’s another table where uplift between flows can be easily examined.</p><figure><img alt="" loading="lazy" width="360" height="98.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F7f7859da8db515c85e30022d7c0699ac28c3de30-720x197.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F7f7859da8db515c85e30022d7c0699ac28c3de30-720x197.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F7f7859da8db515c85e30022d7c0699ac28c3de30-720x197.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Benchmark results.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Sonnet 3.5 rises steadily from a 0.596 score in NaiveText2CypherFlow to 0.616 in NaiveText2CypherRetryFlow, then makes a bigger leap to 0.843 in NaiveText2CypherRetryCheckFlow. GPT-4o shows a similar pattern overall, moving from 0.622 in NaiveText2CypherFlow down slightly to 0.603 in NaiveText2CypherRetryFlow, but then climbing significantly to 0.837 in NaiveText2CypherRetryCheckFlow. These improvements suggest that adding both a retry mechanism and a final verification step markedly boosts answer relevancy.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>The benchmark code can be found</em> <em><a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/benchmark/benchmark_gridsearch.ipynb" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">here</a>.</em></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>Please note that benchmark results may vary by at least 5%, meaning you might observe slightly different outcomes and top performers across different runs.</em></p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Learnings and going to production</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This was a two-month project where I learned a lot along the way. One highlight of the project was achieving 84<strong>% relevancy in a test benchmark</strong>, which is a significant achievement. However, does that mean you will achieve 84% accuracy in production? <strong>Probably not.</strong></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Production environments bring their own set of challenges — real-world data is often noisier, more diverse, and less structured than benchmark datasets. Something we haven’t discussed yet, but you’ll see in practice with real applications and users, is the <strong>need for production-ready steps</strong>. This means not just focusing on achieving high accuracy in controlled benchmarks but ensuring the system is reliable, adaptable, and delivers consistent results in real-world conditions.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In these settings, you’d need to implement some kind of guardrails to stop unrelated questions from going through the text-to-Cypher pipeline.</p><figure><img alt="" loading="lazy" width="360" height="118.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Feed7c6f97cbd307429326765c9d87e528e23edbc-720x237.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Feed7c6f97cbd307429326765c9d87e528e23edbc-720x237.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Feed7c6f97cbd307429326765c9d87e528e23edbc-720x237.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Unrelated question.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">We have an <a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/steps/iterative_planner/guardrails.py" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">example guardrails implementation here</a>. Beyond simply rerouting irrelevant questions, the initial guardrails step can also be used to help educate users by guiding them toward the types of questions they can ask, showcasing the available tools, and demonstrating how to use them effectively.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In the following example, we also highlight the importance of adding a process to map values from user input to the database. This step is crucial for ensuring that user-provided information aligns with the database schema, enabling accurate query execution and minimizing errors caused by mismatched or ambiguous data.</p><figure><img alt="" loading="lazy" width="360" height="114.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F16a8fd5d7b3415e7cfcd83aa38a6798c1aa21909-720x229.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F16a8fd5d7b3415e7cfcd83aa38a6798c1aa21909-720x229.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F16a8fd5d7b3415e7cfcd83aa38a6798c1aa21909-720x229.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Mapping values to database.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This is an example where a user asks for “Science Fiction” movies. The issue arises because the genre is stored as “Sci-Fi” in the database, causing the query to return no results.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">What’s often overlooked is the presence of null values. In real-world data, null values are common and must be accounted for, especially when performing operations like sorting or similar tasks. Failing to handle them properly can lead to unexpected results or errors.</p><figure><img alt="" loading="lazy" width="360" height="200" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe94ca0ddc7d0c944f2b4f71d08e63f95b51b5ff4-720x400.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe94ca0ddc7d0c944f2b4f71d08e63f95b51b5ff4-720x400.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe94ca0ddc7d0c944f2b4f71d08e63f95b51b5ff4-720x400.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Dealing with null values.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In this example, we get a random movie with <code class="SanityPortableText_inlineCode__cI85z">Null</code> value for their rating. To solve this problem, the query would need to have an additional clause <code class="SanityPortableText_inlineCode__cI85z">WHERE m.imdbRating IS NOT NULL</code> .</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are also cases where the missing information isn’t just a data issue but a schema limitation. For example, if we’re asking for Oscar-winning movies, but the schema doesn’t include any information about awards, the query simply cannot return the desired results.</p><figure><img alt="" loading="lazy" width="360" height="137.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ffa69f770c7d04ba2d2a036740922a145d4af29c0-720x275.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ffa69f770c7d04ba2d2a036740922a145d4af29c0-720x275.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Ffa69f770c7d04ba2d2a036740922a145d4af29c0-720x275.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Missing data.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Since LLMs are trained to please the user, the LLM still comes with something that fits the the schema but is invalid. I don’t know yet how to best handle such examples.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The last thing I want to mention is the query planning part. Remember before I used the following plan query to answer the question:</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu"><em>Who made more movies in the 2000s, Tom Hanks or Tom Cruise, and for the winner find their highest profiting movie.</em></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">The plan was:</p><pre><code>plan = [
<span class="hljs-comment"># 2 steps in parallel</span>
    [
        <span class="hljs-string">&quot;List all movies made by Tom Hanks in the 2000s.&quot;</span>,
        <span class="hljs-string">&quot;List all movies made by Tom Cruise in the 2000s.&quot;</span>,
    ],
<span class="hljs-comment"># Second step</span>
    [<span class="hljs-string">&quot;Find the highest profiting movie among winner of step 1&quot;</span>],
]</code></pre><p class="Text_text__zPO0D Text_text-size-16__PkjFu">It looks impressive, but the reality is that Cypher is highly flexible, and GPT-4o can handle this in a single query.</p><figure><img alt="" loading="lazy" width="360" height="206.5" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbca7f3e7169554dfd488bb9b9469550d5ac91f9d-720x413.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbca7f3e7169554dfd488bb9b9469550d5ac91f9d-720x413.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fbca7f3e7169554dfd488bb9b9469550d5ac91f9d-720x413.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">I’d say parallelism is definitely overkill in this case. You can include a query planner if you’re dealing with complex question types that truly require it, but keep in mind that many multi-hop questions can be efficiently handled with a single Cypher statement.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">This example highlights a different issue: the final answer is ambiguous because the LLM was only provided with limited information. Specifically, the winner, Tom Cruise, for the movie <em>War of the Worlds</em>. In this case, the reasoning already occurred within the database, so the LLM isn’t required to handle that logic. However, LLMs tend to operate this way by default, which underscores the importance of providing the LLM with the full context to ensure accurate and unambiguous responses.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Lastly, you will also have to think about how to handle questions that return a lot of results.</p><figure><img alt="" loading="lazy" width="349.5" height="338" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F4c7483c0e34efe784e7c01ba5115ac9b3b694a49-699x676.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=384&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F4c7483c0e34efe784e7c01ba5115ac9b3b694a49-699x676.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F4c7483c0e34efe784e7c01ba5115ac9b3b694a49-699x676.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=750&amp;q=75"/><figcaption>Returning a lot of results.</figcaption></figure><p class="Text_text__zPO0D Text_text-size-16__PkjFu">In our implementations, we enforce a hard limit of 100 records for results. While this helps manage data volume, it can still be excessive in some cases and may even mislead the LLM during its reasoning process.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Additionally, all the agents presented in this blog are not conversational. You’d likely need a question-rewriting step at the beginning to make them conversational, could be part of the guardrails step. And if you have a large graph schema that you cannot pass it whole in the prompt, you have to come up with a system that dynamically fetches relevant graph schema.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">There are a lot of things to be aware of when going to production!</p><h2 class="Text_text__zPO0D Text_text-size-48__A2f8Q">Summary</h2><p class="Text_text__zPO0D Text_text-size-16__PkjFu">Agents can be incredibly useful, but it’s best to start simple and avoid diving into overly complex implementations from the beginning. Focus on establishing a solid benchmark to effectively evaluate and compare different architectures. When it comes to tool outputs, consider minimizing their use or sticking to the simplest tools possible, as many agents struggle to handle tool outputs effectively, often requiring manual parsing.</p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">I would love to see some of the implementations you can come up with! Additionally, you can connect the project to your Neo4j database and start experimenting.</p><ul><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://github.com/tomasonjo-labs/text2cypher_llama_agent?source=post_page-----b81f8d372f19--------------------------------" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">Check this code out on GitHub</a></li><li class="Text_text__zPO0D Text_text-size-16__PkjFu"><a href="https://text2cypher-llama-agent.up.railway.app/?source=post_page-----b81f8d372f19--------------------------------" rel="noreferrer noopener" class="SanityPortableText_link__QA4Ze">And try out the web UI</a></li></ul></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9c496e451841627addd5e7afd5428e907bb9e5e4-1256x634.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9c496e451841627addd5e7afd5428e907bb9e5e4-1256x634.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F9c496e451841627addd5e7afd5428e907bb9e5e4-1256x634.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/constructing-a-knowledge-graph-with-llamaindex-and-memgraph">Constructing a Knowledge Graph with LlamaIndex and Memgraph</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-11-21</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F257208ac7bda29e95139ab4736474d8022317f37-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F257208ac7bda29e95139ab4736474d8022317f37-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F257208ac7bda29e95139ab4736474d8022317f37-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/customizing-property-graph-index-in-llamaindex">Customizing property graph index in LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-06-11</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe8afb8b2b1304f867e0becfc4d5ddbbd9dd94ec1-1784x1044.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe8afb8b2b1304f867e0becfc4d5ddbbd9dd94ec1-1784x1044.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe8afb8b2b1304f867e0becfc4d5ddbbd9dd94ec1-1784x1044.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms">Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-05-29</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2025-01-15T17:51:00Z","_id":"ee6dffc8-decc-43f7-ae7b-3085570313eb","_rev":"05dtDS0H5iRVsxYMarZmzK","_type":"blogPost","_updatedAt":"2025-05-21T20:40:24Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:51:08Z","_id":"72d3c868-ff34-414f-bef2-e8c64c71164d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Tomaz Bratanic","slug":{"_type":"slug","current":"tomaz-bratanic"}}],"featured":false,"image":{"_type":"image","asset":{"_ref":"image-a9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516-webp","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/a9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516.webp","publishedDate":"2025-01-15","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-9c496e451841627addd5e7afd5428e907bb9e5e4-1256x634-png","_type":"reference"}},"publishedDate":"2024-11-21","slug":"constructing-a-knowledge-graph-with-llamaindex-and-memgraph","title":"Constructing a Knowledge Graph with LlamaIndex and Memgraph"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-257208ac7bda29e95139ab4736474d8022317f37-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-06-11","slug":"customizing-property-graph-index-in-llamaindex","title":"Customizing property graph index in LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e8afb8b2b1304f867e0becfc4d5ddbbd9dd94ec1-1784x1044-png","_type":"reference"}},"publishedDate":"2024-05-29","slug":"introducing-the-property-graph-index-a-powerful-new-way-to-build-knowledge-graphs-with-llms","title":"Introducing the Property Graph Index: A Powerful New Way to Build Knowledge Graphs with LLMs"}],"slug":{"_type":"slug","current":"building-knowledge-graph-agents-with-llamaindex-workflows"},"tags":[{"_createdAt":"2024-05-29T17:02:40Z","_id":"067170d7-e13f-45c7-bf89-74d521336787","_rev":"9mesbPtC9rOYXgtTRMRqBa","_type":"blogTag","_updatedAt":"2024-05-29T17:02:40Z","slug":{"_type":"slug","current":"knowledge-graphs"},"title":"Knowledge Graphs"}],"text":[{"_key":"cfbac97b7af7","_type":"block","children":[{"_key":"0700041700720","_type":"span","marks":[],"text":"Retrieval-augmented generation (RAG) is here to stay, and for good reason. It’s a powerful framework that blends advanced language models with targeted information retrieval techniques, enabling quicker access to relevant data and producing more accurate, context-aware responses. While RAG applications often focus on "},{"_key":"0700041700721","_type":"span","marks":["strong"],"text":"unstructured data"},{"_key":"0700041700722","_type":"span","marks":[],"text":", I’m a big fan of integrating "},{"_key":"0700041700723","_type":"span","marks":["strong"],"text":"structured data"},{"_key":"0700041700724","_type":"span","marks":[],"text":" into the mix, a vital yet frequently overlooked approach. One of my favorite ways to do this is by leveraging graph databases like "},{"_key":"0700041700725","_type":"span","marks":["8b035a4fd56e"],"text":"Neo4j"},{"_key":"0700041700726","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"8b035a4fd56e","_type":"link","href":"https://neo4j.com/"}],"style":"normal"},{"_key":"3fcff957852f","_type":"block","children":[{"_key":"f2eeb851eccd0","_type":"span","marks":[],"text":"Often, the go-to approach for retrieving data from a graph is "},{"_key":"f2eeb851eccd1","_type":"span","marks":["strong"],"text":"text2cypher"},{"_key":"f2eeb851eccd2","_type":"span","marks":[],"text":", where natural language queries are automatically converted into Cypher statements to query the graph database. This technique relies on a language model (or rule-based system) that interprets user queries, infers their underlying intent, and translates them into valid Cypher queries, enabling RAG applications to retrieve the relevant information from the knowledge graph and produce accurate answers."}],"markDefs":[],"style":"normal"},{"_key":"a1a6537b608f","_type":"image","asset":{"_ref":"image-3dc91aa952bae365e07cb40a463020e009598182-700x345-webp","_type":"reference"},"caption":"Generating Cypher using LLMs. Image credits: https://neo4j.com/developer-blog/fine-tuned-text2cypher-2024-model/"},{"_key":"8e29479c31bb","_type":"block","children":[{"_key":"1cb8fb21ab510","_type":"span","marks":[],"text":"Text2cypher offers remarkable flexibility because it allows users to formulate questions in natural language without having to know the underlying graph schema or Cypher syntax. However, due to the nuances of language interpretation and the need for precise schema-specific details, its accuracy can still be lacking as shown in "},{"_key":"9a8bc39f9a49","_type":"span","marks":["5033a8149799"],"text":"this text2cypher article"},{"_key":"dd2a3e465b45","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"5033a8149799","_type":"link","href":"https://medium.com/neo4j/benchmarking-using-the-neo4j-text2cypher-2024-dataset-d77be96ab65a"}],"style":"normal"},{"_key":"d0d5f5f6f0c7","_type":"block","children":[{"_key":"3d29692039890","_type":"span","marks":[],"text":"The most important results of the benchmark are shown in the following visualization."}],"markDefs":[],"style":"normal"},{"_key":"de9d492f60cb","_type":"image","asset":{"_ref":"image-c75a9d32eb3787339408e3581f7ccda9faaa1dac-700x591-webp","_type":"reference"},"caption":"Text2cypher benchmark results. Image from https://medium.com/neo4j/benchmarking-using-the-neo4j-text2cypher-2024-dataset-d77be96ab65a"},{"_key":"b0e414b01ef5","_type":"block","children":[{"_key":"3e462cadfb740","_type":"span","marks":[],"text":"From a high-level perspective, the benchmark compares three groups of models:"}],"markDefs":[],"style":"normal"},{"_key":"7c1b8b1cfd54","_type":"block","children":[{"_key":"882939ee53c40","_type":"span","marks":[],"text":"Fine-tuned models for text2cypher tasks"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"414079c8fda8","_type":"block","children":[{"_key":"caf35afd6b3f0","_type":"span","marks":[],"text":"Open foundational models"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"0f7a34102682","_type":"block","children":[{"_key":"2106b69ec96b0","_type":"span","marks":[],"text":"Closed foundational models"}],"level":1,"listItem":"bullet","markDefs":[],"style":"normal"},{"_key":"9a9d8822f3e8","_type":"block","children":[{"_key":"aae386f731df0","_type":"span","marks":[],"text":"The benchmark evaluates their performance on generating correct Cypher queries using two metrics: GoogleBLEU (top chart) and ExactMatch (bottom chart)."}],"markDefs":[],"style":"normal"},{"_key":"0401328349f5","_type":"block","children":[{"_key":"5ed30d8a0a640","_type":"span","marks":[],"text":"The "},{"_key":"5ed30d8a0a641","_type":"span","marks":["em"],"text":"GoogleBLEU"},{"_key":"5ed30d8a0a642","_type":"span","marks":[],"text":" metric measures the degree of overlap (in terms of n-grams) between the generated query and a reference query. Higher scores generally indicate closer alignment to the reference, but they do not necessarily guarantee that the query will run correctly in a database context."}],"markDefs":[],"style":"normal"},{"_key":"70430ebefc63","_type":"block","children":[{"_key":"947f3e69b4800","_type":"span","marks":["em"],"text":"ExactMatch"},{"_key":"947f3e69b4801","_type":"span","marks":[],"text":", on the other hand, is an execution-based metric. It indicates the percentage of generated queries that exactly match the correct query text, implying they produce the same results when executed. This makes ExactMatch a stricter measure of correctness and more directly tied to the actual utility of the query in a real-world setting."}],"markDefs":[],"style":"normal"},{"_key":"9a68eacd74a8","_type":"block","children":[{"_key":"078f493ee56d0","_type":"span","marks":[],"text":"Despite some promising results with fine-tuning, the overall accuracy levels illustrate that text2cypher remains an evolving technology. Some models still struggle to generate fully correct queries in every case, underscoring the need for further improvement in this area."}],"markDefs":[],"style":"normal"},{"_key":"6f92e8fbf8c8","_type":"block","children":[{"_key":"d9f0916877fb0","_type":"span","marks":[],"text":"In this post, we’ll experiment with "},{"_key":"d9f0916877fb1","_type":"span","marks":["9b81b9009974"],"text":"LlamaIndex Workflows"},{"_key":"d9f0916877fb2","_type":"span","marks":[],"text":" to implement more agentic strategies for text2cypher. Instead of relying on a single-shot query generation, which is typically how most benchmarks are run, we’ll try a multi-step approach that allows for retries or alternative query formulations. By incorporating these extra steps and fallback options, we’re aiming to boost overall accuracy and reduce the instances of flawed Cypher generation."}],"markDefs":[{"_key":"9b81b9009974","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/workflow/"}],"style":"normal"},{"_key":"e457ba99a8fe","_type":"block","children":[{"_key":"52418fb3f4450","_type":"span","marks":["em"],"text":"The code is available on"},{"_key":"52418fb3f4451","_type":"span","marks":[],"text":" "},{"_key":"52418fb3f4452","_type":"span","marks":["1eeb85e47b4a","em"],"text":"GitHub"},{"_key":"52418fb3f4453","_type":"span","marks":["em"],"text":". We also have a hosted version of the application available"},{"_key":"52418fb3f4454","_type":"span","marks":[],"text":" "},{"_key":"52418fb3f4455","_type":"span","marks":["58dbad92e522","em"],"text":"here"},{"_key":"52418fb3f4456","_type":"span","marks":["em"],"text":". Thanks to"},{"_key":"52418fb3f4457","_type":"span","marks":[],"text":" "},{"_key":"52418fb3f4458","_type":"span","marks":["66e8b7c790f5","em"],"text":"Anej Gorkic"},{"_key":"52418fb3f4459","_type":"span","marks":[],"text":" "},{"_key":"52418fb3f44510","_type":"span","marks":["em"],"text":"for contributing to the application and helping with debugging :)"}],"markDefs":[{"_key":"1eeb85e47b4a","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent"},{"_key":"58dbad92e522","_type":"link","href":"https://text2cypher-llama-agent.up.railway.app/"},{"_key":"66e8b7c790f5","_type":"link","href":"https://github.com/easwee"}],"style":"normal"},{"_key":"b7a7d3cfe13a","_type":"image","asset":{"_ref":"image-deec5a7d4af4bb833277ecc97113d8c4e844c35a-720x408-webp","_type":"reference"},"caption":"Hosted web application with all the agents is available on: https://text2cypher-llama-agent.up.railway.app/"},{"_key":"b9b9de8e97c3","_type":"block","children":[{"_key":"45ca879c823a0","_type":"span","marks":[],"text":"LlamaIndex Workflows"}],"markDefs":[],"style":"h2"},{"_key":"4efc5228a477","_type":"block","children":[{"_key":"26d4d510d6300","_type":"span","marks":["04922c618cc7"],"text":"LlamaIndex Workflows"},{"_key":"26d4d510d6301","_type":"span","marks":[],"text":" are a practical way to organize multi-step AI processes by connecting different operations through an event-driven system. They help break down complex tasks into smaller, manageable pieces that can communicate with each other in a structured way. Each step in a workflow handles specific events and produces new ones, creating a chain of operations that can accomplish tasks like document processing, question answering, or content generation. The system handles the coordination between steps automatically, making it easier to build and maintain complex AI applications."}],"markDefs":[{"_key":"04922c618cc7","_type":"link","href":"https://docs.llamaindex.ai/en/stable/module_guides/workflow/"}],"style":"normal"},{"_key":"66e94958283e","_type":"block","children":[{"_key":"be6b097d3ef00","_type":"span","marks":[],"text":"Naive text2cypher flow"}],"markDefs":[],"style":"h3"},{"_key":"83925b076b3e","_type":"block","children":[{"_key":"05d584ee51710","_type":"span","marks":[],"text":"The naive text2cypher architecture is a streamlined approach to converting natural language questions into Cypher queries for Neo4j graph databases. It operates through a three-stage workflow: first, it generates a Cypher query from the input question using few-shot learning with similar examples stored in a vector database. The system then executes the generated Cypher query against the graph database. Finally, it processes the database results through a language model to generate a natural language response that directly answers the original question. This architecture maintains a simple yet effective pipeline, leveraging vector similarity search for example fewshot retrieval and LLM for both Cypher query generation and response formatting."}],"markDefs":[],"style":"normal"},{"_key":"d584510e6cfc","_type":"block","children":[{"_key":"1e922efa050c0","_type":"span","marks":[],"text":"Here is the visualized naive text2cypher workflow."}],"markDefs":[],"style":"normal"},{"_key":"77cb7fa638f0","_type":"image","asset":{"_ref":"image-54fea43edd1e72de2f07d9d584ecf5d531bef2ea-719x386-webp","_type":"reference"},"caption":"Naive text2cypher flow."},{"_key":"22f44c157150","_type":"block","children":[{"_key":"b79483bcdfd70","_type":"span","marks":[],"text":"It is worth noting that most Neo4j schema generation methods struggle with "},{"_key":"b79483bcdfd71","_type":"span","marks":["em"],"text":"multi-labeled nodes"},{"_key":"b79483bcdfd72","_type":"span","marks":[],"text":". This issue arises not only due to added complexity but also because of the combinatorial explosion of labels, which can overwhelm the prompt. To mitigate this, we exclude the "},{"_key":"b79483bcdfd73","_type":"span","marks":["code"],"text":"Actor"},{"_key":"b79483bcdfd74","_type":"span","marks":[],"text":" and "},{"_key":"b79483bcdfd75","_type":"span","marks":["code"],"text":"Director"},{"_key":"b79483bcdfd76","_type":"span","marks":[],"text":" labels in the schema generation process."}],"markDefs":[],"style":"normal"},{"_key":"31cee8778b72","_type":"codeBlock","code":"schema = graph_store.get_schema_str(exclude_types=[\"Actor\", \"Director\"])","language":"python"},{"_key":"ac611c975d75","_type":"block","children":[{"_key":"93618dbdb6740","_type":"span","marks":[],"text":"The pipeline starts with the "},{"_key":"93618dbdb6741","_type":"span","marks":["code"],"text":"generate_cypher"},{"_key":"93618dbdb6742","_type":"span","marks":[],"text":"step."}],"markDefs":[],"style":"normal"},{"_key":"53f64c9a56eb","_type":"codeBlock","code":"@step\nasync def generate_cypher(self, ctx: Context, ev: StartEvent) -\u003e ExecuteCypherEvent:\n    question = ev.input\n    # Cypher query generation using an LLM\n    cypher_query = await generate_cypher_step(\n        self.llm, question, self.few_shot_retriever\n    )\n    # Streaming event information to the web UI.\n    ctx.write_event_to_stream(\n        SseEvent(\n            label=\"Cypher generation\",\n            message=f\"Generated Cypher: {cypher_query}\",\n        )\n    )\n\n    # Return for the next step\n    return ExecuteCypherEvent(question=question, cypher=cypher_query)","language":"python"},{"_key":"e41905cf3921","_type":"block","children":[{"_key":"244255a021430","_type":"span","marks":[],"text":"The generate_cypher step takes a natural language question and transforms it into a Cypher query by utilizing a language model and retrieving similar examples from a vector store. The step also streams the generated Cypher query back to the user interface in real-time, providing immediate feedback on the query generation process. You can inspect the whole code and prompts "},{"_key":"244255a021431","_type":"span","marks":["c7187e48600b"],"text":"here"},{"_key":"244255a021432","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"c7187e48600b","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/naive_text2cypher.py"}],"style":"normal"},{"_key":"674bda644e8e","_type":"block","children":[{"_key":"b7a0f4686f6e0","_type":"span","marks":[],"text":"Naive text2cypher with retry flow"}],"markDefs":[],"style":"h2"},{"_key":"924f2df0bae3","_type":"block","children":[{"_key":"08ad990a13ae0","_type":"span","marks":[],"text":"This enhanced version of text2cypher with retry builds upon the original architecture by adding a self-correction mechanism. When a generated Cypher query fails to execute, instead of failing outright, the system attempts to fix the query by feeding the error information back to the language model in the "},{"_key":"08ad990a13ae1","_type":"span","marks":["code"],"text":"CorrectCypherEvent"},{"_key":"08ad990a13ae2","_type":"span","marks":[],"text":"step. This makes the system more resilient and capable of handling initial mistakes, similar to how a human might revise their approach after receiving error feedback."}],"markDefs":[],"style":"normal"},{"_key":"64009e39ccbd","_type":"block","children":[{"_key":"3c5be61990820","_type":"span","marks":[],"text":"Here is the visualized naive text2cypher with retry workflow."}],"markDefs":[],"style":"normal"},{"_key":"50723a141206","_type":"image","asset":{"_ref":"image-17544d9a7d2a51a5ee51e4f4429dd6808867e5ef-720x391-webp","_type":"reference"},"caption":"Naive text2cypher with retry flow."},{"_key":"822d0b7b1e4b","_type":"block","children":[{"_key":"82349ab0a6d30","_type":"span","marks":[],"text":"Let’s take a look at the "},{"_key":"82349ab0a6d31","_type":"span","marks":["code"],"text":"ExecuteCypherEvent"},{"_key":"82349ab0a6d32","_type":"span","marks":[],"text":" ."}],"markDefs":[],"style":"normal"},{"_key":"ffa20c53700d","_type":"codeBlock","code":"@step\nasync def execute_query(\n    self, ctx: Context, ev: ExecuteCypherEvent\n) -\u003e SummarizeEvent | CorrectCypherEvent:\n    # Get global var\n    retries = await ctx.get(\"retries\")\n    try:\n        database_output = str(graph_store.structured_query(ev.cypher))\n    except Exception as e:\n        database_output = str(e)\n        # Retry\n        if retries \u003c self.max_retries:\n            await ctx.set(\"retries\", retries + 1)\n            return CorrectCypherEvent(\n                question=ev.question, cypher=ev.cypher, error=database_output\n            )\n\n    return SummarizeEvent(\n        question=ev.question, cypher=ev.cypher, context=database_output\n    )","language":"python"},{"_key":"3be29b9080ec","_type":"block","children":[{"_key":"fdadc9d396810","_type":"span","marks":[],"text":"The execute function first attempts to run the query, and if successful, passes the results forward for summarization. However, if something goes wrong, it doesn’t give up immediately — instead, it checks if it has any retry attempts left and, if so, sends the query back for correction along with information about what went wrong. This creates a more forgiving system that can learn from its mistakes, much like how we might revise our approach after receiving feedback. You can inspect the whole code and prompts "},{"_key":"fdadc9d396811","_type":"span","marks":["8bac9bda6604"],"text":"here"},{"_key":"fdadc9d396812","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"8bac9bda6604","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/naive_text2cypher_retry.py"}],"style":"normal"},{"_key":"6b7af7a5f8c2","_type":"block","children":[{"_key":"94c6d865c0eb0","_type":"span","marks":[],"text":"Naive text2cypher with retry and evaluation flow"}],"markDefs":[],"style":"h3"},{"_key":"5037df435056","_type":"block","children":[{"_key":"1381919102130","_type":"span","marks":[],"text":"Building on the "},{"_key":"1381919102131","_type":"span","marks":["strong"],"text":"naive text2cypher with retry"},{"_key":"1381919102132","_type":"span","marks":[],"text":" flow, this enhanced version adds an "},{"_key":"1381919102133","_type":"span","marks":["strong"],"text":"evaluation"},{"_key":"1381919102134","_type":"span","marks":[],"text":" phase that checks if the query results are sufficient to answer the user’s question. If the results are deemed inadequate, the system sends the query back for correction with information on how to improve it. If the results are acceptable, the flow proceeds to the final summarization step. This extra layer of validation further bolsters the resilience of the pipeline, ensuring that the user ultimately receives the most accurate and complete answer possible."}],"markDefs":[],"style":"normal"},{"_key":"efab90c4326a","_type":"image","asset":{"_ref":"image-a93183fe7953a69d1f1153941e7e7233b8f9f3cd-709x449-webp","_type":"reference"},"caption":"Naive text2cypher with retry and evaluation flow."},{"_key":"82e268b3304e","_type":"block","children":[{"_key":"e7f7a937f4be0","_type":"span","marks":[],"text":"The additional evaluation step is implemented in the following manner:"}],"markDefs":[],"style":"normal"},{"_key":"2caf37462edb","_type":"codeBlock","code":"@step\nasync def evaluate_context(\n    self, ctx: Context, ev: EvaluateEvent\n) -\u003e SummarizeEvent | CorrectCypherEvent:\n    # Get global var\n    retries = await ctx.get(\"retries\")\n    evaluation = await evaluate_database_output_step(\n        self.llm, ev.question, ev.cypher, ev.context\n    )\n    if retries \u003c self.max_retries and not evaluation == \"Ok\":\n        await ctx.set(\"retries\", retries + 1)\n        return CorrectCypherEvent(\n            question=ev.question, cypher=ev.cypher, error=evaluation\n        )\n    return SummarizeEvent(\n        question=ev.question, cypher=ev.cypher, context=ev.context\n    )","language":"python"},{"_key":"5bea593fd7c5","_type":"block","children":[{"_key":"16c4a74589170","_type":"span","marks":[],"text":"The function "},{"_key":"16c4a74589171","_type":"span","marks":["code"],"text":"evaluate_check"},{"_key":"16c4a74589172","_type":"span","marks":[],"text":"is a simple check that determines whether the query results adequately address the user’s question. If the evaluation indicates the results are insufficient and there are retry attempts remaining, it returns a "},{"_key":"16c4a74589173","_type":"span","marks":["code"],"text":"CorrectCypherEvent"},{"_key":"16c4a74589174","_type":"span","marks":[],"text":"so the query can be refined. Otherwise, it proceeds with a "},{"_key":"16c4a74589175","_type":"span","marks":["code"],"text":"SummarizeEvent"},{"_key":"16c4a74589176","_type":"span","marks":[],"text":",indicating that the results are suitable for final summarization."}],"markDefs":[],"style":"normal"},{"_key":"7691b53f8767","_type":"block","children":[{"_key":"9fef27a494be0","_type":"span","marks":["em"],"text":"I later realized it would be an excellent idea to capture instances where the flow successfully self-healed by correcting invalid Cypher statements. These examples could then be leveraged as dynamic few-shot prompts for future Cypher generation. This approach would enable the agent to not only self-heal but also continuously self-learn and improve over time. The example code to store these fewshot examples can be found"},{"_key":"9fef27a494be1","_type":"span","marks":[],"text":" "},{"_key":"9fef27a494be2","_type":"span","marks":["654f89829559","em"],"text":"here"},{"_key":"9fef27a494be3","_type":"span","marks":[],"text":" "},{"_key":"9fef27a494be4","_type":"span","marks":["em"],"text":"and is implemented currently for this flow only (as it gives the best self-healing accuracy)."}],"markDefs":[{"_key":"654f89829559","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/text2cypher_retry_check.py#L163"}],"style":"normal"},{"_key":"10296bfb96e0","_type":"codeBlock","code":"@step\nasync def summarize_answer(self, ctx: Context, ev: SummarizeEvent) -\u003e StopEvent:\n    retries = await ctx.get(\"retries\")\n    # If retry was successful:\n    if retries \u003e 0 and check_ok(ev.evaluation):\n        # print(f\"Learned new example: {ev.question}, {ev.cypher}\")\n        # Store success retries to be used as fewshots!\n        store_fewshot_example(ev.question, ev.cypher, self.llm.model)","language":"python"},{"_key":"e17c710de60a","_type":"block","children":[{"_key":"9c6f3a5e4c960","_type":"span","marks":[],"text":"Iterative planner flow"}],"markDefs":[],"style":"h3"},{"_key":"68122a623661","_type":"block","children":[{"_key":"3305c5e25ab80","_type":"span","marks":[],"text":"The final flow is the most complex, and coincidentally, the one I ambitiously designed first. I’ve kept it in the code so you can learn from my exploration."}],"markDefs":[],"style":"normal"},{"_key":"adfe7cbab472","_type":"block","children":[{"_key":"f11a7c092e5e0","_type":"span","marks":[],"text":"The iterative planner flow introduces a more sophisticated approach by implementing an "},{"_key":"f11a7c092e5e1","_type":"span","marks":["strong"],"text":"iterative planning system"},{"_key":"f11a7c092e5e2","_type":"span","marks":[],"text":". Instead of directly generating a Cypher query, it first creates a plan of sub-queries, validates each subquery Cypher statement before execution, and includes an information checking mechanism that can modify the plan if the initial results are insufficient. The system can make up to three iterations of information gathering, each time refining its approach based on previous results. This creates a more thorough question-answering system that can handle complex queries by breaking them down into manageable steps and validating the information at each stage."}],"markDefs":[],"style":"normal"},{"_key":"61f0326bec01","_type":"block","children":[{"_key":"7b102333e7d20","_type":"span","marks":[],"text":"Here is the visualized iterative planner workflow."}],"markDefs":[],"style":"normal"},{"_key":"91b9e7171ec2","_type":"image","asset":{"_ref":"image-a9bea7ec41e5dc8374f7b0ceb81ab0ac0c8ed51d-720x516-webp","_type":"reference"},"caption":"Iterative planning flow."},{"_key":"3aded7f8b7e8","_type":"block","children":[{"_key":"9bd8d540c41d0","_type":"span","marks":[],"text":"Let’s examine the query planner prompt. I was quite ambitious when I began. I expected the LLMs to produce the following response."}],"markDefs":[],"style":"normal"},{"_key":"0898e9b93e2a","_type":"codeBlock","code":"class SubqueriesOutput(BaseModel):\n    \"\"\"Defines the output format for transforming a question into parallel-optimized retrieval steps.\"\"\"\n\n    plan: List[List[str]] = Field(\n        description=(\n            \"\"\"A list of query groups where:\n        - Each group (inner list) contains queries that can be executed in parallel\n        - Groups are ordered by dependency (earlier groups must be executed before later ones)\n        - Each query must be a specific information retrieval request\n        - Split into multiple steps only if intermediate results return ≤25 values\n        - No reasoning or comparison tasks, only data fetching queries\"\"\"\n        )\n    )","language":"python"},{"_key":"49dc299a92a7","_type":"block","children":[{"_key":"b356113f06270","_type":"span","marks":[],"text":"The output represents a structured plan for transforming a complex question into sequential and parallel query steps. Each step consists of a group of queries that can be executed in parallel, with later steps depending on the results of earlier ones. Queries are strictly for information retrieval, avoiding reasoning tasks, and are split into smaller steps if needed to manage result size. For example, the following plan begins by listing movies for two actors in parallel, followed by a step that identifies the highest-grossing movie from the results of the first step."}],"markDefs":[],"style":"normal"},{"_key":"679f1bd50622","_type":"codeBlock","code":"plan = [\n# 2 steps in parallel\n    [\n        \"List all movies made by Tom Hanks in the 2000s.\",\n        \"List all movies made by Tom Cruise in the 2000s.\",\n    ],\n# Second step\n    [\"Find the highest profiting movie among winner of step 1\"],\n]","language":"python"},{"_key":"8a627d949c8c","_type":"block","children":[{"_key":"f90539a331840","_type":"span","marks":[],"text":"This idea is undeniably cool. It’s a smart way to break down complex questions into smaller, actionable steps and even leverage parallelism to optimize retrieval. It sounds like the kind of strategy that could really speed things up. But, in practice, expecting LLMs to execute this reliably is a bit ambitious. Parallelism, while efficient in theory, introduces a lot of complexity. Dependencies, intermediate results, and maintaining logical consistency between parallel steps can easily trip up even advanced models. Sequential execution, though less glamorous, is more reliable for now and significantly reduces the cognitive overhead on the model."}],"markDefs":[],"style":"normal"},{"_key":"423db9ff76ba","_type":"block","children":[{"_key":"58770c1afadb0","_type":"span","marks":[],"text":"Additionally, LLMs often struggle with following structured tool outputs like lists of lists, especially when reasoning about dependencies between steps. Here, I’m curious to see how much prompting alone (without tool outputs) can improve the model’s performance on these tasks"}],"markDefs":[],"style":"normal"},{"_key":"94c26000dfd8","_type":"block","children":[{"_key":"4871adf2aaaf0","_type":"span","marks":[],"text":"The code for the iterative planning flow is available "},{"_key":"4871adf2aaaf1","_type":"span","marks":["62d3c6be449b"],"text":"here"},{"_key":"4871adf2aaaf2","_type":"span","marks":[],"text":"."}],"markDefs":[{"_key":"62d3c6be449b","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/iterative_planner.py"}],"style":"normal"},{"_key":"c493e138cfc8","_type":"block","children":[{"_key":"fab3d47d107c0","_type":"span","marks":[],"text":"Benchmarking"}],"markDefs":[],"style":"h2"},{"_key":"f2be614ab7de","_type":"block","children":[{"_key":"418acdeee9100","_type":"span","marks":[],"text":"Creating a benchmark dataset for evaluating text2ypher agents in the LlamaIndex workflow architecture feels like an exciting step forward."}],"markDefs":[],"style":"normal"},{"_key":"e0d9328e6b22","_type":"block","children":[{"_key":"4cc8acdd33830","_type":"span","marks":[],"text":"We sought an alternative to traditional one-shot Cypher execution metrics, such as ExactMatch mentioned in the beginning, which often fall short in capturing the full potential of workflows like iterative planning. In these workflows, multiple steps are employed to refine queries and retrieve relevant information, making single-step execution metrics inadequate."}],"markDefs":[],"style":"normal"},{"_key":"8c9eed9c4980","_type":"block","children":[{"_key":"306a9ff2a6800","_type":"span","marks":[],"text":"That’s why we chose to use "},{"_key":"306a9ff2a6801","_type":"span","marks":["em"],"text":"answer_relevancy"},{"_key":"306a9ff2a6802","_type":"span","marks":[],"text":" from RAGAS — it feels more aligned with what we want to measure. Here, we use an LLM to generate the answer, and then use LLM as a judge to compare it with ground truth. We’ve prepared a custom dataset of about 50 samples, carefully designed to avoid generating overwhelming outputs, meaning database results that are excessively large or detailed. Such outputs can make it difficult for an LLM judge to evaluate relevance effectively, so keeping results short ensures a fair and focused comparison of single- and multi-step workflows."}],"markDefs":[],"style":"normal"},{"_key":"c44717ecdb4f","_type":"block","children":[{"_key":"fd4586c487810","_type":"span","marks":[],"text":"Here are the results."}],"markDefs":[],"style":"normal"},{"_key":"52bc632eaa77","_type":"image","asset":{"_ref":"image-971a7d83e63ffde44854b41f68d7811a277eb688-720x613-webp","_type":"reference"},"caption":"Benchmark results."},{"_key":"b02c4b3560a1","_type":"block","children":[{"_key":"32f34aad15840","_type":"span","marks":[],"text":"Sonnet 3.5, Deepsek-v3, and GPT-4o emerge as the top three models in terms of answer relevancy, each scoring above 0.80. The "},{"_key":"32f34aad15841","_type":"span","marks":["em"],"text":"NaiveText2CypherRetryCheckFlow"},{"_key":"32f34aad15842","_type":"span","marks":[],"text":" tends to produce the highest relevancy overall, while the "},{"_key":"32f34aad15843","_type":"span","marks":["em"],"text":"IterativePlanningFlow"},{"_key":"32f34aad15844","_type":"span","marks":[],"text":" consistently ranks lower (dropping as low as 0.163)."}],"markDefs":[],"style":"normal"},{"_key":"57280f68b692","_type":"block","children":[{"_key":"ad985ac03c4e0","_type":"span","marks":[],"text":"Although o1 model is fairly accurate, it probably isn’t at the top due to multiple timeouts (set at 90 seconds). Deepsek-v3 stands out as especially promising, given its strong scores paired with relatively low latency. Overall, these results underscore the importance not just of raw accuracy, but also of stability and speed in practical deployment scenarios."}],"markDefs":[],"style":"normal"},{"_key":"392af18de3e3","_type":"block","children":[{"_key":"64513792c7c70","_type":"span","marks":[],"text":"Here’s another table where uplift between flows can be easily examined."}],"markDefs":[],"style":"normal"},{"_key":"9a3bc1b7c60b","_type":"image","asset":{"_ref":"image-7f7859da8db515c85e30022d7c0699ac28c3de30-720x197-webp","_type":"reference"},"caption":"Benchmark results."},{"_key":"6fff72053914","_type":"block","children":[{"_key":"b0037fd412e90","_type":"span","marks":[],"text":"Sonnet 3.5 rises steadily from a 0.596 score in NaiveText2CypherFlow to 0.616 in NaiveText2CypherRetryFlow, then makes a bigger leap to 0.843 in NaiveText2CypherRetryCheckFlow. GPT-4o shows a similar pattern overall, moving from 0.622 in NaiveText2CypherFlow down slightly to 0.603 in NaiveText2CypherRetryFlow, but then climbing significantly to 0.837 in NaiveText2CypherRetryCheckFlow. These improvements suggest that adding both a retry mechanism and a final verification step markedly boosts answer relevancy."}],"markDefs":[],"style":"normal"},{"_key":"03a3f087d7db","_type":"block","children":[{"_key":"428d12193ec60","_type":"span","marks":["em"],"text":"The benchmark code can be found"},{"_key":"428d12193ec61","_type":"span","marks":[],"text":" "},{"_key":"428d12193ec62","_type":"span","marks":["f9f72888f2c5","em"],"text":"here"},{"_key":"428d12193ec63","_type":"span","marks":["em"],"text":"."}],"markDefs":[{"_key":"f9f72888f2c5","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/benchmark/benchmark_gridsearch.ipynb"}],"style":"normal"},{"_key":"7718e2d7659f","_type":"block","children":[{"_key":"d061581dce840","_type":"span","marks":["em"],"text":"Please note that benchmark results may vary by at least 5%, meaning you might observe slightly different outcomes and top performers across different runs."}],"markDefs":[],"style":"normal"},{"_key":"c74273f9880c","_type":"block","children":[{"_key":"e0d0847b696f0","_type":"span","marks":[],"text":"Learnings and going to production"}],"markDefs":[],"style":"h2"},{"_key":"94ad979ff6bf","_type":"block","children":[{"_key":"42959ae30e970","_type":"span","marks":[],"text":"This was a two-month project where I learned a lot along the way. One highlight of the project was achieving 84"},{"_key":"42959ae30e971","_type":"span","marks":["strong"],"text":"% relevancy in a test benchmark"},{"_key":"42959ae30e972","_type":"span","marks":[],"text":", which is a significant achievement. However, does that mean you will achieve 84% accuracy in production? "},{"_key":"42959ae30e973","_type":"span","marks":["strong"],"text":"Probably not."}],"markDefs":[],"style":"normal"},{"_key":"5624115ce6df","_type":"block","children":[{"_key":"89ddd868a7f90","_type":"span","marks":[],"text":"Production environments bring their own set of challenges — real-world data is often noisier, more diverse, and less structured than benchmark datasets. Something we haven’t discussed yet, but you’ll see in practice with real applications and users, is the "},{"_key":"89ddd868a7f91","_type":"span","marks":["strong"],"text":"need for production-ready steps"},{"_key":"89ddd868a7f92","_type":"span","marks":[],"text":". This means not just focusing on achieving high accuracy in controlled benchmarks but ensuring the system is reliable, adaptable, and delivers consistent results in real-world conditions."}],"markDefs":[],"style":"normal"},{"_key":"e982959e9ec5","_type":"block","children":[{"_key":"5eed103f17a00","_type":"span","marks":[],"text":"In these settings, you’d need to implement some kind of guardrails to stop unrelated questions from going through the text-to-Cypher pipeline."}],"markDefs":[],"style":"normal"},{"_key":"c6e79fedcb3e","_type":"image","asset":{"_ref":"image-eed7c6f97cbd307429326765c9d87e528e23edbc-720x237-webp","_type":"reference"},"caption":"Unrelated question."},{"_key":"d448f41c5a32","_type":"block","children":[{"_key":"741454f0c6c80","_type":"span","marks":[],"text":"We have an "},{"_key":"741454f0c6c81","_type":"span","marks":["c5dc864d86f8"],"text":"example guardrails implementation here"},{"_key":"741454f0c6c82","_type":"span","marks":[],"text":". Beyond simply rerouting irrelevant questions, the initial guardrails step can also be used to help educate users by guiding them toward the types of questions they can ask, showcasing the available tools, and demonstrating how to use them effectively."}],"markDefs":[{"_key":"c5dc864d86f8","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent/blob/main/app/workflows/steps/iterative_planner/guardrails.py"}],"style":"normal"},{"_key":"887092fd4241","_type":"block","children":[{"_key":"53eb4afe45290","_type":"span","marks":[],"text":"In the following example, we also highlight the importance of adding a process to map values from user input to the database. This step is crucial for ensuring that user-provided information aligns with the database schema, enabling accurate query execution and minimizing errors caused by mismatched or ambiguous data."}],"markDefs":[],"style":"normal"},{"_key":"24904eb60c61","_type":"image","asset":{"_ref":"image-16a8fd5d7b3415e7cfcd83aa38a6798c1aa21909-720x229-webp","_type":"reference"},"caption":"Mapping values to database."},{"_key":"1c835b0d2ef4","_type":"block","children":[{"_key":"eb1086cac2a20","_type":"span","marks":[],"text":"This is an example where a user asks for “Science Fiction” movies. The issue arises because the genre is stored as “Sci-Fi” in the database, causing the query to return no results."}],"markDefs":[],"style":"normal"},{"_key":"e4c13b7b1deb","_type":"block","children":[{"_key":"c33863c01fe60","_type":"span","marks":[],"text":"What’s often overlooked is the presence of null values. In real-world data, null values are common and must be accounted for, especially when performing operations like sorting or similar tasks. Failing to handle them properly can lead to unexpected results or errors."}],"markDefs":[],"style":"normal"},{"_key":"8ecb84dc2a32","_type":"image","asset":{"_ref":"image-e94ca0ddc7d0c944f2b4f71d08e63f95b51b5ff4-720x400-webp","_type":"reference"},"caption":"Dealing with null values."},{"_key":"e2b641231465","_type":"block","children":[{"_key":"b33b50b063a30","_type":"span","marks":[],"text":"In this example, we get a random movie with "},{"_key":"b33b50b063a31","_type":"span","marks":["code"],"text":"Null"},{"_key":"b33b50b063a32","_type":"span","marks":[],"text":" value for their rating. To solve this problem, the query would need to have an additional clause "},{"_key":"b33b50b063a33","_type":"span","marks":["code"],"text":"WHERE m.imdbRating IS NOT NULL"},{"_key":"b33b50b063a34","_type":"span","marks":[],"text":" ."}],"markDefs":[],"style":"normal"},{"_key":"f3d488c68ec4","_type":"block","children":[{"_key":"0852c60300980","_type":"span","marks":[],"text":"There are also cases where the missing information isn’t just a data issue but a schema limitation. For example, if we’re asking for Oscar-winning movies, but the schema doesn’t include any information about awards, the query simply cannot return the desired results."}],"markDefs":[],"style":"normal"},{"_key":"1ec136a8aa6a","_type":"image","asset":{"_ref":"image-fa69f770c7d04ba2d2a036740922a145d4af29c0-720x275-webp","_type":"reference"},"caption":"Missing data."},{"_key":"70954955de17","_type":"block","children":[{"_key":"9d72f40655b70","_type":"span","marks":[],"text":"Since LLMs are trained to please the user, the LLM still comes with something that fits the the schema but is invalid. I don’t know yet how to best handle such examples."}],"markDefs":[],"style":"normal"},{"_key":"fc7e207088d4","_type":"block","children":[{"_key":"857cdca1e7440","_type":"span","marks":[],"text":"The last thing I want to mention is the query planning part. Remember before I used the following plan query to answer the question:"}],"markDefs":[],"style":"normal"},{"_key":"e321c8557245","_type":"block","children":[{"_key":"e77e9f9b80740","_type":"span","marks":["em"],"text":"Who made more movies in the 2000s, Tom Hanks or Tom Cruise, and for the winner find their highest profiting movie."}],"markDefs":[],"style":"normal"},{"_key":"e445c4d7dacf","_type":"block","children":[{"_key":"29af078094200","_type":"span","marks":[],"text":"The plan was:"}],"markDefs":[],"style":"normal"},{"_key":"d4c0133a7fdd","_type":"codeBlock","code":"plan = [\n# 2 steps in parallel\n    [\n        \"List all movies made by Tom Hanks in the 2000s.\",\n        \"List all movies made by Tom Cruise in the 2000s.\",\n    ],\n# Second step\n    [\"Find the highest profiting movie among winner of step 1\"],\n]","language":"python"},{"_key":"af94ae3e3aee","_type":"block","children":[{"_key":"8f25d58518350","_type":"span","marks":[],"text":"It looks impressive, but the reality is that Cypher is highly flexible, and GPT-4o can handle this in a single query."}],"markDefs":[],"style":"normal"},{"_key":"916d8d7fb506","_type":"image","asset":{"_ref":"image-bca7f3e7169554dfd488bb9b9469550d5ac91f9d-720x413-webp","_type":"reference"}},{"_key":"43db2abb3bd8","_type":"block","children":[{"_key":"427a8eda18570","_type":"span","marks":[],"text":"I’d say parallelism is definitely overkill in this case. You can include a query planner if you’re dealing with complex question types that truly require it, but keep in mind that many multi-hop questions can be efficiently handled with a single Cypher statement."}],"markDefs":[],"style":"normal"},{"_key":"08f3bc0bbdab","_type":"block","children":[{"_key":"3e0361fc79670","_type":"span","marks":[],"text":"This example highlights a different issue: the final answer is ambiguous because the LLM was only provided with limited information. Specifically, the winner, Tom Cruise, for the movie "},{"_key":"3e0361fc79671","_type":"span","marks":["em"],"text":"War of the Worlds"},{"_key":"3e0361fc79672","_type":"span","marks":[],"text":". In this case, the reasoning already occurred within the database, so the LLM isn’t required to handle that logic. However, LLMs tend to operate this way by default, which underscores the importance of providing the LLM with the full context to ensure accurate and unambiguous responses."}],"markDefs":[],"style":"normal"},{"_key":"62f410476dd8","_type":"block","children":[{"_key":"0b13c67292a80","_type":"span","marks":[],"text":"Lastly, you will also have to think about how to handle questions that return a lot of results."}],"markDefs":[],"style":"normal"},{"_key":"aeedb7ab89d8","_type":"image","asset":{"_ref":"image-4c7483c0e34efe784e7c01ba5115ac9b3b694a49-699x676-webp","_type":"reference"},"caption":"Returning a lot of results."},{"_key":"9b7c561806e5","_type":"block","children":[{"_key":"8479922b803c0","_type":"span","marks":[],"text":"In our implementations, we enforce a hard limit of 100 records for results. While this helps manage data volume, it can still be excessive in some cases and may even mislead the LLM during its reasoning process."}],"markDefs":[],"style":"normal"},{"_key":"790c54ad39a5","_type":"block","children":[{"_key":"23923ecca67d0","_type":"span","marks":[],"text":"Additionally, all the agents presented in this blog are not conversational. You’d likely need a question-rewriting step at the beginning to make them conversational, could be part of the guardrails step. And if you have a large graph schema that you cannot pass it whole in the prompt, you have to come up with a system that dynamically fetches relevant graph schema."}],"markDefs":[],"style":"normal"},{"_key":"4e62c64e0130","_type":"block","children":[{"_key":"27c1ec68e14c0","_type":"span","marks":[],"text":"There are a lot of things to be aware of when going to production!"}],"markDefs":[],"style":"normal"},{"_key":"74ada3711a06","_type":"block","children":[{"_key":"038217bc74890","_type":"span","marks":[],"text":"Summary"}],"markDefs":[],"style":"h2"},{"_key":"5d5984826110","_type":"block","children":[{"_key":"53f90c1f3cfc0","_type":"span","marks":[],"text":"Agents can be incredibly useful, but it’s best to start simple and avoid diving into overly complex implementations from the beginning. Focus on establishing a solid benchmark to effectively evaluate and compare different architectures. When it comes to tool outputs, consider minimizing their use or sticking to the simplest tools possible, as many agents struggle to handle tool outputs effectively, often requiring manual parsing."}],"markDefs":[],"style":"normal"},{"_key":"cc3d867616e3","_type":"block","children":[{"_key":"251a8267f7e00","_type":"span","marks":[],"text":"I would love to see some of the implementations you can come up with! Additionally, you can connect the project to your Neo4j database and start experimenting."}],"markDefs":[],"style":"normal"},{"_key":"8b9aa86f134e","_type":"block","children":[{"_key":"d0ab658a1323","_type":"span","marks":["4d4e74ec2794"],"text":"Check this code out on GitHub"}],"level":1,"listItem":"bullet","markDefs":[{"_key":"4d4e74ec2794","_type":"link","href":"https://github.com/tomasonjo-labs/text2cypher_llama_agent?source=post_page-----b81f8d372f19--------------------------------"}],"style":"normal"},{"_key":"fe8bd7d8c924","_type":"block","children":[{"_key":"7af00a0032bc","_type":"span","marks":["508517feed9e"],"text":"And try out the web UI"}],"level":1,"listItem":"bullet","markDefs":[{"_key":"508517feed9e","_type":"link","href":"https://text2cypher-llama-agent.up.railway.app/?source=post_page-----b81f8d372f19--------------------------------"}],"style":"normal"}],"title":"Building knowledge graph agents with LlamaIndex Workflows"},"publishedDate":"Invalid Date"},"params":{"slug":"building-knowledge-graph-agents-with-llamaindex-workflows"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"building-knowledge-graph-agents-with-llamaindex-workflows"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>