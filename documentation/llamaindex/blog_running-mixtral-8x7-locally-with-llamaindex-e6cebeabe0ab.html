<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Running Mixtral 8x7 locally with LlamaIndex and Ollama — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Running Mixtral 8x7 locally with LlamaIndex and Ollama — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Running Mixtral 8x7 locally with LlamaIndex and Ollama — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Running Mixtral 8x7 locally with LlamaIndex and Ollama — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="661" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/llamaindex">LlamaIndex</a> <!-- -->•<!-- --> <!-- -->2023-12-21</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Running Mixtral 8x7 locally with LlamaIndex and Ollama</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/mistral-ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Mistral Ai</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ollama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Ollama</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>You may have heard the fuss about the latest release from European AI powerhouse <a href="https://mistral.ai/" rel="noopener ugc nofollow" target="_blank">Mistral AI</a>: it’s called Mixtral 8x7b, a “mixture of experts” model — eight of them, each trained with 7 billion parameters, hence the name. Released originally as a <a href="https://twitter.com/MistralAI/status/1733150512395038967" rel="noopener ugc nofollow" target="_blank">mic-drop tweet</a> they followed up a few days later with a <a href="https://mistral.ai/news/mixtral-of-experts/" rel="noopener ugc nofollow" target="_blank">blog post</a> that showed it matching or exceeding GPT-3.5 as well as the much larger Llama2 70b on a number of benchmarks.</p><p>Here at LlamaIndex we’re naturally fans of open source software, so open models with permissive licenses like Mixtral are right up our alley. We’ve had a few questions about how to get Mixtral working with LlamaIndex, so this post is here to get you up and running with a totally local model.</p><h1>Step 1: Install Ollama</h1><p>Previously getting a local model installed and working was a huge pain, but with the release of <a href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a>, it’s suddenly a snap! Available for MacOS and Linux (and soon on Windows, though you can use it today on Windows via <a href="https://learn.microsoft.com/en-us/windows/wsl/install" rel="noopener ugc nofollow" target="_blank">Windows Subsystem For Linux</a>), it is itself open source and a <a href="https://ollama.ai/download" rel="noopener ugc nofollow" target="_blank">free download</a>.</p><p>Once downloaded, you can get Mixtral with a single command:</p><pre><span id="2746" class="qd os gt qa b bf qe qf l qg qh">ollama run mixtral</span></pre><p>The first time you run this command it will have to download the model, which can take a long time, so go get a snack. Also note that it requires a hefty 48GB of RAM to run smoothly! If that’s too much for your machine, consider using its smaller but still very capable cousin <strong>Mistral 7b</strong>, which you install and run the same way:</p><pre><span id="9686" class="qd os gt qa b bf qe qf l qg qh">ollama run mistral</span></pre><p>We’ll assume you’re using Mixtral for the rest of this tutorial, but Mistral will also work.</p><p>Once the model is running Ollama will automatically let you chat with it. That’s fun, but what’s the point of having a model if it can’t work with your data? That’s where LlamaIndex comes in. The next few steps will take you through the code line by line, but if you’d prefer to save all the copying and pasting, all of this code is available in an <a href="https://github.com/run-llama/mixtral_ollama" rel="noopener ugc nofollow" target="_blank">open-source repo</a> that you can clone to follow along there.</p><h1>Step 2: Install your dependencies</h1><p>You’re going to need LlamaIndex installed, obviously! We’ll also get you going with a handful of other dependencies that are about to come in handy:</p><pre><span id="6d42" class="qd os gt qa b bf qe qf l qg qh">pip install llama-index qdrant_client torch transformers</span></pre><h1>Step 3: Smoke test</h1><p>If you’ve got Ollama running and LlamaIndex properly installed, the following quick script will make sure everything is in order by asking it a quick “smoke test” question in a script all by itself:</p><pre><span id="d12c" class="qd os gt qa b bf qe qf l qg qh"><span class="hljs-comment"># Just runs .complete to make sure the LLM is listening</span>
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> Ollama

llm = Ollama(model=<span class="hljs-string">"mixtral"</span>)
response = llm.complete(<span class="hljs-string">"Who is Laurie Voss?"</span>)
<span class="hljs-built_in">print</span>(response)</span></pre><h1>Step 4: Load some data and index it</h1><p>Now you’re ready to load in some real data! You can use any data you want; in this case I’m using a <a href="https://www.dropbox.com/scl/fi/6sos49fluvfilj3sqcvoj/tinytweets.json?rlkey=qmxlaqp000kmx8zktvaj4u1vh&amp;dl=0" rel="noopener ugc nofollow" target="_blank">small collection of my own tweets</a> which you can download, or use your own! We’re going to be storing our data in the nifty, open source <a href="https://github.com/qdrant/qdrant" rel="noopener ugc nofollow" target="_blank">Qdrant</a> vector database (which is why we got you to install it earlier). Create a new python file, and load in all our dependencies:</p><pre><span id="0599" class="qd os gt qa b bf qe qf l qg qh"><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path
<span class="hljs-keyword">import</span> qdrant_client
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    VectorStoreIndex,
    ServiceContext,
    download_loader,
)
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> Ollama
<span class="hljs-keyword">from</span> llama_index.storage.storage_context <span class="hljs-keyword">import</span> StorageContext
<span class="hljs-keyword">from</span> llama_index.vector_stores.qdrant <span class="hljs-keyword">import</span> QdrantVectorStore</span></pre><p>Then load our tweets out of our JSON file using a nifty JSONReader from <a href="https://llamahub.ai/l/file-json?from=all" rel="noopener ugc nofollow" target="_blank">LlamaHub</a>, our convenient collection of open source data connectors. This will give you a pile of documents ready to be embedded and indexed:</p><pre><span id="9a4d" class="qd os gt qa b bf qe qf l qg qh">JSONReader = download_loader("JSONReader")
loader = JSONReader()
documents = loader.load_data(Path('./data/tinytweets.json'))</span></pre><p>Get Qdrant ready for action by initializing it and passing it into a Storage Context we’ll be using later:</p><pre><span id="dcc0" class="qd os gt qa b bf qe qf l qg qh">client = qdrant_client.QdrantClient(
    path="./qdrant_data"
)
vector_store = QdrantVectorStore(client=client, collection_name="tweets")
storage_context = StorageContext.from_defaults(vector_store=vector_store)</span></pre><p>Now set up our Service Context. We’ll be passing it Mixtral as the LLM so we can test that things are working once we’ve finished indexing; indexing itself doesn’t need Mixtral. By passing <code class="cw qi qj qk qa b">embed_model="local"</code>we’re specifying that LlamaIndex will embed your data locally, which is why you needed <code class="cw qi qj qk qa b">torch</code> and <code class="cw qi qj qk qa b">transformers</code>.</p><pre><span id="b93a" class="qd os gt qa b bf qe qf l qg qh">llm = Ollama(model="mixtral")
service_context = ServiceContext.from_defaults(llm=llm,embed_model="local")</span></pre><p>Now bring it all together: build the index from the documents you loaded using the service and storage contexts you already set up, and give it a query:</p><pre><span id="0767" class="qd os gt qa b bf qe qf l qg qh">index = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)

query_engine = index.as_query_engine()
response = query_engine.query(<span class="hljs-string">"What does the author think about Star Trek? Give details."</span>)
<span class="hljs-built_in">print</span>(response)</span></pre><p>Ollama will need to fire up Mixtral to answer the query, which can take a little while, so be patient! You should get output something like this (but with more details):</p><pre><span id="607d" class="qd os gt qa b bf qe qf l qg qh">Based on the provided context information, the author has a mixed opinion about Star Trek.</span></pre><h1>Verify our index</h1><p>Now to prove it’s not all smoke and mirrors, let’s use our pre-built index. Start a new python file and load in dependencies again:</p><pre><span id="b12a" class="qd os gt qa b bf qe qf l qg qh"><span class="hljs-keyword">import</span> qdrant_client
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    VectorStoreIndex,
    ServiceContext,
)
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> Ollama
<span class="hljs-keyword">from</span> llama_index.vector_stores.qdrant <span class="hljs-keyword">import</span> QdrantVectorStore</span></pre><p>This time we won’t need to load the data, that’s already done! We will need the Qdrant client and of course Mixtral again:</p><pre><span id="48fb" class="qd os gt qa b bf qe qf l qg qh">client = qdrant_client.QdrantClient(
    path="./qdrant_data"
)
vector_store = QdrantVectorStore(client=client, collection_name="tweets")

llm = Ollama(model="mixtral")
service_context = ServiceContext.from_defaults(llm=llm,embed_model="local")</span></pre><p>This time instead of creating our index from documents we load it directly from the vector store using <code class="cw qi qj qk qa b">from_vector_store</code>. We’re also passing <code class="cw qi qj qk qa b">similarity_top_k=20</code> to the query engine; this will mean it will fetch 20 tweets at a time (the default is 2) to get more context and better answer the question.</p><pre><span id="b083" class="qd os gt qa b bf qe qf l qg qh">index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)
query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">20</span>)
response = query_engine.query(<span class="hljs-string">"Does the author like SQL? Give details."</span>)
<span class="hljs-built_in">print</span>(response)</span></pre><h1>Build a little web service</h1><p>It’s no good having an index that just runs as a script! Let’s make an API out of this thing. We’ll need two new dependencies:</p><pre><span id="bd6a" class="qd os gt qa b bf qe qf l qg qh">pip install flask flask-cors</span></pre><p>Load in our dependencies as before into a new file:</p><pre><span id="0499" class="qd os gt qa b bf qe qf l qg qh"><span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, request, jsonify
<span class="hljs-keyword">from</span> flask_cors <span class="hljs-keyword">import</span> CORS, cross_origin
<span class="hljs-keyword">import</span> qdrant_client
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> Ollama
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> (
    VectorStoreIndex,
    ServiceContext,
)
<span class="hljs-keyword">from</span> llama_index.vector_stores.qdrant <span class="hljs-keyword">import</span> QdrantVectorStore</span></pre><p>Get the vector store, the LLM and the index loaded:</p><pre><span id="b229" class="qd os gt qa b bf qe qf l qg qh"># re-initialize the vector store
client = qdrant_client.QdrantClient(
    path="./qdrant_data"
)
vector_store = QdrantVectorStore(client=client, collection_name="tweets")

# get the LLM again
llm = Ollama(model="mixtral")
service_context = ServiceContext.from_defaults(llm=llm,embed_model="local")
# load the index from the vector store
index = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)</span></pre><p>Set up a really basic Flask server:</p><pre><span id="3c1d" class="qd os gt qa b bf qe qf l qg qh">app = Flask(__name__)
cors = CORS(app)
app.config[<span class="hljs-string">'CORS_HEADERS'</span>] = <span class="hljs-string">'Content-Type'</span>

<span class="hljs-comment"># This is just so you can easily tell the app is running</span>
<span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">'/'</span></span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">hello_world</span>():
    <span class="hljs-keyword">return</span> <span class="hljs-string">'Hello, World!'</span></span></pre><p>And add a route that accepts a query (as form data), queries the LLM and returns the response:</p><pre><span id="8542" class="qd os gt qa b bf qe qf l qg qh"><span class="hljs-meta">@app.route(<span class="hljs-params"><span class="hljs-string">'/process_form'</span>, methods=[<span class="hljs-string">'POST'</span>]</span>)</span>
<span class="hljs-meta">@cross_origin()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">process_form</span>():
    query = request.form.get(<span class="hljs-string">'query'</span>)
    <span class="hljs-keyword">if</span> query <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        query_engine = index.as_query_engine(similarity_top_k=<span class="hljs-number">20</span>)
        response = query_engine.query(query)
        <span class="hljs-keyword">return</span> jsonify({<span class="hljs-string">"response"</span>: <span class="hljs-built_in">str</span>(response)})
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> jsonify({<span class="hljs-string">"error"</span>: <span class="hljs-string">"query field is missing"</span>}), <span class="hljs-number">400</span>

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    app.run()</span></pre><p>Note those last two lines, they’re important! <code class="cw qi qj qk qa b">flask run</code> is incompatible with the way LlamaIndex loads dependencies, so you will need to run this API directly like so (assuming your file is called <code class="cw qi qj qk qa b">app.py</code>)</p><pre><span id="c3fa" class="qd os gt qa b bf qe qf l qg qh">python app.py</span></pre><p>With your API up and running, you can use cURL to send a request and verify it:</p><pre><span id="6bda" class="qd os gt qa b bf qe qf l qg qh">curl --location '<span class="hljs-symbol">&amp;lt;</span>http://127.0.0.1:5000/process_form<span class="hljs-symbol">&amp;gt;</span>' \\
--form 'query="What does the author think about Star Trek?"'</span></pre><h1>You’re done!</h1><p>We covered a few things here:</p><ul><li>Getting Ollama to run Mixtral locally</li><li>Using LlamaIndex to query Mixtral 8x7b</li><li>Building and querying an index over your data using Qdrant vector store</li><li>Wrapping your index into a very simple web API</li><li>All open-source, free, and running locally!</li></ul><p>I hope this was a fun, quick introduction to running local models with LlamaIndex!</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"c120a9b6-78b0-4cd2-812c-4ee332f0fae3","_rev":"Ys5IzmCaJ2UnW2RAX7Uc3D","_type":"blogPost","_updatedAt":"2025-05-21T20:39:39Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-20T20:23:12Z","_id":"363ec4e9-0b8f-48d2-ba6a-567a9c527c3d","_rev":"rGZ2nN6K5mjOGJOoWaUhNb","_type":"people","_updatedAt":"2024-02-25T00:45:24Z","image":{"_type":"image","asset":{"_ref":"image-89523511cf20d73e3f10077add50128d077ed520-176x176-png","_type":"reference"}},"name":"LlamaIndex","slug":{"_type":"slug","current":"llamaindex"}}],"featured":false,"htmlContent":"\u003cp\u003eYou may have heard the fuss about the latest release from European AI powerhouse \u003ca href=\"https://mistral.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eMistral AI\u003c/a\u003e: it’s called Mixtral 8x7b, a “mixture of experts” model — eight of them, each trained with 7 billion parameters, hence the name. Released originally as a \u003ca href=\"https://twitter.com/MistralAI/status/1733150512395038967\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emic-drop tweet\u003c/a\u003e they followed up a few days later with a \u003ca href=\"https://mistral.ai/news/mixtral-of-experts/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eblog post\u003c/a\u003e that showed it matching or exceeding GPT-3.5 as well as the much larger Llama2 70b on a number of benchmarks.\u003c/p\u003e\u003cp\u003eHere at LlamaIndex we’re naturally fans of open source software, so open models with permissive licenses like Mixtral are right up our alley. We’ve had a few questions about how to get Mixtral working with LlamaIndex, so this post is here to get you up and running with a totally local model.\u003c/p\u003e\u003ch1\u003eStep 1: Install Ollama\u003c/h1\u003e\u003cp\u003ePreviously getting a local model installed and working was a huge pain, but with the release of \u003ca href=\"https://ollama.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOllama\u003c/a\u003e, it’s suddenly a snap! Available for MacOS and Linux (and soon on Windows, though you can use it today on Windows via \u003ca href=\"https://learn.microsoft.com/en-us/windows/wsl/install\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWindows Subsystem For Linux\u003c/a\u003e), it is itself open source and a \u003ca href=\"https://ollama.ai/download\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efree download\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOnce downloaded, you can get Mixtral with a single command:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"2746\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eollama run mixtral\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe first time you run this command it will have to download the model, which can take a long time, so go get a snack. Also note that it requires a hefty 48GB of RAM to run smoothly! If that’s too much for your machine, consider using its smaller but still very capable cousin \u003cstrong\u003eMistral 7b\u003c/strong\u003e, which you install and run the same way:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9686\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eollama run mistral\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe’ll assume you’re using Mixtral for the rest of this tutorial, but Mistral will also work.\u003c/p\u003e\u003cp\u003eOnce the model is running Ollama will automatically let you chat with it. That’s fun, but what’s the point of having a model if it can’t work with your data? That’s where LlamaIndex comes in. The next few steps will take you through the code line by line, but if you’d prefer to save all the copying and pasting, all of this code is available in an \u003ca href=\"https://github.com/run-llama/mixtral_ollama\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eopen-source repo\u003c/a\u003e that you can clone to follow along there.\u003c/p\u003e\u003ch1\u003eStep 2: Install your dependencies\u003c/h1\u003e\u003cp\u003eYou’re going to need LlamaIndex installed, obviously! We’ll also get you going with a handful of other dependencies that are about to come in handy:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6d42\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003epip install llama-index qdrant_client torch transformers\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eStep 3: Smoke test\u003c/h1\u003e\u003cp\u003eIf you’ve got Ollama running and LlamaIndex properly installed, the following quick script will make sure everything is in order by asking it a quick “smoke test” question in a script all by itself:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d12c\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Just runs .complete to make sure the LLM is listening\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Ollama\n\nllm = Ollama(model=\u003cspan class=\"hljs-string\"\u003e\"mixtral\"\u003c/span\u003e)\nresponse = llm.complete(\u003cspan class=\"hljs-string\"\u003e\"Who is Laurie Voss?\"\u003c/span\u003e)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(response)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eStep 4: Load some data and index it\u003c/h1\u003e\u003cp\u003eNow you’re ready to load in some real data! You can use any data you want; in this case I’m using a \u003ca href=\"https://www.dropbox.com/scl/fi/6sos49fluvfilj3sqcvoj/tinytweets.json?rlkey=qmxlaqp000kmx8zktvaj4u1vh\u0026amp;dl=0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esmall collection of my own tweets\u003c/a\u003e which you can download, or use your own! We’re going to be storing our data in the nifty, open source \u003ca href=\"https://github.com/qdrant/qdrant\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eQdrant\u003c/a\u003e vector database (which is why we got you to install it earlier). Create a new python file, and load in all our dependencies:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0599\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e pathlib \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Path\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e qdrant_client\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    VectorStoreIndex,\n    ServiceContext,\n    download_loader,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Ollama\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.storage.storage_context \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e StorageContext\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores.qdrant \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e QdrantVectorStore\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThen load our tweets out of our JSON file using a nifty JSONReader from \u003ca href=\"https://llamahub.ai/l/file-json?from=all\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaHub\u003c/a\u003e, our convenient collection of open source data connectors. This will give you a pile of documents ready to be embedded and indexed:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9a4d\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eJSONReader = download_loader(\"JSONReader\")\nloader = JSONReader()\ndocuments = loader.load_data(Path('./data/tinytweets.json'))\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eGet Qdrant ready for action by initializing it and passing it into a Storage Context we’ll be using later:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"dcc0\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow set up our Service Context. We’ll be passing it Mixtral as the LLM so we can test that things are working once we’ve finished indexing; indexing itself doesn’t need Mixtral. By passing \u003ccode class=\"cw qi qj qk qa b\"\u003eembed_model=\"local\"\u003c/code\u003ewe’re specifying that LlamaIndex will embed your data locally, which is why you needed \u003ccode class=\"cw qi qj qk qa b\"\u003etorch\u003c/code\u003e and \u003ccode class=\"cw qi qj qk qa b\"\u003etransformers\u003c/code\u003e.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b93a\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003ellm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow bring it all together: build the index from the documents you loaded using the service and storage contexts you already set up, and give it a query:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0767\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eindex = VectorStoreIndex.from_documents(documents,service_context=service_context,storage_context=storage_context)\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\u003cspan class=\"hljs-string\"\u003e\"What does the author think about Star Trek? Give details.\"\u003c/span\u003e)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(response)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOllama will need to fire up Mixtral to answer the query, which can take a little while, so be patient! You should get output something like this (but with more details):\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"607d\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eBased on the provided context information, the author has a mixed opinion about Star Trek.\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eVerify our index\u003c/h1\u003e\u003cp\u003eNow to prove it’s not all smoke and mirrors, let’s use our pre-built index. Start a new python file and load in dependencies again:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b12a\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e qdrant_client\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    VectorStoreIndex,\n    ServiceContext,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Ollama\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores.qdrant \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e QdrantVectorStore\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis time we won’t need to load the data, that’s already done! We will need the Qdrant client and of course Mixtral again:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"48fb\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThis time instead of creating our index from documents we load it directly from the vector store using \u003ccode class=\"cw qi qj qk qa b\"\u003efrom_vector_store\u003c/code\u003e. We’re also passing \u003ccode class=\"cw qi qj qk qa b\"\u003esimilarity_top_k=20\u003c/code\u003e to the query engine; this will mean it will fetch 20 tweets at a time (the default is 2) to get more context and better answer the question.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b083\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\nquery_engine = index.as_query_engine(similarity_top_k=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\nresponse = query_engine.query(\u003cspan class=\"hljs-string\"\u003e\"Does the author like SQL? Give details.\"\u003c/span\u003e)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(response)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eBuild a little web service\u003c/h1\u003e\u003cp\u003eIt’s no good having an index that just runs as a script! Let’s make an API out of this thing. We’ll need two new dependencies:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"bd6a\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003epip install flask flask-cors\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eLoad in our dependencies as before into a new file:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0499\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e flask \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Flask, request, jsonify\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e flask_cors \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e CORS, cross_origin\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e qdrant_client\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Ollama\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    VectorStoreIndex,\n    ServiceContext,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores.qdrant \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e QdrantVectorStore\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eGet the vector store, the LLM and the index loaded:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b229\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003e# re-initialize the vector store\nclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"tweets\")\n\n# get the LLM again\nllm = Ollama(model=\"mixtral\")\nservice_context = ServiceContext.from_defaults(llm=llm,embed_model=\"local\")\n# load the index from the vector store\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store,service_context=service_context)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eSet up a really basic Flask server:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"3c1d\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003eapp = Flask(__name__)\ncors = CORS(app)\napp.config[\u003cspan class=\"hljs-string\"\u003e'CORS_HEADERS'\u003c/span\u003e] = \u003cspan class=\"hljs-string\"\u003e'Content-Type'\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# This is just so you can easily tell the app is running\u003c/span\u003e\n\u003cspan class=\"hljs-meta\"\u003e@app.route(\u003cspan class=\"hljs-params\"\u003e\u003cspan class=\"hljs-string\"\u003e'/'\u003c/span\u003e\u003c/span\u003e)\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ehello_world\u003c/span\u003e():\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e'Hello, World!'\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAnd add a route that accepts a query (as form data), queries the LLM and returns the response:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"8542\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003e\u003cspan class=\"hljs-meta\"\u003e@app.route(\u003cspan class=\"hljs-params\"\u003e\u003cspan class=\"hljs-string\"\u003e'/process_form'\u003c/span\u003e, methods=[\u003cspan class=\"hljs-string\"\u003e'POST'\u003c/span\u003e]\u003c/span\u003e)\u003c/span\u003e\n\u003cspan class=\"hljs-meta\"\u003e@cross_origin()\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eprocess_form\u003c/span\u003e():\n    query = request.form.get(\u003cspan class=\"hljs-string\"\u003e'query'\u003c/span\u003e)\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e query \u003cspan class=\"hljs-keyword\"\u003eis\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n        query_engine = index.as_query_engine(similarity_top_k=\u003cspan class=\"hljs-number\"\u003e20\u003c/span\u003e)\n        response = query_engine.query(query)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e jsonify({\u003cspan class=\"hljs-string\"\u003e\"response\"\u003c/span\u003e: \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e(response)})\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e jsonify({\u003cspan class=\"hljs-string\"\u003e\"error\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"query field is missing\"\u003c/span\u003e}), \u003cspan class=\"hljs-number\"\u003e400\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e'__main__'\u003c/span\u003e:\n    app.run()\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNote those last two lines, they’re important! \u003ccode class=\"cw qi qj qk qa b\"\u003eflask run\u003c/code\u003e is incompatible with the way LlamaIndex loads dependencies, so you will need to run this API directly like so (assuming your file is called \u003ccode class=\"cw qi qj qk qa b\"\u003eapp.py\u003c/code\u003e)\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c3fa\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003epython app.py\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWith your API up and running, you can use cURL to send a request and verify it:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6bda\" class=\"qd os gt qa b bf qe qf l qg qh\"\u003ecurl --location '\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003ehttp://127.0.0.1:5000/process_form\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e' \\\\\n--form 'query=\"What does the author think about Star Trek?\"'\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eYou’re done!\u003c/h1\u003e\u003cp\u003eWe covered a few things here:\u003c/p\u003e\u003cul\u003e\u003cli\u003eGetting Ollama to run Mixtral locally\u003c/li\u003e\u003cli\u003eUsing LlamaIndex to query Mixtral 8x7b\u003c/li\u003e\u003cli\u003eBuilding and querying an index over your data using Qdrant vector store\u003c/li\u003e\u003cli\u003eWrapping your index into a very simple web API\u003c/li\u003e\u003cli\u003eAll open-source, free, and running locally!\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eI hope this was a fun, quick introduction to running local models with LlamaIndex!\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/6b35d8aa957da1165454d971f5c392990cec8f7c-1598x1322.png","publishedDate":"2023-12-21","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"}],"slug":{"_type":"slug","current":"running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"8e2df150-e231-4eea-9f83-70337f217ba3","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"mistral-ai"},"title":"Mistral Ai"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"5323f2c8-7cf8-4772-94d3-4ef701fe28ad","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"ollama"},"title":"Ollama"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"}],"title":"Running Mixtral 8x7 locally with LlamaIndex and Ollama"},"publishedDate":"Invalid Date"},"params":{"slug":"running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"running-mixtral-8x7-locally-with-llamaindex-e6cebeabe0ab"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>