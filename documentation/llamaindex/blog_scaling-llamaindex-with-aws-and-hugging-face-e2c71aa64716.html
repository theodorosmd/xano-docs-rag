<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Scaling LlamaIndex with AWS and Hugging Face — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Scaling LlamaIndex with AWS and Hugging Face — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Scaling LlamaIndex with AWS and Hugging Face — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Scaling LlamaIndex with AWS and Hugging Face — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="218.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/logan-markewich">Logan Markewich</a> <!-- -->•<!-- --> <!-- -->2024-01-02</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Scaling LlamaIndex with AWS and Hugging Face</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/aws"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AWS</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/etl"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Etl</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/data-processing"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Data Processing</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/hugging-face"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Hugging Face</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>Over the holidays, I was running some retrieval benchmarks with LlamaIndex. I found myself rebuilding an index repeatedly with 30K documents, and finding waiting 10–20 minutes each time was too grating.</p><p>So to solve this, issue, I decided to bite the bullet and figure out how to deploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my data. This brought the processing time down to around 5 minutes!</p><figure><figcaption class="om fe on ny nz oo op be b bf z dt">Proposed system architecture</figcaption><img src="/blog/images/1*mRGtcSW8UdGOulg1wMSOHg.png" alt="" width="700" height="318"></figure><p>If you want to skip the detailed steps, you can jump to the code at the following repository:</p><p><a href="https://github.com/run-llama/llamaindex_aws_ingestion" rel="noopener ugc nofollow" target="_blank">https://github.com/run-llama/llamaindex_aws_ingestion</a></p><p><strong>NOTE:</strong> I am not an AWS expert, and had zero experience with it before this project. There are likely ways to improve upon the design I came up with. This blog merely documents my first foray into getting a system working on AWS. My hope is that this helps other people get started, and opens the door for other engineers deploying more scale-able systems.</p><h1>Step 1: Figuring out how AWS works</h1><p>To use AWS effectively, there are several packages and tools that you will need:</p><ol><li><a href="https://portal.aws.amazon.com/billing/signup#/start/email" rel="noopener ugc nofollow" target="_blank">AWS account signup</a></li><li><a href="https://docs.aws.amazon.com/eks/latest/userguide/setting-up.html" rel="noopener ugc nofollow" target="_blank">Install AWS CLI</a></li><li>Used to authenticate your AWS account for CLI tools</li><li><a href="https://eksctl.io/installation/" rel="noopener ugc nofollow" target="_blank">Install eksctl</a></li><li>Used to create <code class="cw qc qd qe qf b">EKS</code> clusters easily</li><li><a href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html" rel="noopener ugc nofollow" target="_blank">Install kubectl</a></li><li>Used to configure and debug deployments, pods, services, etc.</li><li><a href="https://www.docker.com/products/docker-desktop/" rel="noopener ugc nofollow" target="_blank">Install Docker</a></li></ol><p>As you will see, nearly all AWS deployments revolve around <code class="cw qc qd qe qf b">yaml</code> files that describe what you are deploying and how they connect together, as well as some CLI commands to actually run the deployment.</p><p>If at any time you aren’t sure what’s going on, I found it helpful to visit the AWS dashboard and explore the resources I had actually deployed. Usually, you will want to visit. I had the pages below favourited in AWS. Also, remember to set your region properly in the top right!</p><figure><figcaption class="om fe on ny nz oo op be b bf z dt">My AWS console favourites</figcaption><img src="/blog/images/1*zoJ97EUzIvBLuPjqBr8jtg.png" alt="" width="700" height="46"></figure><h2>Note on how deployments work</h2><p>For a majority of deployments, you will typically have</p><ol><li>The cluster</li><li>The deployed app, scaled to X replicas</li><li>A load balancer, to balance the incoming requests between X replicas</li></ol><p>In the examples below, most will have a <code class="cw qc qd qe qf b">yaml</code> for the deployed app, a <code class="cw qc qd qe qf b">yaml</code> for the load balancer, and a command to create the cluster you want to run on.</p><h2>Helpful CLI Commands</h2><p>A few CLI commands proved to be extremely helpful for debugging and monitoring deployments.</p><pre><span id="d9d5" class="qz os gt qf b bf ra rb l rc rd"># get the state of pods/deployments
kubectl get pods
kubectl get deployments

# useful for seeing logs/events of pods + full yaml config
kubectl describe pod &lt;pod name&gt;
kubectl logs &lt;pod name&gt;

# list clusters kubectl knows about
kubectl config get-contexts

# switch kubectl to another cluster
kubectl config use-context &lt;context name&gt;

# delete things
kubectl delete &lt;pod/deployment/service&gt; &lt;name&gt;</span></pre><h1>Step 2: Deploying Text Embeddings Interface</h1><p>In order to run embeddings fast, we will deploy an embeddings server using HuggingFace’s <a href="https://github.com/huggingface/text-embeddings-inference" rel="noopener ugc nofollow" target="_blank">Text Embedding Interface</a> (TEI). This server has production-level features and optimizations out-of-the-box, including continuous batching, flash-attention, rust implementation, and more. HuggingFace provides prebuilt docker images to simplify deployment.</p><p>However, the first step to running embeddings fast is to have a GPU. If you just signed up for AWS, you will have to request a quota increase. For me, I requested a few times for G5 instances (which run an Nvidia A10G GPU), and after a few days of testing on CPU, AWS gave me access to use up to 4 G5 instances.</p><p>Once you have a quota for GPU instances (like G5 nodes), you can create your cluster and deploy</p><pre><span id="bfc5" class="qz os gt qf b bf ra rb l rc rd">eksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1
sleep 5
kubectl create -f ./tei-deployment.yaml
sleep 5
kubectl create -f ./tei-service.yaml
sleep 5
echo "Embeddings URL is: <span class="hljs-symbol">&amp;lt;</span>http://$<span class="hljs-symbol">&amp;gt;</span>(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"</span></pre><p>The code above will create a cluster, a deployment (i.e. our TEI server) and a load balancer server.</p><p>You can see the yaml configs in <a href="https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/tei" rel="noopener ugc nofollow" target="_blank">the repo</a>, and you can edit them as needed.</p><p><strong>NOTE:</strong> Make sure to write down the URL printed at the end! If you forget, you can get the URL in the <code class="cw qc qd qe qf b">EKS</code> page on AWS. You’ll want the external IP for the load balancer.</p><h1>Step 3: Deploying RabbitMQ</h1><p>RabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each <code class="cw qc qd qe qf b">Document</code> object is processed independently by a consumer.</p><p>Deploying RabbitMQ on <code class="cw qc qd qe qf b">EKS</code> was a little tricky, but using the RabbitMQ operator installed with <code class="cw qc qd qe qf b">krew</code>, many things are abstracted away.</p><p>First, you need to create your cluster. For whatever reason, this didn’t work unless I also specified the zones</p><pre><span id="4d82" class="qz os gt qf b bf ra rb l rc rd">eksctl create cluster \
  --name mqCluster \
  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f</span></pre><p>Since RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use <code class="cw qc qd qe qf b">EBS</code> for storage. This was a frustrating step to figure out since most existing guides skip this detail!</p><pre><span id="fb1b" class="qz os gt qf b bf ra rb l rc rd">eksctl utils associate-iam-oidc-provider \
  --cluster=mqCluster \
  --region us-east<span class="hljs-number">-1</span> \
  --approve
sleep <span class="hljs-number">5</span>
eksctl create iamserviceaccount \
    --name ebs-csi-controller-sa \
    --<span class="hljs-keyword">namespace</span> kube-system \
    --cluster mqCluster \
    --role-name AmazonEKS_EBS_CSI_DriverRole \
    --role-only \
    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
    --approve
sleep <span class="hljs-number">5</span>
eksctl create addon \
  --name aws-ebs-csi-driver \
  --cluster mqCluster \
  --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \
  --force</span></pre><p>From there, we can install the RabbitMQ operator and create our deployment</p><pre><span id="4f79" class="qz os gt qf b bf ra rb l rc rd">kubectl apply -f <span class="hljs-symbol">&amp;lt;</span>https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml<span class="hljs-symbol">&amp;gt;</span>
sleep 5
kubectl apply -f rabbitmqcluster.yaml
sleep 5
echo "RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')"</span></pre><p>As usual, the code for all this can be found in the <a href="https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/rabbitmq" rel="noopener ugc nofollow" target="_blank">git repo</a>.</p><p><strong>NOTE:</strong> Make sure to write down the URL printed at the end! If you forget, you can get the URL in the <code class="cw qc qd qe qf b">EKS</code> page on AWS. You’ll want the external IP for the load balancer.</p><p>You can monitor your RabbitMQ queues by visiting “&lt;rabbitmq_url&gt;:15672” and signing in with “guest”/”guest”.</p><h1>Step 4: Deploying IngestionPipeline Workers</h1><p>This is where the real meat of work comes in. We need to create a <code class="cw qc qd qe qf b">consumer</code> that will endlessly pull from our <code class="cw qc qd qe qf b">RabbitMQ</code> queue, ingest data with the help of TEI, and then put that data into our vector db.</p><p>To do this, we can make a FastAPI server that does two things</p><ol><li>Starts a thread to consume from our queue</li><li>Starts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.)</li></ol><p>First, we write our code, as you can see in <a href="https://github.com/run-llama/llamaindex_aws_ingestion/blob/main/worker/worker.py" rel="noopener ugc nofollow" target="_blank">worker.py</a></p><p>Then, we dockerize our app by creating a simple <a href="https://github.com/run-llama/llamaindex_aws_ingestion/blob/main/worker/Dockerfile" rel="noopener ugc nofollow" target="_blank">Dockerfile</a> and running:</p><pre><span id="5eee" class="qz os gt qf b bf ra rb l rc rd">docker build -t &lt;image_name&gt; .
docker tag &lt;image_name&gt;:latest &lt;image_name&gt;:&lt;version&gt;
docker push &lt;image_name&gt;:&lt;version&gt;</span></pre><p>With our app dockerized, we can complete the <code class="cw qc qd qe qf b">worker-deployment.yaml</code> file by filling in</p><ul><li>Our embeddings URL under <code class="cw qc qd qe qf b">TEI_URL</code></li><li>Our rabbit-mq URL under <code class="cw qc qd qe qf b">RABBITMQ_URL</code></li><li>Our image name under container image</li><li>Our cluster details (in this case, a weaviate URL and API key)</li></ul><p>With the <code class="cw qc qd qe qf b">yaml</code> file complete, now we can properly deploy the worker</p><pre><span id="2b4a" class="qz os gt qf b bf ra rb l rc rd">eksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f
sleep 5
kubectl create -f ./worker-deployment.yaml
sleep 5
kubectl create -f ./worker-service.yaml</span></pre><h1>Step 5: Making a User-Facing Lambda Function</h1><p>Our lambda function will rely on a single external dependency — <code class="cw qc qd qe qf b">pika</code> — which is used to communicate with RabbitMQ.</p><p>Create a python file called <code class="cw qc qd qe qf b">lambda_function.py</code> with the following code:</p><pre><span id="830d" class="qz os gt qf b bf ra rb l rc rd"><span class="hljs-keyword">import</span> pika
<span class="hljs-keyword">import</span> json

<span class="hljs-keyword">def</span> <span class="hljs-title function_">lambda_handler</span>(<span class="hljs-params">event, context</span>):
    <span class="hljs-keyword">try</span>:
        body = json.loads(event.get(<span class="hljs-string">'body'</span>, <span class="hljs-string">'{}'</span>))
    <span class="hljs-keyword">except</span>:
        body = event.get(<span class="hljs-string">'body'</span>, {})
        
    user = body.get(<span class="hljs-string">'user'</span>, <span class="hljs-string">''</span>)
    documents = body.get(<span class="hljs-string">'documents'</span>, [])
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> user <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> documents:
        <span class="hljs-keyword">return</span> {
            <span class="hljs-string">'statusCode'</span>: <span class="hljs-number">400</span>,
            <span class="hljs-string">'body'</span>: json.dumps(<span class="hljs-string">'Missing user or documents'</span>)
        }
    
    credentials = pika.PlainCredentials(<span class="hljs-string">"guest"</span>, <span class="hljs-string">"guest"</span>)
    parameters = pika.ConnectionParameters(
        host=<span class="hljs-string">"hostname.amazonaws.com"</span>, 
        port=<span class="hljs-number">5672</span>, 
        credentials=credentials
    )
    
    connection = pika.BlockingConnection(parameters=parameters)
    channel = connection.channel()
    channel.queue_declare(queue=<span class="hljs-string">'etl'</span>)

    <span class="hljs-keyword">for</span> document <span class="hljs-keyword">in</span> documents:
        data = {
            <span class="hljs-string">'user'</span>: user,
            <span class="hljs-string">'documents'</span>: [document]
        }
        channel.basic_publish(
            exchange=<span class="hljs-string">""</span>, 
            routing_key=<span class="hljs-string">'etl'</span>, 
            body=json.dumps(data)
        )

    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">'statusCode'</span>: <span class="hljs-number">200</span>,
        <span class="hljs-string">'body'</span>: json.dumps(<span class="hljs-string">'Documents queued for ingestion'</span>)
    }</span></pre><p>The function above processes incoming requests, and publishes each document as a single message in our rabbitmq cluster.</p><p>To deploy a lambda file with dependencies, we need to create a zip of our lambda function + all dependencies. To do this, we can create a <code class="cw qc qd qe qf b">requirements.txt</code> file with our dependencies and run:</p><pre><span id="9737" class="qz os gt qf b bf ra rb l rc rd">pip install -r requirements.txt -t .
zip -r9 ../ingestion_lambda.zip . -x "*.git*" "*setup.sh*" "*requirements.txt*" "*.zip*"</span></pre><p>With our code and zip file in hand, head over to the Lambda AWS page in your browser.</p><ol><li>Select <code class="cw qc qd qe qf b">Create function</code></li><li>Give it a name, select a python runtime (I used Python 3.11)</li><li>Click <code class="cw qc qd qe qf b">Create function</code> at the bottom</li><li>In the code editor, you’ll see an <code class="cw qc qd qe qf b">Upload from</code> button — click that, and upload your zip file</li><li>Click test, give the test a name, and paste the following JSON</li></ol><pre><span id="8f73" class="qz os gt qf b bf ra rb l rc rd">{
    <span class="hljs-string">"body"</span>: {<span class="hljs-string">"user"</span>: <span class="hljs-string">"Test"</span>, <span class="hljs-string">"documents"</span>: [{<span class="hljs-string">"text"</span>: <span class="hljs-string">"test"</span>}]}
}</span></pre><p>Once the test works, the <code class="cw qc qd qe qf b">Deploy</code> button will not be grayed out, and you can click it.</p><p>Your public URL will be listed in the upper right pane under <code class="cw qc qd qe qf b">Function URL</code> — this is the URL you can use to call your lambda function from anywhere!</p><h1>Step 6: Reap the Scaling Benefits</h1><p>Now, we can run our system end-to-end!</p><p>To ingest data, you can run:</p><pre><span id="977a" class="qz os gt qf b bf ra rb l rc rd"><span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> Document, SimpleDirectoryReader

documents = SimpleDirectoryReader(<span class="hljs-string">"./data"</span>).load_data()

<span class="hljs-comment"># this will also be the namespace for the vector store </span>
<span class="hljs-comment"># -- for weaviate, it needs to start with a captial and only alpha-numeric</span>
user = <span class="hljs-string">"Loganm"</span> 

<span class="hljs-comment"># upload in batches</span>
<span class="hljs-keyword">for</span> batch_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(documents), <span class="hljs-number">30</span>):
  documents_batch = documents[batch_idx:batch_idx+<span class="hljs-number">30</span>]
  body = {
    <span class="hljs-string">'user'</span>: user,
    <span class="hljs-string">'documents'</span>: [doc.json() <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> documents_batch]
  }

 <span class="hljs-comment"># use the URL of our lambda function here</span>
 response = requests.post(<span class="hljs-string">"&amp;lt;lambda_url&amp;gt;"</span>, json=body)
 <span class="hljs-built_in">print</span>(response.text)</span></pre><p>Then, to use our data:</p><pre><span id="2fdb" class="qz os gt qf b bf ra rb l rc rd"><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> VectorStoreIndex
<span class="hljs-keyword">from</span> llama_index.vector_stores <span class="hljs-keyword">import</span> WeaviateVectorStore
<span class="hljs-keyword">import</span> weaviate

auth_config = weaviate.AuthApiKey(api_key=<span class="hljs-string">"..."</span>)
client = weaviate.Client(url=<span class="hljs-string">"..."</span>, auth_client_secret=auth_config)
vector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=<span class="hljs-string">"&amp;lt;user&amp;gt;"</span>)
index = VectorStoreIndex.from_vector_store(vector_store)</span></pre><h1>Step 7: Clean-up</h1><p>AWS doesn’t make it easy to estimate costs of all this. But after running and testing things for a few days, I had only spent ~$40CAD. This included leaving some services running overnight (whoops!).</p><p>When you are done with your deployment, you’ll want to delete the resources so that you aren’t charged for things you aren’t using. To delete my clusters, I ran the following:</p><pre><span id="90e4" class="qz os gt qf b bf ra rb l rc rd">eksctl delete cluster embeddings
eksctl delete cluster mq-worker
kubectl rabbitmq delete production-rabbitmqcluster</span></pre><p>Then, in the AWS UI console, I deleted any remaining resources on the <code class="cw qc qd qe qf b">EC2</code> and <code class="cw qc qd qe qf b">CloudFormation</code> pages, as well as double-checking that everything was deleted on the <code class="cw qc qd qe qf b">EKS</code> page.</p><h1>Conclusion</h1><p>Using this setup, I was able to reduce index-construction times for creating large indexes dramatically. Before, it would take about 10–20 minutes to create the index for 25K documents, and with this setup (2 rabbitmq nodes, 2 workers, 2 embeddings), it was down to 5 minutes! And with more scaling, it could be even faster.</p><h1>Next Steps</h1><p>From here, there are several improvements that I can think of</p><ul><li>better secrets management</li><li>adding auto-scaling</li><li>adding a retrieval lambda function (would require making a docker image for lambda + llama-index)</li><li>adding queue stats to the fastapi server</li><li>deploying redis for document management on the IngestionPipeline</li></ul><p>I encourage anyone to take this work and build off it. Be sure share any improvement on the github repository as well!</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-02-27-4b9102a0f824">LlamaIndex Newsletter 2024–02–27</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-27</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3">Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-24</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/introducing-llamacloud-and-llamaparse-af8cedf9006b">Introducing LlamaCloud and LlamaParse</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-20</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81f41e8a6fe34f544e518f6e137dbf559e8885da-1024x1024.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81f41e8a6fe34f544e518f6e137dbf559e8885da-1024x1024.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F81f41e8a6fe34f544e518f6e137dbf559e8885da-1024x1024.jpg%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4">LlamaIndex Newsletter 2024–02–20: introducing LlamaCloud</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-02-20</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"7bb38663-fa2e-494f-8ab9-3caf6f608455","_rev":"TLgH6AcXrxoqw75SBDhiEf","_type":"blogPost","_updatedAt":"2025-05-21T20:38:17Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:51:10Z","_id":"ab6d3b0c-ba72-4fca-a2f0-ad05d4de202d","_rev":"aMTdiZulL6GCCCCaWLcqxF","_type":"people","_updatedAt":"2025-05-07T16:40:57Z","image":{"_type":"image","asset":{"_ref":"image-724a0ce035b56ce15b3e802b31929a82bacd0b42-2041x2041-jpg","_type":"reference"}},"name":"Logan Markewich","position":"Head of Open Source","slug":{"_type":"slug","current":"logan-markewich"}}],"featured":false,"htmlContent":"\u003cp\u003eOver the holidays, I was running some retrieval benchmarks with LlamaIndex. I found myself rebuilding an index repeatedly with 30K documents, and finding waiting 10–20 minutes each time was too grating.\u003c/p\u003e\u003cp\u003eSo to solve this, issue, I decided to bite the bullet and figure out how to deploy LlamaIndex to AWS, and create a scalable ETL pipeline for indexing my data. This brought the processing time down to around 5 minutes!\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"om fe on ny nz oo op be b bf z dt\"\u003eProposed system architecture\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*mRGtcSW8UdGOulg1wMSOHg.png\" alt=\"\" width=\"700\" height=\"318\"\u003e\u003c/figure\u003e\u003cp\u003eIf you want to skip the detailed steps, you can jump to the code at the following repository:\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/run-llama/llamaindex_aws_ingestion\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehttps://github.com/run-llama/llamaindex_aws_ingestion\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e I am not an AWS expert, and had zero experience with it before this project. There are likely ways to improve upon the design I came up with. This blog merely documents my first foray into getting a system working on AWS. My hope is that this helps other people get started, and opens the door for other engineers deploying more scale-able systems.\u003c/p\u003e\u003ch1\u003eStep 1: Figuring out how AWS works\u003c/h1\u003e\u003cp\u003eTo use AWS effectively, there are several packages and tools that you will need:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003ca href=\"https://portal.aws.amazon.com/billing/signup#/start/email\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAWS account signup\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.aws.amazon.com/eks/latest/userguide/setting-up.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eInstall AWS CLI\u003c/a\u003e\u003c/li\u003e\u003cli\u003eUsed to authenticate your AWS account for CLI tools\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://eksctl.io/installation/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eInstall eksctl\u003c/a\u003e\u003c/li\u003e\u003cli\u003eUsed to create \u003ccode class=\"cw qc qd qe qf b\"\u003eEKS\u003c/code\u003e clusters easily\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eInstall kubectl\u003c/a\u003e\u003c/li\u003e\u003cli\u003eUsed to configure and debug deployments, pods, services, etc.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://www.docker.com/products/docker-desktop/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eInstall Docker\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAs you will see, nearly all AWS deployments revolve around \u003ccode class=\"cw qc qd qe qf b\"\u003eyaml\u003c/code\u003e files that describe what you are deploying and how they connect together, as well as some CLI commands to actually run the deployment.\u003c/p\u003e\u003cp\u003eIf at any time you aren’t sure what’s going on, I found it helpful to visit the AWS dashboard and explore the resources I had actually deployed. Usually, you will want to visit. I had the pages below favourited in AWS. Also, remember to set your region properly in the top right!\u003c/p\u003e\u003cfigure\u003e\u003cfigcaption class=\"om fe on ny nz oo op be b bf z dt\"\u003eMy AWS console favourites\u003c/figcaption\u003e\u003cimg src=\"/blog/images/1*zoJ97EUzIvBLuPjqBr8jtg.png\" alt=\"\" width=\"700\" height=\"46\"\u003e\u003c/figure\u003e\u003ch2\u003eNote on how deployments work\u003c/h2\u003e\u003cp\u003eFor a majority of deployments, you will typically have\u003c/p\u003e\u003col\u003e\u003cli\u003eThe cluster\u003c/li\u003e\u003cli\u003eThe deployed app, scaled to X replicas\u003c/li\u003e\u003cli\u003eA load balancer, to balance the incoming requests between X replicas\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eIn the examples below, most will have a \u003ccode class=\"cw qc qd qe qf b\"\u003eyaml\u003c/code\u003e for the deployed app, a \u003ccode class=\"cw qc qd qe qf b\"\u003eyaml\u003c/code\u003e for the load balancer, and a command to create the cluster you want to run on.\u003c/p\u003e\u003ch2\u003eHelpful CLI Commands\u003c/h2\u003e\u003cp\u003eA few CLI commands proved to be extremely helpful for debugging and monitoring deployments.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d9d5\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003e# get the state of pods/deployments\nkubectl get pods\nkubectl get deployments\n\n# useful for seeing logs/events of pods + full yaml config\nkubectl describe pod \u0026lt;pod name\u0026gt;\nkubectl logs \u0026lt;pod name\u0026gt;\n\n# list clusters kubectl knows about\nkubectl config get-contexts\n\n# switch kubectl to another cluster\nkubectl config use-context \u0026lt;context name\u0026gt;\n\n# delete things\nkubectl delete \u0026lt;pod/deployment/service\u0026gt; \u0026lt;name\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eStep 2: Deploying Text Embeddings Interface\u003c/h1\u003e\u003cp\u003eIn order to run embeddings fast, we will deploy an embeddings server using HuggingFace’s \u003ca href=\"https://github.com/huggingface/text-embeddings-inference\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eText Embedding Interface\u003c/a\u003e (TEI). This server has production-level features and optimizations out-of-the-box, including continuous batching, flash-attention, rust implementation, and more. HuggingFace provides prebuilt docker images to simplify deployment.\u003c/p\u003e\u003cp\u003eHowever, the first step to running embeddings fast is to have a GPU. If you just signed up for AWS, you will have to request a quota increase. For me, I requested a few times for G5 instances (which run an Nvidia A10G GPU), and after a few days of testing on CPU, AWS gave me access to use up to 4 G5 instances.\u003c/p\u003e\u003cp\u003eOnce you have a quota for GPU instances (like G5 nodes), you can create your cluster and deploy\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"bfc5\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003eeksctl create cluster --name embeddings --node-type=g5.xlarge --nodes 1\nsleep 5\nkubectl create -f ./tei-deployment.yaml\nsleep 5\nkubectl create -f ./tei-service.yaml\nsleep 5\necho \"Embeddings URL is: \u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003ehttp://$\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e(kubectl get svc tei-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe code above will create a cluster, a deployment (i.e. our TEI server) and a load balancer server.\u003c/p\u003e\u003cp\u003eYou can see the yaml configs in \u003ca href=\"https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/tei\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ethe repo\u003c/a\u003e, and you can edit them as needed.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e Make sure to write down the URL printed at the end! If you forget, you can get the URL in the \u003ccode class=\"cw qc qd qe qf b\"\u003eEKS\u003c/code\u003e page on AWS. You’ll want the external IP for the load balancer.\u003c/p\u003e\u003ch1\u003eStep 3: Deploying RabbitMQ\u003c/h1\u003e\u003cp\u003eRabbitMQ is where we will queue documents to be ingested. RabbitMQ is a message broker system that allows for powerful yet simple queuing of tasks. Since some ingestion tasks (like metadata extraction, embeddings) can be slow, the more naive approach of a REST API would leave connections open while data is processed. Instead, using a queue allows us to quickly upload data and offload processing to scalable message consumer(s). It also allows us to add parallelism with ease, where in our system, each \u003ccode class=\"cw qc qd qe qf b\"\u003eDocument\u003c/code\u003e object is processed independently by a consumer.\u003c/p\u003e\u003cp\u003eDeploying RabbitMQ on \u003ccode class=\"cw qc qd qe qf b\"\u003eEKS\u003c/code\u003e was a little tricky, but using the RabbitMQ operator installed with \u003ccode class=\"cw qc qd qe qf b\"\u003ekrew\u003c/code\u003e, many things are abstracted away.\u003c/p\u003e\u003cp\u003eFirst, you need to create your cluster. For whatever reason, this didn’t work unless I also specified the zones\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4d82\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003eeksctl create cluster \\\n  --name mqCluster \\\n  --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eSince RabbitMQ needs storage, and each replica needs to share the same storage, we should give our cluster permission to provision and use \u003ccode class=\"cw qc qd qe qf b\"\u003eEBS\u003c/code\u003e for storage. This was a frustrating step to figure out since most existing guides skip this detail!\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"fb1b\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003eeksctl utils associate-iam-oidc-provider \\\n  --cluster=mqCluster \\\n  --region us-east\u003cspan class=\"hljs-number\"\u003e-1\u003c/span\u003e \\\n  --approve\nsleep \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e\neksctl create iamserviceaccount \\\n    --name ebs-csi-controller-sa \\\n    --\u003cspan class=\"hljs-keyword\"\u003enamespace\u003c/span\u003e kube-system \\\n    --cluster mqCluster \\\n    --role-name AmazonEKS_EBS_CSI_DriverRole \\\n    --role-only \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve\nsleep \u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e\neksctl create addon \\\n  --name aws-ebs-csi-driver \\\n  --cluster mqCluster \\\n  --service-account-role-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):role/AmazonEKS_EBS_CSI_DriverRole \\\n  --force\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eFrom there, we can install the RabbitMQ operator and create our deployment\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4f79\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003ekubectl apply -f \u003cspan class=\"hljs-symbol\"\u003e\u0026amp;lt;\u003c/span\u003ehttps://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml\u003cspan class=\"hljs-symbol\"\u003e\u0026amp;gt;\u003c/span\u003e\nsleep 5\nkubectl apply -f rabbitmqcluster.yaml\nsleep 5\necho \"RabbitMQ URL is: $(kubectl get svc production-rabbitmqcluster -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAs usual, the code for all this can be found in the \u003ca href=\"https://github.com/run-llama/llamaindex_aws_ingestion/tree/main/rabbitmq\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003egit repo\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e Make sure to write down the URL printed at the end! If you forget, you can get the URL in the \u003ccode class=\"cw qc qd qe qf b\"\u003eEKS\u003c/code\u003e page on AWS. You’ll want the external IP for the load balancer.\u003c/p\u003e\u003cp\u003eYou can monitor your RabbitMQ queues by visiting “\u0026lt;rabbitmq_url\u0026gt;:15672” and signing in with “guest”/”guest”.\u003c/p\u003e\u003ch1\u003eStep 4: Deploying IngestionPipeline Workers\u003c/h1\u003e\u003cp\u003eThis is where the real meat of work comes in. We need to create a \u003ccode class=\"cw qc qd qe qf b\"\u003econsumer\u003c/code\u003e that will endlessly pull from our \u003ccode class=\"cw qc qd qe qf b\"\u003eRabbitMQ\u003c/code\u003e queue, ingest data with the help of TEI, and then put that data into our vector db.\u003c/p\u003e\u003cp\u003eTo do this, we can make a FastAPI server that does two things\u003c/p\u003e\u003col\u003e\u003cli\u003eStarts a thread to consume from our queue\u003c/li\u003e\u003cli\u003eStarts a webserver, to enable us to specify a readiness check, and gives us room to add more features in the future (i.e. probing queue status, logs, etc.)\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eFirst, we write our code, as you can see in \u003ca href=\"https://github.com/run-llama/llamaindex_aws_ingestion/blob/main/worker/worker.py\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eworker.py\u003c/a\u003e\u003c/p\u003e\u003cp\u003eThen, we dockerize our app by creating a simple \u003ca href=\"https://github.com/run-llama/llamaindex_aws_ingestion/blob/main/worker/Dockerfile\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDockerfile\u003c/a\u003e and running:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5eee\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003edocker build -t \u0026lt;image_name\u0026gt; .\ndocker tag \u0026lt;image_name\u0026gt;:latest \u0026lt;image_name\u0026gt;:\u0026lt;version\u0026gt;\ndocker push \u0026lt;image_name\u0026gt;:\u0026lt;version\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWith our app dockerized, we can complete the \u003ccode class=\"cw qc qd qe qf b\"\u003eworker-deployment.yaml\u003c/code\u003e file by filling in\u003c/p\u003e\u003cul\u003e\u003cli\u003eOur embeddings URL under \u003ccode class=\"cw qc qd qe qf b\"\u003eTEI_URL\u003c/code\u003e\u003c/li\u003e\u003cli\u003eOur rabbit-mq URL under \u003ccode class=\"cw qc qd qe qf b\"\u003eRABBITMQ_URL\u003c/code\u003e\u003c/li\u003e\u003cli\u003eOur image name under container image\u003c/li\u003e\u003cli\u003eOur cluster details (in this case, a weaviate URL and API key)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWith the \u003ccode class=\"cw qc qd qe qf b\"\u003eyaml\u003c/code\u003e file complete, now we can properly deploy the worker\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"2b4a\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003eeksctl create cluster --name mq-workers --zones us-east-1a,us-east-1b,us-east-1c,us-east-1d,us-east-1f\nsleep 5\nkubectl create -f ./worker-deployment.yaml\nsleep 5\nkubectl create -f ./worker-service.yaml\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eStep 5: Making a User-Facing Lambda Function\u003c/h1\u003e\u003cp\u003eOur lambda function will rely on a single external dependency — \u003ccode class=\"cw qc qd qe qf b\"\u003epika\u003c/code\u003e — which is used to communicate with RabbitMQ.\u003c/p\u003e\u003cp\u003eCreate a python file called \u003ccode class=\"cw qc qd qe qf b\"\u003elambda_function.py\u003c/code\u003e with the following code:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"830d\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e pika\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e json\n\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003elambda_handler\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eevent, context\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003etry\u003c/span\u003e:\n        body = json.loads(event.get(\u003cspan class=\"hljs-string\"\u003e'body'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e'{}'\u003c/span\u003e))\n    \u003cspan class=\"hljs-keyword\"\u003eexcept\u003c/span\u003e:\n        body = event.get(\u003cspan class=\"hljs-string\"\u003e'body'\u003c/span\u003e, {})\n        \n    user = body.get(\u003cspan class=\"hljs-string\"\u003e'user'\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e''\u003c/span\u003e)\n    documents = body.get(\u003cspan class=\"hljs-string\"\u003e'documents'\u003c/span\u003e, [])\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e user \u003cspan class=\"hljs-keyword\"\u003eor\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003enot\u003c/span\u003e documents:\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\n            \u003cspan class=\"hljs-string\"\u003e'statusCode'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e400\u003c/span\u003e,\n            \u003cspan class=\"hljs-string\"\u003e'body'\u003c/span\u003e: json.dumps(\u003cspan class=\"hljs-string\"\u003e'Missing user or documents'\u003c/span\u003e)\n        }\n    \n    credentials = pika.PlainCredentials(\u003cspan class=\"hljs-string\"\u003e\"guest\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"guest\"\u003c/span\u003e)\n    parameters = pika.ConnectionParameters(\n        host=\u003cspan class=\"hljs-string\"\u003e\"hostname.amazonaws.com\"\u003c/span\u003e, \n        port=\u003cspan class=\"hljs-number\"\u003e5672\u003c/span\u003e, \n        credentials=credentials\n    )\n    \n    connection = pika.BlockingConnection(parameters=parameters)\n    channel = connection.channel()\n    channel.queue_declare(queue=\u003cspan class=\"hljs-string\"\u003e'etl'\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e document \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e documents:\n        data = {\n            \u003cspan class=\"hljs-string\"\u003e'user'\u003c/span\u003e: user,\n            \u003cspan class=\"hljs-string\"\u003e'documents'\u003c/span\u003e: [document]\n        }\n        channel.basic_publish(\n            exchange=\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e, \n            routing_key=\u003cspan class=\"hljs-string\"\u003e'etl'\u003c/span\u003e, \n            body=json.dumps(data)\n        )\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e {\n        \u003cspan class=\"hljs-string\"\u003e'statusCode'\u003c/span\u003e: \u003cspan class=\"hljs-number\"\u003e200\u003c/span\u003e,\n        \u003cspan class=\"hljs-string\"\u003e'body'\u003c/span\u003e: json.dumps(\u003cspan class=\"hljs-string\"\u003e'Documents queued for ingestion'\u003c/span\u003e)\n    }\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe function above processes incoming requests, and publishes each document as a single message in our rabbitmq cluster.\u003c/p\u003e\u003cp\u003eTo deploy a lambda file with dependencies, we need to create a zip of our lambda function + all dependencies. To do this, we can create a \u003ccode class=\"cw qc qd qe qf b\"\u003erequirements.txt\u003c/code\u003e file with our dependencies and run:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9737\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003epip install -r requirements.txt -t .\nzip -r9 ../ingestion_lambda.zip . -x \"*.git*\" \"*setup.sh*\" \"*requirements.txt*\" \"*.zip*\"\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWith our code and zip file in hand, head over to the Lambda AWS page in your browser.\u003c/p\u003e\u003col\u003e\u003cli\u003eSelect \u003ccode class=\"cw qc qd qe qf b\"\u003eCreate function\u003c/code\u003e\u003c/li\u003e\u003cli\u003eGive it a name, select a python runtime (I used Python 3.11)\u003c/li\u003e\u003cli\u003eClick \u003ccode class=\"cw qc qd qe qf b\"\u003eCreate function\u003c/code\u003e at the bottom\u003c/li\u003e\u003cli\u003eIn the code editor, you’ll see an \u003ccode class=\"cw qc qd qe qf b\"\u003eUpload from\u003c/code\u003e button — click that, and upload your zip file\u003c/li\u003e\u003cli\u003eClick test, give the test a name, and paste the following JSON\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003cspan id=\"8f73\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003e{\n    \u003cspan class=\"hljs-string\"\u003e\"body\"\u003c/span\u003e: {\u003cspan class=\"hljs-string\"\u003e\"user\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Test\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"documents\"\u003c/span\u003e: [{\u003cspan class=\"hljs-string\"\u003e\"text\"\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"test\"\u003c/span\u003e}]}\n}\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOnce the test works, the \u003ccode class=\"cw qc qd qe qf b\"\u003eDeploy\u003c/code\u003e button will not be grayed out, and you can click it.\u003c/p\u003e\u003cp\u003eYour public URL will be listed in the upper right pane under \u003ccode class=\"cw qc qd qe qf b\"\u003eFunction URL\u003c/code\u003e — this is the URL you can use to call your lambda function from anywhere!\u003c/p\u003e\u003ch1\u003eStep 6: Reap the Scaling Benefits\u003c/h1\u003e\u003cp\u003eNow, we can run our system end-to-end!\u003c/p\u003e\u003cp\u003eTo ingest data, you can run:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"977a\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e requests\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Document, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\u003cspan class=\"hljs-string\"\u003e\"./data\"\u003c/span\u003e).load_data()\n\n\u003cspan class=\"hljs-comment\"\u003e# this will also be the namespace for the vector store \u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# -- for weaviate, it needs to start with a captial and only alpha-numeric\u003c/span\u003e\nuser = \u003cspan class=\"hljs-string\"\u003e\"Loganm\"\u003c/span\u003e \n\n\u003cspan class=\"hljs-comment\"\u003e# upload in batches\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e batch_idx \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003erange\u003c/span\u003e(\u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e, \u003cspan class=\"hljs-built_in\"\u003elen\u003c/span\u003e(documents), \u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e):\n  documents_batch = documents[batch_idx:batch_idx+\u003cspan class=\"hljs-number\"\u003e30\u003c/span\u003e]\n  body = {\n    \u003cspan class=\"hljs-string\"\u003e'user'\u003c/span\u003e: user,\n    \u003cspan class=\"hljs-string\"\u003e'documents'\u003c/span\u003e: [doc.json() \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e doc \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e documents_batch]\n  }\n\n \u003cspan class=\"hljs-comment\"\u003e# use the URL of our lambda function here\u003c/span\u003e\n response = requests.post(\u003cspan class=\"hljs-string\"\u003e\"\u0026amp;lt;lambda_url\u0026amp;gt;\"\u003c/span\u003e, json=body)\n \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(response.text)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThen, to use our data:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"2fdb\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e VectorStoreIndex\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e WeaviateVectorStore\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e weaviate\n\nauth_config = weaviate.AuthApiKey(api_key=\u003cspan class=\"hljs-string\"\u003e\"...\"\u003c/span\u003e)\nclient = weaviate.Client(url=\u003cspan class=\"hljs-string\"\u003e\"...\"\u003c/span\u003e, auth_client_secret=auth_config)\nvector_store = WeaviateVectorStore(weaviate_client=client, class_prefix=\u003cspan class=\"hljs-string\"\u003e\"\u0026amp;lt;user\u0026amp;gt;\"\u003c/span\u003e)\nindex = VectorStoreIndex.from_vector_store(vector_store)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eStep 7: Clean-up\u003c/h1\u003e\u003cp\u003eAWS doesn’t make it easy to estimate costs of all this. But after running and testing things for a few days, I had only spent ~$40CAD. This included leaving some services running overnight (whoops!).\u003c/p\u003e\u003cp\u003eWhen you are done with your deployment, you’ll want to delete the resources so that you aren’t charged for things you aren’t using. To delete my clusters, I ran the following:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"90e4\" class=\"qz os gt qf b bf ra rb l rc rd\"\u003eeksctl delete cluster embeddings\neksctl delete cluster mq-worker\nkubectl rabbitmq delete production-rabbitmqcluster\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThen, in the AWS UI console, I deleted any remaining resources on the \u003ccode class=\"cw qc qd qe qf b\"\u003eEC2\u003c/code\u003e and \u003ccode class=\"cw qc qd qe qf b\"\u003eCloudFormation\u003c/code\u003e pages, as well as double-checking that everything was deleted on the \u003ccode class=\"cw qc qd qe qf b\"\u003eEKS\u003c/code\u003e page.\u003c/p\u003e\u003ch1\u003eConclusion\u003c/h1\u003e\u003cp\u003eUsing this setup, I was able to reduce index-construction times for creating large indexes dramatically. Before, it would take about 10–20 minutes to create the index for 25K documents, and with this setup (2 rabbitmq nodes, 2 workers, 2 embeddings), it was down to 5 minutes! And with more scaling, it could be even faster.\u003c/p\u003e\u003ch1\u003eNext Steps\u003c/h1\u003e\u003cp\u003eFrom here, there are several improvements that I can think of\u003c/p\u003e\u003cul\u003e\u003cli\u003ebetter secrets management\u003c/li\u003e\u003cli\u003eadding auto-scaling\u003c/li\u003e\u003cli\u003eadding a retrieval lambda function (would require making a docker image for lambda + llama-index)\u003c/li\u003e\u003cli\u003eadding queue stats to the fastapi server\u003c/li\u003e\u003cli\u003edeploying redis for document management on the IngestionPipeline\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eI encourage anyone to take this work and build off it. Be sure share any improvement on the github repository as well!\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/3ab95769458ff0ada70144bf9d37e605a4c97fa4-964x437.png","publishedDate":"2024-01-02","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-39a875fc787fe28f9e44db8769c6fab9c31c7e17-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-02-27","slug":"llamaindex-newsletter-2024-02-27-4b9102a0f824","title":"LlamaIndex Newsletter 2024–02–27"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-34ef148ef2e331372f8f0db7a8e9c9c11a76b504-1600x646-png","_type":"reference"}},"publishedDate":"2024-02-24","slug":"bridging-the-gap-in-crisis-counseling-introducing-counselor-copilot-db42e26ab4f3","title":"Bridging the Gap in Crisis Counseling: Introducing Counselor Copilot"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-65e2a4a5037cc464566a13e7828fbb905fd33b38-960x863-png","_type":"reference"}},"publishedDate":"2024-02-20","slug":"introducing-llamacloud-and-llamaparse-af8cedf9006b","title":"Introducing LlamaCloud and LlamaParse"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-81f41e8a6fe34f544e518f6e137dbf559e8885da-1024x1024-jpg","_type":"reference"}},"publishedDate":"2024-02-20","slug":"llamaindex-newsletter-2024-02-20-introducing-llamacloud-30511f4662f4","title":"LlamaIndex Newsletter 2024–02–20: introducing LlamaCloud"}],"slug":{"_type":"slug","current":"scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"},"tags":[{"_createdAt":"2024-02-22T20:19:12Z","_id":"b735578c-8c5a-4fdf-827e-54a3997362bb","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"aws"},"title":"AWS"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"68c30083-0646-4c56-9164-e1923f33023e","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"etl"},"title":"Etl"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"3de7df95-dda1-4e37-a816-8d8356ad31c2","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"data-processing"},"title":"Data Processing"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"00b61f11-d13e-413d-aae0-734df49c2444","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"hugging-face"},"title":"Hugging Face"}],"title":"Scaling LlamaIndex with AWS and Hugging Face"},"publishedDate":"Invalid Date"},"params":{"slug":"scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"scaling-llamaindex-with-aws-and-hugging-face-e2c71aa64716"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>