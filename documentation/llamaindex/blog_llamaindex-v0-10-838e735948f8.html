<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>LlamaIndex v0.10 — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="LlamaIndex v0.10 — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="LlamaIndex v0.10 — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="LlamaIndex v0.10 — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="1040.5" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/llamaindex">LlamaIndex</a> <!-- -->•<!-- --> <!-- -->2024-02-12</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">LlamaIndex v0.10</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/upgrade"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Upgrade</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>Today we’re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date (<a href="https://github.com/run-llama/llama_index/pull/10537" rel="noopener ugc nofollow" target="_blank">see this gargantuan PR</a>), and it takes a massive step towards making LlamaIndex a next-generation, <strong>production-ready data framework</strong> for your LLM applications.</p><p>LlamaIndex v0.10 contains some major updates:</p><ol><li><strong>We have created a </strong><code class="cw oq or os ot b"><strong>llama-index-core</strong></code><strong> package, and split all integrations and templates into separate packages:</strong> Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still use<code class="cw oq or os ot b">from llama_index.llms.openai import OpenAI</code> for a LLM.</li><li><a href="https://llamahub.ai/" rel="noopener ugc nofollow" target="_blank"><strong>LlamaHub</strong></a><strong> will be the central hub for all integrations: </strong>the former <a href="https://github.com/run-llama/llama-hub" rel="noopener ugc nofollow" target="_blank">llama-hub</a> repo itself is consolidated into the main <a href="https://github.com/run-llama/llama_index/tree/main/llama-index-integrations" rel="noopener ugc nofollow" target="_blank">llama_index</a> repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned!</li><li><strong>ServiceContext is deprecated:</strong> Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default.</li></ol><p>Upgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we’ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10.</p><p>Check out the below sections for more details, and go to the very last section for resource links to everything.</p><h1>Splitting into `llama-index-core` and integration packages</h1><p>The first and biggest change we’ve made is a massive packaging refactor.</p><p>LlamaIndex has evolved into a broad toolkit containing hundreds of integrations:</p><ul><li>150+ data loaders</li><li>35+ agent tools</li><li>50+ LlamaPack templates</li><li>50+ LLMs</li><li>25+ embeddings</li><li>40+ vector stores</li></ul><p>and more across the <code class="cw oq or os ot b">llama_index</code> and <code class="cw oq or os ot b">llama-hub</code> repos. The rapid growth of our ecosystem has been awesome to see, but it’s also come with growing pains:</p><ul><li>Many of the integrations lack proper tests</li><li>Users are responsible for figuring out dependencies</li><li>If an integration updates, users will have to update their entire <code class="cw oq or os ot b">llama-index</code> Python package.</li></ul><p>In response to this, we’ve done the following.</p><ul><li><strong>Created </strong><code class="cw oq or os ot b"><strong>llama-index-core</strong></code><strong> :</strong> This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations.</li><li><strong>Created separate packages for all integrations/templates:</strong> Every integration is now available as a separate package. This includes <em class="qd">all</em> integrations, including those on LlamaHub! See <a href="https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139" rel="noopener ugc nofollow" target="_blank">our Notion registry</a> page for a full list of all packages.</li></ul><p>The <code class="cw oq or os ot b">llama-index</code> package still exists, and it imports <code class="cw oq or os ot b">llama-index-core</code> and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages (<code class="cw oq or os ot b">llama-index-llms-openai</code>, <code class="cw oq or os ot b">llama-index-embeddings-openai</code>, and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in <code class="cw oq or os ot b">llama-index-readers-file</code>).</p><p><strong>NOTE:</strong> if you don’t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining <code class="cw oq or os ot b">llama-index-legacy</code> (pinned to the latest release 0.9.48) for the foreseeable future.</p><h2>Revamped Folder Structure</h2><p>We’ve completely revamped the folder structure in the <code class="cw oq or os ot b">llama_index</code> repo. The most important folders you should care about are:</p><ul><li><code class="cw oq or os ot b"><strong>llama-index-core</strong></code> : This folder contains all core LlamaIndex abstractions.</li><li><code class="cw oq or os ot b"><strong>llama-index-integrations</strong></code>: This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.</li><li><code class="cw oq or os ot b"><strong>llama-index-packs</strong></code> : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user’s application.</li></ul><p>Other folders:</p><ul><li><code class="cw oq or os ot b">llama-index-legacy</code> : contains the legacy LlamaIndex code.</li><li><code class="cw oq or os ot b">llama-index-experimental</code> : contains experimental features. Largely unused right now (outside parameter tuning).</li><li><code class="cw oq or os ot b">llama-index-finetuning</code> : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.</li></ul><p>The sub-directories in <code class="cw oq or os ot b">integrations</code> and <code class="cw oq or os ot b">packs</code> represent individual packages. The name of the folder corresponds to the package name. For instance, <code class="cw oq or os ot b">llama-index-integrations/llms/llama-index-llms-gemini</code> corresponds to the <code class="cw oq or os ot b">llama-index-llms-gemini</code> PyPI package.</p><p>Within each package folder, the source files are arranged in the same paths that you use to import them. For example, in the Gemini LLM package, you’ll see a folder called <code class="cw oq or os ot b">llama_index/llms/gemini</code> containing the source files. This folder structure is what allows you to <strong>preserve the top-level </strong><code class="cw oq or os ot b"><strong>llama_index</strong></code><strong> namespace during importing.</strong> In the case of Gemini LLM, you would pip install <code class="cw oq or os ot b">llama-index-llms-gemini</code> and then import using <code class="cw oq or os ot b">from llama_index.llms.gemini import Gemini</code>.</p><p>Every one of these subfolders also has the resources needed to packagify it: <code class="cw oq or os ot b">pyproject.toml</code>, <code class="cw oq or os ot b">poetry.lock</code>, and a <code class="cw oq or os ot b">Makefile</code> , along with a script to automatically create a package.</p><p>If you’re looking to contribute an integration or pack, don’t worry! We have a <a href="https://docs.llamaindex.ai/en/stable/contributing/contributing.html" rel="noopener ugc nofollow" target="_blank">full contributing guide</a> designed to make this as seamless as possible, make sure to check it out.</p><h1>Integrations</h1><p><em class="qd">All</em> third-party integrations are now under <code class="cw oq or os ot b">llama-index-integrations</code>. There are 19 folders in here. The main integration categories are:</p><ul><li><code class="cw oq or os ot b">llms</code></li><li><code class="cw oq or os ot b">embeddings</code></li><li><code class="cw oq or os ot b">multi_modal_llms</code></li><li><code class="cw oq or os ot b">readers</code></li><li><code class="cw oq or os ot b">tools</code></li><li><code class="cw oq or os ot b">vector_stores</code></li></ul><p>For completeness here are all the other categories: <code class="cw oq or os ot b">agent</code>, <code class="cw oq or os ot b">callbacks</code>, <code class="cw oq or os ot b">evaluation</code>, <code class="cw oq or os ot b">extractors</code>, <code class="cw oq or os ot b">graph_stores</code>, <code class="cw oq or os ot b">indices</code>, <code class="cw oq or os ot b">output_parsers</code>, <code class="cw oq or os ot b">postprocessor</code>, <code class="cw oq or os ot b">program</code>, <code class="cw oq or os ot b">question_gen</code>, <code class="cw oq or os ot b">response_synthesizers</code>, <code class="cw oq or os ot b">retrievers</code>, <code class="cw oq or os ot b">storage</code>, <code class="cw oq or os ot b">tools</code>.</p><p>The integrations in the most common categories can be found in our temporary <a href="https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139" rel="noopener ugc nofollow" target="_blank"><strong>Notion package registry page</strong></a>. All integrations can be found in <a href="https://github.com/run-llama/llama_index/tree/main/llama-index-integrations" rel="noopener ugc nofollow" target="_blank">our Github repo</a>. The folder name of each integration package corresponds to the name of the package — so if you find an integration you like, you now know how to pip install it!</p><p>We are actively working to make all integrations viewable on LlamaHub. Our vision for LlamaHub is to be <em class="qd">the</em> hub for all third-party integrations.</p><p>If you’re interested in contributing a package, see our <code class="cw oq or os ot b">contributing</code> section below!</p><h1>Usage Example</h1><p>Here is a simple example of installing and using an Anthropic LLM.</p><p><code class="cw oq or os ot b">pip install llama-index-llms-anthropic</code></p><pre><span id="8760" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-keyword">from</span> llama_index.llms.anthropic <span class="hljs-keyword">import</span> Anthropic
llm = Anthropic(api_key=<span class="hljs-string">"&amp;lt;api_key&amp;gt;"</span>)</span></pre><p>Here is an example of using a data loader.</p><p><code class="cw oq or os ot b">pip install llama-index-readers-notion</code></p><pre><span id="e19a" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-keyword">from</span> llama_index.readers.notion <span class="hljs-keyword">import</span> NotionPageReader
integration_token = os.getenv(<span class="hljs-string">"NOTION_INTEGRATION_TOKEN"</span>)
page_ids = [<span class="hljs-string">"&amp;lt;page_id&amp;gt;"</span>]
reader = NotionPageReader(integration_token=integration_token)
documents = reader.load_data(page_ids=page_ids)</span></pre><p>Here is an example of using a LlamaPack:</p><p><code class="cw oq or os ot b">pip install llama-index-packs-sentence-window-retriever</code></p><pre><span id="ec7c" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-keyword">from</span> llama_index.packs.sentence_window_retriever <span class="hljs-keyword">import</span> SentenceWindowRetrieverPack
<span class="hljs-comment"># create the pack</span>
<span class="hljs-comment"># get documents from any data loader</span>
sentence_window_retriever_pack = SentenceWindowRetrieverPack(
  documents
)
response = sentence_window_retriever_pack.run(<span class="hljs-string">"Tell me a bout a Music celebritiy."</span>)</span></pre><h1>Dealing with Breaking Changes</h1><p>This update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these:</p><pre><span id="31a0" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-comment"># no more using `llama_index.llms` as a top-level package</span>
<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-comment"># no more using `llama_index.vector_stores` as a top-level package</span>
<span class="hljs-keyword">from</span> llama_index.vector_stores <span class="hljs-keyword">import</span> PineconeVectorStore
<span class="hljs-comment"># llama_hub imports are now no longer supported.</span>
<span class="hljs-keyword">from</span> llama_hub.slack.base <span class="hljs-keyword">import</span> SlackReader</span></pre><p>Instead you can do these:</p><pre><span id="f96d" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-keyword">from</span> llama_index.llms.openai <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-keyword">from</span> llama_index.vector_stores.pinecone <span class="hljs-keyword">import</span> PineconeVectorStore
<span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> no longer import a separate llama_hub package</span>
<span class="hljs-keyword">from</span> llama_index.readers.slack <span class="hljs-keyword">import</span> SlackReader</span></pre><p>See our <a href="https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77" rel="noopener ugc nofollow" target="_blank">migration guide</a> (also described below) for more details.</p><h1>LlamaHub as a Central Hub for Integrations</h1><p>With these packaging updates, we’re expanding the concept of LlamaHub to become a central hub of <em class="qd">all</em> LlamaIndex integrations to fulfill its vision of becoming an integration site at the center of the LLM ecosystem. This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, embeddings, vector stores, callbacks, and more.</p><p><strong>This effort is still a WIP.</strong> If you go to <a href="http://llamahub.ai" rel="noopener ugc nofollow" target="_blank">llamahub.ai</a> today, you’ll see that the site has not been updated yet, and it still contains the current set of integrations (data loaders, tools, LlamaPacks, datasets). Rest assured we’ll be updating the site in a few weeks; in the meantime check out our <a href="https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139" rel="noopener ugc nofollow" target="_blank">Notion package registry</a> / <a href="https://github.com/run-llama/llama_index/tree/main/llama-index-integrations" rel="noopener ugc nofollow" target="_blank">repo</a> for a list of all integrations/packages.</p><h2>Sunsetting llama-hub repo</h2><p>Since all integrations have been moved to the llama_index repo, we are sunsetting the llama-hub repo (but <a href="https://llamahub.ai/" rel="noopener ugc nofollow" target="_blank">LlamaHub</a> itself lives on!). We did the painstaking work of migrating and packaging all existing llama-hub integrations. For all future contributions please submit directly to the llama_index repo!</p><h2>`download` syntax</h2><p>A popular UX for fetching integrations through LlamaHub has been the <code class="cw oq or os ot b">download</code> syntax: <code class="cw oq or os ot b">download_loader</code> , <code class="cw oq or os ot b">download_llama_pack</code> , and more.</p><p><strong>This will still work, but have different behavior. Check out the details below:</strong></p><ul><li><code class="cw oq or os ot b">download_llama_pack</code> : Will download a pack under <code class="cw oq or os ot b">llama-index-packs</code> to a local file on your disk. This allows you to directly use and modify the source code from the template.</li><li>Every other download function <code class="cw oq or os ot b">download_loader</code> , <code class="cw oq or os ot b">download_tool</code> : This will directly run pip install on the relevant integration package.</li></ul><h1>Deprecating ServiceContext</h1><p>Last but not least, we are deprecating our <code class="cw oq or os ot b">ServiceContext</code> construct and as a result improving the developer experience of LlamaIndex.</p><p>Our <code class="cw oq or os ot b">ServiceContext</code> object existed as a general configuration container containing an LLM, embedding model, callback, and more; it was created before we had proper LLM, embedding, prompt abstractions and was meant to be an intermediate user-facing layer to let users define these parameters.</p><p>Over time however, this object became increasingly difficult to use. Passing in an entire <code class="cw oq or os ot b">service_context</code> container to any module (index, retriever, post-processor, etc.) made it hard to reason about which component was actually getting used. Since all modules use OpenAI by default, users were getting asked to unnecessarily specify their OpenAI key even in cases where they’d want to use a local model (because the embedding model default was still OpenAI). It was also laborious to import and type out.</p><p>Another related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the <code class="cw oq or os ot b">service_context</code> to <em class="qd">all</em> modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.</p><p>Therefore we’ve made the following changes:</p><ol><li><strong>ServiceContext is now deprecated:</strong> You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.</li><li><strong>You can now define global settings:</strong> Define this once, and don’t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.</li></ol><p>All references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.</p><h1>Usage Example</h1><p>To build a <code class="cw oq or os ot b">VectorStoreIndex</code> and then query it, you can now pass in the embedding model and LLM directly</p><pre><span id="9560" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-keyword">from</span> llama_index.embeddings.openai <span class="hljs-keyword">import</span> OpenAIEmbedding
<span class="hljs-keyword">from</span> llama_index.llms.openai <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-keyword">from</span> llama_index.core.callbacks <span class="hljs-keyword">import</span> CallbackManager

embed_model = OpenAIEmbedding()
llm = OpenAI()
callback_manager = CallbackManager()
index = VectorStoreIndex.from_documents(
 documents, embed_model=embed_model, callback_manager=callback_manager
)
query_engine = index.as_query_engine(llm=llm)</span></pre><p>Or you can define a global settings object</p><pre><span id="a11f" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-keyword">from</span> llama_index.core.settings <span class="hljs-keyword">import</span> Settings
Settings.llm = llm
Settings.embed_model = embed_model
Settings.callback_manager = callback_manager
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()</span></pre><h1>Contributing to LlamaIndex v0.10</h1><p>v0.10 makes the <code class="cw oq or os ot b">llama_index</code> repo the central place for all community contributions, whether you are interested in contributing core refactors, or integrations/packs!</p><p>If you’re contributing an integration/pack, v0.10 makes it way easier for you to contribute something that can be independently versioned, tested, and packaged.</p><p>We have utility scripts to make the package creation process for an integration or pack effortless:</p><pre><span id="1b83" class="rb pa gt ot b bf rc rd l re rf"><span class="hljs-meta"># create a new pack</span>
cd ./llama-index-packs
llamaindex-cli <span class="hljs-keyword">new</span>-package --kind <span class="hljs-string">"packs"</span> --name <span class="hljs-string">"my new pack"</span>

<span class="hljs-meta"># create a new integration</span>
cd ./llama-index-integrations/readers
llamaindex-cli <span class="hljs-keyword">new</span>-pacakge --kind <span class="hljs-string">"readers"</span> --name <span class="hljs-string">"new reader"</span></span></pre><p>Take a look at our updated <a href="https://docs.llamaindex.ai/en/latest/contributing/contributing.html#contribute-a-pack-reader-tool-or-dataset-formerly-from-llama-hub" rel="noopener ugc nofollow" target="_blank">contributing guide here</a> for more details.</p><h1>Migration to v0.10</h1><p>If you want to use LlamaIndex v0.10, you will need to do two main things:</p><ol><li>Adjust imports to fit the new package structure for core modules/integrations</li><li>Deprecate ServiceContext</li></ol><p>Luckily, we’ve created a comprehensive migration guide that also contains a CLI tool to <em class="qd">automatically</em> upgrade your existing code and notebooks to v0.10!</p><p>Just do</p><pre><span id="a5f8" class="rb pa gt ot b bf rc rd l re rf">llamaindex-cli upgrade &lt;source-dir&gt;</span></pre><p><a href="https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77" rel="noopener ugc nofollow" target="_blank">Check out the full migration guide here.</a></p><h1>Next Steps</h1><p>We’ve painstakingly revamped all of our README, documentation and notebooks to reflect these v0.10 changes. Check out the below section for a compiled list of all resources.</p><h2>Documentation</h2><p><a href="https://docs.llamaindex.ai/en/stable/" rel="noopener ugc nofollow" target="_blank">v0.10 Documentation</a></p><p><a href="https://docs.llamaindex.ai/en/stable/getting_started/installation.html" rel="noopener ugc nofollow" target="_blank">v0.10 Installation Guide</a></p><p><a href="https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html" rel="noopener ugc nofollow" target="_blank">v0.10 Quickstart</a></p><p><a href="https://docs.llamaindex.ai/en/stable/contributing/contributing.html" rel="noopener ugc nofollow" target="_blank">Updated Contribution Guide</a></p><p>Temporary <a href="https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139" rel="noopener ugc nofollow" target="_blank">v0.10 Package Registry</a></p><p><a href="https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77" rel="noopener ugc nofollow" target="_blank">v0.10 Migration Guide</a></p><h2>Repo</h2><p><a href="https://github.com/run-llama/llama_index/tree/main" rel="noopener ugc nofollow" target="_blank">Repo README</a></p><p><a href="https://github.com/run-llama/llama_index/tree/main/llama-index-integrations" rel="noopener ugc nofollow" target="_blank">llama-index-integrations</a></p><p><a href="https://github.com/run-llama/llama_index/tree/main/llama-index-packs" rel="noopener ugc nofollow" target="_blank">llama-index-packs</a></p><h2>Example Notebooks</h2><p>These are mostly to show our updated import syntax.</p><ul><li><a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html" rel="noopener ugc nofollow" target="_blank">Sub-Question Query Engine</a> (primarily uses core)</li><li><a href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo.html" rel="noopener ugc nofollow" target="_blank">Weaviate Vector Store Demo</a></li><li><a href="https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html" rel="noopener ugc nofollow" target="_blank">OpenAI Agent over RAG Pipelines</a></li></ul><h1>Bug reports</h1><p>We’ll be actively monitoring our <a href="https://github.com/run-llama/llama_index/issues" rel="noopener ugc nofollow" target="_blank">Github Issues</a> and <a href="https://discord.gg/dGcwcsnxhU" rel="noopener ugc nofollow" target="_blank">Discord</a>. If you run into any issues don’t hesitate to hop into either of these channels!</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"7bdf5dbb-fed3-4dae-83d0-96b10d07eb90","_rev":"TLgH6AcXrxoqw75SBDhiHc","_type":"blogPost","_updatedAt":"2025-05-21T20:38:19Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-20T20:23:12Z","_id":"363ec4e9-0b8f-48d2-ba6a-567a9c527c3d","_rev":"rGZ2nN6K5mjOGJOoWaUhNb","_type":"people","_updatedAt":"2024-02-25T00:45:24Z","image":{"_type":"image","asset":{"_ref":"image-89523511cf20d73e3f10077add50128d077ed520-176x176-png","_type":"reference"}},"name":"LlamaIndex","slug":{"_type":"slug","current":"llamaindex"}}],"featured":false,"htmlContent":"\u003cp\u003eToday we’re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date (\u003ca href=\"https://github.com/run-llama/llama_index/pull/10537\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003esee this gargantuan PR\u003c/a\u003e), and it takes a massive step towards making LlamaIndex a next-generation, \u003cstrong\u003eproduction-ready data framework\u003c/strong\u003e for your LLM applications.\u003c/p\u003e\u003cp\u003eLlamaIndex v0.10 contains some major updates:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eWe have created a \u003c/strong\u003e\u003ccode class=\"cw oq or os ot b\"\u003e\u003cstrong\u003ellama-index-core\u003c/strong\u003e\u003c/code\u003e\u003cstrong\u003e package, and split all integrations and templates into separate packages:\u003c/strong\u003e Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still use\u003ccode class=\"cw oq or os ot b\"\u003efrom llama_index.llms.openai import OpenAI\u003c/code\u003e for a LLM.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eLlamaHub\u003c/strong\u003e\u003c/a\u003e\u003cstrong\u003e will be the central hub for all integrations: \u003c/strong\u003ethe former \u003ca href=\"https://github.com/run-llama/llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ellama-hub\u003c/a\u003e repo itself is consolidated into the main \u003ca href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ellama_index\u003c/a\u003e repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned!\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eServiceContext is deprecated:\u003c/strong\u003e Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eUpgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we’ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10.\u003c/p\u003e\u003cp\u003eCheck out the below sections for more details, and go to the very last section for resource links to everything.\u003c/p\u003e\u003ch1\u003eSplitting into `llama-index-core` and integration packages\u003c/h1\u003e\u003cp\u003eThe first and biggest change we’ve made is a massive packaging refactor.\u003c/p\u003e\u003cp\u003eLlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\u003c/p\u003e\u003cul\u003e\u003cli\u003e150+ data loaders\u003c/li\u003e\u003cli\u003e35+ agent tools\u003c/li\u003e\u003cli\u003e50+ LlamaPack templates\u003c/li\u003e\u003cli\u003e50+ LLMs\u003c/li\u003e\u003cli\u003e25+ embeddings\u003c/li\u003e\u003cli\u003e40+ vector stores\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eand more across the \u003ccode class=\"cw oq or os ot b\"\u003ellama_index\u003c/code\u003e and \u003ccode class=\"cw oq or os ot b\"\u003ellama-hub\u003c/code\u003e repos. The rapid growth of our ecosystem has been awesome to see, but it’s also come with growing pains:\u003c/p\u003e\u003cul\u003e\u003cli\u003eMany of the integrations lack proper tests\u003c/li\u003e\u003cli\u003eUsers are responsible for figuring out dependencies\u003c/li\u003e\u003cli\u003eIf an integration updates, users will have to update their entire \u003ccode class=\"cw oq or os ot b\"\u003ellama-index\u003c/code\u003e Python package.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eIn response to this, we’ve done the following.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eCreated \u003c/strong\u003e\u003ccode class=\"cw oq or os ot b\"\u003e\u003cstrong\u003ellama-index-core\u003c/strong\u003e\u003c/code\u003e\u003cstrong\u003e :\u003c/strong\u003e This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eCreated separate packages for all integrations/templates:\u003c/strong\u003e Every integration is now available as a separate package. This includes \u003cem class=\"qd\"\u003eall\u003c/em\u003e integrations, including those on LlamaHub! See \u003ca href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eour Notion registry\u003c/a\u003e page for a full list of all packages.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe \u003ccode class=\"cw oq or os ot b\"\u003ellama-index\u003c/code\u003e package still exists, and it imports \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-core\u003c/code\u003e and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages (\u003ccode class=\"cw oq or os ot b\"\u003ellama-index-llms-openai\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-embeddings-openai\u003c/code\u003e, and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-readers-file\u003c/code\u003e).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e if you don’t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-legacy\u003c/code\u003e (pinned to the latest release 0.9.48) for the foreseeable future.\u003c/p\u003e\u003ch2\u003eRevamped Folder Structure\u003c/h2\u003e\u003cp\u003eWe’ve completely revamped the folder structure in the \u003ccode class=\"cw oq or os ot b\"\u003ellama_index\u003c/code\u003e repo. The most important folders you should care about are:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003e\u003cstrong\u003ellama-index-core\u003c/strong\u003e\u003c/code\u003e : This folder contains all core LlamaIndex abstractions.\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003e\u003cstrong\u003ellama-index-integrations\u003c/strong\u003e\u003c/code\u003e: This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003e\u003cstrong\u003ellama-index-packs\u003c/strong\u003e\u003c/code\u003e : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user’s application.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eOther folders:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003ellama-index-legacy\u003c/code\u003e : contains the legacy LlamaIndex code.\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003ellama-index-experimental\u003c/code\u003e : contains experimental features. Largely unused right now (outside parameter tuning).\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003ellama-index-finetuning\u003c/code\u003e : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe sub-directories in \u003ccode class=\"cw oq or os ot b\"\u003eintegrations\u003c/code\u003e and \u003ccode class=\"cw oq or os ot b\"\u003epacks\u003c/code\u003e represent individual packages. The name of the folder corresponds to the package name. For instance, \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-integrations/llms/llama-index-llms-gemini\u003c/code\u003e corresponds to the \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-llms-gemini\u003c/code\u003e PyPI package.\u003c/p\u003e\u003cp\u003eWithin each package folder, the source files are arranged in the same paths that you use to import them. For example, in the Gemini LLM package, you’ll see a folder called \u003ccode class=\"cw oq or os ot b\"\u003ellama_index/llms/gemini\u003c/code\u003e containing the source files. This folder structure is what allows you to \u003cstrong\u003epreserve the top-level \u003c/strong\u003e\u003ccode class=\"cw oq or os ot b\"\u003e\u003cstrong\u003ellama_index\u003c/strong\u003e\u003c/code\u003e\u003cstrong\u003e namespace during importing.\u003c/strong\u003e In the case of Gemini LLM, you would pip install \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-llms-gemini\u003c/code\u003e and then import using \u003ccode class=\"cw oq or os ot b\"\u003efrom llama_index.llms.gemini import Gemini\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eEvery one of these subfolders also has the resources needed to packagify it: \u003ccode class=\"cw oq or os ot b\"\u003epyproject.toml\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003epoetry.lock\u003c/code\u003e, and a \u003ccode class=\"cw oq or os ot b\"\u003eMakefile\u003c/code\u003e , along with a script to automatically create a package.\u003c/p\u003e\u003cp\u003eIf you’re looking to contribute an integration or pack, don’t worry! We have a \u003ca href=\"https://docs.llamaindex.ai/en/stable/contributing/contributing.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003efull contributing guide\u003c/a\u003e designed to make this as seamless as possible, make sure to check it out.\u003c/p\u003e\u003ch1\u003eIntegrations\u003c/h1\u003e\u003cp\u003e\u003cem class=\"qd\"\u003eAll\u003c/em\u003e third-party integrations are now under \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-integrations\u003c/code\u003e. There are 19 folders in here. The main integration categories are:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003ellms\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003eembeddings\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003emulti_modal_llms\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003ereaders\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003etools\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003evector_stores\u003c/code\u003e\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eFor completeness here are all the other categories: \u003ccode class=\"cw oq or os ot b\"\u003eagent\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003ecallbacks\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eevaluation\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eextractors\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003egraph_stores\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eindices\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eoutput_parsers\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003epostprocessor\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eprogram\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003equestion_gen\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eresponse_synthesizers\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003eretrievers\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003estorage\u003c/code\u003e, \u003ccode class=\"cw oq or os ot b\"\u003etools\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eThe integrations in the most common categories can be found in our temporary \u003ca href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003e\u003cstrong\u003eNotion package registry page\u003c/strong\u003e\u003c/a\u003e. All integrations can be found in \u003ca href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eour Github repo\u003c/a\u003e. The folder name of each integration package corresponds to the name of the package — so if you find an integration you like, you now know how to pip install it!\u003c/p\u003e\u003cp\u003eWe are actively working to make all integrations viewable on LlamaHub. Our vision for LlamaHub is to be \u003cem class=\"qd\"\u003ethe\u003c/em\u003e hub for all third-party integrations.\u003c/p\u003e\u003cp\u003eIf you’re interested in contributing a package, see our \u003ccode class=\"cw oq or os ot b\"\u003econtributing\u003c/code\u003e section below!\u003c/p\u003e\u003ch1\u003eUsage Example\u003c/h1\u003e\u003cp\u003eHere is a simple example of installing and using an Anthropic LLM.\u003c/p\u003e\u003cp\u003e\u003ccode class=\"cw oq or os ot b\"\u003epip install llama-index-llms-anthropic\u003c/code\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"8760\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms.anthropic \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Anthropic\nllm = Anthropic(api_key=\u003cspan class=\"hljs-string\"\u003e\"\u0026amp;lt;api_key\u0026amp;gt;\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eHere is an example of using a data loader.\u003c/p\u003e\u003cp\u003e\u003ccode class=\"cw oq or os ot b\"\u003epip install llama-index-readers-notion\u003c/code\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e19a\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.readers.notion \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e NotionPageReader\nintegration_token = os.getenv(\u003cspan class=\"hljs-string\"\u003e\"NOTION_INTEGRATION_TOKEN\"\u003c/span\u003e)\npage_ids = [\u003cspan class=\"hljs-string\"\u003e\"\u0026amp;lt;page_id\u0026amp;gt;\"\u003c/span\u003e]\nreader = NotionPageReader(integration_token=integration_token)\ndocuments = reader.load_data(page_ids=page_ids)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eHere is an example of using a LlamaPack:\u003c/p\u003e\u003cp\u003e\u003ccode class=\"cw oq or os ot b\"\u003epip install llama-index-packs-sentence-window-retriever\u003c/code\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ec7c\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.packs.sentence_window_retriever \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SentenceWindowRetrieverPack\n\u003cspan class=\"hljs-comment\"\u003e# create the pack\u003c/span\u003e\n\u003cspan class=\"hljs-comment\"\u003e# get documents from any data loader\u003c/span\u003e\nsentence_window_retriever_pack = SentenceWindowRetrieverPack(\n  documents\n)\nresponse = sentence_window_retriever_pack.run(\u003cspan class=\"hljs-string\"\u003e\"Tell me a bout a Music celebritiy.\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eDealing with Breaking Changes\u003c/h1\u003e\u003cp\u003eThis update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"31a0\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-comment\"\u003e# no more using `llama_index.llms` as a top-level package\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\u003cspan class=\"hljs-comment\"\u003e# no more using `llama_index.vector_stores` as a top-level package\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e PineconeVectorStore\n\u003cspan class=\"hljs-comment\"\u003e# llama_hub imports are now no longer supported.\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_hub.slack.base \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SlackReader\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eInstead you can do these:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"f96d\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms.openai \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores.pinecone \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e PineconeVectorStore\n\u003cspan class=\"hljs-comment\"\u003e# \u003cspan class=\"hljs-doctag\"\u003eNOTE:\u003c/span\u003e no longer import a separate llama_hub package\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.readers.slack \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SlackReader\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eSee our \u003ca href=\"https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003emigration guide\u003c/a\u003e (also described below) for more details.\u003c/p\u003e\u003ch1\u003eLlamaHub as a Central Hub for Integrations\u003c/h1\u003e\u003cp\u003eWith these packaging updates, we’re expanding the concept of LlamaHub to become a central hub of \u003cem class=\"qd\"\u003eall\u003c/em\u003e LlamaIndex integrations to fulfill its vision of becoming an integration site at the center of the LLM ecosystem. This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, embeddings, vector stores, callbacks, and more.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThis effort is still a WIP.\u003c/strong\u003e If you go to \u003ca href=\"http://llamahub.ai\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ellamahub.ai\u003c/a\u003e today, you’ll see that the site has not been updated yet, and it still contains the current set of integrations (data loaders, tools, LlamaPacks, datasets). Rest assured we’ll be updating the site in a few weeks; in the meantime check out our \u003ca href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eNotion package registry\u003c/a\u003e / \u003ca href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003erepo\u003c/a\u003e for a list of all integrations/packages.\u003c/p\u003e\u003ch2\u003eSunsetting llama-hub repo\u003c/h2\u003e\u003cp\u003eSince all integrations have been moved to the llama_index repo, we are sunsetting the llama-hub repo (but \u003ca href=\"https://llamahub.ai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eLlamaHub\u003c/a\u003e itself lives on!). We did the painstaking work of migrating and packaging all existing llama-hub integrations. For all future contributions please submit directly to the llama_index repo!\u003c/p\u003e\u003ch2\u003e`download` syntax\u003c/h2\u003e\u003cp\u003eA popular UX for fetching integrations through LlamaHub has been the \u003ccode class=\"cw oq or os ot b\"\u003edownload\u003c/code\u003e syntax: \u003ccode class=\"cw oq or os ot b\"\u003edownload_loader\u003c/code\u003e , \u003ccode class=\"cw oq or os ot b\"\u003edownload_llama_pack\u003c/code\u003e , and more.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eThis will still work, but have different behavior. Check out the details below:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw oq or os ot b\"\u003edownload_llama_pack\u003c/code\u003e : Will download a pack under \u003ccode class=\"cw oq or os ot b\"\u003ellama-index-packs\u003c/code\u003e to a local file on your disk. This allows you to directly use and modify the source code from the template.\u003c/li\u003e\u003cli\u003eEvery other download function \u003ccode class=\"cw oq or os ot b\"\u003edownload_loader\u003c/code\u003e , \u003ccode class=\"cw oq or os ot b\"\u003edownload_tool\u003c/code\u003e : This will directly run pip install on the relevant integration package.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eDeprecating ServiceContext\u003c/h1\u003e\u003cp\u003eLast but not least, we are deprecating our \u003ccode class=\"cw oq or os ot b\"\u003eServiceContext\u003c/code\u003e construct and as a result improving the developer experience of LlamaIndex.\u003c/p\u003e\u003cp\u003eOur \u003ccode class=\"cw oq or os ot b\"\u003eServiceContext\u003c/code\u003e object existed as a general configuration container containing an LLM, embedding model, callback, and more; it was created before we had proper LLM, embedding, prompt abstractions and was meant to be an intermediate user-facing layer to let users define these parameters.\u003c/p\u003e\u003cp\u003eOver time however, this object became increasingly difficult to use. Passing in an entire \u003ccode class=\"cw oq or os ot b\"\u003eservice_context\u003c/code\u003e container to any module (index, retriever, post-processor, etc.) made it hard to reason about which component was actually getting used. Since all modules use OpenAI by default, users were getting asked to unnecessarily specify their OpenAI key even in cases where they’d want to use a local model (because the embedding model default was still OpenAI). It was also laborious to import and type out.\u003c/p\u003e\u003cp\u003eAnother related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the \u003ccode class=\"cw oq or os ot b\"\u003eservice_context\u003c/code\u003e to \u003cem class=\"qd\"\u003eall\u003c/em\u003e modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.\u003c/p\u003e\u003cp\u003eTherefore we’ve made the following changes:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eServiceContext is now deprecated:\u003c/strong\u003e You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eYou can now define global settings:\u003c/strong\u003e Define this once, and don’t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eAll references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.\u003c/p\u003e\u003ch1\u003eUsage Example\u003c/h1\u003e\u003cp\u003eTo build a \u003ccode class=\"cw oq or os ot b\"\u003eVectorStoreIndex\u003c/code\u003e and then query it, you can now pass in the embedding model and LLM directly\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9560\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.embeddings.openai \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAIEmbedding\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms.openai \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.core.callbacks \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e CallbackManager\n\nembed_model = OpenAIEmbedding()\nllm = OpenAI()\ncallback_manager = CallbackManager()\nindex = VectorStoreIndex.from_documents(\n documents, embed_model=embed_model, callback_manager=callback_manager\n)\nquery_engine = index.as_query_engine(llm=llm)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOr you can define a global settings object\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a11f\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.core.settings \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Settings\nSettings.llm = llm\nSettings.embed_model = embed_model\nSettings.callback_manager = callback_manager\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eContributing to LlamaIndex v0.10\u003c/h1\u003e\u003cp\u003ev0.10 makes the \u003ccode class=\"cw oq or os ot b\"\u003ellama_index\u003c/code\u003e repo the central place for all community contributions, whether you are interested in contributing core refactors, or integrations/packs!\u003c/p\u003e\u003cp\u003eIf you’re contributing an integration/pack, v0.10 makes it way easier for you to contribute something that can be independently versioned, tested, and packaged.\u003c/p\u003e\u003cp\u003eWe have utility scripts to make the package creation process for an integration or pack effortless:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"1b83\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003e\u003cspan class=\"hljs-meta\"\u003e# create a new pack\u003c/span\u003e\ncd ./llama-index-packs\nllamaindex-cli \u003cspan class=\"hljs-keyword\"\u003enew\u003c/span\u003e-package --kind \u003cspan class=\"hljs-string\"\u003e\"packs\"\u003c/span\u003e --name \u003cspan class=\"hljs-string\"\u003e\"my new pack\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-meta\"\u003e# create a new integration\u003c/span\u003e\ncd ./llama-index-integrations/readers\nllamaindex-cli \u003cspan class=\"hljs-keyword\"\u003enew\u003c/span\u003e-pacakge --kind \u003cspan class=\"hljs-string\"\u003e\"readers\"\u003c/span\u003e --name \u003cspan class=\"hljs-string\"\u003e\"new reader\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eTake a look at our updated \u003ca href=\"https://docs.llamaindex.ai/en/latest/contributing/contributing.html#contribute-a-pack-reader-tool-or-dataset-formerly-from-llama-hub\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003econtributing guide here\u003c/a\u003e for more details.\u003c/p\u003e\u003ch1\u003eMigration to v0.10\u003c/h1\u003e\u003cp\u003eIf you want to use LlamaIndex v0.10, you will need to do two main things:\u003c/p\u003e\u003col\u003e\u003cli\u003eAdjust imports to fit the new package structure for core modules/integrations\u003c/li\u003e\u003cli\u003eDeprecate ServiceContext\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eLuckily, we’ve created a comprehensive migration guide that also contains a CLI tool to \u003cem class=\"qd\"\u003eautomatically\u003c/em\u003e upgrade your existing code and notebooks to v0.10!\u003c/p\u003e\u003cp\u003eJust do\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"a5f8\" class=\"rb pa gt ot b bf rc rd l re rf\"\u003ellamaindex-cli upgrade \u0026lt;source-dir\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003ca href=\"https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eCheck out the full migration guide here.\u003c/a\u003e\u003c/p\u003e\u003ch1\u003eNext Steps\u003c/h1\u003e\u003cp\u003eWe’ve painstakingly revamped all of our README, documentation and notebooks to reflect these v0.10 changes. Check out the below section for a compiled list of all resources.\u003c/p\u003e\u003ch2\u003eDocumentation\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ev0.10 Documentation\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/getting_started/installation.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ev0.10 Installation Guide\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ev0.10 Quickstart\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/contributing/contributing.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eUpdated Contribution Guide\u003c/a\u003e\u003c/p\u003e\u003cp\u003eTemporary \u003ca href=\"https://pretty-sodium-5e0.notion.site/ce81b247649a44e4b6b35dfb24af28a6?v=53b3c2ced7bb4c9996b81b83c9f01139\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ev0.10 Package Registry\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://pretty-sodium-5e0.notion.site/v0-10-0-Migration-Guide-6ede431dcb8841b09ea171e7f133bd77\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ev0.10 Migration Guide\u003c/a\u003e\u003c/p\u003e\u003ch2\u003eRepo\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://github.com/run-llama/llama_index/tree/main\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRepo README\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-integrations\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ellama-index-integrations\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003ca href=\"https://github.com/run-llama/llama_index/tree/main/llama-index-packs\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ellama-index-packs\u003c/a\u003e\u003c/p\u003e\u003ch2\u003eExample Notebooks\u003c/h2\u003e\u003cp\u003eThese are mostly to show our updated import syntax.\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSub-Question Query Engine\u003c/a\u003e (primarily uses core)\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eWeaviate Vector Store Demo\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://docs.llamaindex.ai/en/stable/examples/agent/openai_agent.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eOpenAI Agent over RAG Pipelines\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eBug reports\u003c/h1\u003e\u003cp\u003eWe’ll be actively monitoring our \u003ca href=\"https://github.com/run-llama/llama_index/issues\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGithub Issues\u003c/a\u003e and \u003ca href=\"https://discord.gg/dGcwcsnxhU\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eDiscord\u003c/a\u003e. If you run into any issues don’t hesitate to hop into either of these channels!\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/503b8a293ad96120ef9db9c9c4473045cb8c5eca-2004x2081.png","publishedDate":"2024-02-12","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"}],"slug":{"_type":"slug","current":"llamaindex-v0-10-838e735948f8"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"3fa3e1f7-c1c0-4fed-b4da-4eb17ed92420","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"upgrade"},"title":"Upgrade"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"}],"title":"LlamaIndex v0.10"},"publishedDate":"Invalid Date"},"params":{"slug":"llamaindex-v0-10-838e735948f8"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"llamaindex-v0-10-838e735948f8"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>