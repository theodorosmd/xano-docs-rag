<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Building a Slack bot that learns with LlamaIndex, Qdrant and Render — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Building a Slack bot that learns with LlamaIndex, Qdrant and Render — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Building a Slack bot that learns with LlamaIndex, Qdrant and Render — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Building a Slack bot that learns with LlamaIndex, Qdrant and Render — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="512" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/llamaindex">LlamaIndex</a> <!-- -->•<!-- --> <!-- -->2024-01-25</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Building a Slack bot that learns with LlamaIndex, Qdrant and Render</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/qdrant"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Qdrant</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/render"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Render</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/slack"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Slack</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><p>In this post we’re going to walk you through the process of building and deploying a Slackbot that listens to your conversations, learns from them, and uses that knowledge to answer questions about what’s going on in your Slack workspace. We’ll also deploy it to production on Render!</p><h1>Things you’ll need to start</h1><ul><li>Rudimentary understanding of LlamaIndex. If you haven’t got that, the <a href="https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html" rel="noopener ugc nofollow" target="_blank">starter tutorial</a> in our documentation will give you as much as you need to understand this tutorial and takes only a few minutes.</li><li>A working knowledge of Python, and Python 3.11 or higher installed</li><li>A Slack workspace you can install apps to (so you’ll need to be an admin)</li><li>A clone of <a href="https://github.com/run-llama/llamabot" rel="noopener ugc nofollow" target="_blank">our Slackbot repo</a> on your local machine. We’ll be referring to files in this repo throughout the post.</li></ul><h1>Step 1: Create a Slack app, and install it to your workspace</h1><p>This is the most complicated step, because Slack is very picky about permissions.</p><p>The very first version of your Slackbot is going to be only about 20 lines of code. All it does is provide a “challenge” endpoint that Slack needs to verify your app is available. You can see this code as the file <code class="cw qc qd qe qf b">1_flask.py</code> in the repo. Let's walk through it.</p><p>First we bring in your dependencies. You’ll need to install these with pip or poetry if you don’t have them already.</p><pre><span id="62df" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, request, jsonify</span></pre><p>Now we’ll create your flask app and set it up so it can run in development.</p><pre><span id="691b" class="qo or gt qf b bf qp qq l qr qs">flask_app = Flask(__name__)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    flask_app.run(port=<span class="hljs-number">3000</span>)</span></pre><p>Between those lines we’ll add our basic route: if a POST request is received that contains a JSON object with a <code class="cw qc qd qe qf b">challenge</code> key, we'll return the value of that key. Otherwise we'll do nothing.</p><pre><span id="c25c" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-meta">@flask_app.route(<span class="hljs-params"><span class="hljs-string">"/"</span>, methods=[<span class="hljs-string">"POST"</span>]</span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">slack_challenge</span>():
    <span class="hljs-keyword">if</span> request.json <span class="hljs-keyword">and</span> <span class="hljs-string">"challenge"</span> <span class="hljs-keyword">in</span> request.json:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"Received challenge"</span>)
        <span class="hljs-keyword">return</span> jsonify({<span class="hljs-string">"challenge"</span>: request.json[<span class="hljs-string">"challenge"</span>]})
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"Got unknown request incoming"</span>)
        <span class="hljs-built_in">print</span>(request.json)
    <span class="hljs-keyword">return</span></span></pre><h1>Make your app available to Slack</h1><p>To configure a Slack app, it needs to be running somewhere Slack can see it. So let’s run our Slack app:</p><pre><span id="9edb" class="qo or gt qf b bf qp qq l qr qs">python 1_flask.py</span></pre><p>And we’ll set it up so the world can see it using <a href="https://ngrok.com/" rel="noopener ugc nofollow" target="_blank">ngrok</a>. You’ll need to download and install ngrok for this step. Once you have it installed, run the following command so it can find our app running on port 3000:</p><pre><span id="45a9" class="qo or gt qf b bf qp qq l qr qs">ngrok http 3000</span></pre><p>ngrok will give you an HTTPS url like <code class="cw qc qd qe qf b">https://1bf6-64-38-189-168.ngrok-free.app</code>. Make a note of it, because we need to give that to Slack. Also keep in mind that if you stop ngrok and start it again, this URL will change and you'll need to tell Slack about that. You'll only need this during development.</p><h1>Register your app with Slack</h1><p>Go to the <a href="https://api.slack.com/apps" rel="noopener ugc nofollow" target="_blank">Slack API site</a> and click “Create New App”. You’ll see a screen like this, you’ll want to pick “from scratch”:</p><figure><img src="/blog/images/1*z6jtbvuT9krVCq9joYjDzw.png" alt="" width="700" height="568"></figure><p>Pick a nice friendly name and the workspace you want to install it to. You’ll see a screen like this:</p><figure><img src="/blog/images/1*QkyK71pJcbZDZknI1tagMQ.png" alt="" width="700" height="677"></figure><p>Next you’ll want to set up what permissions your app needs. Click the “Permissions” link in the bottom right:</p><figure><img src="/blog/images/1*ZOHiALXKFHIN4klcW5942w.png" alt="" width="700" height="947"></figure><p>This will bring you to the “scopes” screen where you’ll need to add all the scopes you see in this picture, namely:</p><ul><li>channels:read — the lets your app see what channels are avaialble</li><li>channels:join — this lets your app join channels</li><li>channels:history — this lets your app see previous messages in channels</li><li>chat:write — this lets your app send messages</li><li>users:read — this lets your app see people’s names</li></ul><figure><img src="/blog/images/1*FWueDTZxYh8-kNXYU9PX_Q.png" alt="" width="700" height="990"></figure><p>Once you’ve saved those scopes, scroll up to “Install to workspace” to install your app.</p><p>You now need to tell Slack where your app is so you can receive messages from it. Click the “Event Subscriptions” link in the left nav and fill it out so it looks something like this, specifically:</p><ul><li>Set your Request URL to that URL that ngrok gave you earlier</li><li>Subscribe to the <code class="cw qc qd qe qf b">message.channels</code> event</li></ul><figure><img src="/blog/images/1*D8GcOA9uoSqNz6t9uX6dog.png" alt="" width="700" height="974"></figure><p>If your app is running and ngrok is correctly tunneling, your Request URL should be Verified.</p><p>Phew! That was a lot. Your Slack app is now registered and Slack will send it messages. But to get those messages, you have to tell it to join a channel.</p><h1>Step 2: Join a channel, and reply to messages</h1><p>To do this we’ll need to extend our app. You can see the final result of this step in <code class="cw qc qd qe qf b">2_join_and_reply.py</code>. Let's walk through what we've added:</p><pre><span id="5982" class="qo or gt qf b bf qp qq l qr qs">import dotenv, os
dotenv.load_dotenv()</span></pre><p>We need some environment variables, so you’ll need to add these lines and install <code class="cw qc qd qe qf b">python-dotenv</code>. You'll also need to create a <code class="cw qc qd qe qf b">.env</code> file in the root of your project with three values:</p><ul><li><code class="cw qc qd qe qf b">OPENAI_API_KEY</code>: your OpenAI API key. You don't need this quite yet but you may as well <a href="https://platform.openai.com/" rel="noopener ugc nofollow" target="_blank">get it now</a>.</li><li><code class="cw qc qd qe qf b">SLACK_BOT_TOKEN</code>: you can find this in the "OAuth and Permissions" section of your Slack app.</li><li><code class="cw qc qd qe qf b">SLACK_SIGNING_SECRET</code>: you can find this in the "Basic Information" section of your Slack app.</li></ul><p>We’re going to use Slack’s handy Python SDK to build our app, so pip install <code class="cw qc qd qe qf b">slack-bolt</code> and then update all our imports:</p><pre><span id="8a8e" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-keyword">from</span> slack_bolt <span class="hljs-keyword">import</span> App
<span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask, request, jsonify
<span class="hljs-keyword">from</span> slack_bolt.adapter.flask <span class="hljs-keyword">import</span> SlackRequestHandler</span></pre><p>Now initialize a Slack Bolt app using those secrets we set just now:</p><pre><span id="27ee" class="qo or gt qf b bf qp qq l qr qs">app = App(
    token=os.environ.get(<span class="hljs-string">"SLACK_BOT_TOKEN"</span>),
    signing_secret=os.environ.get(<span class="hljs-string">"SLACK_SIGNING_SECRET"</span>)
)
handler = SlackRequestHandler(app)</span></pre><p>To listen to messages, the bot has to be in a channel. You can get it to join any and all public channels, but for the purposes of testing I’ve created a channel called <code class="cw qc qd qe qf b">#bot-testing</code> and that's the one it's joining here:</p><pre><span id="be3e" class="qo or gt qf b bf qp qq l qr qs">channel_list = app.client.conversations_list().<span class="hljs-type">data</span>
<span class="hljs-variable">channel</span> <span class="hljs-operator">=</span> next((channel <span class="hljs-keyword">for</span> channel in channel_list.get(<span class="hljs-string">'channels'</span>) <span class="hljs-keyword">if</span> channel.get(<span class="hljs-string">"name"</span>) == <span class="hljs-string">"bot-testing"</span>), None)
channel_id = channel.get(<span class="hljs-string">'id'</span>)
app.client.conversations_join(channel=channel_id)</span></pre><p><code class="cw qc qd qe qf b">app.client</code> is the Bolt framework's Slack WebClient, so you can do anything a WebClient can do directly from within the framework. The final addition here is a very simple message listener:</p><pre><span id="c21c" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-meta">@app.message()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">reply</span>(<span class="hljs-params">message, say</span>):
    <span class="hljs-built_in">print</span>(message)
    say(<span class="hljs-string">"Yes?"</span>)</span></pre><p>In the Bolt framework, the <code class="cw qc qd qe qf b">@app.message</code> decorator tells the framework to trigger this method when it receives a message event. The <code class="cw qc qd qe qf b">say</code> parameter is a function that will send a message back to the channel the message came from. So this code will send a message back to the channel saying "Yes?" every time it receives a message.</p><p>Let’s try it out! Stop running <code class="cw qc qd qe qf b">1_flask.py</code> and run <code class="cw qc qd qe qf b">python 2_join_and_reply.py</code> instead. You don't need to restart <code class="cw qc qd qe qf b">ngrok</code>, it will continue to send messages to port 3000 as before. Here's me trying it out:</p><figure><img src="/blog/images/1*VLN-JCG8el8o4NcvttwmpA.png" alt="" width="524" height="418"></figure><p>Success! We have a very annoying bot that replies to every single thing anybody says. We can do better!</p><h1>Step 3: reply only to messages that mention the bot</h1><p>This is a pretty simple change on the surface, but Slack’s incoming message format is a little complicated so we have to add a fair bit of code. You can see the final results in <code class="cw qc qd qe qf b">3_reply_to_mentions.py</code>.</p><p>First, to tell when our bot is being mentioned, we need our bot’s User ID. Under the hood, Slack doesn’t use user names or even @-handles, but a globally unique ID across all Slack installations. We have to get that:</p><pre><span id="337f" class="qo or gt qf b bf qp qq l qr qs">auth_response = app.client.auth_test()
bot_user_id = auth_response["user_id"]</span></pre><p>Now we add an annoyingly complicated chunk of code that parses through Slack’s message object to see what user is mentioned in an incoming message. If it’s the bot, the bot replies, otherwise it just ignores the message. As we go further, we’ll treat messages to the bot as “queries” and any other message as a “fact” for it to store, but we won’t be storing it just yet.</p><pre><span id="18cf" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-meta">@app.message()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">reply</span>(<span class="hljs-params">message, say</span>):
    <span class="hljs-keyword">if</span> message.get(<span class="hljs-string">'blocks'</span>):
        <span class="hljs-keyword">for</span> block <span class="hljs-keyword">in</span> message.get(<span class="hljs-string">'blocks'</span>):
            <span class="hljs-keyword">if</span> block.get(<span class="hljs-string">'type'</span>) == <span class="hljs-string">'rich_text'</span>:
                <span class="hljs-keyword">for</span> rich_text_section <span class="hljs-keyword">in</span> block.get(<span class="hljs-string">'elements'</span>):
                    <span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> rich_text_section.get(<span class="hljs-string">'elements'</span>):
                        <span class="hljs-keyword">if</span> element.get(<span class="hljs-string">'type'</span>) == <span class="hljs-string">'user'</span> <span class="hljs-keyword">and</span> element.get(<span class="hljs-string">'user_id'</span>) == bot_user_id:
                            <span class="hljs-keyword">for</span> element <span class="hljs-keyword">in</span> rich_text_section.get(<span class="hljs-string">'elements'</span>):
                                <span class="hljs-keyword">if</span> element.get(<span class="hljs-string">'type'</span>) == <span class="hljs-string">'text'</span>:
                                    query = element.get(<span class="hljs-string">'text'</span>)
                                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Somebody asked the bot: <span class="hljs-subst">{query}</span>"</span>)
                                    say(<span class="hljs-string">"Yes?"</span>)
                                    <span class="hljs-keyword">return</span>
    <span class="hljs-comment"># otherwise do something else with it</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Saw a fact: "</span>, message.get(<span class="hljs-string">'text'</span>))</span></pre><p>Oof. That took a while to get right! But now our bot only replies when it’s mentioned:</p><figure><img src="/blog/images/1*Nob9nVc3BEtxBq9e3BCMLQ.png" alt="" width="626" height="268"></figure><h1>Step 4: use LlamaIndex to store facts and answer questions</h1><p>We’re all the way at step 4 and we still haven’t done anything with LlamaIndex! But now’s the time. In <code class="cw qc qd qe qf b">4_incremental_rag.py</code> you'll see a demonstration of a simple command-line Python script that uses LlamaIndex to store facts and answer questions. I won't walk you through every line (the script has helpful comments for that), but let's look at the important ones. Remember to <code class="cw qc qd qe qf b">pip install llama-index</code>!</p><p>First we create a new <code class="cw qc qd qe qf b">VectorStoreIndex</code>, an in-memory <a href="https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#vector-store-index" rel="noopener ugc nofollow" target="_blank">vector store</a> where we'll be storing our facts. It's empty to start with.</p><pre><span id="1358" class="qo or gt qf b bf qp qq l qr qs">index = VectorStoreIndex([])</span></pre><p>Next we create 3 <code class="cw qc qd qe qf b">Document</code> objects and insert them each into our index. Real documents can be huge blocks of text, whole PDFs, even images, but these are just some simple, Slack-message-sized facts.</p><pre><span id="4923" class="qo or gt qf b bf qp qq l qr qs">doc1 = Document(text="Molly is a cat")
doc2 = Document(text="Doug is a dog")
doc3 = Document(text="Carl is a rat")

index.insert(doc1)
index.insert(doc2)
index.insert(doc3)</span></pre><p>And finally we create a <a href="https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html" rel="noopener ugc nofollow" target="_blank">query engine</a> from our index and ask it a question:</p><pre><span id="c918" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-comment"># run a query</span>
query_engine = index.as_query_engine()
response = query_engine.query(<span class="hljs-string">"Who is Molly?"</span>)
<span class="hljs-built_in">print</span>(response)</span></pre><p>The result is “Molly is a cat” plus a whole lot of debugging info because we turned on noisy debugging in <code class="cw qc qd qe qf b">4_incremental_rag.py</code>. You can see the prompt we sent to the LLM, the context it retrieved from the index, and the response it generated and sent back to us.</p><figure><img src="/blog/images/1*EXc4mvyUgOc2VgV4ENrDPw.png" alt="" width="700" height="323"></figure><h1>Step 5: use LlamaIndex to store facts and answer questions in Slack</h1><p>In <code class="cw qc qd qe qf b">5_rag_in_slack.py</code> we are combining the two things we had before: script 3, where we reply to queries, and script 4, where we store facts and answer questions. Once again we won't walk through every line, but here are the important changes:</p><p>First <code class="cw qc qd qe qf b">pip install llama-index</code> if you didn't already, and bring in your deps. Initialize your index while you're at it:</p><pre><span id="05ed" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> VectorStoreIndex, Document

index = VectorStoreIndex([])</span></pre><p>Where previously we were just replying with “Yes?” (line 73) let’s instead send a query to the query engine and reply with the response:</p><pre><span id="d686" class="qo or gt qf b bf qp qq l qr qs">query = element.get(<span class="hljs-string">'text'</span>)
query_engine = index.as_query_engine()
response = query_engine.query(query)
say(str(response))</span></pre><p>And where previously we were just noting that we’d seen a fact (line 82), let’s store it in the index:</p><pre><span id="fe76" class="qo or gt qf b bf qp qq l qr qs">index.insert(Document(text=message.get(<span class="hljs-string">'text'</span>)))</span></pre><p>The result is a Slackbot that can answer questions about what it’s been told:</p><figure><img src="/blog/images/1*Zk-kD7S8nNZNSzTzi-_MSQ.png" alt="" width="700" height="396"></figure><p>Amazing! You can easily imagine a bot that listens to everybody’s conversations and is able to answer questions about things people said weeks or months ago, saving everybody time and effort searching through old messages.</p><h1>Step 6: persist our memory</h1><p>Our bot has a critical flaw though: the index is stored only in memory. If we restart the bot, it forgets everything:</p><figure><img src="/blog/images/1*2q-qOH4dVV0ehOJG2h6ycg.png" alt="" width="700" height="514"></figure><p>In <code class="cw qc qd qe qf b">6_qdrant.py</code> we bring in <a href="https://qdrant.tech/" rel="noopener ugc nofollow" target="_blank">Qdrant</a>, an open-source, local vector database that stores these facts on disk instead. That way if we restart our bot it remembers what was said before. <code class="cw qc qd qe qf b">pip install qdrant-client</code> and bring in some new deps:</p><pre><span id="1f81" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-keyword">import</span> qdrant_client
<span class="hljs-keyword">from</span> llama_index.vector_stores.qdrant <span class="hljs-keyword">import</span> QdrantVectorStore</span></pre><p>Now we’ll initialize the Qdrant client, attach it to a storage context, and give that storage context to our index when we initialize it:</p><pre><span id="b019" class="qo or gt qf b bf qp qq l qr qs">client = qdrant_client.QdrantClient(
    path="./qdrant_data"
)
vector_store = QdrantVectorStore(client=client, collection_name="slack_messages")
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex([],storage_context=storage_context)</span></pre><p>That’s it for this step! Your bot now survives reboots, and remembers that I typoed “Doug” as “Dough” and was too lazy to fix it for the screenshot:</p><figure><img src="/blog/images/1*QvqZmlSuiifdmQTzueSaGA.png" alt="" width="700" height="379"></figure><h1>Step 7: make recent messages more important</h1><p>We now have a pretty capable bot! But it has a subtle problem: people can say conflicting things, and it doesn’t have a way to decide who was “right”, such as when I change my mind about what the dog’s name should be:</p><figure><img src="/blog/images/1*Chf0RarxYCr00pKC2HuL2g.png" alt="" width="700" height="281"></figure><p>In real Slack conversations, as a situation evolves people might move from saying a project is “in planning” to “underway” to “launched”. So we need a way to tell the bot that more recent messages are more important than older ones.</p><p>To make this happen we have to do quite a bit of refactoring, the final results of which you can see in <code class="cw qc qd qe qf b">7_recency.py</code>. First we need a bunch of new deps:</p><pre><span id="f751" class="qo or gt qf b bf qp qq l qr qs"><span class="hljs-keyword">import</span> datetime, uuid
<span class="hljs-keyword">from</span> llama_index.schema <span class="hljs-keyword">import</span> TextNode
<span class="hljs-keyword">from</span> llama_index.prompts <span class="hljs-keyword">import</span> PromptTemplate
<span class="hljs-keyword">from</span> llama_index.postprocessor <span class="hljs-keyword">import</span> FixedRecencyPostprocessor
<span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> set_global_handler</span></pre><p>To make recent messages more important, we have to know when a message was sent. To do that we are going to stop inserting <code class="cw qc qd qe qf b">Documents</code> into the index and instead insert <code class="cw qc qd qe qf b">Nodes</code>, to which we're going to attach the timestamp as metadata (under the hood, our Documents were always being converted into Nodes anyway so this doesn't change much):</p><pre><span id="856c" class="qo or gt qf b bf qp qq l qr qs">dt_object = datetime.datetime.fromtimestamp(<span class="hljs-type">float</span>(message.get(<span class="hljs-string">'ts'</span>)))
formatted_time = dt_object.strftime(<span class="hljs-string">'%Y-%m-%d %H:%M:%S'</span>)

# get the message <span class="hljs-type">text</span>
<span class="hljs-variable">text</span> <span class="hljs-operator">=</span> message.get(<span class="hljs-string">'text'</span>)
# create a node with <span class="hljs-type">metadata</span>
<span class="hljs-variable">node</span> <span class="hljs-operator">=</span> TextNode(
    text=text,
    id_=str(uuid.uuid4()),
    metadata={
        <span class="hljs-string">"when"</span>: formatted_time
    }
)
index.insert_nodes([node])</span></pre><p>I’ve also factored out the reply logic from message handling into its own function, <code class="cw qc qd qe qf b">answer_question</code>, just to make things a little easier to read. The first thing we're going to change is the prompt that we give to our LLM: we have to tell it that more recent messages are important. To do this we create a prompt template:</p><pre><span id="28ed" class="qo or gt qf b bf qp qq l qr qs">template = (
    <span class="hljs-string">"Your context is a series of chat messages. Each one is tagged with 'who:' \n"</span>
    <span class="hljs-string">"indicating who was speaking and 'when:' indicating when they said it, \n"</span>
    <span class="hljs-string">"followed by a line break and then what they said. There can be up to 20 chat messages.\n"</span>
    <span class="hljs-string">"The messages are sorted by recency, so the most recent one is first in the list.\n"</span>
    <span class="hljs-string">"The most recent messages should take precedence over older ones.\n"</span>
    <span class="hljs-string">"---------------------\n"</span>
    <span class="hljs-string">"{context_str}"</span>
    <span class="hljs-string">"\n---------------------\n"</span>
    <span class="hljs-string">"You are a helpful AI assistant who has been listening to everything everyone has been saying. \n"</span>
    <span class="hljs-string">"Given the most relevant chat messages above, please answer this question: {query_str}\n"</span>
)
qa_template = <span class="hljs-title class_">PromptTemplate</span>(template)</span></pre><p>The fun thing about working with LLMs is how often you end up just describing what you’re doing in English and that being what you send to the LLM. A prompt template will automatically get the <code class="cw qc qd qe qf b">context_str</code> and <code class="cw qc qd qe qf b">query_str</code> from the query engine. But we have to set this template on our query engine, like so:</p><pre><span id="81ea" class="qo or gt qf b bf qp qq l qr qs">query_engine.update_prompts(
    {"response_synthesizer:text_qa_template": qa_template}
)</span></pre><p>Now there’s two more things we’re going to change. We’re going to take the results we get from the vector store and sort them by recency, something LlamaIndex has a built-in class for. It’s called the <code class="cw qc qd qe qf b">FixedRecencyPostprocessor</code>. We tell it the key that holds the timestamp (which we defined earlier on the nodes, above) and how many results it should return:</p><pre><span id="4492" class="qo or gt qf b bf qp qq l qr qs">postprocessor = FixedRecencyPostprocessor(
    top_k=20, 
    date_key="when", # the key in the metadata to find the date
    service_context=ServiceContext.from_defaults()
)</span></pre><p>Then we need to create our query engine with the postprocessor attached:</p><pre><span id="6e1f" class="qo or gt qf b bf qp qq l qr qs">query_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor])</span></pre><p>While we were at it we did our final thing, which was pass <code class="cw qc qd qe qf b">similarity_top_k=20</code>, which means the vector store will give us 20 Slack messages as context (the default is just 2, because usually the chunks of text in a Node are a lot bigger).</p><p>Tada! Now the bot knows to take more recent statements as the truth.</p><figure><img src="/blog/images/1*ZFpJSOdrjdb-zvYFo22wLg.png" alt="" width="700" height="367"></figure><h1>Step 8: draw the rest of the owl</h1><p>This bot is working pretty well now, but I was having such fun when building it I got carried away and added two more features:</p><ul><li>I attached metadata about <em class="re">who</em> was speaking, not just when, so the bot can answer questions like “What did Logan say about the project?”</li><li>My colleagues interacting with the bot tried to ask follow-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it’s in a thread, and treat replies in a thread as follow-up questions, even if the user doesn’t mention the bot directly:</li></ul><figure><img src="/blog/images/1*5UcgwlMvyOnFn-zLpfEhTg.png" alt="" width="700" height="377"></figure><p>The code to make both of those happen is in <code class="cw qc qd qe qf b">8_rest_of_the_owl.py</code> but I'm not going to be stepping through it line by line. We have to deploy this thing!</p><h1>Step 9: deploy to Render</h1><p>Until now we’ve been working with local scripts running through the ngrok tunnel, but even the most dedicated coder turns their laptop off sometimes. Let’s put this thing on a real server.</p><h1>Login to Render</h1><p>We’ll be deploying to <a href="https://render.com/" rel="noopener ugc nofollow" target="_blank">Render</a>, a Python-friendly hosting service that’s free for small projects. Sign up for an account (I recommend logging in with GitHub).</p><h1>Create a new GitHub repository</h1><p>Render deploys things from GitHub repositories, so you’ll need to create a new one and copy 2 files from our existing repo into it:</p><ul><li><code class="cw qc qd qe qf b">pyproject.toml</code></li><li><code class="cw qc qd qe qf b">8_rest_of_the_owl.py</code> which we're going to rename to "app.py" for simplicity.</li></ul><p>Commit those and push them up to GitHub.</p><h1>Create a new Render web service</h1><p>In Render, create a new web service. Connect it to the repo on GitHub you just created:</p><figure><img src="/blog/images/1*ezvvGYoT_erwiJ_6fUPwDA.png" alt="" width="700" height="675"></figure><p>Render will probably automatically detect that this is a Python app but you should make sure the following settings are correct:</p><ul><li>Name: any name you choose</li><li>Region: any region is fine</li><li>Branch: main</li><li>Root directory: (blank, meaning root)</li><li>Runtime: Python 3</li><li>Build command: <code class="cw qc qd qe qf b">poetry install</code></li><li>Start command: <code class="cw qc qd qe qf b">gunicorn app:flask_app</code> (this will definitely need to be set)</li></ul><p>You’ll also need to scroll down and set some environment variables:</p><ul><li>PYTHON_VERSION: 3.11.6 (or whatever version you’re using)</li><li>OPENAI_API_KEY: your OpenAI API key</li><li>SLACK_BOT_TOKEN: your Slack bot token</li><li>SLACK_SIGNING_SECRET: your Slack signing secret from before</li></ul><p>Then click deploy and away you go!</p><figure><img src="/blog/images/1*kogMt0pPsSyHv_M0D4wBDw.png" alt="" width="700" height="491"></figure><p>You now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations!</p><h1>What next?</h1><p>There’s a whole bunch of features you could add to this bot, roughly in increasing order of difficulty:</p><ul><li>Join every channel instead of just one, clearly!</li><li>Add a way to tell the bot to forget things (delete nodes)</li><li>Give the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar</li><li>Give the bot “tags” so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way</li><li>Add multi-modal abilities, so the bot can read images and even reply with generated images</li><li>And tons more!</li></ul><p>This bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"a255f5fc-e84a-41ae-b5a8-9422c4c482e6","_rev":"05dtDS0H5iRVsxYMarZNu3","_type":"blogPost","_updatedAt":"2025-05-21T20:39:09Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-20T20:23:12Z","_id":"363ec4e9-0b8f-48d2-ba6a-567a9c527c3d","_rev":"rGZ2nN6K5mjOGJOoWaUhNb","_type":"people","_updatedAt":"2024-02-25T00:45:24Z","image":{"_type":"image","asset":{"_ref":"image-89523511cf20d73e3f10077add50128d077ed520-176x176-png","_type":"reference"}},"name":"LlamaIndex","slug":{"_type":"slug","current":"llamaindex"}}],"featured":false,"htmlContent":"\u003cp\u003eIn this post we’re going to walk you through the process of building and deploying a Slackbot that listens to your conversations, learns from them, and uses that knowledge to answer questions about what’s going on in your Slack workspace. We’ll also deploy it to production on Render!\u003c/p\u003e\u003ch1\u003eThings you’ll need to start\u003c/h1\u003e\u003cul\u003e\u003cli\u003eRudimentary understanding of LlamaIndex. If you haven’t got that, the \u003ca href=\"https://docs.llamaindex.ai/en/stable/getting_started/starter_example.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003estarter tutorial\u003c/a\u003e in our documentation will give you as much as you need to understand this tutorial and takes only a few minutes.\u003c/li\u003e\u003cli\u003eA working knowledge of Python, and Python 3.11 or higher installed\u003c/li\u003e\u003cli\u003eA Slack workspace you can install apps to (so you’ll need to be an admin)\u003c/li\u003e\u003cli\u003eA clone of \u003ca href=\"https://github.com/run-llama/llamabot\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eour Slackbot repo\u003c/a\u003e on your local machine. We’ll be referring to files in this repo throughout the post.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eStep 1: Create a Slack app, and install it to your workspace\u003c/h1\u003e\u003cp\u003eThis is the most complicated step, because Slack is very picky about permissions.\u003c/p\u003e\u003cp\u003eThe very first version of your Slackbot is going to be only about 20 lines of code. All it does is provide a “challenge” endpoint that Slack needs to verify your app is available. You can see this code as the file \u003ccode class=\"cw qc qd qe qf b\"\u003e1_flask.py\u003c/code\u003e in the repo. Let's walk through it.\u003c/p\u003e\u003cp\u003eFirst we bring in your dependencies. You’ll need to install these with pip or poetry if you don’t have them already.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"62df\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e flask \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Flask, request, jsonify\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow we’ll create your flask app and set it up so it can run in development.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"691b\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eflask_app = Flask(__name__)\n\n\u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e __name__ == \u003cspan class=\"hljs-string\"\u003e\"__main__\"\u003c/span\u003e:\n    flask_app.run(port=\u003cspan class=\"hljs-number\"\u003e3000\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eBetween those lines we’ll add our basic route: if a POST request is received that contains a JSON object with a \u003ccode class=\"cw qc qd qe qf b\"\u003echallenge\u003c/code\u003e key, we'll return the value of that key. Otherwise we'll do nothing.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c25c\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-meta\"\u003e@flask_app.route(\u003cspan class=\"hljs-params\"\u003e\u003cspan class=\"hljs-string\"\u003e\"/\"\u003c/span\u003e, methods=[\u003cspan class=\"hljs-string\"\u003e\"POST\"\u003c/span\u003e]\u003c/span\u003e)\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003eslack_challenge\u003c/span\u003e():\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e request.json \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"challenge\"\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e request.json:\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Received challenge\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e jsonify({\u003cspan class=\"hljs-string\"\u003e\"challenge\"\u003c/span\u003e: request.json[\u003cspan class=\"hljs-string\"\u003e\"challenge\"\u003c/span\u003e]})\n    \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Got unknown request incoming\"\u003c/span\u003e)\n        \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(request.json)\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eMake your app available to Slack\u003c/h1\u003e\u003cp\u003eTo configure a Slack app, it needs to be running somewhere Slack can see it. So let’s run our Slack app:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"9edb\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003epython 1_flask.py\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAnd we’ll set it up so the world can see it using \u003ca href=\"https://ngrok.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003engrok\u003c/a\u003e. You’ll need to download and install ngrok for this step. Once you have it installed, run the following command so it can find our app running on port 3000:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"45a9\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003engrok http 3000\u003c/span\u003e\u003c/pre\u003e\u003cp\u003engrok will give you an HTTPS url like \u003ccode class=\"cw qc qd qe qf b\"\u003ehttps://1bf6-64-38-189-168.ngrok-free.app\u003c/code\u003e. Make a note of it, because we need to give that to Slack. Also keep in mind that if you stop ngrok and start it again, this URL will change and you'll need to tell Slack about that. You'll only need this during development.\u003c/p\u003e\u003ch1\u003eRegister your app with Slack\u003c/h1\u003e\u003cp\u003eGo to the \u003ca href=\"https://api.slack.com/apps\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eSlack API site\u003c/a\u003e and click “Create New App”. You’ll see a screen like this, you’ll want to pick “from scratch”:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*z6jtbvuT9krVCq9joYjDzw.png\" alt=\"\" width=\"700\" height=\"568\"\u003e\u003c/figure\u003e\u003cp\u003ePick a nice friendly name and the workspace you want to install it to. You’ll see a screen like this:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*QkyK71pJcbZDZknI1tagMQ.png\" alt=\"\" width=\"700\" height=\"677\"\u003e\u003c/figure\u003e\u003cp\u003eNext you’ll want to set up what permissions your app needs. Click the “Permissions” link in the bottom right:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*ZOHiALXKFHIN4klcW5942w.png\" alt=\"\" width=\"700\" height=\"947\"\u003e\u003c/figure\u003e\u003cp\u003eThis will bring you to the “scopes” screen where you’ll need to add all the scopes you see in this picture, namely:\u003c/p\u003e\u003cul\u003e\u003cli\u003echannels:read — the lets your app see what channels are avaialble\u003c/li\u003e\u003cli\u003echannels:join — this lets your app join channels\u003c/li\u003e\u003cli\u003echannels:history — this lets your app see previous messages in channels\u003c/li\u003e\u003cli\u003echat:write — this lets your app send messages\u003c/li\u003e\u003cli\u003eusers:read — this lets your app see people’s names\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*FWueDTZxYh8-kNXYU9PX_Q.png\" alt=\"\" width=\"700\" height=\"990\"\u003e\u003c/figure\u003e\u003cp\u003eOnce you’ve saved those scopes, scroll up to “Install to workspace” to install your app.\u003c/p\u003e\u003cp\u003eYou now need to tell Slack where your app is so you can receive messages from it. Click the “Event Subscriptions” link in the left nav and fill it out so it looks something like this, specifically:\u003c/p\u003e\u003cul\u003e\u003cli\u003eSet your Request URL to that URL that ngrok gave you earlier\u003c/li\u003e\u003cli\u003eSubscribe to the \u003ccode class=\"cw qc qd qe qf b\"\u003emessage.channels\u003c/code\u003e event\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*D8GcOA9uoSqNz6t9uX6dog.png\" alt=\"\" width=\"700\" height=\"974\"\u003e\u003c/figure\u003e\u003cp\u003eIf your app is running and ngrok is correctly tunneling, your Request URL should be Verified.\u003c/p\u003e\u003cp\u003ePhew! That was a lot. Your Slack app is now registered and Slack will send it messages. But to get those messages, you have to tell it to join a channel.\u003c/p\u003e\u003ch1\u003eStep 2: Join a channel, and reply to messages\u003c/h1\u003e\u003cp\u003eTo do this we’ll need to extend our app. You can see the final result of this step in \u003ccode class=\"cw qc qd qe qf b\"\u003e2_join_and_reply.py\u003c/code\u003e. Let's walk through what we've added:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"5982\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eimport dotenv, os\ndotenv.load_dotenv()\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe need some environment variables, so you’ll need to add these lines and install \u003ccode class=\"cw qc qd qe qf b\"\u003epython-dotenv\u003c/code\u003e. You'll also need to create a \u003ccode class=\"cw qc qd qe qf b\"\u003e.env\u003c/code\u003e file in the root of your project with three values:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw qc qd qe qf b\"\u003eOPENAI_API_KEY\u003c/code\u003e: your OpenAI API key. You don't need this quite yet but you may as well \u003ca href=\"https://platform.openai.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eget it now\u003c/a\u003e.\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qc qd qe qf b\"\u003eSLACK_BOT_TOKEN\u003c/code\u003e: you can find this in the \"OAuth and Permissions\" section of your Slack app.\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qc qd qe qf b\"\u003eSLACK_SIGNING_SECRET\u003c/code\u003e: you can find this in the \"Basic Information\" section of your Slack app.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eWe’re going to use Slack’s handy Python SDK to build our app, so pip install \u003ccode class=\"cw qc qd qe qf b\"\u003eslack-bolt\u003c/code\u003e and then update all our imports:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"8a8e\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e slack_bolt \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e App\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e flask \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e Flask, request, jsonify\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e slack_bolt.adapter.flask \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e SlackRequestHandler\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow initialize a Slack Bolt app using those secrets we set just now:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"27ee\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eapp = App(\n    token=os.environ.get(\u003cspan class=\"hljs-string\"\u003e\"SLACK_BOT_TOKEN\"\u003c/span\u003e),\n    signing_secret=os.environ.get(\u003cspan class=\"hljs-string\"\u003e\"SLACK_SIGNING_SECRET\"\u003c/span\u003e)\n)\nhandler = SlackRequestHandler(app)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eTo listen to messages, the bot has to be in a channel. You can get it to join any and all public channels, but for the purposes of testing I’ve created a channel called \u003ccode class=\"cw qc qd qe qf b\"\u003e#bot-testing\u003c/code\u003e and that's the one it's joining here:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"be3e\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003echannel_list = app.client.conversations_list().\u003cspan class=\"hljs-type\"\u003edata\u003c/span\u003e\n\u003cspan class=\"hljs-variable\"\u003echannel\u003c/span\u003e \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e next((channel \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e channel in channel_list.get(\u003cspan class=\"hljs-string\"\u003e'channels'\u003c/span\u003e) \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e channel.get(\u003cspan class=\"hljs-string\"\u003e\"name\"\u003c/span\u003e) == \u003cspan class=\"hljs-string\"\u003e\"bot-testing\"\u003c/span\u003e), None)\nchannel_id = channel.get(\u003cspan class=\"hljs-string\"\u003e'id'\u003c/span\u003e)\napp.client.conversations_join(channel=channel_id)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode class=\"cw qc qd qe qf b\"\u003eapp.client\u003c/code\u003e is the Bolt framework's Slack WebClient, so you can do anything a WebClient can do directly from within the framework. The final addition here is a very simple message listener:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c21c\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-meta\"\u003e@app.message()\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ereply\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003emessage, say\u003c/span\u003e):\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(message)\n    say(\u003cspan class=\"hljs-string\"\u003e\"Yes?\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eIn the Bolt framework, the \u003ccode class=\"cw qc qd qe qf b\"\u003e@app.message\u003c/code\u003e decorator tells the framework to trigger this method when it receives a message event. The \u003ccode class=\"cw qc qd qe qf b\"\u003esay\u003c/code\u003e parameter is a function that will send a message back to the channel the message came from. So this code will send a message back to the channel saying \"Yes?\" every time it receives a message.\u003c/p\u003e\u003cp\u003eLet’s try it out! Stop running \u003ccode class=\"cw qc qd qe qf b\"\u003e1_flask.py\u003c/code\u003e and run \u003ccode class=\"cw qc qd qe qf b\"\u003epython 2_join_and_reply.py\u003c/code\u003e instead. You don't need to restart \u003ccode class=\"cw qc qd qe qf b\"\u003engrok\u003c/code\u003e, it will continue to send messages to port 3000 as before. Here's me trying it out:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*VLN-JCG8el8o4NcvttwmpA.png\" alt=\"\" width=\"524\" height=\"418\"\u003e\u003c/figure\u003e\u003cp\u003eSuccess! We have a very annoying bot that replies to every single thing anybody says. We can do better!\u003c/p\u003e\u003ch1\u003eStep 3: reply only to messages that mention the bot\u003c/h1\u003e\u003cp\u003eThis is a pretty simple change on the surface, but Slack’s incoming message format is a little complicated so we have to add a fair bit of code. You can see the final results in \u003ccode class=\"cw qc qd qe qf b\"\u003e3_reply_to_mentions.py\u003c/code\u003e.\u003c/p\u003e\u003cp\u003eFirst, to tell when our bot is being mentioned, we need our bot’s User ID. Under the hood, Slack doesn’t use user names or even @-handles, but a globally unique ID across all Slack installations. We have to get that:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"337f\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eauth_response = app.client.auth_test()\nbot_user_id = auth_response[\"user_id\"]\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow we add an annoyingly complicated chunk of code that parses through Slack’s message object to see what user is mentioned in an incoming message. If it’s the bot, the bot replies, otherwise it just ignores the message. As we go further, we’ll treat messages to the bot as “queries” and any other message as a “fact” for it to store, but we won’t be storing it just yet.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"18cf\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-meta\"\u003e@app.message()\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ereply\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003emessage, say\u003c/span\u003e):\n    \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e message.get(\u003cspan class=\"hljs-string\"\u003e'blocks'\u003c/span\u003e):\n        \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e block \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e message.get(\u003cspan class=\"hljs-string\"\u003e'blocks'\u003c/span\u003e):\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e block.get(\u003cspan class=\"hljs-string\"\u003e'type'\u003c/span\u003e) == \u003cspan class=\"hljs-string\"\u003e'rich_text'\u003c/span\u003e:\n                \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e rich_text_section \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e block.get(\u003cspan class=\"hljs-string\"\u003e'elements'\u003c/span\u003e):\n                    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e element \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e rich_text_section.get(\u003cspan class=\"hljs-string\"\u003e'elements'\u003c/span\u003e):\n                        \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e element.get(\u003cspan class=\"hljs-string\"\u003e'type'\u003c/span\u003e) == \u003cspan class=\"hljs-string\"\u003e'user'\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eand\u003c/span\u003e element.get(\u003cspan class=\"hljs-string\"\u003e'user_id'\u003c/span\u003e) == bot_user_id:\n                            \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e element \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e rich_text_section.get(\u003cspan class=\"hljs-string\"\u003e'elements'\u003c/span\u003e):\n                                \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e element.get(\u003cspan class=\"hljs-string\"\u003e'type'\u003c/span\u003e) == \u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e:\n                                    query = element.get(\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e)\n                                    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Somebody asked the bot: \u003cspan class=\"hljs-subst\"\u003e{query}\u003c/span\u003e\"\u003c/span\u003e)\n                                    say(\u003cspan class=\"hljs-string\"\u003e\"Yes?\"\u003c/span\u003e)\n                                    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e\n    \u003cspan class=\"hljs-comment\"\u003e# otherwise do something else with it\u003c/span\u003e\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003e\"Saw a fact: \"\u003c/span\u003e, message.get(\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e))\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOof. That took a while to get right! But now our bot only replies when it’s mentioned:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*Nob9nVc3BEtxBq9e3BCMLQ.png\" alt=\"\" width=\"626\" height=\"268\"\u003e\u003c/figure\u003e\u003ch1\u003eStep 4: use LlamaIndex to store facts and answer questions\u003c/h1\u003e\u003cp\u003eWe’re all the way at step 4 and we still haven’t done anything with LlamaIndex! But now’s the time. In \u003ccode class=\"cw qc qd qe qf b\"\u003e4_incremental_rag.py\u003c/code\u003e you'll see a demonstration of a simple command-line Python script that uses LlamaIndex to store facts and answer questions. I won't walk you through every line (the script has helpful comments for that), but let's look at the important ones. Remember to \u003ccode class=\"cw qc qd qe qf b\"\u003epip install llama-index\u003c/code\u003e!\u003c/p\u003e\u003cp\u003eFirst we create a new \u003ccode class=\"cw qc qd qe qf b\"\u003eVectorStoreIndex\u003c/code\u003e, an in-memory \u003ca href=\"https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#vector-store-index\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003evector store\u003c/a\u003e where we'll be storing our facts. It's empty to start with.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"1358\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eindex = VectorStoreIndex([])\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNext we create 3 \u003ccode class=\"cw qc qd qe qf b\"\u003eDocument\u003c/code\u003e objects and insert them each into our index. Real documents can be huge blocks of text, whole PDFs, even images, but these are just some simple, Slack-message-sized facts.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4923\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003edoc1 = Document(text=\"Molly is a cat\")\ndoc2 = Document(text=\"Doug is a dog\")\ndoc3 = Document(text=\"Carl is a rat\")\n\nindex.insert(doc1)\nindex.insert(doc2)\nindex.insert(doc3)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAnd finally we create a \u003ca href=\"https://docs.llamaindex.ai/en/stable/understanding/querying/querying.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003equery engine\u003c/a\u003e from our index and ask it a question:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c918\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-comment\"\u003e# run a query\u003c/span\u003e\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\u003cspan class=\"hljs-string\"\u003e\"Who is Molly?\"\u003c/span\u003e)\n\u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(response)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe result is “Molly is a cat” plus a whole lot of debugging info because we turned on noisy debugging in \u003ccode class=\"cw qc qd qe qf b\"\u003e4_incremental_rag.py\u003c/code\u003e. You can see the prompt we sent to the LLM, the context it retrieved from the index, and the response it generated and sent back to us.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*EXc4mvyUgOc2VgV4ENrDPw.png\" alt=\"\" width=\"700\" height=\"323\"\u003e\u003c/figure\u003e\u003ch1\u003eStep 5: use LlamaIndex to store facts and answer questions in Slack\u003c/h1\u003e\u003cp\u003eIn \u003ccode class=\"cw qc qd qe qf b\"\u003e5_rag_in_slack.py\u003c/code\u003e we are combining the two things we had before: script 3, where we reply to queries, and script 4, where we store facts and answer questions. Once again we won't walk through every line, but here are the important changes:\u003c/p\u003e\u003cp\u003eFirst \u003ccode class=\"cw qc qd qe qf b\"\u003epip install llama-index\u003c/code\u003e if you didn't already, and bring in your deps. Initialize your index while you're at it:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"05ed\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e VectorStoreIndex, Document\n\nindex = VectorStoreIndex([])\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWhere previously we were just replying with “Yes?” (line 73) let’s instead send a query to the query engine and reply with the response:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d686\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003equery = element.get(\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e)\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(query)\nsay(str(response))\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eAnd where previously we were just noting that we’d seen a fact (line 82), let’s store it in the index:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"fe76\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eindex.insert(Document(text=message.get(\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e)))\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe result is a Slackbot that can answer questions about what it’s been told:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*Zk-kD7S8nNZNSzTzi-_MSQ.png\" alt=\"\" width=\"700\" height=\"396\"\u003e\u003c/figure\u003e\u003cp\u003eAmazing! You can easily imagine a bot that listens to everybody’s conversations and is able to answer questions about things people said weeks or months ago, saving everybody time and effort searching through old messages.\u003c/p\u003e\u003ch1\u003eStep 6: persist our memory\u003c/h1\u003e\u003cp\u003eOur bot has a critical flaw though: the index is stored only in memory. If we restart the bot, it forgets everything:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*2q-qOH4dVV0ehOJG2h6ycg.png\" alt=\"\" width=\"700\" height=\"514\"\u003e\u003c/figure\u003e\u003cp\u003eIn \u003ccode class=\"cw qc qd qe qf b\"\u003e6_qdrant.py\u003c/code\u003e we bring in \u003ca href=\"https://qdrant.tech/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eQdrant\u003c/a\u003e, an open-source, local vector database that stores these facts on disk instead. That way if we restart our bot it remembers what was said before. \u003ccode class=\"cw qc qd qe qf b\"\u003epip install qdrant-client\u003c/code\u003e and bring in some new deps:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"1f81\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e qdrant_client\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.vector_stores.qdrant \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e QdrantVectorStore\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow we’ll initialize the Qdrant client, attach it to a storage context, and give that storage context to our index when we initialize it:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"b019\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003eclient = qdrant_client.QdrantClient(\n    path=\"./qdrant_data\"\n)\nvector_store = QdrantVectorStore(client=client, collection_name=\"slack_messages\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex([],storage_context=storage_context)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThat’s it for this step! Your bot now survives reboots, and remembers that I typoed “Doug” as “Dough” and was too lazy to fix it for the screenshot:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*QvqZmlSuiifdmQTzueSaGA.png\" alt=\"\" width=\"700\" height=\"379\"\u003e\u003c/figure\u003e\u003ch1\u003eStep 7: make recent messages more important\u003c/h1\u003e\u003cp\u003eWe now have a pretty capable bot! But it has a subtle problem: people can say conflicting things, and it doesn’t have a way to decide who was “right”, such as when I change my mind about what the dog’s name should be:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*Chf0RarxYCr00pKC2HuL2g.png\" alt=\"\" width=\"700\" height=\"281\"\u003e\u003c/figure\u003e\u003cp\u003eIn real Slack conversations, as a situation evolves people might move from saying a project is “in planning” to “underway” to “launched”. So we need a way to tell the bot that more recent messages are more important than older ones.\u003c/p\u003e\u003cp\u003eTo make this happen we have to do quite a bit of refactoring, the final results of which you can see in \u003ccode class=\"cw qc qd qe qf b\"\u003e7_recency.py\u003c/code\u003e. First we need a bunch of new deps:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"f751\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003e\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e datetime, uuid\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.schema \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e TextNode\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.prompts \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e PromptTemplate\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.postprocessor \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e FixedRecencyPostprocessor\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e set_global_handler\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eTo make recent messages more important, we have to know when a message was sent. To do that we are going to stop inserting \u003ccode class=\"cw qc qd qe qf b\"\u003eDocuments\u003c/code\u003e into the index and instead insert \u003ccode class=\"cw qc qd qe qf b\"\u003eNodes\u003c/code\u003e, to which we're going to attach the timestamp as metadata (under the hood, our Documents were always being converted into Nodes anyway so this doesn't change much):\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"856c\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003edt_object = datetime.datetime.fromtimestamp(\u003cspan class=\"hljs-type\"\u003efloat\u003c/span\u003e(message.get(\u003cspan class=\"hljs-string\"\u003e'ts'\u003c/span\u003e)))\nformatted_time = dt_object.strftime(\u003cspan class=\"hljs-string\"\u003e'%Y-%m-%d %H:%M:%S'\u003c/span\u003e)\n\n# get the message \u003cspan class=\"hljs-type\"\u003etext\u003c/span\u003e\n\u003cspan class=\"hljs-variable\"\u003etext\u003c/span\u003e \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e message.get(\u003cspan class=\"hljs-string\"\u003e'text'\u003c/span\u003e)\n# create a node with \u003cspan class=\"hljs-type\"\u003emetadata\u003c/span\u003e\n\u003cspan class=\"hljs-variable\"\u003enode\u003c/span\u003e \u003cspan class=\"hljs-operator\"\u003e=\u003c/span\u003e TextNode(\n    text=text,\n    id_=str(uuid.uuid4()),\n    metadata={\n        \u003cspan class=\"hljs-string\"\u003e\"when\"\u003c/span\u003e: formatted_time\n    }\n)\nindex.insert_nodes([node])\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eI’ve also factored out the reply logic from message handling into its own function, \u003ccode class=\"cw qc qd qe qf b\"\u003eanswer_question\u003c/code\u003e, just to make things a little easier to read. The first thing we're going to change is the prompt that we give to our LLM: we have to tell it that more recent messages are important. To do this we create a prompt template:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"28ed\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003etemplate = (\n    \u003cspan class=\"hljs-string\"\u003e\"Your context is a series of chat messages. Each one is tagged with 'who:' \\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"indicating who was speaking and 'when:' indicating when they said it, \\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"followed by a line break and then what they said. There can be up to 20 chat messages.\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"The messages are sorted by recency, so the most recent one is first in the list.\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"The most recent messages should take precedence over older ones.\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"---------------------\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"{context_str}\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"\\n---------------------\\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"You are a helpful AI assistant who has been listening to everything everyone has been saying. \\n\"\u003c/span\u003e\n    \u003cspan class=\"hljs-string\"\u003e\"Given the most relevant chat messages above, please answer this question: {query_str}\\n\"\u003c/span\u003e\n)\nqa_template = \u003cspan class=\"hljs-title class_\"\u003ePromptTemplate\u003c/span\u003e(template)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThe fun thing about working with LLMs is how often you end up just describing what you’re doing in English and that being what you send to the LLM. A prompt template will automatically get the \u003ccode class=\"cw qc qd qe qf b\"\u003econtext_str\u003c/code\u003e and \u003ccode class=\"cw qc qd qe qf b\"\u003equery_str\u003c/code\u003e from the query engine. But we have to set this template on our query engine, like so:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"81ea\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003equery_engine.update_prompts(\n    {\"response_synthesizer:text_qa_template\": qa_template}\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow there’s two more things we’re going to change. We’re going to take the results we get from the vector store and sort them by recency, something LlamaIndex has a built-in class for. It’s called the \u003ccode class=\"cw qc qd qe qf b\"\u003eFixedRecencyPostprocessor\u003c/code\u003e. We tell it the key that holds the timestamp (which we defined earlier on the nodes, above) and how many results it should return:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4492\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003epostprocessor = FixedRecencyPostprocessor(\n    top_k=20, \n    date_key=\"when\", # the key in the metadata to find the date\n    service_context=ServiceContext.from_defaults()\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eThen we need to create our query engine with the postprocessor attached:\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"6e1f\" class=\"qo or gt qf b bf qp qq l qr qs\"\u003equery_engine = index.as_query_engine(similarity_top_k=20, node_postprocessors=[postprocessor])\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWhile we were at it we did our final thing, which was pass \u003ccode class=\"cw qc qd qe qf b\"\u003esimilarity_top_k=20\u003c/code\u003e, which means the vector store will give us 20 Slack messages as context (the default is just 2, because usually the chunks of text in a Node are a lot bigger).\u003c/p\u003e\u003cp\u003eTada! Now the bot knows to take more recent statements as the truth.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*ZFpJSOdrjdb-zvYFo22wLg.png\" alt=\"\" width=\"700\" height=\"367\"\u003e\u003c/figure\u003e\u003ch1\u003eStep 8: draw the rest of the owl\u003c/h1\u003e\u003cp\u003eThis bot is working pretty well now, but I was having such fun when building it I got carried away and added two more features:\u003c/p\u003e\u003cul\u003e\u003cli\u003eI attached metadata about \u003cem class=\"re\"\u003ewho\u003c/em\u003e was speaking, not just when, so the bot can answer questions like “What did Logan say about the project?”\u003c/li\u003e\u003cli\u003eMy colleagues interacting with the bot tried to ask follow-up questions in a thread, like we do with each other. So I added a way for the bot to understand that it’s in a thread, and treat replies in a thread as follow-up questions, even if the user doesn’t mention the bot directly:\u003c/li\u003e\u003c/ul\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*5UcgwlMvyOnFn-zLpfEhTg.png\" alt=\"\" width=\"700\" height=\"377\"\u003e\u003c/figure\u003e\u003cp\u003eThe code to make both of those happen is in \u003ccode class=\"cw qc qd qe qf b\"\u003e8_rest_of_the_owl.py\u003c/code\u003e but I'm not going to be stepping through it line by line. We have to deploy this thing!\u003c/p\u003e\u003ch1\u003eStep 9: deploy to Render\u003c/h1\u003e\u003cp\u003eUntil now we’ve been working with local scripts running through the ngrok tunnel, but even the most dedicated coder turns their laptop off sometimes. Let’s put this thing on a real server.\u003c/p\u003e\u003ch1\u003eLogin to Render\u003c/h1\u003e\u003cp\u003eWe’ll be deploying to \u003ca href=\"https://render.com/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eRender\u003c/a\u003e, a Python-friendly hosting service that’s free for small projects. Sign up for an account (I recommend logging in with GitHub).\u003c/p\u003e\u003ch1\u003eCreate a new GitHub repository\u003c/h1\u003e\u003cp\u003eRender deploys things from GitHub repositories, so you’ll need to create a new one and copy 2 files from our existing repo into it:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003ccode class=\"cw qc qd qe qf b\"\u003epyproject.toml\u003c/code\u003e\u003c/li\u003e\u003cli\u003e\u003ccode class=\"cw qc qd qe qf b\"\u003e8_rest_of_the_owl.py\u003c/code\u003e which we're going to rename to \"app.py\" for simplicity.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eCommit those and push them up to GitHub.\u003c/p\u003e\u003ch1\u003eCreate a new Render web service\u003c/h1\u003e\u003cp\u003eIn Render, create a new web service. Connect it to the repo on GitHub you just created:\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*ezvvGYoT_erwiJ_6fUPwDA.png\" alt=\"\" width=\"700\" height=\"675\"\u003e\u003c/figure\u003e\u003cp\u003eRender will probably automatically detect that this is a Python app but you should make sure the following settings are correct:\u003c/p\u003e\u003cul\u003e\u003cli\u003eName: any name you choose\u003c/li\u003e\u003cli\u003eRegion: any region is fine\u003c/li\u003e\u003cli\u003eBranch: main\u003c/li\u003e\u003cli\u003eRoot directory: (blank, meaning root)\u003c/li\u003e\u003cli\u003eRuntime: Python 3\u003c/li\u003e\u003cli\u003eBuild command: \u003ccode class=\"cw qc qd qe qf b\"\u003epoetry install\u003c/code\u003e\u003c/li\u003e\u003cli\u003eStart command: \u003ccode class=\"cw qc qd qe qf b\"\u003egunicorn app:flask_app\u003c/code\u003e (this will definitely need to be set)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eYou’ll also need to scroll down and set some environment variables:\u003c/p\u003e\u003cul\u003e\u003cli\u003ePYTHON_VERSION: 3.11.6 (or whatever version you’re using)\u003c/li\u003e\u003cli\u003eOPENAI_API_KEY: your OpenAI API key\u003c/li\u003e\u003cli\u003eSLACK_BOT_TOKEN: your Slack bot token\u003c/li\u003e\u003cli\u003eSLACK_SIGNING_SECRET: your Slack signing secret from before\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThen click deploy and away you go!\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*kogMt0pPsSyHv_M0D4wBDw.png\" alt=\"\" width=\"700\" height=\"491\"\u003e\u003c/figure\u003e\u003cp\u003eYou now have a production Slack bot listening to messages, remembering, learning, and replying. Congratulations!\u003c/p\u003e\u003ch1\u003eWhat next?\u003c/h1\u003e\u003cp\u003eThere’s a whole bunch of features you could add to this bot, roughly in increasing order of difficulty:\u003c/p\u003e\u003cul\u003e\u003cli\u003eJoin every channel instead of just one, clearly!\u003c/li\u003e\u003cli\u003eAdd a way to tell the bot to forget things (delete nodes)\u003c/li\u003e\u003cli\u003eGive the bot the ability to use more than one index, such as an index of your documentation, or connected to your email, or your calendar\u003c/li\u003e\u003cli\u003eGive the bot “tags” so it can attach metadata to nodes and answer questions only with (or ignore) things that have been tagged a certain way\u003c/li\u003e\u003cli\u003eAdd multi-modal abilities, so the bot can read images and even reply with generated images\u003c/li\u003e\u003cli\u003eAnd tons more!\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThis bot is a lot of fun to play with and was a lot of fun to build, I hope you enjoyed learning about Slackbots and LlamaIndex as much as I enjoyed writing this tutorial!\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/67cd05c6c3de12c07cf3642ed6549dedcdc71e44-1024x1024.png","publishedDate":"2024-01-25","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"}],"slug":{"_type":"slug","current":"building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"b7aec1dd-a146-49dd-bd67-196d779a52dd","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"qdrant"},"title":"Qdrant"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"ab848ac2-08ba-45a7-af17-27c558f47c8e","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"render"},"title":"Render"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"9886c2e4-8e09-4afb-8914-8e658933673f","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"slack"},"title":"Slack"}],"title":"Building a Slack bot that learns with LlamaIndex, Qdrant and Render"},"publishedDate":"Invalid Date"},"params":{"slug":"building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"building-a-slack-bot-that-learns-with-llamaindex-qdrant-and-render-c88d4aa72840"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>