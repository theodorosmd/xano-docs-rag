<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="477" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/ravi-theja">Ravi Theja</a> <!-- -->•<!-- --> <!-- -->2023-12-15</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/open-source"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Open Source</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/gpt-4"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Gpt 4</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/openai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">OpenAI</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><h1>Introduction</h1><p>Evaluation is a critical component in enhancing your Retrieval-Augmented Generation (RAG) pipeline, traditionally reliant on GPT-4. However, the open-source <a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" rel="noopener ugc nofollow" target="_blank">Prometheus model</a> has recently emerged as a notable alternative for such evaluation tasks.</p><p>In this blog post, we will demonstrate how to effectively use the Prometheus model for evaluation purposes, integrating it smoothly with the LlamaIndex framework by comparing it with GPT-4 evaluation. Our primary focus will be on assessing RAG using our standard metrics: Correctness, Faithfulness, and Context Relevancy. To provide a clearer understanding, here’s what each metric entails:</p><ol><li><strong>Correctness</strong>: Assesses whether the generated answer aligns with the reference answer, given the query (this necessitates labeled data).</li><li><strong>Faithfulness</strong>: Measures if the answer remains true to the retrieved contexts, essentially checking for the absence of hallucinations.</li><li><strong>Context Relevancy</strong>: Evaluate the relevance of both the retrieved context and the answer to the query.</li></ol><p>For an in-depth exploration, our documentation is available <a href="https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p>For those who are exploring the Prometheus model for the first time, the paper summary by <a href="https://www.linkedin.com/in/nerdai/" rel="noopener ugc nofollow" target="_blank">Andrei</a> is an excellent resource to gain a better understanding.</p><figure><img src="/blog/images/1*W8U1VmBa_O38064ffw7w1g.png" alt="" width="700" height="651"></figure><p>A crucial aspect to remember when using the Prometheus model is its dependence on rubric scores within the prompt for effective evaluation. An example of such Rubric scores in the context of <code class="cw py pz qa qb b">Correctness Evaluation</code>is as follows:</p><blockquote><p id="26fb" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj">###Score Rubrics: 
Score 1: If the generated answer is not relevant to the user query and reference answer. 
Score 2: If the generated answer is according to reference answer but not relevant to user query. 
Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes. 
Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise. 
Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.</p></blockquote><p>You’ll find comprehensive details on this in the prompts section of this tutorial.</p><p>For a detailed walkthrough of the code, feel free to follow along with our <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab Notebook</a> accompanying this blog post. In the notebook, we conducted evaluations on both the <code class="cw py pz qa qb b">Paul Graham Essay Text</code>and the <code class="cw py pz qa qb b">Llama2 Paper</code>. However, for this blog post, we’ll focus exclusively on the Llama2 Paper, as it revealed some particularly interesting insights.</p><h1>Outline:</h1><ol><li>Setup Evaluation Pipeline.</li></ol><ul><li>Download Dataset.</li><li>Define LLMs (Prometheus, GPT-4) needed for evaluation.</li><li>Define Correctness, Faithfulness, and Relevancy prompt templates.</li><li>Define Prometheus, GPT-4 Evaluators, and Batch Eval Runner.</li><li>Run the Correctness, Faithfulness, and Relevancy Evaluation over the Llama2 dataset.</li></ul><p>2. Results</p><ul><li>Correctness Evaluation score distribution between Prometheus and GPT-4.</li><li>Feedback comparison between Prometheus and GPT-4 for correctness evaluation.</li><li>Faithfulness and Relevancy Evaluation scores with Prometheus and GPT-4.</li><li>Hamming Distance comparison between Prometheus and GPT-4.</li><li>Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy</li></ul><p>3. Summary with Cost Analysis.</p><h1>Setup Evaluation Pipeline</h1><p>Please be aware that certain functions mentioned here are not defined in detail within the blog post. We have showcased only the essential parts of the pipeline to provide an overview of its setup. For a comprehensive code walkthrough, we recommend visiting our <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab Notebook</a>.</p><h2>Download Dataset</h2><p>We will use the Llama2 paper dataset from Llama Datasets which contains 100 questions and their reference answers.</p><pre><span id="87c5" class="qz nb gt qb b bf ra rb l rc rd"><span class="hljs-keyword">from</span> llama_index.llama_dataset <span class="hljs-keyword">import</span> download_llama_dataset

llama2_rag_dataset, llama2_documents = download_llama_dataset(
    <span class="hljs-string">"Llama2PaperDataset"</span>, <span class="hljs-string">"./data/llama2"</span>
)</span></pre><h2>Define Prometheus LLM hosted on HuggingFace And OpenAI for creating an Index (RAG) pipeline</h2><p>We need to host the model on HF Inference endpoint using Nvidia A100 GPU, 80 GB RAM.</p><pre><span id="74bd" class="qz nb gt qb b bf ra rb l rc rd"><span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> HuggingFaceInferenceAPI
<span class="hljs-keyword">import</span> os

HF_TOKEN = <span class="hljs-string">"YOUR HF TOKEN"</span>
HF_ENDPOINT_URL = <span class="hljs-string">"HF END POINT URL"</span>

prometheus_llm = HuggingFaceInferenceAPI(
    model_name=HF_ENDPOINT_URL,
    token=HF_TOKEN,
    temperature=<span class="hljs-number">0.1</span>,
    do_sample=<span class="hljs-literal">True</span>,
    top_p=<span class="hljs-number">0.95</span>,
    top_k=<span class="hljs-number">40</span>,
    repetition_penalty=<span class="hljs-number">1.1</span>,
)

os.environ[<span class="hljs-string">"OPENAI_API_KEY"</span>] = <span class="hljs-string">"YOUR OPENAI API KEY"</span>

<span class="hljs-keyword">from</span> llama_index.llms <span class="hljs-keyword">import</span> OpenAI

gpt4_llm = OpenAI(<span class="hljs-string">"gpt-4"</span>)</span></pre><h2>Prompt templates.</h2><p>We will use the same prompts for the Prometheus model and GPT-4 to make consistent performance comparisons.</p><p><strong>Correctness Evaluation Prompt:</strong></p><pre><span id="0cb1" class="qz nb gt qb b bf ra rb l rc rd">prometheus_correctness_eval_prompt_template = <span class="hljs-string">""</span><span class="hljs-string">"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 
   1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. 
   2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric. 
   3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'
   4. Please do not generate any other opening, closing, and explanations. 
   5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.

   ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}
   
   ###Generate answer to evaluate: {generated_answer} 

   ###Reference Answer (Score 5): {reference_answer}
            
   ###Score Rubrics: 
   Score 1: If the generated answer is not relevant to the user query and reference answer.
   Score 2: If the generated answer is according to reference answer but not relevant to user query.
   Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.
   Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.
   Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.
    
   ###Feedback:"</span><span class="hljs-string">""</span>
</span></pre><p><strong>Faithfulness Evaluation Prompt:</strong></p><pre><span id="650f" class="qz nb gt qb b bf ra rb l rc rd">prometheus_faithfulness_eval_prompt_template= <span class="hljs-string">""</span><span class="hljs-string">"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.
1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.
2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.
3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.
4. The output format should look as follows: "</span><span class="hljs-title class_">Feedback</span>: (write a feedback <span class="hljs-keyword">for</span> criteria) [<span class="hljs-variable constant_">RESULT</span>] (<span class="hljs-variable constant_">YES</span> or <span class="hljs-variable constant_">NO</span>)”
<span class="hljs-number">5.</span> <span class="hljs-title class_">Please</span> <span class="hljs-keyword">do</span> not generate any other opening, closing, and explanations.

###<span class="hljs-title class_">The</span> instruction to <span class="hljs-attr">evaluate</span>: <span class="hljs-title class_">Your</span> task is to evaluate <span class="hljs-keyword">if</span> the given piece <span class="hljs-keyword">of</span> information is supported by context.

###<span class="hljs-title class_">Information</span>: {query_str}

###<span class="hljs-title class_">Context</span>: {context_str}

###<span class="hljs-title class_">Score</span> <span class="hljs-title class_">Rubrics</span>:
<span class="hljs-title class_">Score</span> <span class="hljs-attr">YES</span>: <span class="hljs-title class_">If</span> the given piece <span class="hljs-keyword">of</span> information is supported by context.
<span class="hljs-title class_">Score</span> <span class="hljs-attr">NO</span>: <span class="hljs-title class_">If</span> the given piece <span class="hljs-keyword">of</span> information is not supported by context

###<span class="hljs-title class_">Feedback</span>: <span class="hljs-string">""</span><span class="hljs-string">"

prometheus_faithfulness_refine_prompt_template= "</span><span class="hljs-string">""</span>###<span class="hljs-title class_">Task</span> <span class="hljs-title class_">Description</span>: <span class="hljs-title class_">An</span> instruction (might include an <span class="hljs-title class_">Input</span> inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.
<span class="hljs-number">1.</span> <span class="hljs-title class_">You</span> are provided <span class="hljs-keyword">with</span> evaluation task <span class="hljs-keyword">with</span> the help <span class="hljs-keyword">of</span> information, context information and an existing answer.
<span class="hljs-number">2.</span> <span class="hljs-title class_">Write</span> a detailed feedback based on evaluation task and the given score rubric, not evaluating <span class="hljs-keyword">in</span> general.
<span class="hljs-number">3.</span> <span class="hljs-title class_">After</span> writing a feedback, write a score that is <span class="hljs-variable constant_">YES</span> or <span class="hljs-variable constant_">NO</span>. <span class="hljs-title class_">You</span> should refer to the score rubric.
<span class="hljs-number">4.</span> <span class="hljs-title class_">The</span> output format should look <span class="hljs-keyword">as</span> <span class="hljs-attr">follows</span>: <span class="hljs-string">"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)"</span>
<span class="hljs-number">5.</span> <span class="hljs-title class_">Please</span> <span class="hljs-keyword">do</span> not generate any other opening, closing, and explanations.

###<span class="hljs-title class_">The</span> instruction to <span class="hljs-attr">evaluate</span>: <span class="hljs-title class_">If</span> the information is present <span class="hljs-keyword">in</span> the context and also provided <span class="hljs-keyword">with</span> an existing answer.

###<span class="hljs-title class_">Existing</span> <span class="hljs-attr">answer</span>: {existing_answer}

###<span class="hljs-title class_">Information</span>: {query_str}

###<span class="hljs-title class_">Context</span>: {context_msg}

###<span class="hljs-title class_">Score</span> <span class="hljs-title class_">Rubrics</span>:
<span class="hljs-title class_">Score</span> <span class="hljs-attr">YES</span>: <span class="hljs-title class_">If</span> the existing answer is already <span class="hljs-variable constant_">YES</span> or <span class="hljs-title class_">If</span> the <span class="hljs-title class_">Information</span> is present <span class="hljs-keyword">in</span> the context.
<span class="hljs-title class_">Score</span> <span class="hljs-attr">NO</span>: <span class="hljs-title class_">If</span> the existing answer is <span class="hljs-variable constant_">NO</span> and <span class="hljs-title class_">If</span> the <span class="hljs-title class_">Information</span> is not present <span class="hljs-keyword">in</span> the context.

###<span class="hljs-title class_">Feedback</span>: <span class="hljs-string">""</span><span class="hljs-string">"</span></span></pre><p><strong>Relevancy Evaluation Prompt:</strong></p><pre><span id="8df3" class="qz nb gt qb b bf ra rb l rc rd">prometheus_relevancy_eval_prompt_template = <span class="hljs-string">""</span><span class="hljs-string">"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. 
       1. You are provided with evaluation task with the help of a query with response and context.
       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. 
       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. 
       4. The output format should look as follows: "</span><span class="hljs-title class_">Feedback</span>: (write a feedback <span class="hljs-keyword">for</span> criteria) [<span class="hljs-variable constant_">RESULT</span>] (<span class="hljs-variable constant_">YES</span> or <span class="hljs-variable constant_">NO</span>)” 
       <span class="hljs-number">5.</span> <span class="hljs-title class_">Please</span> <span class="hljs-keyword">do</span> not generate any other opening, closing, and explanations. 

        ###<span class="hljs-title class_">The</span> instruction to <span class="hljs-attr">evaluate</span>: <span class="hljs-title class_">Your</span> task is to evaluate <span class="hljs-keyword">if</span> the response <span class="hljs-keyword">for</span> the query is <span class="hljs-keyword">in</span> line <span class="hljs-keyword">with</span> the context information provided.

        ###<span class="hljs-title class_">Query</span> and <span class="hljs-title class_">Response</span>: {query_str} 

        ###<span class="hljs-title class_">Context</span>: {context_str}
            
        ###<span class="hljs-title class_">Score</span> <span class="hljs-title class_">Rubrics</span>: 
        <span class="hljs-title class_">Score</span> <span class="hljs-attr">YES</span>: <span class="hljs-title class_">If</span> the response <span class="hljs-keyword">for</span> the query is <span class="hljs-keyword">in</span> line <span class="hljs-keyword">with</span> the context information provided.
        <span class="hljs-title class_">Score</span> <span class="hljs-attr">NO</span>: <span class="hljs-title class_">If</span> the response <span class="hljs-keyword">for</span> the query is not <span class="hljs-keyword">in</span> line <span class="hljs-keyword">with</span> the context information provided.
    
        ###<span class="hljs-title class_">Feedback</span>: <span class="hljs-string">""</span><span class="hljs-string">"

prometheus_relevancy_refine_prompt_template = "</span><span class="hljs-string">""</span>###<span class="hljs-title class_">Task</span> <span class="hljs-title class_">Description</span>: <span class="hljs-title class_">An</span> instruction (might include an <span class="hljs-title class_">Input</span> inside it), a query <span class="hljs-keyword">with</span> response, context, an existing answer, and a score rubric representing a evaluation criteria are given. 
   <span class="hljs-number">1.</span> <span class="hljs-title class_">You</span> are provided <span class="hljs-keyword">with</span> evaluation task <span class="hljs-keyword">with</span> the help <span class="hljs-keyword">of</span> a query <span class="hljs-keyword">with</span> response and context and an existing answer.
   <span class="hljs-number">2.</span> <span class="hljs-title class_">Write</span> a detailed feedback based on evaluation task and the given score rubric, not evaluating <span class="hljs-keyword">in</span> general. 
   <span class="hljs-number">3.</span> <span class="hljs-title class_">After</span> writing a feedback, write a score that is <span class="hljs-variable constant_">YES</span> or <span class="hljs-variable constant_">NO</span>. <span class="hljs-title class_">You</span> should refer to the score rubric. 
   <span class="hljs-number">4.</span> <span class="hljs-title class_">The</span> output format should look <span class="hljs-keyword">as</span> <span class="hljs-attr">follows</span>: <span class="hljs-string">"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)"</span> 
   <span class="hljs-number">5.</span> <span class="hljs-title class_">Please</span> <span class="hljs-keyword">do</span> not generate any other opening, closing, and explanations. 

   ###<span class="hljs-title class_">The</span> instruction to <span class="hljs-attr">evaluate</span>: <span class="hljs-title class_">Your</span> task is to evaluate <span class="hljs-keyword">if</span> the response <span class="hljs-keyword">for</span> the query is <span class="hljs-keyword">in</span> line <span class="hljs-keyword">with</span> the context information provided.

   ###<span class="hljs-title class_">Query</span> and <span class="hljs-title class_">Response</span>: {query_str} 

   ###<span class="hljs-title class_">Context</span>: {context_str}
            
   ###<span class="hljs-title class_">Score</span> <span class="hljs-title class_">Rubrics</span>: 
   <span class="hljs-title class_">Score</span> <span class="hljs-attr">YES</span>: <span class="hljs-title class_">If</span> the existing answer is already <span class="hljs-variable constant_">YES</span> or <span class="hljs-title class_">If</span> the response <span class="hljs-keyword">for</span> the query is <span class="hljs-keyword">in</span> line <span class="hljs-keyword">with</span> the context information provided.
   <span class="hljs-title class_">Score</span> <span class="hljs-attr">NO</span>: <span class="hljs-title class_">If</span> the existing answer is <span class="hljs-variable constant_">NO</span> and <span class="hljs-title class_">If</span> the response <span class="hljs-keyword">for</span> the query is <span class="hljs-keyword">in</span> line <span class="hljs-keyword">with</span> the context information provided.
    
   ###<span class="hljs-title class_">Feedback</span>: <span class="hljs-string">""</span><span class="hljs-string">"</span></span></pre><h2>Define Correctness, FaithFulness, Relevancy Evaluators</h2><pre><span id="9f97" class="qz nb gt qb b bf ra rb l rc rd"><span class="hljs-keyword">from</span> llama_index <span class="hljs-keyword">import</span> ServiceContext
<span class="hljs-keyword">from</span> llama_index.evaluation <span class="hljs-keyword">import</span> (
    CorrectnessEvaluator,
    FaithfulnessEvaluator,
    RelevancyEvaluator,
)
<span class="hljs-keyword">from</span> llama_index.callbacks <span class="hljs-keyword">import</span> CallbackManager, TokenCountingHandler
<span class="hljs-keyword">import</span> tiktoken

<span class="hljs-comment"># Provide Prometheus model in service_context</span>
prometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)

<span class="hljs-comment"># CorrectnessEvaluator with Prometheus model</span>
prometheus_correctness_evaluator = CorrectnessEvaluator(
    service_context=prometheus_service_context,
    parser_function=parser_function,
    eval_template=prometheus_correctness_eval_prompt_template,
)

<span class="hljs-comment"># FaithfulnessEvaluator with Prometheus model</span>
prometheus_faithfulness_evaluator = FaithfulnessEvaluator(
    service_context=prometheus_service_context,
    eval_template=prometheus_faithfulness_eval_prompt_template,
    refine_template=prometheus_faithfulness_refine_prompt_template,
)

<span class="hljs-comment"># RelevancyEvaluator with Prometheus model</span>
prometheus_relevancy_evaluator = RelevancyEvaluator(
    service_context=prometheus_service_context,
    eval_template=prometheus_relevancy_eval_prompt_template,
    refine_template=prometheus_relevancy_refine_prompt_template,
)

<span class="hljs-comment"># Set the encoding model to `gpt-4` for token counting.</span>
token_counter = TokenCountingHandler(
    tokenizer=tiktoken.encoding_for_model(<span class="hljs-string">"gpt-4"</span>).encode
)

callback_manager = CallbackManager([token_counter])

<span class="hljs-comment"># Provide GPT-4 model in service_context</span>
gpt4_service_context = ServiceContext.from_defaults(
    llm=gpt4_llm, callback_manager=callback_manager
)

<span class="hljs-comment"># CorrectnessEvaluator with GPT-4 model</span>
gpt4_correctness_evaluator = CorrectnessEvaluator(
    service_context=gpt4_service_context,
    <span class="hljs-comment"># parser_function=parser_function,</span>
)

<span class="hljs-comment"># FaithfulnessEvaluator with GPT-4 model</span>
gpt4_faithfulness_evaluator = FaithfulnessEvaluator(
    service_context=gpt4_service_context,
    eval_template=prometheus_faithfulness_eval_prompt_template,
    refine_template=prometheus_faithfulness_refine_prompt_template,
)

<span class="hljs-comment"># RelevancyEvaluator with GPT-4 model</span>
gpt4_relevancy_evaluator = RelevancyEvaluator(
    service_context=gpt4_service_context,
    eval_template=prometheus_relevancy_eval_prompt_template,
    refine_template=prometheus_relevancy_refine_prompt_template,
)

<span class="hljs-comment"># create a dictionary of evaluators</span>
prometheus_evaluators = {
    <span class="hljs-string">"correctness"</span>: prometheus_correctness_evaluator,
    <span class="hljs-string">"faithfulness"</span>: prometheus_faithfulness_evaluator,
    <span class="hljs-string">"relevancy"</span>: prometheus_relevancy_evaluator,
}

gpt4_evaluators = {
    <span class="hljs-string">"correctness"</span>: gpt4_correctness_evaluator,
    <span class="hljs-string">"faithfulness"</span>: gpt4_faithfulness_evaluator,
    <span class="hljs-string">"relevancy"</span>: gpt4_relevancy_evaluator,
}</span></pre><h2>Function to run batch evaluations on defined evaluators</h2><pre><span id="b1fb" class="qz nb gt qb b bf ra rb l rc rd"><span class="hljs-keyword">from</span> llama_index.evaluation <span class="hljs-keyword">import</span> BatchEvalRunner


<span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_eval_runner</span>(<span class="hljs-params">
    evaluators, query_engine, questions, reference=<span class="hljs-literal">None</span>, num_workers=<span class="hljs-number">8</span>
</span>):
    batch_runner = BatchEvalRunner(
        evaluators, workers=num_workers, show_progress=<span class="hljs-literal">True</span>
    )

    eval_results = <span class="hljs-keyword">await</span> batch_runner.aevaluate_queries(
        query_engine, queries=questions, reference=reference
    )

    <span class="hljs-keyword">return</span> eval_results</span></pre><h2>Get Query Engine, Questions, and References.</h2><pre><span id="d93d" class="qz nb gt qb b bf ra rb l rc rd">query_engine, rag_dataset = create_query_engine_rag_dataset(<span class="hljs-string">"./data/llama2"</span>)

questions = [example.query <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> rag_dataset.examples]

reference = [[example.reference_answer] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> rag_dataset.examples]</span></pre><h2>Compute Correctness, Faithfulness, and Relevancy Evaluation.</h2><pre><span id="72a2" class="qz nb gt qb b bf ra rb l rc rd">prometheus_eval_results = await batch_eval_runner(
    prometheus_evaluators, query_engine, questions, reference
)

gpt4_eval_results = await batch_eval_runner(
    gpt4_evaluators, query_engine, questions, reference
)</span></pre><h1>Results</h1><h2>Correctness Evaluation score distribution.</h2><h2>With the Prometheus Model:</h2><blockquote><p id="e31c" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj">3.0: 56.0, 
1.0: 26.0, 
5.0: 9.0, 
4.0: 8.0, 
2.0: 1.0</p></blockquote><h2>With GPT-4 Model:</h2><blockquote><p id="d4f1" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj">4.5: 57.99, 
1.0: 6.0, 
4.0: 12.0, 
5.0: 10.0, 
2.0: 5.0, 
3.5: 5.0, 
2.5: 3.0, 
3.0: 1.0</p></blockquote><h2><strong>Observation:</strong></h2><p>Prometheus Evaluator gives more <code class="cw py pz qa qb b">3.0</code> scores compared to GPT-4 which gives <code class="cw py pz qa qb b">4.5</code>. Let’s investigate it with the help of feedback.</p><h2>Feedback comparison between Prometheus and GPT-4 for correctness.</h2><blockquote><p id="c272" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">Query:</em></strong><em class="gt"> Based on the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models,” what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed?</em></p><p id="3a16" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">Generated Answer:</em></strong><em class="gt"> The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.</em></p><p id="2c5f" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">Reference Answer:</em></strong><em class="gt"> The two primary objectives achieved in the work described in the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models” are: 1. The development and release of a collection of pretrained and fine-tuned large language models (LLMs) specifically optimized for dialogue use cases. 2. The demonstration that these fine-tuned LLMs, referred to as Llama 2-Chat, outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models, particularly in terms of helpfulness and safety based on human evaluations. The range of parameters for the large language models developed in this work is from 7 billion to 70 billion parameters.</em></p><p id="cd6c" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">Prometheus Feedback:</em></strong><em class="gt"> The generated response is relevant to the user query and correctly identifies the two primary objectives of the work described in the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models.” However, it does not mention the demonstration of the fine-tuned LLMs outperforming open-source chat models on most benchmarks tested, which is a key point in the reference response. The range of parameters for the large language models developed is correctly identified, but the response does not mention the specific models referred to as Llama 2-Chat. So the overall score is 3.</em></p><p id="6fc5" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">Prometheus Score:</em></strong><em class="gt"> 3.0</em></p><p id="a3f6" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">GPT-4 Feedback:</em></strong><em class="gt"> The generated answer is relevant and almost fully correct. It correctly identifies the two primary objectives and the range of parameters for the large language models. However, it misses the detail about Llama 2-Chat outperforming other models on most benchmarks and potentially being a suitable substitute for closed-source models.</em></p><p id="8c97" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong><em class="gt">GPT-4 Score:</em></strong><em class="gt"> 4.5</em></p></blockquote><p><strong>Observation:</strong></p><p>The feedback from Prometheus is a little more precise compared to GPT-4 and it penalizes and gives a score of <code class="cw py pz qa qb b">3.0</code>but GPT-4 gives a score of <code class="cw py pz qa qb b">4.5</code>. Prometheus penalizes more even if some of the text in the reference answer is missed in the generated answer.</p><h2>Faithfulness and Relevancy Evaluation scores.</h2><h2>With the Prometheus Model:</h2><blockquote><p id="c4ea" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj">faithfulness Score: 0.39 
relevancy Score: 0.57</p></blockquote><h2>With GPT-4 Model:</h2><blockquote><p id="c9d6" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj">faithfulness Score: 0.93
relevancy Score: 0.98</p></blockquote><p><strong>Observation:</strong></p><p>We see a higher faithfulness and relevancy score with GPT-4 compared to the Prometheus model. Let’s investigate it through feedback.</p><h2>Hamming Distance comparison between Prometheus and GPT-4:</h2><blockquote><p id="48b6" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj">Faithfulness Hamming Distance: 58 
Relevancy Hamming Distance: 41</p></blockquote><h2>Observation:</h2><p>The comparison reveals that approximately <code class="cw py pz qa qb b">42%</code> of the scores in case of <code class="cw py pz qa qb b">Faithfulness</code> and <code class="cw py pz qa qb b">59%</code> in case of <code class="cw py pz qa qb b">Relevancy</code> are common between Prometheus and GPT-4 evaluations. This indicates a decent amount of correlation in terms of faithfulness and relevance scoring between the Prometheus and GPT-4 models.</p><h2>Feedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy</h2><blockquote><p id="ac48" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Query:</strong> Based on the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models,” what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.</p><p id="be60" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Context-1:</strong> Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron∗Louis Martin†Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗ GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com †Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023</p><p id="7e4b" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Context-2: </strong>(2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto transparency and safety, we plan to make further improvements to Llama 2-Chat in future work. 36</p><p id="07ab" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Prometheus Faithfulness Feedback:</strong> The information provided in the context is not supported by the given information. The context is about the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. However, the information provided in the context does not align with the given information. The context does not mention the range of parameters for the large language models developed, which is the primary objective mentioned in the information. The context only talks about the development and release of Llama 2 and its optimization for dialogue use cases, but it does not provide any information about the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO</p><p id="98bd" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Prometheus Faithfulness Score:</strong> 0.0</p><p id="9617" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Prometheus Relevancy Feedback:</strong> The response is not in line with the context information provided. The query asked for the two primary objectives achieved in the work and the range of parameters for the large language models developed. However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO</p><p id="dfd9" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>Prometheus Relevancy Score:</strong> 0.0</p><p id="871f" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>GPT-4 Faithfulness Feedback:</strong> The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YES</p><p id="2c3d" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>GPT-4 Faithfulness Score:</strong> 1.0</p><p id="efdb" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>GPT-4 Relevancy Feedback:</strong> The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YES</p><p id="fc08" class="ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj"><strong>GPT-4 Relevancy Score:</strong> 1.0</p></blockquote><h1>Observation:</h1><ol><li>Prometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information.</li><li>GPT-4: Evaluates it correctly, unlike the Prometheus model.</li></ol><h1>Summary</h1><ol><li>The cost for evaluation (approx.): <code class="cw py pz qa qb b">$1.5</code> for Prometheus Model and <code class="cw py pz qa qb b">$15</code> for GPT4.</li><li>The Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application.</li><li>If a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4.</li><li>The faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4.</li></ol><h1><strong>Note:</strong></h1><ul><li>You can check detailed analysis with code on <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab Notebook</a>.</li><li>The endpoint on HF is served on AWS Nvidia A100G · 1x GPU · 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.)</li><li>We used the <a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" rel="noopener ugc nofollow" target="_blank">Prometheus model</a> for the analysis here. We also made a similar analysis with the <a href="https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ" rel="noopener ugc nofollow" target="_blank">GPTQ Quantized version</a> of the <a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" rel="noopener ugc nofollow" target="_blank">Prometheus model</a> and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open-sourcing the model and <a href="https://twitter.com/TheBlokeAI" rel="noopener ugc nofollow" target="_blank">Tom Jobbins</a> for the quantized version of the model.</li></ul><h1>References:</h1><ul><li><a href="https://arxiv.org/abs/2310.08491" rel="noopener ugc nofollow" target="_blank">Prometheus paper</a>.</li><li><a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" rel="noopener ugc nofollow" target="_blank">Prometheus model on HuggingFace.</a></li></ul></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Fe1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-19">LlamaIndex Newsletter 2024-03-19</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"842f7f73-08d5-4ffb-960b-0e11f0b8464c","_rev":"05dtDS0H5iRVsxYMarZIT8","_type":"blogPost","_updatedAt":"2025-05-21T20:38:28Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:58:55Z","_id":"60575af5-a5c2-40f6-9aab-d5e02da9c000","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Ravi Theja","slug":{"_type":"slug","current":"ravi-theja"}}],"featured":false,"htmlContent":"\u003ch1\u003eIntroduction\u003c/h1\u003e\u003cp\u003eEvaluation is a critical component in enhancing your Retrieval-Augmented Generation (RAG) pipeline, traditionally reliant on GPT-4. However, the open-source \u003ca href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePrometheus model\u003c/a\u003e has recently emerged as a notable alternative for such evaluation tasks.\u003c/p\u003e\u003cp\u003eIn this blog post, we will demonstrate how to effectively use the Prometheus model for evaluation purposes, integrating it smoothly with the LlamaIndex framework by comparing it with GPT-4 evaluation. Our primary focus will be on assessing RAG using our standard metrics: Correctness, Faithfulness, and Context Relevancy. To provide a clearer understanding, here’s what each metric entails:\u003c/p\u003e\u003col\u003e\u003cli\u003e\u003cstrong\u003eCorrectness\u003c/strong\u003e: Assesses whether the generated answer aligns with the reference answer, given the query (this necessitates labeled data).\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eFaithfulness\u003c/strong\u003e: Measures if the answer remains true to the retrieved contexts, essentially checking for the absence of hallucinations.\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eContext Relevancy\u003c/strong\u003e: Evaluate the relevance of both the retrieved context and the answer to the query.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eFor an in-depth exploration, our documentation is available \u003ca href=\"https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eFor those who are exploring the Prometheus model for the first time, the paper summary by \u003ca href=\"https://www.linkedin.com/in/nerdai/\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eAndrei\u003c/a\u003e is an excellent resource to gain a better understanding.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*W8U1VmBa_O38064ffw7w1g.png\" alt=\"\" width=\"700\" height=\"651\"\u003e\u003c/figure\u003e\u003cp\u003eA crucial aspect to remember when using the Prometheus model is its dependence on rubric scores within the prompt for effective evaluation. An example of such Rubric scores in the context of \u003ccode class=\"cw py pz qa qb b\"\u003eCorrectness Evaluation\u003c/code\u003eis as follows:\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"26fb\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e###Score Rubrics: \nScore 1: If the generated answer is not relevant to the user query and reference answer. \nScore 2: If the generated answer is according to reference answer but not relevant to user query. \nScore 3: If the generated answer is relevant to the user query and reference answer but contains mistakes. \nScore 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise. \nScore 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003eYou’ll find comprehensive details on this in the prompts section of this tutorial.\u003c/p\u003e\u003cp\u003eFor a detailed walkthrough of the code, feel free to follow along with our \u003ca href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Colab Notebook\u003c/a\u003e accompanying this blog post. In the notebook, we conducted evaluations on both the \u003ccode class=\"cw py pz qa qb b\"\u003ePaul Graham Essay Text\u003c/code\u003eand the \u003ccode class=\"cw py pz qa qb b\"\u003eLlama2 Paper\u003c/code\u003e. However, for this blog post, we’ll focus exclusively on the Llama2 Paper, as it revealed some particularly interesting insights.\u003c/p\u003e\u003ch1\u003eOutline:\u003c/h1\u003e\u003col\u003e\u003cli\u003eSetup Evaluation Pipeline.\u003c/li\u003e\u003c/ol\u003e\u003cul\u003e\u003cli\u003eDownload Dataset.\u003c/li\u003e\u003cli\u003eDefine LLMs (Prometheus, GPT-4) needed for evaluation.\u003c/li\u003e\u003cli\u003eDefine Correctness, Faithfulness, and Relevancy prompt templates.\u003c/li\u003e\u003cli\u003eDefine Prometheus, GPT-4 Evaluators, and Batch Eval Runner.\u003c/li\u003e\u003cli\u003eRun the Correctness, Faithfulness, and Relevancy Evaluation over the Llama2 dataset.\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e2. Results\u003c/p\u003e\u003cul\u003e\u003cli\u003eCorrectness Evaluation score distribution between Prometheus and GPT-4.\u003c/li\u003e\u003cli\u003eFeedback comparison between Prometheus and GPT-4 for correctness evaluation.\u003c/li\u003e\u003cli\u003eFaithfulness and Relevancy Evaluation scores with Prometheus and GPT-4.\u003c/li\u003e\u003cli\u003eHamming Distance comparison between Prometheus and GPT-4.\u003c/li\u003e\u003cli\u003eFeedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e3. Summary with Cost Analysis.\u003c/p\u003e\u003ch1\u003eSetup Evaluation Pipeline\u003c/h1\u003e\u003cp\u003ePlease be aware that certain functions mentioned here are not defined in detail within the blog post. We have showcased only the essential parts of the pipeline to provide an overview of its setup. For a comprehensive code walkthrough, we recommend visiting our \u003ca href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Colab Notebook\u003c/a\u003e.\u003c/p\u003e\u003ch2\u003eDownload Dataset\u003c/h2\u003e\u003cp\u003eWe will use the Llama2 paper dataset from Llama Datasets which contains 100 questions and their reference answers.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"87c5\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llama_dataset \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e download_llama_dataset\n\nllama2_rag_dataset, llama2_documents = download_llama_dataset(\n    \u003cspan class=\"hljs-string\"\u003e\"Llama2PaperDataset\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"./data/llama2\"\u003c/span\u003e\n)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eDefine Prometheus LLM hosted on HuggingFace And OpenAI for creating an Index (RAG) pipeline\u003c/h2\u003e\u003cp\u003eWe need to host the model on HF Inference endpoint using Nvidia A100 GPU, 80 GB RAM.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"74bd\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e HuggingFaceInferenceAPI\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e os\n\nHF_TOKEN = \u003cspan class=\"hljs-string\"\u003e\"YOUR HF TOKEN\"\u003c/span\u003e\nHF_ENDPOINT_URL = \u003cspan class=\"hljs-string\"\u003e\"HF END POINT URL\"\u003c/span\u003e\n\nprometheus_llm = HuggingFaceInferenceAPI(\n    model_name=HF_ENDPOINT_URL,\n    token=HF_TOKEN,\n    temperature=\u003cspan class=\"hljs-number\"\u003e0.1\u003c/span\u003e,\n    do_sample=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e,\n    top_p=\u003cspan class=\"hljs-number\"\u003e0.95\u003c/span\u003e,\n    top_k=\u003cspan class=\"hljs-number\"\u003e40\u003c/span\u003e,\n    repetition_penalty=\u003cspan class=\"hljs-number\"\u003e1.1\u003c/span\u003e,\n)\n\nos.environ[\u003cspan class=\"hljs-string\"\u003e\"OPENAI_API_KEY\"\u003c/span\u003e] = \u003cspan class=\"hljs-string\"\u003e\"YOUR OPENAI API KEY\"\u003c/span\u003e\n\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.llms \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e OpenAI\n\ngpt4_llm = OpenAI(\u003cspan class=\"hljs-string\"\u003e\"gpt-4\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003ePrompt templates.\u003c/h2\u003e\u003cp\u003eWe will use the same prompts for the Prometheus model and GPT-4 to make consistent performance comparisons.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eCorrectness Evaluation Prompt:\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0cb1\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003eprometheus_correctness_eval_prompt_template = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"###Task Description: An instruction (might include an Input inside it), a query, a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. \n   1. Write a detailed feedback that assesses the quality of the response strictly based on the given score rubric, not evaluating in general. \n   2. After writing a feedback, write a score that is either 1 or 2 or 3 or 4 or 5. You should refer to the score rubric. \n   3. The output format should look as follows: 'Feedback: (write a feedback for criteria) [RESULT] (1 or 2 or 3 or 4 or 5)'\n   4. Please do not generate any other opening, closing, and explanations. \n   5. Only evaluate on common things between generated answer and reference answer. Don't evaluate on things which are present in reference answer but not in generated answer.\n\n   ###The instruction to evaluate: Your task is to evaluate the generated answer and reference answer for the query: {query}\n   \n   ###Generate answer to evaluate: {generated_answer} \n\n   ###Reference Answer (Score 5): {reference_answer}\n            \n   ###Score Rubrics: \n   Score 1: If the generated answer is not relevant to the user query and reference answer.\n   Score 2: If the generated answer is according to reference answer but not relevant to user query.\n   Score 3: If the generated answer is relevant to the user query and reference answer but contains mistakes.\n   Score 4: If the generated answer is relevant to the user query and has the exact same metrics as the reference answer, but it is not as concise.\n   Score 5: If the generated answer is relevant to the user query and fully correct according to the reference answer.\n    \n   ###Feedback:\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\n\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eFaithfulness Evaluation Prompt:\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"650f\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003eprometheus_faithfulness_eval_prompt_template= \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"###Task Description: An instruction (might include an Input inside it), an information, a context, and a score rubric representing evaluation criteria are given.\n1. You are provided with evaluation task with the help of information, context information to give result based on score rubrics.\n2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general.\n3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric.\n4. The output format should look as follows: \"\u003c/span\u003e\u003cspan class=\"hljs-title class_\"\u003eFeedback\u003c/span\u003e: (write a feedback \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e criteria) [\u003cspan class=\"hljs-variable constant_\"\u003eRESULT\u003c/span\u003e] (\u003cspan class=\"hljs-variable constant_\"\u003eYES\u003c/span\u003e or \u003cspan class=\"hljs-variable constant_\"\u003eNO\u003c/span\u003e)”\n\u003cspan class=\"hljs-number\"\u003e5.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePlease\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edo\u003c/span\u003e not generate any other opening, closing, and explanations.\n\n###\u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e instruction to \u003cspan class=\"hljs-attr\"\u003eevaluate\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eYour\u003c/span\u003e task is to evaluate \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e the given piece \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e information is supported by context.\n\n###\u003cspan class=\"hljs-title class_\"\u003eInformation\u003c/span\u003e: {query_str}\n\n###\u003cspan class=\"hljs-title class_\"\u003eContext\u003c/span\u003e: {context_str}\n\n###\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRubrics\u003c/span\u003e:\n\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eYES\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the given piece \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e information is supported by context.\n\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eNO\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the given piece \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e information is not supported by context\n\n###\u003cspan class=\"hljs-title class_\"\u003eFeedback\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n\nprometheus_faithfulness_refine_prompt_template= \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e###\u003cspan class=\"hljs-title class_\"\u003eTask\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDescription\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eAn\u003c/span\u003e instruction (might include an \u003cspan class=\"hljs-title class_\"\u003eInput\u003c/span\u003e inside it), a information, a context information, an existing answer, and a score rubric representing a evaluation criteria are given.\n\u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eYou\u003c/span\u003e are provided \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e evaluation task \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the help \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e information, context information and an existing answer.\n\u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eWrite\u003c/span\u003e a detailed feedback based on evaluation task and the given score rubric, not evaluating \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e general.\n\u003cspan class=\"hljs-number\"\u003e3.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAfter\u003c/span\u003e writing a feedback, write a score that is \u003cspan class=\"hljs-variable constant_\"\u003eYES\u003c/span\u003e or \u003cspan class=\"hljs-variable constant_\"\u003eNO\u003c/span\u003e. \u003cspan class=\"hljs-title class_\"\u003eYou\u003c/span\u003e should refer to the score rubric.\n\u003cspan class=\"hljs-number\"\u003e4.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e output format should look \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efollows\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\u003c/span\u003e\n\u003cspan class=\"hljs-number\"\u003e5.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePlease\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edo\u003c/span\u003e not generate any other opening, closing, and explanations.\n\n###\u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e instruction to \u003cspan class=\"hljs-attr\"\u003eevaluate\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the information is present \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e the context and also provided \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e an existing answer.\n\n###\u003cspan class=\"hljs-title class_\"\u003eExisting\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eanswer\u003c/span\u003e: {existing_answer}\n\n###\u003cspan class=\"hljs-title class_\"\u003eInformation\u003c/span\u003e: {query_str}\n\n###\u003cspan class=\"hljs-title class_\"\u003eContext\u003c/span\u003e: {context_msg}\n\n###\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRubrics\u003c/span\u003e:\n\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eYES\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the existing answer is already \u003cspan class=\"hljs-variable constant_\"\u003eYES\u003c/span\u003e or \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the \u003cspan class=\"hljs-title class_\"\u003eInformation\u003c/span\u003e is present \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e the context.\n\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eNO\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the existing answer is \u003cspan class=\"hljs-variable constant_\"\u003eNO\u003c/span\u003e and \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the \u003cspan class=\"hljs-title class_\"\u003eInformation\u003c/span\u003e is not present \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e the context.\n\n###\u003cspan class=\"hljs-title class_\"\u003eFeedback\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eRelevancy Evaluation Prompt:\u003c/strong\u003e\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"8df3\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003eprometheus_relevancy_eval_prompt_template = \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n       1. You are provided with evaluation task with the help of a query with response and context.\n       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n       4. The output format should look as follows: \"\u003c/span\u003e\u003cspan class=\"hljs-title class_\"\u003eFeedback\u003c/span\u003e: (write a feedback \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e criteria) [\u003cspan class=\"hljs-variable constant_\"\u003eRESULT\u003c/span\u003e] (\u003cspan class=\"hljs-variable constant_\"\u003eYES\u003c/span\u003e or \u003cspan class=\"hljs-variable constant_\"\u003eNO\u003c/span\u003e)” \n       \u003cspan class=\"hljs-number\"\u003e5.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePlease\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edo\u003c/span\u003e not generate any other opening, closing, and explanations. \n\n        ###\u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e instruction to \u003cspan class=\"hljs-attr\"\u003eevaluate\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eYour\u003c/span\u003e task is to evaluate \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e the response \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e the query is \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e line \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the context information provided.\n\n        ###\u003cspan class=\"hljs-title class_\"\u003eQuery\u003c/span\u003e and \u003cspan class=\"hljs-title class_\"\u003eResponse\u003c/span\u003e: {query_str} \n\n        ###\u003cspan class=\"hljs-title class_\"\u003eContext\u003c/span\u003e: {context_str}\n            \n        ###\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRubrics\u003c/span\u003e: \n        \u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eYES\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the response \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e the query is \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e line \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the context information provided.\n        \u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eNO\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the response \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e the query is not \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e line \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the context information provided.\n    \n        ###\u003cspan class=\"hljs-title class_\"\u003eFeedback\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\n\nprometheus_relevancy_refine_prompt_template = \"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e###\u003cspan class=\"hljs-title class_\"\u003eTask\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eDescription\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eAn\u003c/span\u003e instruction (might include an \u003cspan class=\"hljs-title class_\"\u003eInput\u003c/span\u003e inside it), a query \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n   \u003cspan class=\"hljs-number\"\u003e1.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eYou\u003c/span\u003e are provided \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e evaluation task \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the help \u003cspan class=\"hljs-keyword\"\u003eof\u003c/span\u003e a query \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e response and context and an existing answer.\n   \u003cspan class=\"hljs-number\"\u003e2.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eWrite\u003c/span\u003e a detailed feedback based on evaluation task and the given score rubric, not evaluating \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e general. \n   \u003cspan class=\"hljs-number\"\u003e3.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eAfter\u003c/span\u003e writing a feedback, write a score that is \u003cspan class=\"hljs-variable constant_\"\u003eYES\u003c/span\u003e or \u003cspan class=\"hljs-variable constant_\"\u003eNO\u003c/span\u003e. \u003cspan class=\"hljs-title class_\"\u003eYou\u003c/span\u003e should refer to the score rubric. \n   \u003cspan class=\"hljs-number\"\u003e4.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e output format should look \u003cspan class=\"hljs-keyword\"\u003eas\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003efollows\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\"\u003c/span\u003e \n   \u003cspan class=\"hljs-number\"\u003e5.\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003ePlease\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edo\u003c/span\u003e not generate any other opening, closing, and explanations. \n\n   ###\u003cspan class=\"hljs-title class_\"\u003eThe\u003c/span\u003e instruction to \u003cspan class=\"hljs-attr\"\u003eevaluate\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eYour\u003c/span\u003e task is to evaluate \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e the response \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e the query is \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e line \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the context information provided.\n\n   ###\u003cspan class=\"hljs-title class_\"\u003eQuery\u003c/span\u003e and \u003cspan class=\"hljs-title class_\"\u003eResponse\u003c/span\u003e: {query_str} \n\n   ###\u003cspan class=\"hljs-title class_\"\u003eContext\u003c/span\u003e: {context_str}\n            \n   ###\u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eRubrics\u003c/span\u003e: \n   \u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eYES\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the existing answer is already \u003cspan class=\"hljs-variable constant_\"\u003eYES\u003c/span\u003e or \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the response \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e the query is \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e line \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the context information provided.\n   \u003cspan class=\"hljs-title class_\"\u003eScore\u003c/span\u003e \u003cspan class=\"hljs-attr\"\u003eNO\u003c/span\u003e: \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the existing answer is \u003cspan class=\"hljs-variable constant_\"\u003eNO\u003c/span\u003e and \u003cspan class=\"hljs-title class_\"\u003eIf\u003c/span\u003e the response \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e the query is \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e line \u003cspan class=\"hljs-keyword\"\u003ewith\u003c/span\u003e the context information provided.\n    \n   ###\u003cspan class=\"hljs-title class_\"\u003eFeedback\u003c/span\u003e: \u003cspan class=\"hljs-string\"\u003e\"\"\u003c/span\u003e\u003cspan class=\"hljs-string\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eDefine Correctness, FaithFulness, Relevancy Evaluators\u003c/h2\u003e\u003cpre\u003e\u003cspan id=\"9f97\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e ServiceContext\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.evaluation \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e (\n    CorrectnessEvaluator,\n    FaithfulnessEvaluator,\n    RelevancyEvaluator,\n)\n\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.callbacks \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e CallbackManager, TokenCountingHandler\n\u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e tiktoken\n\n\u003cspan class=\"hljs-comment\"\u003e# Provide Prometheus model in service_context\u003c/span\u003e\nprometheus_service_context = ServiceContext.from_defaults(llm=prometheus_llm)\n\n\u003cspan class=\"hljs-comment\"\u003e# CorrectnessEvaluator with Prometheus model\u003c/span\u003e\nprometheus_correctness_evaluator = CorrectnessEvaluator(\n    service_context=prometheus_service_context,\n    parser_function=parser_function,\n    eval_template=prometheus_correctness_eval_prompt_template,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# FaithfulnessEvaluator with Prometheus model\u003c/span\u003e\nprometheus_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# RelevancyEvaluator with Prometheus model\u003c/span\u003e\nprometheus_relevancy_evaluator = RelevancyEvaluator(\n    service_context=prometheus_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# Set the encoding model to `gpt-4` for token counting.\u003c/span\u003e\ntoken_counter = TokenCountingHandler(\n    tokenizer=tiktoken.encoding_for_model(\u003cspan class=\"hljs-string\"\u003e\"gpt-4\"\u003c/span\u003e).encode\n)\n\ncallback_manager = CallbackManager([token_counter])\n\n\u003cspan class=\"hljs-comment\"\u003e# Provide GPT-4 model in service_context\u003c/span\u003e\ngpt4_service_context = ServiceContext.from_defaults(\n    llm=gpt4_llm, callback_manager=callback_manager\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# CorrectnessEvaluator with GPT-4 model\u003c/span\u003e\ngpt4_correctness_evaluator = CorrectnessEvaluator(\n    service_context=gpt4_service_context,\n    \u003cspan class=\"hljs-comment\"\u003e# parser_function=parser_function,\u003c/span\u003e\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# FaithfulnessEvaluator with GPT-4 model\u003c/span\u003e\ngpt4_faithfulness_evaluator = FaithfulnessEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_faithfulness_eval_prompt_template,\n    refine_template=prometheus_faithfulness_refine_prompt_template,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# RelevancyEvaluator with GPT-4 model\u003c/span\u003e\ngpt4_relevancy_evaluator = RelevancyEvaluator(\n    service_context=gpt4_service_context,\n    eval_template=prometheus_relevancy_eval_prompt_template,\n    refine_template=prometheus_relevancy_refine_prompt_template,\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# create a dictionary of evaluators\u003c/span\u003e\nprometheus_evaluators = {\n    \u003cspan class=\"hljs-string\"\u003e\"correctness\"\u003c/span\u003e: prometheus_correctness_evaluator,\n    \u003cspan class=\"hljs-string\"\u003e\"faithfulness\"\u003c/span\u003e: prometheus_faithfulness_evaluator,\n    \u003cspan class=\"hljs-string\"\u003e\"relevancy\"\u003c/span\u003e: prometheus_relevancy_evaluator,\n}\n\ngpt4_evaluators = {\n    \u003cspan class=\"hljs-string\"\u003e\"correctness\"\u003c/span\u003e: gpt4_correctness_evaluator,\n    \u003cspan class=\"hljs-string\"\u003e\"faithfulness\"\u003c/span\u003e: gpt4_faithfulness_evaluator,\n    \u003cspan class=\"hljs-string\"\u003e\"relevancy\"\u003c/span\u003e: gpt4_relevancy_evaluator,\n}\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eFunction to run batch evaluations on defined evaluators\u003c/h2\u003e\u003cpre\u003e\u003cspan id=\"b1fb\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003e\u003cspan class=\"hljs-keyword\"\u003efrom\u003c/span\u003e llama_index.evaluation \u003cspan class=\"hljs-keyword\"\u003eimport\u003c/span\u003e BatchEvalRunner\n\n\n\u003cspan class=\"hljs-keyword\"\u003easync\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003ebatch_eval_runner\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n    evaluators, query_engine, questions, reference=\u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e, num_workers=\u003cspan class=\"hljs-number\"\u003e8\u003c/span\u003e\n\u003c/span\u003e):\n    batch_runner = BatchEvalRunner(\n        evaluators, workers=num_workers, show_progress=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e\n    )\n\n    eval_results = \u003cspan class=\"hljs-keyword\"\u003eawait\u003c/span\u003e batch_runner.aevaluate_queries(\n        query_engine, queries=questions, reference=reference\n    )\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e eval_results\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eGet Query Engine, Questions, and References.\u003c/h2\u003e\u003cpre\u003e\u003cspan id=\"d93d\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003equery_engine, rag_dataset = create_query_engine_rag_dataset(\u003cspan class=\"hljs-string\"\u003e\"./data/llama2\"\u003c/span\u003e)\n\nquestions = [example.query \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e example \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e rag_dataset.examples]\n\nreference = [[example.reference_answer] \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e example \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e rag_dataset.examples]\u003c/span\u003e\u003c/pre\u003e\u003ch2\u003eCompute Correctness, Faithfulness, and Relevancy Evaluation.\u003c/h2\u003e\u003cpre\u003e\u003cspan id=\"72a2\" class=\"qz nb gt qb b bf ra rb l rc rd\"\u003eprometheus_eval_results = await batch_eval_runner(\n    prometheus_evaluators, query_engine, questions, reference\n)\n\ngpt4_eval_results = await batch_eval_runner(\n    gpt4_evaluators, query_engine, questions, reference\n)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eResults\u003c/h1\u003e\u003ch2\u003eCorrectness Evaluation score distribution.\u003c/h2\u003e\u003ch2\u003eWith the Prometheus Model:\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"e31c\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e3.0: 56.0, \n1.0: 26.0, \n5.0: 9.0, \n4.0: 8.0, \n2.0: 1.0\u003c/p\u003e\u003c/blockquote\u003e\u003ch2\u003eWith GPT-4 Model:\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"d4f1\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e4.5: 57.99, \n1.0: 6.0, \n4.0: 12.0, \n5.0: 10.0, \n2.0: 5.0, \n3.5: 5.0, \n2.5: 3.0, \n3.0: 1.0\u003c/p\u003e\u003c/blockquote\u003e\u003ch2\u003e\u003cstrong\u003eObservation:\u003c/strong\u003e\u003c/h2\u003e\u003cp\u003ePrometheus Evaluator gives more \u003ccode class=\"cw py pz qa qb b\"\u003e3.0\u003c/code\u003e scores compared to GPT-4 which gives \u003ccode class=\"cw py pz qa qb b\"\u003e4.5\u003c/code\u003e. Let’s investigate it with the help of feedback.\u003c/p\u003e\u003ch2\u003eFeedback comparison between Prometheus and GPT-4 for correctness.\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"c272\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003eQuery:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e Based on the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models,” what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed?\u003c/em\u003e\u003c/p\u003e\u003cp id=\"3a16\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003eGenerated Answer:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"2c5f\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003eReference Answer:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e The two primary objectives achieved in the work described in the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models” are: 1. The development and release of a collection of pretrained and fine-tuned large language models (LLMs) specifically optimized for dialogue use cases. 2. The demonstration that these fine-tuned LLMs, referred to as Llama 2-Chat, outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models, particularly in terms of helpfulness and safety based on human evaluations. The range of parameters for the large language models developed in this work is from 7 billion to 70 billion parameters.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"cd6c\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003ePrometheus Feedback:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e The generated response is relevant to the user query and correctly identifies the two primary objectives of the work described in the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models.” However, it does not mention the demonstration of the fine-tuned LLMs outperforming open-source chat models on most benchmarks tested, which is a key point in the reference response. The range of parameters for the large language models developed is correctly identified, but the response does not mention the specific models referred to as Llama 2-Chat. So the overall score is 3.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"6fc5\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003ePrometheus Score:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e 3.0\u003c/em\u003e\u003c/p\u003e\u003cp id=\"a3f6\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003eGPT-4 Feedback:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e The generated answer is relevant and almost fully correct. It correctly identifies the two primary objectives and the range of parameters for the large language models. However, it misses the detail about Llama 2-Chat outperforming other models on most benchmarks and potentially being a suitable substitute for closed-source models.\u003c/em\u003e\u003c/p\u003e\u003cp id=\"8c97\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003e\u003cem class=\"gt\"\u003eGPT-4 Score:\u003c/em\u003e\u003c/strong\u003e\u003cem class=\"gt\"\u003e 4.5\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003e\u003cstrong\u003eObservation:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe feedback from Prometheus is a little more precise compared to GPT-4 and it penalizes and gives a score of \u003ccode class=\"cw py pz qa qb b\"\u003e3.0\u003c/code\u003ebut GPT-4 gives a score of \u003ccode class=\"cw py pz qa qb b\"\u003e4.5\u003c/code\u003e. Prometheus penalizes more even if some of the text in the reference answer is missed in the generated answer.\u003c/p\u003e\u003ch2\u003eFaithfulness and Relevancy Evaluation scores.\u003c/h2\u003e\u003ch2\u003eWith the Prometheus Model:\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"c4ea\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003efaithfulness Score: 0.39 \nrelevancy Score: 0.57\u003c/p\u003e\u003c/blockquote\u003e\u003ch2\u003eWith GPT-4 Model:\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"c9d6\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003efaithfulness Score: 0.93\nrelevancy Score: 0.98\u003c/p\u003e\u003c/blockquote\u003e\u003cp\u003e\u003cstrong\u003eObservation:\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWe see a higher faithfulness and relevancy score with GPT-4 compared to the Prometheus model. Let’s investigate it through feedback.\u003c/p\u003e\u003ch2\u003eHamming Distance comparison between Prometheus and GPT-4:\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"48b6\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003eFaithfulness Hamming Distance: 58 \nRelevancy Hamming Distance: 41\u003c/p\u003e\u003c/blockquote\u003e\u003ch2\u003eObservation:\u003c/h2\u003e\u003cp\u003eThe comparison reveals that approximately \u003ccode class=\"cw py pz qa qb b\"\u003e42%\u003c/code\u003e of the scores in case of \u003ccode class=\"cw py pz qa qb b\"\u003eFaithfulness\u003c/code\u003e and \u003ccode class=\"cw py pz qa qb b\"\u003e59%\u003c/code\u003e in case of \u003ccode class=\"cw py pz qa qb b\"\u003eRelevancy\u003c/code\u003e are common between Prometheus and GPT-4 evaluations. This indicates a decent amount of correlation in terms of faithfulness and relevance scoring between the Prometheus and GPT-4 models.\u003c/p\u003e\u003ch2\u003eFeedback comparison between Prometheus and GPT-4 for Faithfulness and Relevancy\u003c/h2\u003e\u003cblockquote\u003e\u003cp id=\"ac48\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eQuery:\u003c/strong\u003e Based on the abstract of “Llama 2: Open Foundation and Fine-Tuned Chat Models,” what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.\u003c/p\u003e\u003cp id=\"be60\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eContext-1:\u003c/strong\u003e Llama 2 : Open Foundation and Fine-Tuned Chat Models Hugo Touvron∗Louis Martin†Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗ GenAI, Meta Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed- source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com †Second author Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2 [cs.CL] 19 Jul 2023\u003c/p\u003e\u003cp id=\"7e4b\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eContext-2: \u003c/strong\u003e(2021)alsoilluminatesthedifficultiestiedtochatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models. InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022) and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto continuing our work engaging with the broader policy, academic, and industry community on these issues. 7 Conclusion Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch, wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto transparency and safety, we plan to make further improvements to Llama 2-Chat in future work. 36\u003c/p\u003e\u003cp id=\"07ab\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003ePrometheus Faithfulness Feedback:\u003c/strong\u003e The information provided in the context is not supported by the given information. The context is about the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. However, the information provided in the context does not align with the given information. The context does not mention the range of parameters for the large language models developed, which is the primary objective mentioned in the information. The context only talks about the development and release of Llama 2 and its optimization for dialogue use cases, but it does not provide any information about the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO\u003c/p\u003e\u003cp id=\"98bd\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003ePrometheus Faithfulness Score:\u003c/strong\u003e 0.0\u003c/p\u003e\u003cp id=\"9617\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003ePrometheus Relevancy Feedback:\u003c/strong\u003e The response is not in line with the context information provided. The query asked for the two primary objectives achieved in the work and the range of parameters for the large language models developed. However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO\u003c/p\u003e\u003cp id=\"dfd9\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003ePrometheus Relevancy Score:\u003c/strong\u003e 0.0\u003c/p\u003e\u003cp id=\"871f\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eGPT-4 Faithfulness Feedback:\u003c/strong\u003e The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YES\u003c/p\u003e\u003cp id=\"2c3d\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eGPT-4 Faithfulness Score:\u003c/strong\u003e 1.0\u003c/p\u003e\u003cp id=\"efdb\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eGPT-4 Relevancy Feedback:\u003c/strong\u003e The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YES\u003c/p\u003e\u003cp id=\"fc08\" class=\"ny nz qf oa b ob ox od oe of oy oh oi oj oz ol om on pa op oq or pb ot ou ov gm bj\"\u003e\u003cstrong\u003eGPT-4 Relevancy Score:\u003c/strong\u003e 1.0\u003c/p\u003e\u003c/blockquote\u003e\u003ch1\u003eObservation:\u003c/h1\u003e\u003col\u003e\u003cli\u003ePrometheus: If you compare the feedback and contexts, there is mention of a range of parameters in the context and response but the feedback says the model could not find such information.\u003c/li\u003e\u003cli\u003eGPT-4: Evaluates it correctly, unlike the Prometheus model.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003eSummary\u003c/h1\u003e\u003col\u003e\u003cli\u003eThe cost for evaluation (approx.): \u003ccode class=\"cw py pz qa qb b\"\u003e$1.5\u003c/code\u003e for Prometheus Model and \u003ccode class=\"cw py pz qa qb b\"\u003e$15\u003c/code\u003e for GPT4.\u003c/li\u003e\u003cli\u003eThe Prometheus model, though offering more detailed feedback than GPT-4, occasionally provides incorrect feedback, necessitating cautious application.\u003c/li\u003e\u003cli\u003eIf a generated answer lacks certain facts present in the reference answer, the Prometheus model applies stricter penalties to scores than GPT-4.\u003c/li\u003e\u003cli\u003eThe faithfulness and relevancy feedback of Prometheus shows more hallucinations/ wrong interpretations in the feedback compared to GPT-4.\u003c/li\u003e\u003c/ol\u003e\u003ch1\u003e\u003cstrong\u003eNote:\u003c/strong\u003e\u003c/h1\u003e\u003cul\u003e\u003cli\u003eYou can check detailed analysis with code on \u003ca href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/evaluation/prometheus_evaluation.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Colab Notebook\u003c/a\u003e.\u003c/li\u003e\u003cli\u003eThe endpoint on HF is served on AWS Nvidia A100G · 1x GPU · 80 GB which costs $6.5/h. (We extend our gratitude to the Hugging Face team for their assistance whenever we encounter issues.)\u003c/li\u003e\u003cli\u003eWe used the \u003ca href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePrometheus model\u003c/a\u003e for the analysis here. We also made a similar analysis with the \u003ca href=\"https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGPTQ Quantized version\u003c/a\u003e of the \u003ca href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePrometheus model\u003c/a\u003e and observed a bit more hallucinations in feedback compared to the original unquantized model. Thanks to the authors of the paper for open-sourcing the model and \u003ca href=\"https://twitter.com/TheBlokeAI\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eTom Jobbins\u003c/a\u003e for the quantized version of the model.\u003c/li\u003e\u003c/ul\u003e\u003ch1\u003eReferences:\u003c/h1\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2310.08491\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePrometheus paper\u003c/a\u003e.\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003ePrometheus model on HuggingFace.\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","image":{"_type":"image","asset":{"_ref":"image-431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/431a14b31a8a2d4e5c27ff31cd0f657571687ea7-1026x954.png","publishedDate":"2023-12-15","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-e1c4d777a0138dbccbbc909ab66184688ab914fc-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"llamaindex-newsletter-2024-03-19","title":"LlamaIndex Newsletter 2024-03-19"}],"slug":{"_type":"slug","current":"llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"0cc0ccfb-0d08-4719-860e-fbcc11f095cf","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"open-source"},"title":"Open Source"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:12Z","_id":"56f31c74-33e8-46ba-b349-53a7dd21a7de","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:12Z","slug":{"_type":"slug","current":"gpt-4"},"title":"Gpt 4"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"e171aa9d-bc85-4645-8a08-eabe04c530c7","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"openai"},"title":"OpenAI"}],"title":"LlamaIndex: RAG Evaluation Showdown with GPT-4 vs. Open-Source Prometheus Model"},"publishedDate":"Invalid Date"},"params":{"slug":"llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"llamaindex-rag-evaluation-showdown-with-gpt-4-vs-open-source-prometheus-model-14cdca608277"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>