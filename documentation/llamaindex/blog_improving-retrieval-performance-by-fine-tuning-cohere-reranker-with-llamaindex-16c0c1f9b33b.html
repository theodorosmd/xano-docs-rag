<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><title>Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data</title><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/site.webmanifest"/><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#da532c"/><meta name="theme-color" content="#ffffff"/><meta name="title" content="Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta name="description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:title" content="Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="og:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="og:image" content="https://cdn.sanity.io/images/7m9jw85w/production/08273b147aada29f472350a94f12affaa128f757-342x208.png"/><meta property="twitter:card" content="summary_large_image"/><meta property="twitter:title" content="Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex — LlamaIndex - Build Knowledge Assistants over your Enterprise Data"/><meta property="twitter:description" content="LlamaIndex is a simple, flexible framework for building knowledge assistants using LLMs connected to your enterprise data."/><meta property="twitter:image" content="https://cdn.sanity.io/images/7m9jw85w/production/08273b147aada29f472350a94f12affaa128f757-342x208.png"/><link rel="alternate" type="application/rss+xml" href="https://www.llamaindex.ai/blog/feed"/><meta name="next-head-count" content="20"/><script>
            (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-WWRFB36R');
            </script><link rel="preload" href="/_next/static/css/41c9222e47d080c9.css" as="style"/><link rel="stylesheet" href="/_next/static/css/41c9222e47d080c9.css" data-n-g=""/><link rel="preload" href="/_next/static/css/97c33c8d95f1230e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/97c33c8d95f1230e.css" data-n-p=""/><link rel="preload" href="/_next/static/css/e009059e80bf60c5.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e009059e80bf60c5.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-1b629d9c8fb16f34.js" defer=""></script><script src="/_next/static/chunks/framework-df1f68dff096b68a.js" defer=""></script><script src="/_next/static/chunks/main-eca7952a704663f8.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c7c49437be49d2ad.js" defer=""></script><script src="/_next/static/chunks/d9067523-4985945b21298365.js" defer=""></script><script src="/_next/static/chunks/41155975-60c12da9ce9fa0b2.js" defer=""></script><script src="/_next/static/chunks/cb355538-cee2ea45674d9de3.js" defer=""></script><script src="/_next/static/chunks/9494-dff62cb53535dd7d.js" defer=""></script><script src="/_next/static/chunks/4063-39a391a51171ff87.js" defer=""></script><script src="/_next/static/chunks/6889-edfa85b69b88a372.js" defer=""></script><script src="/_next/static/chunks/5575-11ee0a29eaffae61.js" defer=""></script><script src="/_next/static/chunks/3444-95c636af25a42734.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-82c8e764e69afd2c.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_buildManifest.js" defer=""></script><script src="/_next/static/C8J-EMc_4OCN1ch65l4fl/_ssgManifest.js" defer=""></script></head><body><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WWRFB36R" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div id="__next"><div class="__variable_d65c78 __variable_b1ea77 __variable_eb7534"><a class="Announcement_announcement__2ohK8" href="http://48755185.hs-sites.com/llamaindex-0">Meet LlamaIndex at the Databricks Data + AI Summit!<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M8.293 5.293a1 1 0 0 1 1.414 0l6 6a1 1 0 0 1 0 1.414l-6 6a1 1 0 0 1-1.414-1.414L13.586 12 8.293 6.707a1 1 0 0 1 0-1.414Z" clip-rule="evenodd"></path></svg></a><header class="Header_header__hO3lJ"><button class="Hamburger_hamburger__17auO Header_hamburger__lUulX"><svg width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.5 14H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" id="hamburger-stroke-top" class="Hamburger_hamburgerStrokeMiddle__I7VpD"></path><path d="M3.5 7H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeTop__oOhFM"></path><path d="M3.5 21H24.5" stroke="#212121" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="Hamburger_hamburgerStrokeBottom__GIQR2"></path></svg></button><a aria-label="Homepage" href="/"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" class="Header_logo__e5KhT" style="color:transparent" src="/llamaindex.svg"/></a><nav aria-label="Main" data-orientation="horizontal" dir="ltr" style="--content-position:0px"><div style="position:relative"><ul data-orientation="horizontal" class="Nav_MenuList__PrCDJ" dir="ltr"><li><button id="radix-:R6tm:-trigger-radix-:R5mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R5mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Products</button></li><li><button id="radix-:R6tm:-trigger-radix-:R9mtm:" data-state="closed" aria-expanded="false" aria-controls="radix-:R6tm:-content-radix-:R9mtm:" class="Nav_Trigger__ws43x" data-radix-collection-item="">Solutions</button></li><li><a class="Nav_Link__ZrzFc" href="/community" data-radix-collection-item="">Community</a></li><li><a class="Nav_Link__ZrzFc" href="/pricing" data-radix-collection-item="">Pricing</a></li><li><a class="Nav_Link__ZrzFc" href="/blog" data-radix-collection-item="">Blog</a></li><li><a class="Nav_Link__ZrzFc" href="/customers" data-radix-collection-item="">Customer stories</a></li><li><a class="Nav_Link__ZrzFc" href="/careers" data-radix-collection-item="">Careers</a></li></ul></div><div class="Nav_ViewportPosition__jmyHM"></div></nav><div class="Header_secondNav__YJvm8"><nav><a href="/contact" class="Link_link__71cl8 Link_link-variant-tertiary__BYxn_ Header_bookADemo__qCuxV">Book a demo</a></nav><a href="https://cloud.llamaindex.ai/" class="Button_button-variant-default__Oi__n Button_button__aJ0V6 Header_button__1HFhY" data-tracking-variant="default"> <!-- -->Get started</a></div><div class="MobileMenu_mobileMenu__g5Fa6"><nav class="MobileMenu_nav__EmtTw"><ul><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Products<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaparse"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.6654 1.66675V6.66675H16.6654M8.33203 10.8334L6.66536 12.5001L8.33203 14.1667M11.6654 14.1667L13.332 12.5001L11.6654 10.8334M12.082 1.66675H4.9987C4.55667 1.66675 4.13275 1.84234 3.82019 2.1549C3.50763 2.46746 3.33203 2.89139 3.33203 3.33341V16.6667C3.33203 17.1088 3.50763 17.5327 3.82019 17.8453C4.13275 18.1578 4.55667 18.3334 4.9987 18.3334H14.9987C15.4407 18.3334 15.8646 18.1578 16.1772 17.8453C16.4898 17.5327 16.6654 17.1088 16.6654 16.6667V6.25008L12.082 1.66675Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Document parsing</div><p class="MobileMenu_ListItemText__n_MHY">The first and leading GenAI-native parser over your most complex data.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/llamaextract"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M11.668 1.66675V5.00008C11.668 5.44211 11.8436 5.86603 12.1561 6.17859C12.4687 6.49115 12.8926 6.66675 13.3346 6.66675H16.668M3.33464 5.83341V3.33341C3.33464 2.89139 3.51023 2.46746 3.82279 2.1549C4.13535 1.84234 4.55927 1.66675 5.0013 1.66675H12.5013L16.668 5.83341V16.6667C16.668 17.1088 16.4924 17.5327 16.1798 17.8453C15.8672 18.1578 15.4433 18.3334 15.0013 18.3334L5.05379 18.3326C4.72458 18.3755 4.39006 18.3191 4.09312 18.1706C3.79618 18.0221 3.55034 17.7884 3.38713 17.4992M4.16797 9.16675L1.66797 11.6667M1.66797 11.6667L4.16797 14.1667M1.66797 11.6667H10.0013" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Data extraction</div><p class="MobileMenu_ListItemText__n_MHY">Extract structured data from documents using a schema-driven engine.</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/enterprise"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M9.16667 15.8333C12.8486 15.8333 15.8333 12.8486 15.8333 9.16667C15.8333 5.48477 12.8486 2.5 9.16667 2.5C5.48477 2.5 2.5 5.48477 2.5 9.16667C2.5 12.8486 5.48477 15.8333 9.16667 15.8333Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M17.5 17.5L13.875 13.875" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Knowledge Management</div><p class="MobileMenu_ListItemText__n_MHY">Connect, transform, and index your enterprise data into an agent-accessible knowledge base</p></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/framework"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.0013 6.66659V3.33325H6.66797M1.66797 11.6666H3.33464M16.668 11.6666H18.3346M12.5013 10.8333V12.4999M7.5013 10.8333V12.4999M5.0013 6.66659H15.0013C15.9218 6.66659 16.668 7.41278 16.668 8.33325V14.9999C16.668 15.9204 15.9218 16.6666 15.0013 16.6666H5.0013C4.08083 16.6666 3.33464 15.9204 3.33464 14.9999V8.33325C3.33464 7.41278 4.08083 6.66659 5.0013 6.66659Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Agent Framework</div><p class="MobileMenu_ListItemText__n_MHY">Orchestrate and deploy multi-agent applications over your data with the #1 agent framework.</p></a></li></ul></details></li><li><details class="MobileMenu_ListItem__yMtVi"><summary class="MobileMenu_ListItemHeading___yPC6">Solutions<!-- --> <span class="MobileMenu_icon__6gmaF"><svg width="11" height="6" viewBox="0 0 11 6" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10 1L5.5 5L1 1" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></summary><ul class="MobileMenu_List__XjJr0"><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/finance"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 6.66675H8.33073C7.8887 6.66675 7.46478 6.84234 7.15222 7.1549C6.83966 7.46746 6.66406 7.89139 6.66406 8.33342C6.66406 8.77544 6.83966 9.19937 7.15222 9.51193C7.46478 9.82449 7.8887 10.0001 8.33073 10.0001H11.6641C12.1061 10.0001 12.53 10.1757 12.8426 10.4882C13.1551 10.8008 13.3307 11.2247 13.3307 11.6667C13.3307 12.1088 13.1551 12.5327 12.8426 12.8453C12.53 13.1578 12.1061 13.3334 11.6641 13.3334H6.66406M9.9974 15.0001V5.00008M18.3307 10.0001C18.3307 14.6025 14.5998 18.3334 9.9974 18.3334C5.39502 18.3334 1.66406 14.6025 1.66406 10.0001C1.66406 5.39771 5.39502 1.66675 9.9974 1.66675C14.5998 1.66675 18.3307 5.39771 18.3307 10.0001Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Financial Analysts</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/administrative-operations"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.66406 6.66659V15.8333C1.66406 16.2753 1.83966 16.6992 2.15222 17.0118C2.46478 17.3243 2.8887 17.4999 3.33073 17.4999H14.9974M16.6641 14.1666C17.1061 14.1666 17.53 13.991 17.8426 13.6784C18.1551 13.3659 18.3307 12.9419 18.3307 12.4999V7.49992C18.3307 7.05789 18.1551 6.63397 17.8426 6.32141C17.53 6.00885 17.1061 5.83325 16.6641 5.83325H13.4141C13.1353 5.83598 12.8604 5.76876 12.6143 5.63774C12.3683 5.50671 12.159 5.31606 12.0057 5.08325L11.3307 4.08325C11.179 3.85281 10.9724 3.66365 10.7295 3.53275C10.4866 3.40185 10.215 3.3333 9.93906 3.33325H6.66406C6.22204 3.33325 5.79811 3.50885 5.48555 3.82141C5.17299 4.13397 4.9974 4.55789 4.9974 4.99992V12.4999C4.9974 12.9419 5.17299 13.3659 5.48555 13.6784C5.79811 13.991 6.22204 14.1666 6.66406 14.1666H16.6641Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Administrative Operations</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/engineering"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M13.3307 15L18.3307 10L13.3307 5M6.66406 5L1.66406 10L6.66406 15" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Engineering &amp; R&amp;D</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/customer-support"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M14.9974 7.50008H16.6641C17.1061 7.50008 17.53 7.67568 17.8426 7.98824C18.1551 8.3008 18.3307 8.72472 18.3307 9.16675V18.3334L14.9974 15.0001H9.9974C9.55537 15.0001 9.13145 14.8245 8.81888 14.5119C8.50632 14.1994 8.33073 13.7754 8.33073 13.3334V12.5001M11.6641 7.50008C11.6641 7.94211 11.4885 8.36603 11.1759 8.67859C10.8633 8.99115 10.4394 9.16675 9.9974 9.16675H4.9974L1.66406 12.5001V3.33341C1.66406 2.41675 2.41406 1.66675 3.33073 1.66675H9.9974C10.4394 1.66675 10.8633 1.84234 11.1759 2.1549C11.4885 2.46746 11.6641 2.89139 11.6641 3.33341V7.50008Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Customer Support</div></a></li><li><a class="MobileMenu_ListItemLink__dnvmV" href="/solutions/healthcare-pharma"><div class="MobileMenu_ListItemHeading___yPC6"><svg width="20" height="20" viewBox="0 0 20 20" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M17.0128 3.81671C16.5948 3.39719 16.098 3.06433 15.551 2.8372C15.004 2.61008 14.4176 2.49316 13.8253 2.49316C13.2331 2.49316 12.6466 2.61008 12.0996 2.8372C11.5527 3.06433 11.0559 3.39719 10.6378 3.81671L9.99617 4.46671L9.3545 3.81671C8.93643 3.39719 8.43967 3.06433 7.89268 2.8372C7.3457 2.61008 6.75926 2.49316 6.167 2.49316C5.57474 2.49316 4.9883 2.61008 4.44132 2.8372C3.89433 3.06433 3.39756 3.39719 2.9795 3.81671C1.21283 5.58338 1.1045 8.56671 3.3295 10.8334L9.99617 17.5L16.6628 10.8334C18.8878 8.56671 18.7795 5.58338 17.0128 3.81671Z" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path><path d="M2.91406 9.99992H7.91406L8.33073 9.16659L9.9974 12.9166L11.6641 7.08325L12.9141 9.99992H17.0807" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round"></path></svg>Healthcare / Pharma</div></a></li></ul></details></li><li><a class="MobileMenu_Link__5frcx" href="/community">Community</a></li><li><a class="MobileMenu_Link__5frcx" href="/pricing">Pricing</a></li><li><a class="MobileMenu_Link__5frcx" href="/blog">Blog</a></li><li><a class="MobileMenu_Link__5frcx" href="/customers">Customer stories</a></li><li><a class="MobileMenu_Link__5frcx" href="/careers">Careers</a></li></ul></nav><a href="/contact" class="Button_button-variant-ghost__o2AbG Button_button__aJ0V6" data-tracking-variant="ghost"> <!-- -->Talk to us</a><ul class="Socials_socials__8Y_s5 Socials_socials-theme-dark__Hq8lc MobileMenu_socials__JykCO"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul><p class="Text_text__zPO0D Text_text-size-16__PkjFu MobileMenu_copyright__nKVOs">© <!-- -->2025<!-- --> LlamaIndex</p></div></header><main><section class="BlogPost_post__JHNzd"><img alt="" loading="lazy" width="800" height="104" decoding="async" data-nimg="1" class="BlogPost_featuredImage__KGxwX" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F08273b147aada29f472350a94f12affaa128f757-342x208.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F08273b147aada29f472350a94f12affaa128f757-342x208.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F08273b147aada29f472350a94f12affaa128f757-342x208.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=1920&amp;q=75"/><p class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-600__fKYth BlogPost_date__6uxQw"><a class="BlogPost_author__mesdl" href="/blog/author/ravi-theja">Ravi Theja</a> <!-- -->•<!-- --> <!-- -->2023-11-16</p><h1 class="Text_text__zPO0D Text_text-size-32__koGps BlogPost_title__b2lqJ">Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex</h1><ul class="BlogPost_tags__13pBH"><li><a class="Badge_badge___1ssn" href="/blog/tag/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Llamaindex</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">AI</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/llm"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">LLM</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/cohere"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Cohere</span></a></li><li><a class="Badge_badge___1ssn" href="/blog/tag/fine-tuning"><span class="Text_text__zPO0D Text_text-size-16__PkjFu Text_text-weight-500__f2npw">Fine Tuning</span></a></li></ul><div class="BlogPost_htmlPost__Z5oDL"><h1>Introduction:</h1><p>Achieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is heavily dependent on robust retrieval performance. As we explored in our previous <a href="https://medium.com/llamaindex-blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83" rel="noopener">blog post</a>, rerankers have a significant impact on boosting retrieval performance. But what if we could take it a step further? What if our reranker was not just any reranker, but one tuned specifically to our domain or dataset? Could this specialization enhance the retrieval performance even more?</p><p>To answer these questions, we turn to CohereAI’s beta release of fine-tuning reranker(Custom reranker) models. By integrating these with LlamaIndex, we now offer the ability to build your very own Cohere custom reranker using our streamlined process.</p><p>In this blog post, we’ll guide you through the steps to create a Cohere custom reranker with LlamaIndex and evaluate the retrieval performance.</p><p>For a hands-on walkthrough, you can follow the tutorial on <a href="https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/finetuning/rerankers/cohere_custom_reranker.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab Notebook</a>.</p><p>Let’s start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex.</p><blockquote><p id="44a2" class="oi oj pp ok b ol ph on oo op pi or os ot pj ov ow ox pk oz pa pb pl pd pe pf gm bj">N<!-- -->OTE: This is a guide for fine-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. We suggest experimenting with your dataset and various parameters before deciding to incorporate it into your RAG pipeline.</p></blockquote><h1>Setting Up the Environment</h1><pre><span id="5f24" class="pz nl gt pw b bf qa qb l qc qd">!pip install llama-index cohere pypdf</span></pre><h1>Setting Up the Keys</h1><pre><span id="9827" class="pz nl gt pw b bf qa qb l qc qd">openai_api_key = 'YOUR OPENAI API KEY'
cohere_api_key = 'YOUR COHEREAI API KEY'

import os

os.environ["OPENAI_API_KEY"] = openai_api_key
os.environ["COHERE_API_KEY"] = cohere_api_key</span></pre><h1>Download the Data</h1><p>We will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC Filings for evaluation.</p><pre><span id="726f" class="pz nl gt pw b bf qa qb l qc qd">!mkdir -p 'data/10k/'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'
!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'</span></pre><h1>Load the Data</h1><pre><span id="bb7a" class="pz nl gt pw b bf qa qb l qc qd">lyft_docs = SimpleDirectoryReader(input_files=['./data/10k/lyft_2021.pdf']).load_data()
uber_docs = SimpleDirectoryReader(input_files=['./data/10k/uber_2021.pdf']).load_data()</span></pre><h1>Data Curation</h1><p><strong>Create Nodes.</strong></p><p>The <a href="https://docs.cohere.com/docs/rerank-models" rel="noopener ugc nofollow" target="_blank">documentation</a> mentions that Query + Relevant Passage/ Query + Hard Negatives should be less than 510 tokens. To accommodate that we limit <code class="cw qe qf qg pw b">chunk_size</code>to 400 tokens. (Each chunk will eventually be treated as a Relevant Passage/ Hard Negative)</p><pre><span id="e284" class="pz nl gt pw b bf qa qb l qc qd"># Limit chunk size to 400
node_parser = SimpleNodeParser.from_defaults(chunk_size=400)

# Create nodes
lyft_nodes = node_parser.get_nodes_from_documents(lyft_docs)
uber_nodes = node_parser.get_nodes_from_documents(uber_docs)</span></pre><p>We will use gpt-4 to create questions from chunks.</p><pre><span id="47d3" class="pz nl gt pw b bf qa qb l qc qd">llm = OpenAI(api_key=openai_api_key, temperature=0, model='gpt-4')</span></pre><p>Prompt to generate questions from each Node/ chunk.</p><pre><span id="7066" class="pz nl gt pw b bf qa qb l qc qd"># Prompt to generate questions
qa_generate_prompt_tmpl = """\
Context information is below.

---------------------
{context_str}
---------------------

Given the context information and not prior knowledge.
generate only questions based on the below query.

You are a Professor. Your task is to setup \
{num_questions_per_chunk} questions for an upcoming \
quiz/examination. The questions should be diverse in nature \
across the document. The questions should not contain options, not start with Q1/ Q2. \
Restrict the questions to the context information provided.\
"""</span></pre><p>It expects a minimum of 256 (Query + Relevant passage) pairs with or without hard negatives for training and 64 pairs for validation. Please note that the validation is optional.</p><p><strong>Training:</strong> We use the first 256 nodes from Lyft for creating training pairs.</p><p><strong>Validation:</strong> We will use the next 64 nodes from Lyft for validation.</p><p><strong>Testing: </strong>We will use the first 150 nodes from Uber.</p><pre><span id="7d01" class="pz nl gt pw b bf qa qb l qc qd"><span class="hljs-comment"># Training dataset</span>
qa_dataset_lyft_train = generate_question_context_pairs(
    lyft_nodes[:<span class="hljs-number">256</span>], llm=llm, num_questions_per_chunk=<span class="hljs-number">1</span>, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl
)

<span class="hljs-comment"># Save [Optional]</span>
qa_dataset_lyft_train.save_json(<span class="hljs-string">"lyft_train_dataset.json"</span>)

<span class="hljs-comment"># Validation dataset</span>
qa_dataset_lyft_val = generate_question_context_pairs(
    lyft_nodes[<span class="hljs-number">257</span>:<span class="hljs-number">321</span>], llm=llm, num_questions_per_chunk=<span class="hljs-number">1</span>, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl
)

<span class="hljs-comment"># Save [Optional]</span>
qa_dataset_lyft_val.save_json(<span class="hljs-string">"lyft_val_dataset.json"</span>)

<span class="hljs-comment"># Testing dataset</span>
qa_dataset_uber_val = generate_question_context_pairs(
    uber_nodes[:<span class="hljs-number">150</span>], llm=llm, num_questions_per_chunk=<span class="hljs-number">1</span>, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl
)

<span class="hljs-comment"># Save [Optional]</span>
qa_dataset_uber_val.save_json(<span class="hljs-string">"uber_val_dataset.json"</span>)</span></pre><p>Now that we have compiled questions from each chunk, we will format the data according to the specifications required for training and validation.</p><h1>Data Format and Requirements</h1><p>For both training and validation, it currently accepts data in the format of triplets, every row should have the following</p><p><strong>query: </strong>This represents the question or target.</p><p><strong>relevant_passages:</strong> This represents a list of documents or passages that contain information that answers the query. For every query, there must be at least one relevant_passage</p><p><strong>hard_negatives:</strong> This represents chunks or passages that don’t contain answers for the query. It should be noted that Hard negatives are optional but providing at least ~5 hard negatives will lead to meaningful improvement.</p><p>You can check the <a href="https://docs.cohere.com/docs/rerank-models" rel="noopener ugc nofollow" target="_blank">documentation</a> for more details.</p><p>We need to have an embedding model for creating hard negatives with a cosine similarity approach.</p><pre><span id="05a9" class="pz nl gt pw b bf qa qb l qc qd"># Initialize the Cohere embedding model which we use it for creating Hard Negatives.
embed_model = CohereEmbedding(
    cohere_api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_document",
)</span></pre><p>Let’s create 3 datasets.</p><ol><li>Dataset without hard negatives.</li><li>Dataset with hard negatives selected at random.</li><li>Dataset with hard negatives selected based on cosine similarity.</li></ol><pre><span id="b74a" class="pz nl gt pw b bf qa qb l qc qd"># Train and val datasets without hard negatives.
generate_cohere_reranker_finetuning_dataset(
    qa_dataset_lyft_train,
    finetune_dataset_file_name = "train.jsonl"
)

generate_cohere_reranker_finetuning_dataset(
    qa_dataset_lyft_val,
    finetune_dataset_file_name = "val.jsonl"
)

# Train and val datasets with hard negatives selected at random.
generate_cohere_reranker_finetuning_dataset(
    qa_dataset_lyft_train,
    num_negatives = 5,
    hard_negatives_gen_method = "random",
    finetune_dataset_file_name = "train_5_random.jsonl",
    embed_model = embed_model,
)

generate_cohere_reranker_finetuning_dataset(
    qa_dataset_lyft_val,
    num_negatives = 5,
    hard_negatives_gen_method = "random",
    finetune_dataset_file_name = "val_5_random.jsonl",
    embed_model = embed_model,
)

# Train and val datasets with hard negatives selected based on cosine similarity.
generate_cohere_reranker_finetuning_dataset(
    qa_dataset_lyft_train,
    num_negatives = 5,
    hard_negatives_gen_method = "cosine_similarity",
    finetune_dataset_file_name = "train_5_cosine_similarity.jsonl",
    embed_model = embed_model,
)

generate_cohere_reranker_finetuning_dataset(
    qa_dataset_lyft_val,
    num_negatives = 5,
    hard_negatives_gen_method = "cosine_similarity",
    finetune_dataset_file_name = "val_5_cosine_similarity.jsonl",
    embed_model = embed_model,
)</span></pre><h1>Fine-tuning Reranker (Custom Reranker)</h1><p>With our training and validation datasets ready, we’re set to proceed with the training process. Be aware that this training is expected to take approximately 25 to 45 minutes.</p><pre><span id="0413" class="pz nl gt pw b bf qa qb l qc qd"># Reranker model with 0 hard negatives.
finetune_model_no_hard_negatives = CohereRerankerFinetuneEngine(
    train_file_name="train.jsonl",
    val_file_name="val.jsonl",
    model_name="lyft_reranker_0_hard_negatives1",
    model_type="RERANK",
    base_model="english",
    api_key = cohere_api_key
)
finetune_model_no_hard_negatives.finetune()

# Reranker model with 5 hard negatives selected at random
finetune_model_random_hard_negatives = CohereRerankerFinetuneEngine(
    train_file_name="train_5_random.jsonl",
    val_file_name="val_5_random.jsonl",
    model_name="lyft_reranker_5_random_hard_negatives1",
    model_type="RERANK",
    base_model="english",
)
finetune_model_random_hard_negatives.finetune()

# Reranker model with 5 hard negatives selected based on cosine similarity
finetune_model_cosine_hard_negatives = CohereRerankerFinetuneEngine(
    train_file_name="train_5_cosine_similarity.jsonl",
    val_file_name="val_5_cosine_similarity.jsonl",
    model_name="lyft_reranker_5_cosine_hard_negatives1",
    model_type="RERANK",
    base_model="english",
)
finetune_model_cosine_hard_negatives.finetune()</span></pre><p>Once the jobs are submitted, you can check the training status in the <code class="cw qe qf qg pw b">models</code> section of the <a href="https://dashboard.cohere.com/models" rel="noopener ugc nofollow" target="_blank">dashboard</a>. You can check the status of the job in the dashboard and you should see an image something similar to the following one.</p><figure><img src="/blog/images/1*CFAtQH8dwzjnUrzVktqfbA.png" alt="" width="700" height="552"></figure><p>You then need to get the Cohere Reranker model for testing.</p><pre><span id="ee22" class="pz nl gt pw b bf qa qb l qc qd">reranker_base = CohereRerank(top_n=<span class="hljs-number">5</span>)
reranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(
    top_n=<span class="hljs-number">5</span>
)
reranker_model_5_random = (
    finetune_model_random_hard_negatives.get_finetuned_model(top_n=<span class="hljs-number">5</span>)
)
reranker_model_5_cosine = (
    finetune_model_cosine_hard_negatives.get_finetuned_model(top_n=<span class="hljs-number">5</span>)
)</span></pre><h1>Testing</h1><p>We will conduct tests on the first 150 nodes from Uber using the following different rerankers.</p><ol><li>Without Reranker.</li><li>Cohere Reranker.</li><li>Fine-tuned reranker (Custom reranker) without hard negatives.</li><li>Fine-tuned reranker (Custom reranker) with hard negatives selected at random.</li><li>Fine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity.</li></ol><p>Let’s define the rerankers.</p><pre><span id="dade" class="pz nl gt pw b bf qa qb l qc qd">RERANKERS = {
    "WithoutReranker": "None",
    "CohereRerank": reranker_base,
    "CohereRerank_0": reranker_model_0,
    "CohereRerank_5_random": reranker_model_5_random,
    "CohereRerank_5_cosine": reranker_model_5_cosine,
}</span></pre><p>Create an Index and Retriever for evaluation purposes.</p><pre><span id="4724" class="pz nl gt pw b bf qa qb l qc qd"># Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.
index_embed_model = CohereEmbedding(
    cohere_api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_document",
)

query_embed_model = CohereEmbedding(
    cohere_api_key=cohere_api_key,
    model_name="embed-english-v3.0",
    input_type="search_query",
)

service_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)
service_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)

vector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)
vector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query)</span></pre><p>Define a function to display the results</p><pre><span id="c645" class="pz nl gt pw b bf qa qb l qc qd"><span class="hljs-keyword">def</span> <span class="hljs-title function_">display_results</span>(<span class="hljs-params">embedding_name, reranker_name, eval_results</span>):
    <span class="hljs-string">"""Display results from evaluate."""</span>

    metric_dicts = []
    <span class="hljs-keyword">for</span> eval_result <span class="hljs-keyword">in</span> eval_results:
        metric_dict = eval_result.metric_vals_dict
        metric_dicts.append(metric_dict)

    full_df = pd.DataFrame(metric_dicts)

    hit_rate = full_df[<span class="hljs-string">"hit_rate"</span>].mean()
    mrr = full_df[<span class="hljs-string">"mrr"</span>].mean()

    metric_df = pd.DataFrame(
        {<span class="hljs-string">"Embedding"</span>: [embedding_name], <span class="hljs-string">"Reranker"</span>: [reranker_name], <span class="hljs-string">"hit_rate"</span>: [hit_rate], <span class="hljs-string">"mrr"</span>: [mrr]}
    )

    <span class="hljs-keyword">return</span> metric_df</span></pre><p>Loop over different rerankers and evaluate retrieval performance using Custom Retriever.</p><pre><span id="d3cd" class="pz nl gt pw b bf qa qb l qc qd">results_df = pd.DataFrame()

embed_name = <span class="hljs-string">'CohereEmbedding'</span>

<span class="hljs-comment"># Loop over rerankers</span>
<span class="hljs-keyword">for</span> rerank_name, reranker <span class="hljs-keyword">in</span> RERANKERS.items():

    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"Running Evaluation for Reranker: <span class="hljs-subst">{rerank_name}</span>"</span>)

    <span class="hljs-comment"># Define Retriever</span>
    <span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomRetriever</span>(<span class="hljs-title class_ inherited__">BaseRetriever</span>):
        <span class="hljs-string">"""Custom retriever that performs both Vector search and Knowledge Graph search"""</span>

        <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">
            self,
            vector_retriever: VectorIndexRetriever,
        </span>) -&amp;gt; <span class="hljs-literal">None</span>:
            <span class="hljs-string">"""Init params."""</span>

            self._vector_retriever = vector_retriever

        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_retrieve</span>(<span class="hljs-params">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class="hljs-type">List</span>[NodeWithScore]:
            <span class="hljs-string">"""Retrieve nodes given query."""</span>

            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)

            <span class="hljs-keyword">if</span> reranker != <span class="hljs-string">'None'</span>:
                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)
            <span class="hljs-keyword">else</span>:
                retrieved_nodes = retrieved_nodes[:<span class="hljs-number">5</span>]

            <span class="hljs-keyword">return</span> retrieved_nodes

        <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_aretrieve</span>(<span class="hljs-params">self, query_bundle: QueryBundle</span>) -&amp;gt; <span class="hljs-type">List</span>[NodeWithScore]:
            <span class="hljs-string">"""Asynchronously retrieve nodes given query.
            """</span>
            <span class="hljs-keyword">return</span> self._retrieve(query_bundle)

        <span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">aretrieve</span>(<span class="hljs-params">self, str_or_query_bundle: QueryType</span>) -&amp;gt; <span class="hljs-type">List</span>[NodeWithScore]:
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(str_or_query_bundle, <span class="hljs-built_in">str</span>):
                str_or_query_bundle = QueryBundle(str_or_query_bundle)
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> self._aretrieve(str_or_query_bundle)

    custom_retriever = CustomRetriever(vector_retriever)

    retriever_evaluator = RetrieverEvaluator.from_metric_names(
        [<span class="hljs-string">"mrr"</span>, <span class="hljs-string">"hit_rate"</span>], retriever=custom_retriever
    )
    eval_results = <span class="hljs-keyword">await</span> retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)

    current_df = display_results(embed_name, rerank_name, eval_results)
    results_df = pd.concat([results_df, current_df], ignore_index=<span class="hljs-literal">True</span>)</span></pre><h1>Results:</h1><figure><img src="/blog/images/1*Jv2U5pxMZWZrXjhfuXAqxg.png" alt="" width="700" height="142"></figure><p>From the above table (1- without reranker, 2 — with base cohere reranker, 3–5: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It’s crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker.</p><h1>Summary:</h1><p>In this blog post, we’ve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community’s use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.</p></div><div class="BlogPost_relatedPosts__0z6SN"><h2 class="Text_text__zPO0D Text_text-align-center__HhKqo Text_text-size-16__PkjFu Text_text-weight-400__5ENkK Text_text-family-spaceGrotesk__E4zcE BlogPost_relatedPostsTitle___JIrW">Related articles</h2><ul class="BlogPost_relatedPostsList__uOKzB"><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2Faa21c9d353919277d4fce16f174e54280bda8660-1920x832.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/jamba-instruct-s-256k-context-window-on-llamaindex">Jamba-Instruct&#x27;s 256k context window on LlamaIndex</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-07-31</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024.webp%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-04-02">LlamaIndex Newsletter 2024-04-02</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-04-02</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F67e9da6888edfa6119225413068198422f1eaf77-1024x1024.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/llamaindex-newsletter-2024-03-26">LlamaIndex Newsletter 2024-03-26</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-26</p></div></li><li><div class="CardBlog_card__mm0Zw"><div class="CardBlog_thumbnail__XCu_R"><img alt="" loading="lazy" width="400" height="280" decoding="async" data-nimg="1" class="CardBlog_image__I2Asd" style="color:transparent" srcSet="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=640&amp;q=75 1x, /_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75 2x" src="/_next/image?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2F7m9jw85w%2Fproduction%2F23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561.png%3Ffit%3Dmax%26auto%3Dformat&amp;w=828&amp;q=75"/></div><p class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth CardBlog_title__qC51U"><a href="/blog/supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations">Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations</a></p><p class="Text_text__zPO0D Text_text-size-16__PkjFu">2024-03-19</p></div></li></ul></div></section></main><footer class="Footer_footer__eNA9m"><div class="Footer_navContainer__7bvx4"><div class="Footer_logoContainer__3EpzI"><img alt="LlamaIndex" loading="lazy" width="213" height="42" decoding="async" data-nimg="1" style="color:transparent" src="/llamaindex.svg"/><div class="Footer_socialContainer__GdOgk"><ul class="Socials_socials__8Y_s5"><li><a href="https://github.com/run-llama/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M524.531,69.836a1.5,1.5,0,0,0-.764-.7A485.065,485.065,0,0,0,404.081,32.03a1.816,1.816,0,0,0-1.923.91,337.461,337.461,0,0,0-14.9,30.6,447.848,447.848,0,0,0-134.426,0,309.541,309.541,0,0,0-15.135-30.6,1.89,1.89,0,0,0-1.924-.91A483.689,483.689,0,0,0,116.085,69.137a1.712,1.712,0,0,0-.788.676C39.068,183.651,18.186,294.69,28.43,404.354a2.016,2.016,0,0,0,.765,1.375A487.666,487.666,0,0,0,176.02,479.918a1.9,1.9,0,0,0,2.063-.676A348.2,348.2,0,0,0,208.12,430.4a1.86,1.86,0,0,0-1.019-2.588,321.173,321.173,0,0,1-45.868-21.853,1.885,1.885,0,0,1-.185-3.126c3.082-2.309,6.166-4.711,9.109-7.137a1.819,1.819,0,0,1,1.9-.256c96.229,43.917,200.41,43.917,295.5,0a1.812,1.812,0,0,1,1.924.233c2.944,2.426,6.027,4.851,9.132,7.16a1.884,1.884,0,0,1-.162,3.126,301.407,301.407,0,0,1-45.89,21.83,1.875,1.875,0,0,0-1,2.611,391.055,391.055,0,0,0,30.014,48.815,1.864,1.864,0,0,0,2.063.7A486.048,486.048,0,0,0,610.7,405.729a1.882,1.882,0,0,0,.765-1.352C623.729,277.594,590.933,167.465,524.531,69.836ZM222.491,337.58c-28.972,0-52.844-26.587-52.844-59.239S193.056,219.1,222.491,219.1c29.665,0,53.306,26.82,52.843,59.239C275.334,310.993,251.924,337.58,222.491,337.58Zm195.38,0c-28.971,0-52.843-26.587-52.843-59.239S388.437,219.1,417.871,219.1c29.667,0,53.307,26.82,52.844,59.239C470.715,310.993,447.538,337.58,417.871,337.58Z"></path></svg></a></li><li><a href="https://twitter.com/llama_index"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path></svg></a></li><li><a href="https://www.linkedin.com/company/91154103/"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"></path></svg></a></li></ul></div></div><div class="Footer_nav__BLEuE"><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/">LlamaIndex</a></h3><ul><li><a href="/blog"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Blog</span></a></li><li><a href="/partners"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Partners</span></a></li><li><a href="/careers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Careers</span></a></li><li><a href="/contact"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Contact</span></a></li><li><a href="/brand"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Brand</span></a></li><li><a href="https://llamaindex.statuspage.io" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Status</span></a></li><li><a href="https://app.vanta.com/runllama.ai/trust/pkcgbjf8b3ihxjpqdx17nu" target="_blank"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Trust Center</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/enterprise">Enterprise</a></h3><ul><li><a href="https://cloud.llamaindex.ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaCloud</span></a></li><li><a href="https://cloud.llamaindex.ai/parse" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaParse</span></a></li><li><a href="/customers"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Customers</span></a></li><li><a href="/llamacloud-sharepoint-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SharePoint</span></a></li><li><a href="/llamacloud-aws-s3-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">AWS S3</span></a></li><li><a href="/llamacloud-azure-blob-storage-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Azure Blob Storage</span></a></li><li><a href="/llamacloud-google-drive-data-loading-for-generative-ai" data-tracking-variant="link" data-tracking-section="footer"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Google Drive</span></a></li> </ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/framework">Framework</a></h3><ul><li><a href="https://pypi.org/project/llama-index/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python package</span></a></li><li><a href="https://docs.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Python docs</span></a></li><li><a href="https://www.npmjs.com/package/llamaindex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript package</span></a></li><li><a href="https://ts.llamaindex.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">TypeScript docs</span></a></li><li><a href="https://llamahub.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaHub</span></a></li><li><a href="https://github.com/run-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">GitHub</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e"><a href="/community">Community</a></h3><ul><li><a href="/community#newsletter"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Newsletter</span></a></li><li><a href="https://discord.com/invite/eN6D2HQ4aX"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Discord</span></a></li><li><a href="https://www.linkedin.com/company/91154103/"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LinkedIn</span></a></li><li><a href="https://twitter.com/llama_index"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">Twitter/X</span></a></li><li><a href="https://www.youtube.com/@LlamaIndex"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">YouTube</span></a></li><li><a href="https://bsky.app/profile/llamaindex.bsky.social"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">BlueSky</span></a></li></ul></div><div><h3 class="Text_text__zPO0D Text_text-size-20__LNa6Q Text_text-weight-600__fKYth Footer_navHeader__9o83e">Starter projects</h3><ul><li><a href="https://www.npmjs.com/package/create-llama"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">create-llama</span></a></li><li><a href="https://secinsights.ai"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">SEC Insights</span></a></li><li><a href="https://github.com/run-llama/llamabot"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">LlamaBot</span></a></li><li><a href="https://docs.llamaindex.ai/en/stable/use_cases/q_and_a/rag_cli.html"><span class="Text_text__zPO0D Text_text-size-16__PkjFu">RAG CLI</span></a></li></ul></div></div></div><div class="Footer_copyrightContainer__mBKsT"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA">© <!-- -->2025<!-- --> LlamaIndex</p><div class="Footer_legalNav__O1yJA"><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/privacy-notice.pdf">Privacy Notice</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="/files/terms-of-service.pdf">Terms of Service</a></p><p class="Text_text__zPO0D Text_text-size-14__6Qc26 Footer_copyright__vFlrA"><a href="https://bit.ly/llamaindexdpa">Data Processing Addendum</a></p></div></div></footer></div><svg xmlns="http://www.w3.org/2000/svg" class="flt_svg" style="display:none"><defs><filter id="flt_tag"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="2"></feGaussianBlur><feColorMatrix in="blur" result="flt_tag" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="flt_tag" operator="atop"></feComposite></filter><filter id="svg_blur_large"><feGaussianBlur in="SourceGraphic" result="blur" stdDeviation="8"></feGaussianBlur><feColorMatrix in="blur" result="svg_blur_large" values="1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 19 -9"></feColorMatrix><feComposite in="SourceGraphic" in2="svg_blur_large" operator="atop"></feComposite></filter></defs></svg></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"page":{"announcement":{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"},"post":{"_createdAt":"2024-02-22T21:47:07Z","_id":"ffa524d4-b820-41f5-b5a0-c5a2cad1d93a","_rev":"Ys5IzmCaJ2UnW2RAX7V0rS","_type":"blogPost","_updatedAt":"2025-05-21T20:40:40Z","announcement":[{"_createdAt":"2024-12-15T02:26:13Z","_id":"announcement","_rev":"bDjEm7gsnDBrRrmjaaG2yK","_type":"announcement","_updatedAt":"2025-05-19T19:20:19Z","title":"Meet LlamaIndex at the Databricks Data + AI Summit!","url":"http://48755185.hs-sites.com/llamaindex-0"}],"authors":[{"_createdAt":"2024-02-22T19:58:55Z","_id":"60575af5-a5c2-40f6-9aab-d5e02da9c000","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"people","_updatedAt":"2024-02-24T20:08:04Z","name":"Ravi Theja","slug":{"_type":"slug","current":"ravi-theja"}}],"featured":false,"htmlContent":"\u003ch1\u003eIntroduction:\u003c/h1\u003e\u003cp\u003eAchieving an efficient Retrieval-Augmented-Generation (RAG) pipeline is heavily dependent on robust retrieval performance. As we explored in our previous \u003ca href=\"https://medium.com/llamaindex-blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\" rel=\"noopener\"\u003eblog post\u003c/a\u003e, rerankers have a significant impact on boosting retrieval performance. But what if we could take it a step further? What if our reranker was not just any reranker, but one tuned specifically to our domain or dataset? Could this specialization enhance the retrieval performance even more?\u003c/p\u003e\u003cp\u003eTo answer these questions, we turn to CohereAI’s beta release of fine-tuning reranker(Custom reranker) models. By integrating these with LlamaIndex, we now offer the ability to build your very own Cohere custom reranker using our streamlined process.\u003c/p\u003e\u003cp\u003eIn this blog post, we’ll guide you through the steps to create a Cohere custom reranker with LlamaIndex and evaluate the retrieval performance.\u003c/p\u003e\u003cp\u003eFor a hands-on walkthrough, you can follow the tutorial on \u003ca href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/finetuning/rerankers/cohere_custom_reranker.ipynb\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003eGoogle Colab Notebook\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eLet’s start fine-tuning a Cohere reranker (custom reranker) with LlamaIndex.\u003c/p\u003e\u003cblockquote\u003e\u003cp id=\"44a2\" class=\"oi oj pp ok b ol ph on oo op pi or os ot pj ov ow ox pk oz pa pb pl pd pe pf gm bj\"\u003eN\u003c!-- --\u003eOTE: This is a guide for fine-tuning a Cohere reranker (custom reranker). The results presented at the end of this tutorial are unique to the chosen dataset and parameters. We suggest experimenting with your dataset and various parameters before deciding to incorporate it into your RAG pipeline.\u003c/p\u003e\u003c/blockquote\u003e\u003ch1\u003eSetting Up the Environment\u003c/h1\u003e\u003cpre\u003e\u003cspan id=\"5f24\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e!pip install llama-index cohere pypdf\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eSetting Up the Keys\u003c/h1\u003e\u003cpre\u003e\u003cspan id=\"9827\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003eopenai_api_key = 'YOUR OPENAI API KEY'\ncohere_api_key = 'YOUR COHEREAI API KEY'\n\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = openai_api_key\nos.environ[\"COHERE_API_KEY\"] = cohere_api_key\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eDownload the Data\u003c/h1\u003e\u003cp\u003eWe will use Lyft 2021 10K SEC Filings for training and Uber 2021 10K SEC Filings for evaluation.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"726f\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e!mkdir -p 'data/10k/'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eLoad the Data\u003c/h1\u003e\u003cpre\u003e\u003cspan id=\"bb7a\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003elyft_docs = SimpleDirectoryReader(input_files=['./data/10k/lyft_2021.pdf']).load_data()\nuber_docs = SimpleDirectoryReader(input_files=['./data/10k/uber_2021.pdf']).load_data()\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eData Curation\u003c/h1\u003e\u003cp\u003e\u003cstrong\u003eCreate Nodes.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://docs.cohere.com/docs/rerank-models\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edocumentation\u003c/a\u003e mentions that Query + Relevant Passage/ Query + Hard Negatives should be less than 510 tokens. To accommodate that we limit \u003ccode class=\"cw qe qf qg pw b\"\u003echunk_size\u003c/code\u003eto 400 tokens. (Each chunk will eventually be treated as a Relevant Passage/ Hard Negative)\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"e284\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e# Limit chunk size to 400\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=400)\n\n# Create nodes\nlyft_nodes = node_parser.get_nodes_from_documents(lyft_docs)\nuber_nodes = node_parser.get_nodes_from_documents(uber_docs)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eWe will use gpt-4 to create questions from chunks.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"47d3\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003ellm = OpenAI(api_key=openai_api_key, temperature=0, model='gpt-4')\u003c/span\u003e\u003c/pre\u003e\u003cp\u003ePrompt to generate questions from each Node/ chunk.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"7066\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e# Prompt to generate questions\nqa_generate_prompt_tmpl = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge.\ngenerate only questions based on the below query.\n\nYou are a Professor. Your task is to setup \\\n{num_questions_per_chunk} questions for an upcoming \\\nquiz/examination. The questions should be diverse in nature \\\nacross the document. The questions should not contain options, not start with Q1/ Q2. \\\nRestrict the questions to the context information provided.\\\n\"\"\"\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eIt expects a minimum of 256 (Query + Relevant passage) pairs with or without hard negatives for training and 64 pairs for validation. Please note that the validation is optional.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTraining:\u003c/strong\u003e We use the first 256 nodes from Lyft for creating training pairs.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eValidation:\u003c/strong\u003e We will use the next 64 nodes from Lyft for validation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eTesting: \u003c/strong\u003eWe will use the first 150 nodes from Uber.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"7d01\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e\u003cspan class=\"hljs-comment\"\u003e# Training dataset\u003c/span\u003e\nqa_dataset_lyft_train = generate_question_context_pairs(\n    lyft_nodes[:\u003cspan class=\"hljs-number\"\u003e256\u003c/span\u003e], llm=llm, num_questions_per_chunk=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# Save [Optional]\u003c/span\u003e\nqa_dataset_lyft_train.save_json(\u003cspan class=\"hljs-string\"\u003e\"lyft_train_dataset.json\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Validation dataset\u003c/span\u003e\nqa_dataset_lyft_val = generate_question_context_pairs(\n    lyft_nodes[\u003cspan class=\"hljs-number\"\u003e257\u003c/span\u003e:\u003cspan class=\"hljs-number\"\u003e321\u003c/span\u003e], llm=llm, num_questions_per_chunk=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# Save [Optional]\u003c/span\u003e\nqa_dataset_lyft_val.save_json(\u003cspan class=\"hljs-string\"\u003e\"lyft_val_dataset.json\"\u003c/span\u003e)\n\n\u003cspan class=\"hljs-comment\"\u003e# Testing dataset\u003c/span\u003e\nqa_dataset_uber_val = generate_question_context_pairs(\n    uber_nodes[:\u003cspan class=\"hljs-number\"\u003e150\u003c/span\u003e], llm=llm, num_questions_per_chunk=\u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e, qa_generate_prompt_tmpl=qa_generate_prompt_tmpl\n)\n\n\u003cspan class=\"hljs-comment\"\u003e# Save [Optional]\u003c/span\u003e\nqa_dataset_uber_val.save_json(\u003cspan class=\"hljs-string\"\u003e\"uber_val_dataset.json\"\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eNow that we have compiled questions from each chunk, we will format the data according to the specifications required for training and validation.\u003c/p\u003e\u003ch1\u003eData Format and Requirements\u003c/h1\u003e\u003cp\u003eFor both training and validation, it currently accepts data in the format of triplets, every row should have the following\u003c/p\u003e\u003cp\u003e\u003cstrong\u003equery: \u003c/strong\u003eThis represents the question or target.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003erelevant_passages:\u003c/strong\u003e This represents a list of documents or passages that contain information that answers the query. For every query, there must be at least one relevant_passage\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ehard_negatives:\u003c/strong\u003e This represents chunks or passages that don’t contain answers for the query. It should be noted that Hard negatives are optional but providing at least ~5 hard negatives will lead to meaningful improvement.\u003c/p\u003e\u003cp\u003eYou can check the \u003ca href=\"https://docs.cohere.com/docs/rerank-models\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edocumentation\u003c/a\u003e for more details.\u003c/p\u003e\u003cp\u003eWe need to have an embedding model for creating hard negatives with a cosine similarity approach.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"05a9\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e# Initialize the Cohere embedding model which we use it for creating Hard Negatives.\nembed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eLet’s create 3 datasets.\u003c/p\u003e\u003col\u003e\u003cli\u003eDataset without hard negatives.\u003c/li\u003e\u003cli\u003eDataset with hard negatives selected at random.\u003c/li\u003e\u003cli\u003eDataset with hard negatives selected based on cosine similarity.\u003c/li\u003e\u003c/ol\u003e\u003cpre\u003e\u003cspan id=\"b74a\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e# Train and val datasets without hard negatives.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    finetune_dataset_file_name = \"train.jsonl\"\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    finetune_dataset_file_name = \"val.jsonl\"\n)\n\n# Train and val datasets with hard negatives selected at random.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"train_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"random\",\n    finetune_dataset_file_name = \"val_5_random.jsonl\",\n    embed_model = embed_model,\n)\n\n# Train and val datasets with hard negatives selected based on cosine similarity.\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_train,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"train_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)\n\ngenerate_cohere_reranker_finetuning_dataset(\n    qa_dataset_lyft_val,\n    num_negatives = 5,\n    hard_negatives_gen_method = \"cosine_similarity\",\n    finetune_dataset_file_name = \"val_5_cosine_similarity.jsonl\",\n    embed_model = embed_model,\n)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eFine-tuning Reranker (Custom Reranker)\u003c/h1\u003e\u003cp\u003eWith our training and validation datasets ready, we’re set to proceed with the training process. Be aware that this training is expected to take approximately 25 to 45 minutes.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"0413\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e# Reranker model with 0 hard negatives.\nfinetune_model_no_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train.jsonl\",\n    val_file_name=\"val.jsonl\",\n    model_name=\"lyft_reranker_0_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n    api_key = cohere_api_key\n)\nfinetune_model_no_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected at random\nfinetune_model_random_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_random.jsonl\",\n    val_file_name=\"val_5_random.jsonl\",\n    model_name=\"lyft_reranker_5_random_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_random_hard_negatives.finetune()\n\n# Reranker model with 5 hard negatives selected based on cosine similarity\nfinetune_model_cosine_hard_negatives = CohereRerankerFinetuneEngine(\n    train_file_name=\"train_5_cosine_similarity.jsonl\",\n    val_file_name=\"val_5_cosine_similarity.jsonl\",\n    model_name=\"lyft_reranker_5_cosine_hard_negatives1\",\n    model_type=\"RERANK\",\n    base_model=\"english\",\n)\nfinetune_model_cosine_hard_negatives.finetune()\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eOnce the jobs are submitted, you can check the training status in the \u003ccode class=\"cw qe qf qg pw b\"\u003emodels\u003c/code\u003e section of the \u003ca href=\"https://dashboard.cohere.com/models\" rel=\"noopener ugc nofollow\" target=\"_blank\"\u003edashboard\u003c/a\u003e. You can check the status of the job in the dashboard and you should see an image something similar to the following one.\u003c/p\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*CFAtQH8dwzjnUrzVktqfbA.png\" alt=\"\" width=\"700\" height=\"552\"\u003e\u003c/figure\u003e\u003cp\u003eYou then need to get the Cohere Reranker model for testing.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"ee22\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003ereranker_base = CohereRerank(top_n=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\nreranker_model_0 = finetune_model_no_hard_negatives.get_finetuned_model(\n    top_n=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e\n)\nreranker_model_5_random = (\n    finetune_model_random_hard_negatives.get_finetuned_model(top_n=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n)\nreranker_model_5_cosine = (\n    finetune_model_cosine_hard_negatives.get_finetuned_model(top_n=\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e)\n)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eTesting\u003c/h1\u003e\u003cp\u003eWe will conduct tests on the first 150 nodes from Uber using the following different rerankers.\u003c/p\u003e\u003col\u003e\u003cli\u003eWithout Reranker.\u003c/li\u003e\u003cli\u003eCohere Reranker.\u003c/li\u003e\u003cli\u003eFine-tuned reranker (Custom reranker) without hard negatives.\u003c/li\u003e\u003cli\u003eFine-tuned reranker (Custom reranker) with hard negatives selected at random.\u003c/li\u003e\u003cli\u003eFine-tuned reranker (Custom reranker) with hard negatives selected based on cosine similarity.\u003c/li\u003e\u003c/ol\u003e\u003cp\u003eLet’s define the rerankers.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"dade\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003eRERANKERS = {\n    \"WithoutReranker\": \"None\",\n    \"CohereRerank\": reranker_base,\n    \"CohereRerank_0\": reranker_model_0,\n    \"CohereRerank_5_random\": reranker_model_5_random,\n    \"CohereRerank_5_cosine\": reranker_model_5_cosine,\n}\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eCreate an Index and Retriever for evaluation purposes.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"4724\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e# Initialize the Cohere embedding model, `input_type` is different for indexing and retrieval.\nindex_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n)\n\nquery_embed_model = CohereEmbedding(\n    cohere_api_key=cohere_api_key,\n    model_name=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n)\n\nservice_context_index = ServiceContext.from_defaults(llm=None, embed_model=index_embed_model)\nservice_context_query = ServiceContext.from_defaults(llm=None, embed_model=query_embed_model)\n\nvector_index = VectorStoreIndex(uber_nodes[:150], service_context=service_context_index)\nvector_retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=10, service_context=service_context_query)\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eDefine a function to display the results\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"c645\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003e\u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003edisplay_results\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eembedding_name, reranker_name, eval_results\u003c/span\u003e):\n    \u003cspan class=\"hljs-string\"\u003e\"\"\"Display results from evaluate.\"\"\"\u003c/span\u003e\n\n    metric_dicts = []\n    \u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e eval_result \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e eval_results:\n        metric_dict = eval_result.metric_vals_dict\n        metric_dicts.append(metric_dict)\n\n    full_df = pd.DataFrame(metric_dicts)\n\n    hit_rate = full_df[\u003cspan class=\"hljs-string\"\u003e\"hit_rate\"\u003c/span\u003e].mean()\n    mrr = full_df[\u003cspan class=\"hljs-string\"\u003e\"mrr\"\u003c/span\u003e].mean()\n\n    metric_df = pd.DataFrame(\n        {\u003cspan class=\"hljs-string\"\u003e\"Embedding\"\u003c/span\u003e: [embedding_name], \u003cspan class=\"hljs-string\"\u003e\"Reranker\"\u003c/span\u003e: [reranker_name], \u003cspan class=\"hljs-string\"\u003e\"hit_rate\"\u003c/span\u003e: [hit_rate], \u003cspan class=\"hljs-string\"\u003e\"mrr\"\u003c/span\u003e: [mrr]}\n    )\n\n    \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e metric_df\u003c/span\u003e\u003c/pre\u003e\u003cp\u003eLoop over different rerankers and evaluate retrieval performance using Custom Retriever.\u003c/p\u003e\u003cpre\u003e\u003cspan id=\"d3cd\" class=\"pz nl gt pw b bf qa qb l qc qd\"\u003eresults_df = pd.DataFrame()\n\nembed_name = \u003cspan class=\"hljs-string\"\u003e'CohereEmbedding'\u003c/span\u003e\n\n\u003cspan class=\"hljs-comment\"\u003e# Loop over rerankers\u003c/span\u003e\n\u003cspan class=\"hljs-keyword\"\u003efor\u003c/span\u003e rerank_name, reranker \u003cspan class=\"hljs-keyword\"\u003ein\u003c/span\u003e RERANKERS.items():\n\n    \u003cspan class=\"hljs-built_in\"\u003eprint\u003c/span\u003e(\u003cspan class=\"hljs-string\"\u003ef\"Running Evaluation for Reranker: \u003cspan class=\"hljs-subst\"\u003e{rerank_name}\u003c/span\u003e\"\u003c/span\u003e)\n\n    \u003cspan class=\"hljs-comment\"\u003e# Define Retriever\u003c/span\u003e\n    \u003cspan class=\"hljs-keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"hljs-title class_\"\u003eCustomRetriever\u003c/span\u003e(\u003cspan class=\"hljs-title class_ inherited__\"\u003eBaseRetriever\u003c/span\u003e):\n        \u003cspan class=\"hljs-string\"\u003e\"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\u003c/span\u003e\n\n        \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e__init__\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003e\n            self,\n            vector_retriever: VectorIndexRetriever,\n        \u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-literal\"\u003eNone\u003c/span\u003e:\n            \u003cspan class=\"hljs-string\"\u003e\"\"\"Init params.\"\"\"\u003c/span\u003e\n\n            self._vector_retriever = vector_retriever\n\n        \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e_retrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, query_bundle: QueryBundle\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[NodeWithScore]:\n            \u003cspan class=\"hljs-string\"\u003e\"\"\"Retrieve nodes given query.\"\"\"\u003c/span\u003e\n\n            retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e reranker != \u003cspan class=\"hljs-string\"\u003e'None'\u003c/span\u003e:\n                retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n            \u003cspan class=\"hljs-keyword\"\u003eelse\u003c/span\u003e:\n                retrieved_nodes = retrieved_nodes[:\u003cspan class=\"hljs-number\"\u003e5\u003c/span\u003e]\n\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e retrieved_nodes\n\n        \u003cspan class=\"hljs-keyword\"\u003easync\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003e_aretrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, query_bundle: QueryBundle\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[NodeWithScore]:\n            \u003cspan class=\"hljs-string\"\u003e\"\"\"Asynchronously retrieve nodes given query.\n            \"\"\"\u003c/span\u003e\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e self._retrieve(query_bundle)\n\n        \u003cspan class=\"hljs-keyword\"\u003easync\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"hljs-title function_\"\u003earetrieve\u003c/span\u003e(\u003cspan class=\"hljs-params\"\u003eself, str_or_query_bundle: QueryType\u003c/span\u003e) -\u0026amp;gt; \u003cspan class=\"hljs-type\"\u003eList\u003c/span\u003e[NodeWithScore]:\n            \u003cspan class=\"hljs-keyword\"\u003eif\u003c/span\u003e \u003cspan class=\"hljs-built_in\"\u003eisinstance\u003c/span\u003e(str_or_query_bundle, \u003cspan class=\"hljs-built_in\"\u003estr\u003c/span\u003e):\n                str_or_query_bundle = QueryBundle(str_or_query_bundle)\n            \u003cspan class=\"hljs-keyword\"\u003ereturn\u003c/span\u003e \u003cspan class=\"hljs-keyword\"\u003eawait\u003c/span\u003e self._aretrieve(str_or_query_bundle)\n\n    custom_retriever = CustomRetriever(vector_retriever)\n\n    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n        [\u003cspan class=\"hljs-string\"\u003e\"mrr\"\u003c/span\u003e, \u003cspan class=\"hljs-string\"\u003e\"hit_rate\"\u003c/span\u003e], retriever=custom_retriever\n    )\n    eval_results = \u003cspan class=\"hljs-keyword\"\u003eawait\u003c/span\u003e retriever_evaluator.aevaluate_dataset(qa_dataset_uber_val)\n\n    current_df = display_results(embed_name, rerank_name, eval_results)\n    results_df = pd.concat([results_df, current_df], ignore_index=\u003cspan class=\"hljs-literal\"\u003eTrue\u003c/span\u003e)\u003c/span\u003e\u003c/pre\u003e\u003ch1\u003eResults:\u003c/h1\u003e\u003cfigure\u003e\u003cimg src=\"/blog/images/1*Jv2U5pxMZWZrXjhfuXAqxg.png\" alt=\"\" width=\"700\" height=\"142\"\u003e\u003c/figure\u003e\u003cp\u003eFrom the above table (1- without reranker, 2 — with base cohere reranker, 3–5: Fine-tuned rerankers (Custom rerankers)), we can see that the Fine-tuned rerankers (custom rerankers) have resulted in performance improvements. It’s crucial to note that the choice of the optimal number of hard negatives, as well as the decision between random or cosine sampling, should be grounded in empirical evidence. This guide offers a structured approach for improving retrieval systems through the fine-tuning of the Cohere re-ranker.\u003c/p\u003e\u003ch1\u003eSummary:\u003c/h1\u003e\u003cp\u003eIn this blog post, we’ve demonstrated fine-tuning a Cohere reranker (custom reranker) using LlamaIndex, which has improved retrieval performance metrics. We eagerly anticipate the community’s use of these abilities to boost their retrieval efficiency within RAG pipelines. Additionally, there is room for advancement in selecting hard negatives, and we invite the community to contribute.\u003c/p\u003e","image":{"_type":"image","asset":{"_ref":"image-08273b147aada29f472350a94f12affaa128f757-342x208-png","_type":"reference"}},"mainImage":"https://cdn.sanity.io/images/7m9jw85w/production/08273b147aada29f472350a94f12affaa128f757-342x208.png","publishedDate":"2023-11-16","relatedPosts":[{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-aa21c9d353919277d4fce16f174e54280bda8660-1920x832-png","_type":"reference"}},"publishedDate":"2024-07-31","slug":"jamba-instruct-s-256k-context-window-on-llamaindex","title":"Jamba-Instruct's 256k context window on LlamaIndex"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-31290fcec6832b337689a39c17adf5d995ff46b6-1024x1024-webp","_type":"reference"}},"publishedDate":"2024-04-02","slug":"llamaindex-newsletter-2024-04-02","title":"LlamaIndex Newsletter 2024-04-02"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-67e9da6888edfa6119225413068198422f1eaf77-1024x1024-png","_type":"reference"}},"publishedDate":"2024-03-26","slug":"llamaindex-newsletter-2024-03-26","title":"LlamaIndex Newsletter 2024-03-26"},{"featured":false,"image":{"_type":"image","asset":{"_ref":"image-23819f5bd086643f28ca7d2746a9e400f28cdbee-1023x561-png","_type":"reference"}},"publishedDate":"2024-03-19","slug":"supercharge-your-llamaindex-rag-pipeline-with-uptrain-evaluations","title":"Supercharge your LlamaIndex RAG Pipeline with UpTrain Evaluations"}],"slug":{"_type":"slug","current":"improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"},"tags":[{"_createdAt":"2024-02-22T20:19:11Z","_id":"17d4fc95-517c-4f4a-95ce-bf753e802ac4","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"llamaindex"},"title":"Llamaindex"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"d0a79109-34ab-41fa-a8f4-0b3522970c7d","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:11Z","slug":{"_type":"slug","current":"ai"},"title":"AI"},{"_createdAt":"2024-02-22T20:19:11Z","_id":"aa7d304e-787e-4a6c-80cb-8911afd4c788","_rev":"jbUo4a8sS9GhVRG46mMVHT","_type":"blogTag","_updatedAt":"2024-03-13T16:00:26Z","slug":{"_type":"slug","current":"llm"},"title":"LLM"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"51f8e396-1f60-416c-9f2c-e1fb0c019012","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"cohere"},"title":"Cohere"},{"_createdAt":"2024-02-22T20:19:13Z","_id":"397e326e-8d93-43fd-b559-cf89ee05f65c","_rev":"9mLWnJtMvL1OgKPLcXbz07","_type":"blogTag","_updatedAt":"2024-02-22T20:19:13Z","slug":{"_type":"slug","current":"fine-tuning"},"title":"Fine Tuning"}],"title":"Improving Retrieval Performance by Fine-tuning Cohere Reranker with LlamaIndex"},"publishedDate":"Invalid Date"},"params":{"slug":"improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"},"draftMode":false,"token":""},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"improving-retrieval-performance-by-fine-tuning-cohere-reranker-with-llamaindex-16c0c1f9b33b"},"buildId":"C8J-EMc_4OCN1ch65l4fl","isFallback":false,"isExperimentalCompile":false,"gsp":true,"scriptLoader":[]}</script></body></html>