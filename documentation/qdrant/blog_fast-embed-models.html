<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Nirant Kasliwal discusses the efficiency and optimization techniques of FastEmbed, a Python library designed for speedy, lightweight embedding generation in machine learning applications."><meta name=keywords content="vector search engine,neural network,matching,SaaS,approximate nearest neighbor search,image search,recommender system,vectors,knn algorithm,hnsw,vector search,embeddings,similarity,simaes networks,BERT,transformer,word2vec,fasttext,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/blog/fast-embed-models/#article","@type":"Article","abstract":"Nirant Kasliwal discusses the efficiency and optimization techniques of FastEmbed a Python library designed for speedy lightweight embedding generation in machine learning applications","author":{"@type":"Person","name":"Demetrios Brinkmann"},"dateModified":"2024-01-09 11:38:59.693 +0000 UTC","datePublished":"2024-01-09 11:38:59.693 +0000 UTC","description":"Nirant Kasliwal discusses the efficiency and optimization techniques of FastEmbed a Python library designed for speedy lightweight embedding generation in machine learning applications","headline":"FastEmbed: Fast \u0026amp; Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks","image":[""],"name":"FastEmbed: Fast \u0026amp; Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks","url":"https://qdrant.tech/blog/fast-embed-models/","wordCount":"5789"},{"@id":"https://qdrant.tech/blog/fast-embed-models/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestraße 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["vector search engine","neural network","matching","SaaS","approximate nearest neighbor search","image search","recommender system","vectors","knn algorithm","hnsw","vector search","embeddings","similarity","simaes networks","BERT","transformer","word2vec","fasttext","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/blog/fast-embed-models/"><meta property="og:type" content="website"><meta property="og:title" content="FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/blog/fast-embed-models/"><meta name=twitter:title content="FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks - Qdrant"><meta property="og:description" content="Nirant Kasliwal discusses the efficiency and optimization techniques of FastEmbed, a Python library designed for speedy, lightweight embedding generation in machine learning applications."><meta name=twitter:description content="Nirant Kasliwal discusses the efficiency and optimization techniques of FastEmbed, a Python library designed for speedy, lightweight embedding generation in machine learning applications."><meta name=image property="og:image" content="https://qdrant.tech/blog/fast-embed-models/preview/social_preview.jpg"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/blog/fast-embed-models/preview/social_preview.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/blog/fast-embed-models/preview/social_preview.jpg"><meta name=author content="Demetrios Brinkmann"><link rel=canonical href=https://qdrant.tech/blog/fast-embed-models/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script></head><body><main><header class=site-header><section class=top-banner data-start=1749013200 data-end=1750428000 style=display:none><div><span class=top-banner__icon><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><g clip-path="url(#clip0_770_2716)"><path d="M14.598 6.37199C14.486 6.14399 14.254 5.99999 14 5.99999H8.7447L9.3287.739993C9.36204.44266 9.1927.159993 8.91537.0479934 8.63737-.0653399 8.31937.0226601 8.13737.259327L1.4707 8.92599C1.31604 9.12733 1.2887 9.39933 1.40137 9.62733c.11267.22866.34467.37266.59867.37266H7.25537L6.67137 15.26c-.0333299999999994.2973.136.58.41333.692C7.16537 15.9847 7.25004 16 7.33337 16 7.53604 16 7.73337 15.9073 7.86204 15.74L14.5287 7.07333C14.6834 6.87199 14.71 6.59999 14.598 6.37199z" fill="#8547ff"/></g><defs><clipPath id="clip0_770_2716"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg> </span><span class=top-banner__text>Learn how TripAdvisor Drives 2-3x More Revenue with Qdrant-Powered AI at Enterprise Scale </span><a data-metric-loc=banner data-metric-label="Learn how TripAdvisor Drives 2-3x More Revenue with Qdrant-Powered AI at Enterprise Scale Read now" class="link link_light link_sm" href=https://qdrant.tech/blog/case-study-tripadvisor/>Read now</a></div></section><div class="main-menu z-2"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><span>Products</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/qdrant-vector-database/><img src=https://qdrant.tech/img/menu/qdrant-vector-database.svg draggable=false>
<span>Qdrant Vector Database</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/cloud/><img src=https://qdrant.tech/img/menu/qdrant-cloud.svg draggable=false>
<span>Qdrant Cloud</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/hybrid-cloud/><img src=https://qdrant.tech/img/menu/hybrid-cloud.svg draggable=false>
<span>Qdrant Hybrid Cloud</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/enterprise-solutions/><img src=https://qdrant.tech/img/menu/qdrant-enterprise-solutions.svg draggable=false>
<span>Qdrant Enterprise Solutions</span></a></li></ul></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/use-cases/>Use Cases</a><ul class=main-menu__submenu><li class=main-menu__section-link><a class="link link_neutral link_sm" href=https://qdrant.tech/use-cases/>Use Cases</a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/rag/><img src=https://qdrant.tech/img/menu/rag.svg draggable=false>
<span>RAG</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/recommendations/><img src=https://qdrant.tech/img/menu/recommendation-systems.svg draggable=false>
<span>Recommendation Systems</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/advanced-search/><img src=https://qdrant.tech/img/menu/advanced-search.svg draggable=false>
<span>Advanced Search</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/data-analysis-anomaly-detection/><img src=https://qdrant.tech/img/menu/data-analysis-anomaly-detection.svg draggable=false>
<span>Data Analysis & Anomaly Detection</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/ai-agents/><img src=https://qdrant.tech/img/menu/ai-agents.svg draggable=false>
<span>AI Agents</span></a></li></ul></li><li class=main-menu__item><span>Developers</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/documentation/><img src=https://qdrant.tech/img/menu/documentation.svg draggable=false>
<span>Documentation</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/community/><img src=https://qdrant.tech/img/menu/community.svg draggable=false>
<span>Community</span></a></li><li class=main-menu__submenu-item><a href=https://github.com/qdrant/qdrant target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/github.svg draggable=false>
<span>GitHub</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.to/roadmap target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/roadmap.svg draggable=false>
<span>Roadmap</span></a></li><li class=main-menu__submenu-item><a href=https://github.com/qdrant/qdrant/releases target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/changelog.svg draggable=false>
<span>Change Log</span></a></li></ul></li><li class=main-menu__item><span>Resources</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/benchmarks/><img src=https://qdrant.tech/img/menu/benchmarks.svg draggable=false>
<span>Benchmarks</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/blog/><img src=https://qdrant.tech/img/menu/blog.svg draggable=false>
<span>Blog</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/articles/><img src=https://qdrant.tech/img/menu/articles.svg draggable=false>
<span>Articles</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/demo/><img src=https://qdrant.tech/img/menu/demos.svg draggable=false>
<span>Demos</span></a></li><li class=main-menu__submenu-item><a href=https://try.qdrant.tech/events target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/partners.svg draggable=false>
<span>Events</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/qdrant-for-startups/><img src=https://qdrant.tech/img/menu/qdrant-for-startups.svg draggable=false>
<span>Startup Program</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/security/bug-bounty-program/><img src=https://qdrant.tech/img/menu/bug-bounty-program.svg draggable=false>
<span>Bug Bounty Program</span></a></li></ul></li><li class=main-menu__item><span>Company</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/about-us/><img src=https://qdrant.tech/img/menu/about-us.svg draggable=false>
<span>About us</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/customers/><img src=https://qdrant.tech/img/menu/customers.svg draggable=false>
<span>Customers</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/partners/><img src=https://qdrant.tech/img/menu/partners.svg draggable=false>
<span>Partners</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.join.com/ target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/careers.svg draggable=false>
<span>Careers</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/contact-us/><img src=https://qdrant.tech/img/menu/contact-us.svg draggable=false>
<span>Contact us</span></a></li></ul></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/pricing/>Pricing</a></li></ul><div class=main-menu__buttons><a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3">Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm">Get Started</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M19.0713 4.92871 4.92915 19.0708" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M19.0713 19.0708 4.9292 4.92871" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content>Products
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/qdrant-vector-database/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-vector-database.svg)></span>Qdrant Vector Database</li></a><a href=https://qdrant.tech/cloud/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-cloud.svg)></span>Qdrant Cloud</li></a><a href=https://qdrant.tech/hybrid-cloud/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/hybrid-cloud.svg)></span>Qdrant Hybrid Cloud</li></a><a href=https://qdrant.tech/enterprise-solutions/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-enterprise-solutions.svg)></span>Qdrant Enterprise Solutions</li></a></ul></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content>Use Cases
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><li class=menu-mobile__section-link><a class="link link_neutral link_sm" href=https://qdrant.tech/use-cases/>Use Cases</a></li><a href=https://qdrant.tech/rag/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/rag.svg)></span>RAG</li></a><a href=https://qdrant.tech/recommendations/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/recommendation-systems.svg)></span>Recommendation Systems</li></a><a href=https://qdrant.tech/advanced-search/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/advanced-search.svg)></span>Advanced Search</li></a><a href=https://qdrant.tech/data-analysis-anomaly-detection/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/data-analysis-anomaly-detection.svg)></span>Data Analysis & Anomaly Detection</li></a><a href=https://qdrant.tech/ai-agents/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/ai-agents.svg)></span>AI Agents</li></a></ul></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content>Developers
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/documentation/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/documentation.svg)></span>Documentation</li></a><a href=https://qdrant.tech/community/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/community.svg)></span>Community</li></a><a href=https://github.com/qdrant/qdrant><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/github.svg)></span>GitHub</li></a><a href=https://qdrant.to/roadmap><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/roadmap.svg)></span>Roadmap</li></a><a href=https://github.com/qdrant/qdrant/releases><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/changelog.svg)></span>Change Log</li></a></ul></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content>Resources
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/benchmarks/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/benchmarks.svg)></span>Benchmarks</li></a><a href=https://qdrant.tech/blog/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/blog.svg)></span>Blog</li></a><a href=https://qdrant.tech/articles/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/articles.svg)></span>Articles</li></a><a href=https://qdrant.tech/demo/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/demos.svg)></span>Demos</li></a><a href=https://try.qdrant.tech/events><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/partners.svg)></span>Events</li></a><a href=https://qdrant.tech/qdrant-for-startups/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-for-startups.svg)></span>Startup Program</li></a><a href=https://qdrant.tech/security/bug-bounty-program/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/bug-bounty-program.svg)></span>Bug Bounty Program</li></a></ul></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content>Company
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/about-us/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/about-us.svg)></span>About us</li></a><a href=https://qdrant.tech/customers/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/customers.svg)></span>Customers</li></a><a href=https://qdrant.tech/partners/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/partners.svg)></span>Partners</li></a><a href=https://qdrant.join.com/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/careers.svg)></span>Careers</li></a><a href=https://qdrant.tech/contact-us/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/contact-us.svg)></span>Contact us</li></a></ul></li><li class=menu-mobile__item data-path=menu-5><div class=menu-mobile__item-content><a href=https://qdrant.tech/pricing/>Pricing</a></div></li></ul><div class=menu-mobile__controls><a data-metric-loc=mobile_nav href=https://cloud.qdrant.io/login class="button button_outlined button_lg menu-mobile__login">Log in</a>
<a data-metric-loc=mobile_nav href=https://cloud.qdrant.io/signup class="button button_contained button_lg">Get Started</a></div></div></header><progress id=progress class=progress-bar value=0 max=100>0</progress><section class="qdrant-post
qdrant-blog-post"><article id=article class=container><div class=qdrant-post__header><h1 class=qdrant-post__title>FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks</h1><div class=qdrant-post__about><p>Demetrios Brinkmann</p><span>&#183;</span><p>January 09, 2024</p></div><picture class=qdrant-post__preview><img src=https://qdrant.tech/blog/fast-embed-models/preview/title.jpg alt="FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks" loading=lazy></picture></div><div class="row qdrant-post__breadcrumbs"><div class=col-12><ul class=breadcrumbs><li class=breadcrumbs__crumb><a href=https://qdrant.tech/>Home</a></li><li class=breadcrumbs__crumb>/</li><li class=breadcrumbs__crumb><a href=https://qdrant.tech/blog/>Blog</a></li><li class=breadcrumbs__crumb>/</li><li class=breadcrumbs__crumb>FastEmbed: Fast & Lightweight Embedding Generation - Nirant Kasliwal | Vector Space Talks</li></ul></div></div><div class=qdrant-post__body><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><ul><li><a href=#top-takeaways><strong>Top Takeaways:</strong></a></li><li><a href=#show-notes>Show Notes:</a></li><li><a href=#more-quotes-from-nirant>More Quotes from Nirant:</a></li><li><a href=#transcript>Transcript:</a></li></ul></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fqdrant.tech%2Fblog%2Ffast-embed-models%2F&amp;text=FastEmbed:%20Fast%20&%20Lightweight%20Embedding%20Generation%20-%20Nirant%20Kasliwal%20%7c%20Vector%20Space%20Talks" target=_blank rel="noopener noreferrer" title=x><svg width="33" height="33" viewBox="0 0 33 33" fill="none"><path d="M14.959 20.7369l-8.581 9.798H1.625l11.114-12.696 2.22 2.898z" fill="#161e33"/><path d="M17.5508 11.5229l7.857-8.98799h4.75L19.7508 14.4369l-2.2-2.914z" fill="#161e33"/><path d="M31.9877 30.5349h-9.559L1.01172 2.53491H10.8127L31.9877 30.5349zm-8.248-2.843h2.632L9.38272 5.22891h-2.824L23.7397 27.6919z" fill="#161e33"/></svg>
Share on X</a></li><li class=table-of-contents__link><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fqdrant.tech%2Fblog%2Ffast-embed-models%2F" target=_blank rel="noopener noreferrer" title=LinkedIn><svg width="32" height="33" viewBox="0 0 32 33" fill="none"><g clip-path="url(#clip0_1811_16094)"><path d="M30.6667.4375H1.33333C.533333.4375.0.970833.0 1.77083V31.1042C0 31.9042.533333 32.4375 1.33333 32.4375H30.6667C31.4667 32.4375 32 31.9042 32 31.1042V1.77083C32 .970833 31.4667.4375 30.6667.4375zM9.46667 27.7708H4.8V12.4375H9.6V27.7708H9.46667zm-2.4-17.4666c-1.46667.0-2.8-1.20003-2.8-2.80003.0-1.46667 1.2-2.8 2.8-2.8 1.46666.0 2.8 1.2 2.8 2.8s-1.2 2.80003-2.8 2.80003zM27.3333 27.7708h-4.8V20.3042c0-1.7334.0-4-2.4-4-2.5333.0-2.8 1.8666-2.8 3.8666v7.6h-4.8V12.4375h4.5334v2.1333C17.7333 13.3708 19.2 12.1708 21.6 12.1708c4.8.0 5.7333 3.2 5.7333 7.3334v8.2666z" fill="#161e33"/></g><defs><clipPath id="clip0_1811_16094"><rect width="32" height="32" fill="#fff" transform="translate(0 0.4375)"/></clipPath></defs></svg>
Share on LinkedIn</a></li></ul></div><div class=qdrant-post__content><blockquote><p><em>&ldquo;When things are actually similar or how we define similarity. They are close to each other and if they are not, they&rsquo;re far from each other. This is what a model or embedding model tries to do.”</em><br>&ndash; Nirant Kasliwal</p></blockquote><p>Heard about FastEmbed? It&rsquo;s a game-changer. Nirant shares tricks on how to improve your embedding models. You might want to give it a shot!</p><p>Nirant Kasliwal, the creator and maintainer of FastEmbed, has made notable contributions to the Finetuning Cookbook at OpenAI Cookbook. His contributions extend to the field of Natural Language Processing (NLP), with over 5,000 copies of the NLP book sold.</p><p><em><strong>Listen to the episode on <a href="https://open.spotify.com/episode/4QWCyu28SlURZfS2qCeGKf?si=GDHxoOSQQ_W_UVz4IzzC_A" target=_blank rel="noopener nofollow">Spotify</a>, Apple Podcast, Podcast addicts, Castbox. You can also watch this episode on <a href=https://youtu.be/e67jLAx_F2A target=_blank rel="noopener nofollow">YouTube</a>.</strong></em></p><iframe width=560 height=315 src="https://www.youtube.com/embed/e67jLAx_F2A?si=533LvUwRKIt_qWWu" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe src=https://podcasters.spotify.com/pod/show/qdrant-vector-space-talk/embed/episodes/FastEmbed-Fast--Lightweight-Embedding-Generation---Nirant-Kasliwal--Vector-Space-Talks-004-e2c8s3b/a-aal40k6 height=102px width=400px frameborder=0 scrolling=no></iframe><h2 id=top-takeaways><strong>Top Takeaways:</strong></h2><p>Nirant Kasliwal, AI Engineer at Qdrant joins us on Vector Space Talks to dive into FastEmbed, a lightning-quick method for generating embeddings.</p><p>In this episode, Nirant shares insights, tips, and innovative ways to enhance embedding generation.</p><p>5 Keys to Learning from the Episode:</p><ol><li>Nirant introduces some hacker tricks for improving embedding models - you won&rsquo;t want to miss these!</li><li>Learn how quantized embedding models can enhance CPU performance.</li><li>Get an insight into future plans for GPU-friendly quantized models.</li><li>Understand how to select default models in Qdrant based on MTEB benchmark, and how to calibrate them for domain-specific tasks.</li><li>Find out how Fast Embed, a Python library created by Nirant, can solve common challenges in embedding creation and enhance the speed and efficiency of your workloads.</li></ol><blockquote><p>Fun Fact: The largest header or adapter used in production is only about 400-500 KBs &ndash; proof that bigger doesn&rsquo;t always mean better!</p></blockquote><h2 id=show-notes>Show Notes:</h2><p>00:00 Nirant discusses FastEmbed at Vector Space Talks.<br>05:00 Tokens are expensive and slow in open air.<br>08:40 FastEmbed is fast and lightweight.<br>09:49 Supporting multimodal embedding is our plan.<br>15:21 No findings. Enhancing model downloads and performance.<br>16:59 Embed creation on your own compute, not cloud. Control and simplicity are prioritized.<br>21:06 Qdrant is fast for embedding similarity search.<br>24:07 Engineer&rsquo;s mindset: make informed guesses, set budgets.<br>26:11 Optimize embeddings with questions and linear layers.<br>29:55 Fast, cheap inference using mixed precision embeddings.</p><h2 id=more-quotes-from-nirant>More Quotes from Nirant:</h2><p><em>&ldquo;There is the academic way of looking at and then there is the engineer way of looking at it, and then there is the hacker way of looking at it. And I will give you all these three answers in that order.”</em><br>&ndash; Nirant Kasliwal</p><p><em>&ldquo;The engineer&rsquo;s mindset now tells you that the best way to build something is to make an informed guess about what workload or challenges you&rsquo;re going to foresee. Right. Like a civil engineer builds a bridge around how many cars they expect, they&rsquo;re obviously not going to build a bridge to carry a shipload, for instance, or a plane load, which are very different.”</em><br>&ndash; Nirant Kasliwal</p><p><em>&ldquo;I think the more correct way to look at it is that we use the CPU better.”</em><br>&ndash; Nirant Kasliwal</p><h2 id=transcript>Transcript:</h2><p>Demetrios:
Welcome back, everyone, to another vector space talks. Today we&rsquo;ve got my man Nirant coming to us talking about FastEmbed. For those, if this is your first time at our vector space talks, we like to showcase some of the cool stuff that the community in Qdrant is doing, the Qdrant community is doing. And we also like to show off some of the cool stuff that Qdrant itself is coming out with. And this is one of those times that we are showing off what Qdrant itself came out with with FastEmbed. And we&rsquo;ve got my man Nirant around here somewhere. I am going to bring him on stage and I will welcome him by saying Nirant a little bit about his bio, we could say. So, Naran, what&rsquo;s going on, dude? Let me introduce you real fast before we get cracking.</p><p>Demetrios:
And you are a man that wears many hats. You&rsquo;re currently working on the Devrel team at Qdrant, right? I like that shirt that you got there. And you have worked with ML models and embeddings since 2017. That is wild. You are also the creator and maintainer of fast embed. So you&rsquo;re the perfect guy to talk to about this very topic that we are doing today. Now, if anyone has questions, feel free to throw them into the chat and I will ask Nirant as he&rsquo;s going through it. I will also take this moment to encourage anyone who is watching to come and join us in discord, if you are not already there for the Qdrant discord.</p><p>Demetrios:
And secondly, I will encourage you if you have something that you&rsquo;ve been doing with Qdrant or in the vector database space, or in the AI application space and you want to show it off, we would love to have you talk at the vector space talks. So without further ado, Nirant, my man, I&rsquo;m going to kick it over to you and I am going to start it off with what are the challenges with embedding creation today?</p><p>Nirant Kasliwal:
I think embedding creation has it&rsquo;s not a standalone problem, as you might first think like that&rsquo;s a first thought that it&rsquo;s a standalone problem. It&rsquo;s actually two problems. One is a classic compute that how do you take any media? So you can make embeddings from practically any form of media, text, images, video. In theory, you could make it from bunch of things. So I recently saw somebody use soup as a metaphor. So you can make soup from almost anything. So you can make embeddings from almost anything. Now, what do we want to do though? Embedding are ultimately a form of compression.</p><p>Nirant Kasliwal:
So now we want to make sure that the compression captures something of interest to us. In this case, we want to make sure that embeddings capture some form of meaning of, let&rsquo;s say, text or images. And when we do that, what does that capture mean? We want that when things are actually similar or whatever is our definition of similarity. They are close to each other and if they are not, they&rsquo;re far from each other. This is what a model or embedding model tries to do basically in this piece. The model itself is quite often trained and built in a way which retains its ability to learn new things. And you can separate similar embeddings faster and all of those. But when we actually use this in production, we don&rsquo;t need all of those capabilities, we don&rsquo;t need the train time capabilities.</p><p>Nirant Kasliwal:
And that means that all the extra compute and features and everything that you have stored for training time are wasted in production. So that&rsquo;s almost like saying that every time I have to speak to you I start over with hello, I&rsquo;m Nirant and I&rsquo;m a human being. It&rsquo;s extremely infuriating but we do this all the time with embedding and that is what fast embed primarily tries to fix. We say embeddings from the lens of production and we say that how can we make a Python library which is built for speed, efficiency and accuracy? Those are the core ethos in that sense. And I think people really find this relatable as a problem area. So you can see this on our GitHub issues. For instance, somebody says that oh yeah, we actually does what it says and yes, that&rsquo;s a good thing. So for 8 million tokens we took about 3 hours on a MacBook Pro M one while some other Olama embedding took over two days.</p><p>Nirant Kasliwal:
You can expect what 8 million tokens would cost on open air and how slow it would be given that they frequently rate limit you. So for context, we made a 1 million embedding set which was a little more than it was a lot more than 1 million tokens and that took us several hundred of us. It was not expensive, but it was very slow. So as a batch process, if you want to embed a large data set, it&rsquo;s very slow. I think the more colorful version of this somebody wrote on LinkedIn, Prithvira wrote on LinkedIn that your embeddings will go and I love that idea that we have optimized speed so that it just goes fast. That&rsquo;s the idea. So what do we I mean let&rsquo;s put names to these things, right? So one is we want it to be fast and light. And I&rsquo;ll explain what do we mean by light? We want recall to be fast, right? I mean, that&rsquo;s what we started with that what are embedding we want to be make sure that similar things are similar.</p><p>Nirant Kasliwal:
That&rsquo;s what we call recall. We often confuse this with accuracy but in retrieval sense we&rsquo;ll call it recall. We want to make sure it&rsquo;s still easy to use, right? Like there is no reason for this to get complicated. And we are fast, I mean we are very fast. And part of that is let&rsquo;s say we use BGE small En, the English model only. And let&rsquo;s say this is all in tokens per second and the token is model specific. So for instance, the way BGE would count a token might be different from how OpenAI might count a token because the tokenizers are slightly different and they have been trained on slightly different corporates. So that&rsquo;s the idea.</p><p>Nirant Kasliwal:
I would love you to try this so that I can actually brag about you trying it.</p><p>Demetrios:
What was the fine print on that slide? Benchmarks are my second most liked way to brag. What&rsquo;s your first most liked way to brag?</p><p>Nirant Kasliwal:
The best way is that when somebody tells me that they&rsquo;re using it.</p><p>Demetrios:
There we go. So I guess that&rsquo;s an easy way to get people to try and use it.</p><p>Nirant Kasliwal:
Yeah, I would love it if you try it. Tell us how it went for you, where it&rsquo;s working, where it&rsquo;s broken, all of that. I love it if you report issue then say I will even appreciate it if you yell at me because that means you&rsquo;re not ignoring me.</p><p>Demetrios:
That&rsquo;s it. There we go. Bug reports are good to throw off your mojo. Keep it rolling.</p><p>Nirant Kasliwal:
So we said fast and light. So what does light mean? So you will see a lot of these Embedding servers have really large image sizes. When I say image, I mean typically or docker image that can typically go to a few GPS. For instance, in case of sentence transformers, which somebody&rsquo;s checked out with Transformers the package and PyTorch, you get a docker image of roughly five GB. The Ram consumption is not that high by the way. Right. The size is quite large and of that the model is just 400 MB. So your dependencies are very large.</p><p>Nirant Kasliwal:
And every time you do this on, let&rsquo;s say an AWS Lambda, or let&rsquo;s say if you want to do horizontal scaling, your cold start times can go in several minutes. That is very slow and very inefficient if you are working in a workload which is very spiky. And if you were to think about it, people have more queries than, let&rsquo;s say your corpus quite often. So for instance, let&rsquo;s say you are in customer support for an ecommerce food delivery app. Bulk of your order volume will be around lunch and dinner timing. So that&rsquo;s a very spiky load. Similarly, ecommerce companies, which are even in fashion quite often see that people check in on their orders every evening and for instance when they leave from office or when they get home. And that&rsquo;s another spike.</p><p>Nirant Kasliwal:
So whenever you have a spiky load, you want to be able to scale horizontally and you want to be able to do it fast. And that speed comes from being able to be light. And that is why Fast Embed is very light. So you will see here that we call out that Fast Embed is just half a GB versus five GB. So on the extreme cases, this could be a ten x difference in your docker, image sizes and even Ram consumptions recall how good or bad are these embeddings? Right? So we said we are making them fast but do we sacrifice how much performance do we trade off for that? So we did a cosine similarity test with our default embeddings which was VG small en initially and now 1.5 and they&rsquo;re pretty robust. We don&rsquo;t sacrifice a lot of performance. Everyone with me? I need some audio to you.</p><p>Demetrios:
I&rsquo;m totally with you. There is a question that came through the chat if this is the moment to ask it.</p><p>Nirant Kasliwal:
Yes, please go for it.</p><p>Demetrios:
All right it&rsquo;s from a little bit back like a few slides ago. So I&rsquo;m just warning you. Are there any plans to support audio or image sources in fast embed?</p><p>Nirant Kasliwal:
If there is a request for that we do have a plan to support multimodal embedding. We would love to do that. If there&rsquo;s specific model within those, let&rsquo;s say you want Clip or Seglip or a specific audio model, please mention that either on that discord or our GitHub so that we can plan accordingly. So yeah, that&rsquo;s the idea. We need specific suggestions so that we keep adding it. We don&rsquo;t want to have too many models because then that creates confusion for our end users and that is why we take opinated stance and that is actually a good segue. Why do we prioritize that? We want this package to be easy to use so we&rsquo;re always going to try and make the best default choice for you. So this is a very Linux way of saying that we do one thing and we try to do that one thing really well.</p><p>Nirant Kasliwal:
And here, let&rsquo;s say for instance, if you were to look at Qdrant client it&rsquo;s just passing everything as you would. So docs is a list of strings, metadata is a list of dictionaries and IDs again is a list of IDs valid IDs as per the Qdrant Client spec. And the search is also very straightforward. The entire search query is basically just two params. You could even see a very familiar integration which is let&rsquo;s say langchain. I think most people here would have looked at this in some shape or form earlier. This is also very familiar and very straightforward. And under the hood what are we doing is just this one line.</p><p>Nirant Kasliwal:
We have a dot embed which is a generator and we call a list on that so that we actually get a list of embeddings. You will notice that we have a passage and query keys here which means that our retrieval model which we have used as default here, takes these into account that if there is a passage and a query they need to be mapped together and a question and answer context is captured in the model training itself. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.</p><p>Demetrios:
One thing on this, we had Neil&rsquo;s from Cohere on last week and he was talking about Cohere&rsquo;s embed version three, I think, or V three, he was calling it. How does this play with that? Does it is it supported or no?</p><p>Nirant Kasliwal:
As of now, we only support models which are open source so that we can serve those models directly. Embed V three is cloud only at the moment, so that is why it is not supported yet. But that said, we are not opposed to it. In case there&rsquo;s a requirement for that, we are happy to support that so that people can use it seamlessly with Qdrant and fast embed does the heavy lifting of passing it to Qdrant, structuring the schema and all of those for you. So that&rsquo;s perfectly fair. As I ask, if we have folks who would love to try coherent embed V three, we&rsquo;d use that. Also, I think Nils called out that coherent embed V three is compatible with binary quantization. And I think that&rsquo;s the only embedding which officially supports that.</p><p>Nirant Kasliwal:
Okay, we are binary quantization aware and they&rsquo;ve been trained for it. Like compression awareness is, I think, what it was called. So Qdrant supports that. So please of that might be worth it because it saves about 30 x in memory costs. So that&rsquo;s quite powerful.</p><p>Demetrios:
Excellent.</p><p>Nirant Kasliwal:
All right, so behind the scenes, I think this is my favorite part of this. It&rsquo;s also very short. We do literally two things. Why are we fast? We use ONNX runtime as of now, our configurations are such that it runs on CPU and we are still very fast. And that&rsquo;s because of all the multiple processing and ONNX runtime itself at some point in the future. We also want to support GPUs. We had some configuration issues on different Nvidia configurations. As the GPU changes, the OnX runtime does not seamlessly change the GPU.</p><p>Nirant Kasliwal:
So that is why we do not allow that as a provider. But you can pass that. It&rsquo;s not prohibited, it&rsquo;s just not a default. We want to make sure your default is always available and will be available in the happy path, always. And we quantize the models for you. So when we quantize, what it means is we do a bunch of tricks supported by a huge shout out to hugging faces optimum. So we do a bunch of optimizations in the quantization, which is we compress some activations, for instance, gelu. We also do some graph optimizations and we don&rsquo;t really do a lot of dropping the bits, which is let&rsquo;s say 32 to 16 or 64 to 32 kind of quantization only where required.</p><p>Nirant Kasliwal:
Most of these gains come from the graph optimizations themselves. So there are different modes which optimum itself calls out. And if there are folks interested in that, happy to share docs and details around that. Yeah, that&rsquo;s about it. Those are the two things which we do from which we get bulk of these speed gains. And I think this goes back to the question which you opened with. Yes, we do want to support multimodal. We are looking at how we can do an on and export of Clip, which is as robust as Clip.</p><p>Nirant Kasliwal:
So far we have not found anything. I&rsquo;ve spent some time looking at this, the quality of life upgrades. So far, most of our model downloads have been through Google cloud storage hosted by Qdrant. We want to support hugging Face hub so that we can launch new models much, much faster. So we will do that soon. And the next thing is, as I called out, we always want to take performance as a first class citizen. So we are looking at how we can allow you to change or adapt frozen Embeddings, let&rsquo;s say open a Embedding or any other model to your specific domain. So maybe a separate toolkit within Fast Embed which is optional and not a part of the default path, because this is not something which you will use all the time.</p><p>Nirant Kasliwal:
We want to make sure that your training and experience parts are separate. So we will do that. Yeah, that&rsquo;s it. Fast and sweet.</p><p>Demetrios:
Amazing. Like FastEmbed.</p><p>Nirant Kasliwal:
Yes.</p><p>Demetrios:
There was somebody that talked about how you need to be good at your puns and that might be the best thing, best brag worthy stuff you&rsquo;ve got. There&rsquo;s also a question coming through that I want to ask you. Is it true that when we use Qdrant client add Fast Embedding is included? We don&rsquo;t have to do it?</p><p>Nirant Kasliwal:
What do you mean by do it? As in you don&rsquo;t have to specify a Fast Embed model?</p><p>Demetrios:
Yeah, I think it&rsquo;s more just like you don&rsquo;t have to add it on to Qdrant in any way or this is completely separated.</p><p>Nirant Kasliwal:
So this is client side. You own all your data and even when you compress it and send us all the Embedding creation happens on your own compute. This Embedding creation does not happen on Cauldron cloud, it happens on your own compute. It&rsquo;s consistent with the idea that you should have as much control as possible. This is also why, as of now at least, Fast Embed is not a dedicated server. We do not want you to be running two different docker images for Qdrant and Fast Embed. Or let&rsquo;s say two different ports for Qdrant and Discord within the sorry, Qdrant and Fast Embed in the same docker image or server. So, yeah, that is more chaos than we would like.</p><p>Demetrios:
Yeah, and I think if I understood it, I understood that question a little bit differently, where it&rsquo;s just like this comes with Qdrant out of the box.</p><p>Nirant Kasliwal:
Yes, I think that&rsquo;s a good way to look at it. We set all the defaults for you, we select good practices for you and that should work in a vast majority of cases based on the MTEB benchmark, but we cannot guarantee that it will work for every scenario. Let&rsquo;s say our default model is picked for English and it&rsquo;s mostly tested on open domain open web data. So, for instance, if you&rsquo;re doing something domain specific, like medical or legal, it might not work that well. So that is where you might want to still make your own Embeddings. So that&rsquo;s the edge case here.</p><p>Demetrios:
What are some of the other knobs that you might want to be turning when you&rsquo;re looking at using this.</p><p>Nirant Kasliwal:
With Qdrant or without Qdrant?</p><p>Demetrios:
With Qdrant.</p><p>Nirant Kasliwal:
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let&rsquo;s say this is how you would have to manually specify, let&rsquo;s say, passage or query. When you do this, let&rsquo;s say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you&rsquo;re using it with Qdrant or just in isolation as well.</p><p>Nirant Kasliwal:
So that is one knob. The second is, I think it&rsquo;s very commonly recommended, we would recommend that you start with some evaluation, like have maybe let&rsquo;s even just five sentences to begin with and see if they&rsquo;re actually close to each other. And as a very important shout out in Embedding retrieval, when we use Embedding for retrieval or vector similarity search, it&rsquo;s the relative ordering which matters. So, for instance, we cannot say that zero nine is always good. It could also mean that the best match is, let&rsquo;s say, 0.6 in your domain. So there is no absolute cut off for threshold in terms of match. So sometimes people assume that we should set a minimum threshold so that we get no noise. So I would suggest that you calibrate that for your queries and domain.</p><p>Nirant Kasliwal:
And you don&rsquo;t need a lot of queries. Even if you just, let&rsquo;s say, start with five to ten questions, which you handwrite based on your understanding of the domain, you will do a lot better than just picking a threshold at random.</p><p>Demetrios:
This is good to know. Okay, thanks for that. So there&rsquo;s a question coming through in the chat from Shreya asking how is the latency in comparison to elasticsearch?</p><p>Nirant Kasliwal:
Elasticsearch? I believe that&rsquo;s a Qdrant benchmark question and I&rsquo;m not sure how is elastics HNSW index, because I think that will be the fair comparison. I also believe elastics HNSW index puts some limitations on how many vectors they can store with the payload. So it&rsquo;s not an apples to apples comparison. It&rsquo;s almost like comparing, let&rsquo;s say, a single page with the entire book, because that&rsquo;s typically the ratio from what I remember I also might be a few months outdated on this, but I think the intent behind that question is, is Qdrant fast enough for what Qdrant does? It is definitely fast is, which is embedding similarity search. So for that, it&rsquo;s exceptionally fast. It&rsquo;s written in Rust and Twitter for all C. Similar tweets uses this at really large scale. They run a Qdrant instance.</p><p>Nirant Kasliwal:
So I think if a Twitter scale company, which probably does about anywhere between two and 5 million tweets a day, if they can embed and use Qdrant to serve that similarity search, I think most people should be okay with that latency and throughput requirements.</p><p>Demetrios:
It&rsquo;s also in the name. I mean, you called it Fast Embed for a reason, right?</p><p>Nirant Kasliwal:
Yes.</p><p>Demetrios:
So there&rsquo;s another question that I&rsquo;ve got coming through and it&rsquo;s around the model selection and embedding size. And given the variety of models and the embedding sizes available, how do you determine the most suitable models and embedding sizes? You kind of got into this on how yeah, one thing that you can do to turn the knobs are choosing a different model. But how do you go about choosing which model is better? There.</p><p>Nirant Kasliwal:
There is the academic way of looking at and then there is the engineer way of looking at it, and then there is the hacker way of looking at it. And I will give you all these three answers in that order. So the academic and the gold standard way of doing this would probably look something like this. You will go at a known benchmark, which might be, let&rsquo;s say, something like Kilt K-I-L-T or multilingual text embedding benchmark, also known as MTEB or Beer, which is beir one of these three benchmarks. And you will look at their retrieval section and see which one of those marks very close to whatever is your domain or your problem area, basically. So, for instance, let&rsquo;s say you&rsquo;re working in Pharmacology, the ODS that a customer support retrieval task is relevant to. You are near zero unless you are specifically in, I don&rsquo;t know, a Pharmacology subscription app. So that is where you would start.</p><p>Nirant Kasliwal:
This will typically take anywhere between two to 20 hours, depending on how familiar you are with these data sets already. But it&rsquo;s not going to take you, let&rsquo;s say, a month to do this. So just to put a rough order of magnitude, once you have that, you try to take whatever is the best model on that subdomain data set and you see how does it work within your domain and you launch from there. At that point, you switch into the engineer&rsquo;s mindset. The engineer&rsquo;s mindset now tells you that the best way to build something is to make an informed guess about what workload or challenges you&rsquo;re going to foresee. Right. Like a civil engineer builds a bridge around how many cars they expect, they&rsquo;re obviously not going to build a bridge to carry a ship load, for instance, or a plane load, which are very different. So you start with that and you say, okay, this is the number of requests which I expect, this is what my budget is, and your budget will quite often be, let&rsquo;s say, in terms of latency budgets, compute and memory budgets.</p><p>Nirant Kasliwal:
So for instance, one of the reasons I mentioned binary quantization and product quantization is with something like binary quantization you can get 98% recall, but with 30 to 40 x memory savings because it discards all the extraneous bits and just keeps the zero or one bit of the embedding itself. And Qdrant has already measured it for you. So we know that it works for OpenAI and Cohere embeddings for sure. So you might want to use that to just massively scale while keeping your budgets as an engineer. Now, in order to do this, you need to have some sense of three numbers, right? What are your latency requirements, your cost requirements, and your performance requirement. Now, for the performance, which is where engineers are most unfamiliar with, I will give the hacker answer, which is this.</p><p>Demetrios:
Is what I was waiting for. Man, so excited for this one, exactly this. Please tell us the hacker answer.</p><p>Nirant Kasliwal:
The hacker answer is this there are two tricks which I will share. One is write ten questions, figure out the best answer, and see which model gets as many of those ten, right? The second is most embedding models which are larger or equivalent to 768 embeddings, can be optimized and improved by adding a small linear head over it. So for instance, I can take the Open AI embedding, which is 1536 embedding, take my text, pass it through that, and for my own domain, adapt the Open A embedding by adding two or three layers of linear functions, basically, right? Y is equals to MX plus C or Ax plus B y is equals to C, something like that. So it&rsquo;s very simple, you can do it on NumPy, you don&rsquo;t need Torch for it because it&rsquo;s very small. The header or adapter size will typically be in this range of few KBS to be maybe a megabyte, maybe. I think the largest I have used in production is about 400 500 KBS. That&rsquo;s about it. And that will improve your recall several, several times.</p><p>Nirant Kasliwal:
So that&rsquo;s one, that&rsquo;s two tricks. And a third bonus hacker trick is if you&rsquo;re using an LLM, sometimes what you can do is take a question and rewrite it with a prompt and make embeddings from both, and pull candidates from both. And then with Qdrant Async, you can fire both these queries async so that you&rsquo;re not blocked, and then use the answer of both the original question which the user gave and the one which you rewrote using the LLM and see select the results which are there in both, or figure some other combination method. Also, so most Kagglers would be familiar with the idea of ensembling. This is the way to do query inference time ensembling, that&rsquo;s awesome.</p><p>Demetrios:
Okay, dude, I&rsquo;m not going to lie, that was a lot more than I was expecting for that answer.</p><p>Nirant Kasliwal:
Got into the weeds of retrieval there. Sorry.</p><p>Demetrios:
I like it though. I appreciate it. So what about when it comes to the know, we had Andre V, the CTO of Qdrant on here a few weeks ago. He was talking about binary quantization. But then when it comes to quantizing embedding models, in the docs you mentioned like quantized embedding models for fast CPU generation. Can you explain a little bit more about what quantized embedding models are and how they enhance the CPU performance?</p><p>Nirant Kasliwal:
So it&rsquo;s a shorthand to say that they optimize CPU performance. I think the more correct way to look at it is that we use the CPU better. But let&rsquo;s talk about optimization or quantization, which we do here, right? So most of what we do is from optimum and the way optimum call set up is they call these levels. So you can basically go from let&rsquo;s say level zero, which is there are no optimizations to let&rsquo;s say 99 where there&rsquo;s a bunch of extra optimizations happening. And these are different flags which you can switch. And here are some examples which I remember. So for instance, there is a norm layer which you can fuse with the previous operation. Then there are different attention layers which you can fuse with the previous one because you&rsquo;re not going to update them anymore, right? So what we do in training is we update them.</p><p>Nirant Kasliwal:
You know that you&rsquo;re not going to update them because you&rsquo;re using them for inference. So let&rsquo;s say when somebody asks a question, you want that to be converted into an embedding as fast as possible and as cheaply as possible. So you can discard all these extra information which you are most likely to not going to use. So there&rsquo;s a bunch of those things and obviously you can use mixed precision, which most people have heard of with projects, let&rsquo;s say like lounge CPP that you can use FP 16 mixed precision or a bunch of these things. Let&rsquo;s say if you are doing GPU only. So some of these things like FP 16 work better on GPU. The CPU part of that claim comes from how ONNX the runtime which we use allows you to optimize whatever CPU instruction set you are using. So as an example with intel you can say, okay, I&rsquo;m going to use the Vino instruction set or the optimization.</p><p>Nirant Kasliwal:
So when we do quantize it, we do quantization right now with CPUs in mind. So what we would want to do at some point in the future is give you a GPU friendly quantized model and we can do a device check and say, okay, we can see that a GPU is available and download the GPU friendly model first for you. Awesome. Does that answer the. Question.</p><p>Demetrios:
I mean, for me, yeah, but we&rsquo;ll see what the chat says.</p><p>Nirant Kasliwal:
Yes, let&rsquo;s do that.</p><p>Demetrios:
What everybody says there. Dude, this has been great. I really appreciate you coming and walking through everything we need to know, not only about fast embed, but I think about embeddings in general. All right, I will see you later. Thank you so much, Naran. Thank you, everyone, for coming out. If you want to present, please let us know. Hit us up, because we would love to have you at our vector space talks.</p></div></div></article></section><section class=get-started-blogs><div class=container><div class=get-started-blogs__content><div class=row><div class="get-started-blogs__text col-12 col-lg-7"><h3 class=get-started-blogs__title>Get Started with Qdrant Free</h3><a href=https://cloud.qdrant.io/signup class="button button_contained" target=_blank>Get Started</a></div><div class="get-started-blogs__image col-12 col-lg-5"><img src=https://qdrant.tech/img/rocket.svg alt></div></div><div class=get-started-blogs__overlay-top></div></div></div></section></main><footer class=footer><div class=footer__top><div class=container><div class="row justify-content-md-between"><div class="col-12 col-md-6"><a href=https://qdrant.tech/ title="Go to Home Page"><img class=footer__top-logo src=https://qdrant.tech/img/logo-white.png alt="Qdrant Logo"></a></div><div class="col-12 col-md-6 footer__top-social-media-platforms"><a class=footer__top-social-media-link href=https://github.com/qdrant/qdrant target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g clip-path="url(#clip0_1841_958)"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 .299805C5.35.299805.0 5.6498.0 12.2998c0 5.3 3.45 9.8 8.2 11.4C8.8 23.7998 9 23.4498 9 23.0998 9 22.7998 9 22.0498 9 21.0498c-3.35.75-4.05-1.6-4.05-1.6-.55-1.4-1.35-1.75-1.35-1.75-1.1-.75.1-.75.1-.75C4.9 17.0498 5.55 18.1998 5.55 18.1998c1.05 1.85 2.8 1.3 3.5 1C9.15 18.3998 9.45 17.8998 9.8 17.5998 7.15 17.2998 4.35 16.2498 4.35 11.6498c0-1.3.45-2.4 1.25-3.2C5.5 8.1498 5.05 6.9498 5.7 5.2498c0 0 1-.3 3.3 1.25C9.95 6.2498 11 6.0998 12 6.0998S14.05 6.2498 15 6.4998c2.3-1.55 3.3-1.25 3.3-1.25C18.95 6.8998 18.55 8.0998 18.4 8.4498c.75.85 1.25 1.9 1.25 3.2.0 4.6-2.8 5.6-5.5 5.9C14.6 17.8998 14.95 18.6498 14.95 19.7498c0 1.6.0 2.9.0 3.3C14.95 23.3498 15.15 23.7498 15.8 23.6498c4.75-1.55 8.2-6.05 8.2-11.35C24 5.6498 18.65.299805 12 .299805z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_1841_958"><rect width="24" height="24" fill="#fff"/></clipPath></defs></svg>
</a><a class=footer__top-social-media-link href=https://qdrant.to/linkedin target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g clip-path="url(#clip0_1841_961)"><path d="M21.75.75H2.25c-.39782.0-.77936.158035-1.06066.43934C.908035 1.47064.75 1.85218.75 2.25v19.5C.75 22.1478.908035 22.5294 1.18934 22.8107 1.47064 23.092 1.85218 23.25 2.25 23.25h19.5C22.1478 23.25 22.5294 23.092 22.8107 22.8107 23.092 22.5294 23.25 22.1478 23.25 21.75V2.25C23.25 1.85218 23.092 1.47064 22.8107 1.18934 22.5294.908035 22.1478.75 21.75.75zM7.41525 19.9455H4.0305V9.1875H7.41525v10.758zM5.7225 7.71075C5.33338 7.70956 4.95333 7.59309 4.63036 7.37604 4.30739 7.15899 4.05599 6.8511 3.9079 6.49126 3.75981 6.13141 3.72167 5.73575 3.79831 5.35425 3.87495 4.97275 4.06293 4.62251 4.3385 4.34778 4.61408 4.07304 4.96488 3.88613 5.34662 3.81065 5.72835 3.73517 6.1239 3.77451 6.48329 3.92369 6.84268 4.07288 7.1498 4.32522 7.36587 4.64885c.21606.32363.33138.70402.33138 1.09315C7.69735 6.00107 7.6463 6.25762 7.54702 6.49691 7.44774 6.73621 7.30219 6.95355 7.11872 7.13646 6.93525 7.31938 6.71746 7.46426 6.47787 7.56282 6.23827 7.66137 5.98157 7.71164 5.7225 7.71075zM19.9657 19.9455H16.65V14.742c0-1.2652.0-2.8125-1.7625-2.8125s-1.9748 1.3365-1.9748 2.742V20.016H9.6V9.1875h3.102v1.4767H12.7725C13.0924 10.1111 13.5567 9.65537 14.1156 9.34571 14.6746 9.03606 15.3072 8.88415 15.9457 8.90625c3.3848.0 4.0193 2.24995 4.0193 5.13295L19.9657 19.9455z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_1841_961"><rect width="24" height="24" fill="#fff"/></clipPath></defs></svg>
</a><a class=footer__top-social-media-link href=https://qdrant.to/twitter target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M10.8423 15.1515 4.40655 22.5H.841797L9.1773 12.978l1.665 2.1735z" fill="#f0f3fa"/><path d="M12.7881 8.241 18.6808 1.5h3.5625l-7.8052 8.9265-1.65-2.1855z" fill="#f0f3fa"/><path d="M23.6158 22.5H16.4465L.383789 1.5H7.73454l15.88126 21zm-6.186-2.1322h1.974L6.66204 3.5205h-2.118L17.4298 20.3678z" fill="#f0f3fa"/></svg>
</a><a class=footer__top-social-media-link href=https://qdrant.to/discord target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M5 1C2.79086 1 1 2.79086 1 5V19c0 2.2091 1.79086 4 4 4H19c2.2091.0 4-1.7909 4-4V5c0-2.20914-1.7909-4-4-4H5zM16.3683 18.5964S15.8027 17.9208 15.3314 17.3238C16.4701 17.0557 17.4774 16.3935 18.1749 15.4543 17.6098 15.8317 17.0037 16.1438 16.3683 16.3846 15.6374 16.6966 14.873 16.9233 14.0903 17.0602 12.7448 17.3079 11.3649 17.3026 10.0213 17.0445 9.23256 16.8901 8.45954 16.664 7.71193 16.3689 7.08195 16.1284 6.48116 15.8174 5.92097 15.442 6.59328 16.3616 7.56575 17.0173 8.67025 17.2958 8.19895 17.8928 7.61767 18.5998 7.61767 18.5998 4.14571 18.4898 2.82605 16.2091 2.82605 16.2091 2.87704 13.0242 3.65058 9.89242 5.08832 7.05004 6.35356 6.05636 7.89607 5.47998 9.5029 5.40046l.1571.18853c-1.51174.37413-2.92245 1.0768-4.13179 2.05804.0.0.34562-.18853.926900000000001-.4556C7.58438 6.67595 8.78793 6.34193 10.0213 6.20168 10.1093 6.18348 10.1986 6.17297 10.2884 6.17026 11.3412 6.0331 12.4066 6.02255 13.4619 6.13884c1.6595.18933 3.2659.70144 4.7288 1.5075C17.0426 6.71186 15.7093 6.0318 14.2788 5.65114L14.4988 5.39978C16.1056 5.4793 17.6481 6.05568 18.9133 7.04935c1.4378 2.84239 2.2113 5.97415 2.2623 9.15905.0.0-1.3354 2.278-4.8073 2.388zM9.06284 11.2616C8.62563 11.2983 8.21817 11.498 7.9212 11.821 7.62423 12.1439 7.45941 12.5667 7.45941 13.0054c0 .438800000000001.16482.8615.46179 1.1845S8.62563 14.7125 9.06284 14.7493C9.50005 14.7125 9.90751 14.5129 10.2045 14.1899 10.5015 13.8669 10.6663 13.4442 10.6663 13.0054c0-.438699999999999-.1648-.8615-.4618-1.1844C9.90751 11.498 9.50005 11.2983 9.06284 11.2616zm5.73766.0C14.4493 11.2319 14.0974 11.3089 13.7907 11.4825c-.3066.1736-.553699999999999.4358-.7089.7522-.155199999999999.3164-.2112.6723-.1608 1.021C12.9714 13.6045 13.1259 13.93 13.3644 14.1894 13.6028 14.4489 13.9141 14.6304 14.2573 14.71 14.6006 14.7897 14.96 14.7639 15.2883 14.6359c.3284-.1279.6104-.352.8093-.642899999999999C16.2965 13.702 16.4029 13.3578 16.4029 13.0054 16.4124 12.7854 16.3783 12.5657 16.3026 12.3588 16.2269 12.152 16.1112 11.9622 15.962 11.8002 15.8128 11.6381 15.6331 11.5072 15.4332 11.4148 15.2333 11.3223 15.0171 11.2703 14.7971 11.2616H14.8005z" fill="#f0f3fa"/></svg>
</a><a class=footer__top-social-media-link href=https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M23.775 7.1999S23.55 5.5499 22.8 4.7999C21.9 3.8249 20.85 3.8249 20.4 3.7499c-3.375-.225-8.4-.225-8.4-.225s-5.025.0-8.4.225C3.15 3.8249 2.1 3.8249 1.2 4.7999c-.75.75-.975 2.4-.975 2.4S0 9.1499.0 11.0999v1.8c0 1.95.225 3.9.225 3.9s.225 1.65.975 2.4C2.1 20.1749 3.3 20.0999 3.825 20.2499 5.775 20.3999 12 20.4749 12 20.4749S17.025 20.4749 20.4 20.2499C20.85 20.1749 21.9 20.1749 22.8 19.1999 23.55 18.4499 23.775 16.7999 23.775 16.7999S24 14.8499 24 12.8999v-1.8C24 9.1499 23.775 7.1999 23.775 7.1999zm-14.25 7.95v-6.75l6.45 3.375-6.45 3.375z" fill="#f0f3fa"/></svg></a></div></div></div></div><div class=footer__menu><div class=container><nav class=footer__menu-content><div class=footer__menu-section><p class=footer__menu-section-title>Products</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/qdrant-vector-database/>Qdrant Vector Database</a></li><li class=footer__menu-item><a href=https://qdrant.tech/cloud/>Qdrant Cloud</a></li><li class=footer__menu-item><a href=https://qdrant.tech/hybrid-cloud/>Qdrant Hybrid Cloud</a></li><li class=footer__menu-item><a href=https://qdrant.tech/enterprise-solutions/>Qdrant Enterprise Solutions</a></li><li class=footer__menu-item><a href=https://qdrant.tech/pricing/>Pricing</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Use Cases</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/advanced-search/>Advanced Search</a></li><li class=footer__menu-item><a href=https://qdrant.tech/recommendations/>Recommendation Systems</a></li><li class=footer__menu-item><a href=https://qdrant.tech/rag/>Retrieval Augmented Generation</a></li><li class=footer__menu-item><a href=https://qdrant.tech/data-analysis-anomaly-detection/>Data Analysis & Anomaly Detection</a></li><li class=footer__menu-item><a href=https://qdrant.tech/ai-agents/>AI Agents</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Developers</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/documentation/>Documentation</a></li><li class=footer__menu-item><a href=https://qdrant.tech/community/>Community</a></li><li class=footer__menu-item><a href=https://github.com/qdrant/qdrant target=_blank rel="noopener noreferrer nofollow">GitHub</a></li><li class=footer__menu-item><a href=https://qdrant.to/roadmap target=_blank rel="noopener noreferrer nofollow">Roadmap</a></li><li class=footer__menu-item><a href=https://github.com/qdrant/qdrant/releases target=_blank rel="noopener noreferrer nofollow">Change Log</a></li><li class=footer__menu-item><a href=https://status.qdrant.io/ target=_blank rel="noopener noreferrer nofollow">Status Page</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Resources</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/blog/>Blog</a></li><li class=footer__menu-item><a href=https://qdrant.tech/benchmarks/>Benchmarks</a></li><li class=footer__menu-item><a href=https://qdrant.tech/articles/>Articles</a></li><li class=footer__menu-item><a href=https://try.qdrant.tech/events target=_blank rel="noopener noreferrer nofollow">Events</a></li><li class=footer__menu-item><a href=https://qdrant.tech/qdrant-for-startups/>Startup Program</a></li><li class=footer__menu-item><a href=https://qdrant.tech/demo/>Demos</a></li><li class=footer__menu-item><a href=https://qdrant.tech/security/bug-bounty-program/>Bug Bounty</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Company</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/about-us/>About Us</a></li><li class=footer__menu-item><a href=https://qdrant.tech/customers/>Customers</a></li><li class=footer__menu-item><a href=https://qdrant.tech/partners/>Partners</a></li><li class=footer__menu-item><a href=https://qdrant.join.com/ target=_blank rel="noopener noreferrer nofollow">Careers</a></li><li class=footer__menu-item><a href=https://qdrant.tech/contact-us/>Contact Us</a></li></ul></div></nav></div></div><div class=footer__middle><div class=container><div class="align-items-center row"><div class="col-12 col-lg-5"><p class=footer__middle-title>Sign up for Qdrant updates</p><p class=footer__middle-subtitle>We'll occasionally send you best practices for using vector data and similarity search, as well as product news.</p></div><div class="footer__middle-newsletter col-12 col-lg-7"><div id=footer-subscribe-form><script>(function(){const t={region:"eu1",portalId:"139603372",formId:"049d96c6-ef65-4e41-ba69-a3335b9334cf",cssClass:"subscribe-form",submitButtonClass:"button button_contained button_lg",submitText:"Subscribe"},n="weh",s=function(){const e=document.createElement("input");return e.classList.add(n),e.type="text",e.name="my-work-email",e.style.display="none",e.placeholder="Email",e.ariaHidden="true",e},o=function(e){const t=s();e.appendChild(t);const n=e.querySelector('[type="submit"]');t.addEventListener("input",function(){t.value.length>0&&(n.disabled=!0)})},i=["Argentina","Belgium","Canada","Czech Republic","Cyprus","Denmark","Germany","Hungary","Latvia","Liechtenstein","Luxembourg","Netherlands","Norway","France","Finland","Croatia","Bulgaria","Belarus","Bosnia and Herzegovina","Austria","Estonia","Georgia","Greenland","Hong Kong","Israel","Italy","Maldives","Moldova","Monaco","Portugal","Russia","Serbia","Slovakia","Slovenia","Sweden","Switzerland","Türkiye","Ukraine","Macedonia (FYROM)","United Kingdom"];function e(e){const a=e.querySelector('select[name="country"]'),n=e.querySelector(".legal-consent-container .hs-fieldtype-booleancheckbox"),r=e.querySelector(".legal-consent-container > div:nth-child(3)"),c=e.querySelector(".legal-consent-container > div:nth-child(2)");if(!a||!n)return;const s=a.value,l=i.includes(s),o=s&&l,t=n.querySelector('input[type="checkbox"]');n.style.display=o?"block":"none",r&&(r.style.display=o?"block":"none"),c&&(c.style.display=o?"none":"block"),s&&!l?t.checked||t.click():t.checked&&t.click()}try{hbspt.forms.create({...t,formInstanceId:"#footer-subscribe-form",pageId:"",target:"#footer-subscribe-form",onFormReady:function(t){if(!t){console.warn("Form not found.");return}o(t),e(t);const n=t.querySelector('select[name="country"]');n&&n.addEventListener("change",()=>e(t))}})}catch{document.getElementById("footer-subscribe-form").innerHTML='<p class="text-white">Here should be a form but looks like it was blocked on your side. Please, check your trackers blocking policy.</p>'}})()</script></div></div></div></div></div><div class=footer__bottom><div class=container><div class="row g-3"><div class="col-12 col-lg-6 footer__bottom-content"><span class=footer__bottom-copyright>© 2025 Qdrant.</span><div class=footer__bottom-bages><a href=http://qdrant.to/trust-center target=_blank><img src=https://qdrant.tech/img/soc2-badge.png alt=SOC2>
</a><a href=https://heydata.eu/ target=_blank><img src=https://qdrant.tech/img/gdpr-badge.png alt="heyData GDPR">
</a><a href=https://qdrant.tech/# target=_blank><img src=https://qdrant.tech/img/dark-gdpr-badge.png alt=GDPR>
</a><a href=https://qdrant.tech/# target=_blank><img src=https://qdrant.tech/img/hipaa-badge.png alt=HIPAA></a></div></div><div class="col-12 col-lg-6 footer__bottom-links"><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></div></footer><button class="d-none button button_outlined go-to-top-button" id=scrollToTopBtn title="Go to top">Up!</button>
</body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>document.addEventListener("scroll",()=>{const e=document.getElementById("article"),t=document.getElementById("progress"),n=e.scrollHeight-window.innerHeight,s=window.scrollY-e.offsetTop;t.value=Math.min(Math.max(s/n*100,0),100)})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>