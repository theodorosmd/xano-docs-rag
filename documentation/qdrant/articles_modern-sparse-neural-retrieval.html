<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>Modern Sparse Neural Retrieval: From Theory to Practice - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="A comprehensive guide to modern sparse neural retrievers: COIL, TILDEv2, SPLADE, and more. Find out how they work and learn how to use them effectively."><meta name=keywords content="vector search engine,neural network,matching,SaaS,approximate nearest neighbor search,image search,recommender system,vectors,knn algorithm,hnsw,vector search,embeddings,similarity,simaes networks,BERT,transformer,word2vec,fasttext,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/modern-sparse-neural-retrieval/#article","@type":"Article","abstract":"A comprehensive guide to modern sparse neural retrievers: COIL TILDEv2 SPLADE and more Find out how they work and learn how to use them effectively","author":{"@type":"Person","name":"Evgeniya Sukhodolskaya"},"dateModified":"2024-10-23 00:00:00 +0000 UTC","datePublished":"2024-10-23 00:00:00 +0000 UTC","description":"A comprehensive guide to modern sparse neural retrievers: COIL TILDEv2 SPLADE and more Find out how they work and learn how to use them effectively","headline":"Modern Sparse Neural Retrieval: From Theory to Practice","image":["https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/social-preview.png"],"name":"Modern Sparse Neural Retrieval: From Theory to Practice","url":"https://qdrant.tech/articles/modern-sparse-neural-retrieval/","wordCount":"4765"},{"@id":"https://qdrant.tech/articles/modern-sparse-neural-retrieval/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestraße 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["vector search engine","neural network","matching","SaaS","approximate nearest neighbor search","image search","recommender system","vectors","knn algorithm","hnsw","vector search","embeddings","similarity","simaes networks","BERT","transformer","word2vec","fasttext","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/modern-sparse-neural-retrieval/"><meta property="og:type" content="website"><meta property="og:title" content="Modern Sparse Neural Retrieval: From Theory to Practice - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/modern-sparse-neural-retrieval/"><meta name=twitter:title content="Modern Sparse Neural Retrieval: From Theory to Practice - Qdrant"><meta property="og:description" content="A comprehensive guide to modern sparse neural retrievers: COIL, TILDEv2, SPLADE, and more. Find out how they work and learn how to use them effectively."><meta name=twitter:description content="A comprehensive guide to modern sparse neural retrievers: COIL, TILDEv2, SPLADE, and more. Find out how they work and learn how to use them effectively."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/social-preview.png"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/social-preview.png"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/social-preview.png"><meta name=author content="Evgeniya Sukhodolskaya"><link rel=canonical href=https://qdrant.tech/articles/modern-sparse-neural-retrieval/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>Modern Sparse Neural Retrieval: From Theory to Practice</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/machine-learning/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Machine Learning</a><h1 class=documentation-article__header-title>Modern Sparse Neural Retrieval: From Theory to Practice</h1><div class=documentation-article__header-about><p>Evgeniya Sukhodolskaya</p><span>&#183;</span><p>October 23, 2024</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/preview/title.webp type=image/webp><img alt="Modern Sparse Neural Retrieval: From Theory to Practice" src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/preview/title.jpg></picture></div><p>Finding enough time to study all the modern solutions while keeping your production running is rarely feasible.
Dense retrievers, hybrid retrievers, late interaction… How do they work, and where do they fit best?
If only we could compare retrievers as easily as products on Amazon!</p><p>We explored the most popular modern sparse neural retrieval models and broke them down for you.
By the end of this article, you’ll have a clear understanding of the current landscape in sparse neural retrieval and how to navigate through complex, math-heavy research papers with sky-high NDCG scores without getting overwhelmed.</p><p><a href=#sparse-neural-retrieval-evolution>The first part</a> of this article is theoretical, comparing different approaches used in
modern sparse neural retrieval.<br><a href=#splade-in-qdrant>The second part</a> is more practical, showing how the best model in modern sparse neural retrieval, <code>SPLADE++</code>,
can be used in Qdrant and recommendations on when to choose sparse neural retrieval for your solutions.</p><h2 id=sparse-neural-retrieval-as-if-keyword-based-retrievers-understood-meaning>Sparse Neural Retrieval: As If Keyword-Based Retrievers Understood Meaning</h2><p><strong>Keyword-based (lexical) retrievers</strong> like BM25 provide a good explainability.
If a document matches a query, it’s easy to understand why: query terms are present in the document,
and if these are rare terms, they are more important for retrieval.</p><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/LexicalRetrievers.png alt="Keyword-based (Lexical) Retrieval"></p><p>With their mechanism of exact term matching, they are super fast at retrieval.
A simple <strong>inverted index</strong>, which maps back from a term to a list of documents where this term occurs, saves time on checking millions of documents.</p><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/InvertedIndex.png alt="Inverted Index"></p><p>Lexical retrievers are still a strong baseline in retrieval tasks.
However, by design, they’re unable to bridge <strong>vocabulary</strong> and <strong>semantic mismatch</strong> gaps.
Imagine searching for a “<em>tasty cheese</em>” in an online store and not having a chance to get “<em>Gouda</em>” or “<em>Brie</em>” in your shopping basket.</p><p><strong>Dense retrievers</strong>, based on machine learning models which encode documents and queries in dense vector representations,
are capable of breaching this gap and finding you “<em>a piece of Gouda</em>”.</p><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/DenseRetrievers.png alt="Dense Retrieval"></p><p>However, explainability here suffers: why is this query representation close to this document representation?
Why, searching for “<em>cheese</em>”, we’re also offered “<em>mouse traps</em>”? What does each number in this vector representation mean?
Which one of them is capturing the cheesiness?</p><p>Without a solid understanding, balancing result quality and resource consumption becomes challenging.
Since, hypothetically, any document could match a query, relying on an inverted index with exact matching isn’t feasible.
This doesn’t mean dense retrievers are inherently slower. However, lexical retrieval has been around long enough to inspire several effective architectural choices, which are often worth reusing.</p><p>Sooner or later, there should have been somebody who would say,
“<em>Wait, but what if I want something timeproof like BM25 but with semantic understanding?</em>”</p><h2 id=sparse-neural-retrieval-evolution>Sparse Neural Retrieval Evolution</h2><p>Imagine searching for a “<em>flabbergasting murder</em>” story.
”<em>Flabbergasting</em>” is a rarely used word, so a keyword-based retriever, for example, BM25, will assign huge importance to it.
Consequently, there is a high chance that a text unrelated to any crimes but mentioning something “<em>flabbergasting</em>” will pop up in the top results.</p><p>What if we could instead of relying on term frequency in a document as a proxy of term’s importance as it happens in BM25,
directly predict a term’s importance? The goal is for rare but non-impactful terms to be assigned a much smaller weight than important terms with the same frequency, while both would be equally treated in the BM25 scenario.</p><p>How can we determine if one term is more important than another?
Word impact is related to its meaning, and its meaning can be derived from its context (words which surround this particular word).
That’s how dense contextual embedding models come into the picture.</p><p>All the sparse retrievers are based on the idea of taking a model which produces contextual dense vector representations for terms
and teaching it to produce sparse ones. Very often,
<a href=https://huggingface.co/docs/transformers/en/model_doc/bert target=_blank rel="noopener nofollow">Bidirectional Encoder Representations from the Transformers (BERT)</a> is used as a
base model, and a very simple trainable neural network is added on top of it to sparsify the representations out.
Training this small neural network is usually done by sampling from the <a href=https://microsoft.github.io/msmarco/ target=_blank rel="noopener nofollow">MS MARCO</a> dataset a query,
relevant and irrelevant to it documents and shifting the parameters of the neural network in the direction of relevancy.</p><h3 id=the-pioneer-of-sparse-neural-retrieval>The Pioneer Of Sparse Neural Retrieval</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/DeepCT.png alt="Deep Contextualized Term Weighting (DeepCT)">
The authors of one of the first sparse retrievers, the <a href=https://arxiv.org/pdf/1910.10687 target=_blank rel="noopener nofollow"><code>Deep Contextualized Term Weighting framework (DeepCT)</code></a>,
predict an integer word’s impact value separately for each unique word in a document and a query.
They use a linear regression model on top of the contextual representations produced by the basic BERT model, the model&rsquo;s output is rounded.</p><p>When documents are uploaded into a database, the importance of words in a document is predicted by a trained linear regression model
and stored in the inverted index in the same way as term frequencies in BM25 retrievers.
Then, the retrieval process is identical to the BM25 one.</p><p><em><strong>Why is DeepCT not a perfect solution?</strong></em> To train linear regression, the authors needed to provide the true value (<strong>ground truth</strong>)
of each word’s importance so the model could “see” what the right answer should be.
This score is hard to define in a way that it truly expresses the query-document relevancy.
Which score should have the most relevant word to a query when this word is taken from a five-page document? The second relevant? The third?</p><h3 id=sparse-neural-retrieval-on-relevance-objective>Sparse Neural Retrieval on Relevance Objective</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/DeepImpact.png alt=DeepImpact>
It’s much easier to define whether a document as a whole is relevant or irrelevant to a query.
That’s why the <a href=https://arxiv.org/pdf/2104.12016 target=_blank rel="noopener nofollow"><code>DeepImpact</code></a> Sparse Neural Retriever authors directly used the relevancy between a query and a document as a training objective.
They take BERT’s contextualized embeddings of the document’s words, transform them through a simple 2-layer neural network in a single scalar
score and sum these scores up for each word overlapping with a query.
The training objective is to make this score reflect the relevance between the query and the document.</p><p><em><strong>Why is DeepImpact not a perfect solution?</strong></em>
When converting texts into dense vector representations,
the BERT model does not work on a word level. Sometimes, it breaks the words into parts.
For example, the word “<em>vector</em>” will be processed by BERT as one piece, but for some words that, for example,
BERT hasn’t seen before, it is going to cut the word in pieces
<a href=https://huggingface.co/spaces/Xenova/the-tokenizer-playground target=_blank rel="noopener nofollow">as “Qdrant” turns to “Q”, “#dra” and “#nt”</a></p><p>The DeepImpact model (like the DeepCT model) takes the first piece BERT produces for a word and discards the rest.
However, what can one find searching for “<em>Q</em>” instead of “<em>Qdrant</em>”?</p><h3 id=know-thine-tokenization>Know Thine Tokenization</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/TILDEv2.png alt="Term Independent Likelihood MoDEl v2 (TILDE v2)">
To solve the problems of DeepImpact&rsquo;s architecture, the <a href=https://arxiv.org/pdf/2108.08513 target=_blank rel="noopener nofollow"><code>Term Independent Likelihood MoDEl (TILDEv2)</code></a> model generates
sparse encodings on a level of BERT’s representations, not on words level. Aside from that, its authors use the identical architecture
to the DeepImpact model.</p><p><em><strong>Why is TILDEv2 not a perfect solution?</strong></em>
A single scalar importance score value might not be enough to capture all distinct meanings of a word.
<strong>Homonyms</strong> (pizza, cocktail, flower, and female name “<em>Margherita</em>”) are one of the troublemakers in information retrieval.</p><h3 id=sparse-neural-retriever-which-understood-homonyms>Sparse Neural Retriever Which Understood Homonyms</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/COIL.png alt="COntextualized Inverted List (COIL)"></p><p>If one value for the term importance score is insufficient, we could describe the term’s importance in a vector form!
Authors of the <a href=https://arxiv.org/pdf/2104.07186 target=_blank rel="noopener nofollow"><code>COntextualized Inverted List (COIL)</code></a> model based their work on this idea.
Instead of squeezing 768-dimensional BERT’s contextualised embeddings into one value,
they down-project them (through the similar “relevance” training objective) to 32 dimensions.
Moreover, not to miss a detail, they also encode the query terms as vectors.</p><p>For each vector representing a query token, COIL finds the closest match (using the maximum dot product) vector of the same token in a document.
So, for example, if we are searching for “<em>Revolut bank &lt;finance institution></em>” and a document in a database has the sentence
“<em>Vivid bank &lt;finance institution> was moved to the bank of Amstel &lt;river></em>”, out of two “banks”,
the first one will have a bigger value of a dot product with a “<em>bank</em>” in the query, and it will count towards the final score.
The final relevancy score of a document is a sum of scores of query terms matched.</p><p><em><strong>Why is COIL not a perfect solution?</strong></em> This way of defining the importance score captures deeper semantics;
more meaning comes with more values used to describe it.
However, storing 32-dimensional vectors for every term is far more expensive,
and an inverted index does not work as-is with this architecture.</p><h3 id=back-to-the-roots>Back to the Roots</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/UNICOIL.png alt="Universal COntextualized Inverted List (UniCOIL)">
<a href=https://arxiv.org/pdf/2106.14807 target=_blank rel="noopener nofollow"><code>Universal COntextualized Inverted List (UniCOIL)</code></a>, made by the authors of COIL as a follow-up, goes back to producing a scalar value as the importance score
rather than a vector, leaving unchanged all other COIL design decisions.<br>It optimizes resources consumption but the deep semantics understanding tied to COIL architecture is again lost.</p><h2 id=did-we-solve-the-vocabulary-mismatch-yet>Did we Solve the Vocabulary Mismatch Yet?</h2><p>With the retrieval based on the exact matching,
however sophisticated the methods to predict term importance are, we can’t match relevant documents which have no query terms in them.
If you’re searching for “<em>pizza</em>” in a book of recipes, you won’t find “<em>Margherita</em>”.</p><p>A way to solve this problem is through the so-called <strong>document expansion</strong>.
Let’s append words which could be in a potential query searching for this document.
So, the “<em>Margherita</em>” document becomes “<em>Margherita pizza</em>”. Now, exact matching on “<em>pizza</em>” will work!</p><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/DocumentExpansion.png alt="Document Expansion"></p><p>There are two types of document expansion that are used in sparse neural retrieval:
<strong>external</strong> (one model is responsible for expansion, another one for retrieval) and <strong>internal</strong> (all is done by a single model).</p><h3 id=external-document-expansion>External Document Expansion</h3><p>External document expansion uses a <strong>generative model</strong> (Mistral 7B, Chat-GPT, and Claude are all generative models,
generating words based on the input text) to compose additions to documents before converting them to sparse representations
and applying exact matching methods.</p><h4 id=external-document-expansion-with-doct5query>External Document Expansion with docT5query</h4><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/docT5queryDocumentExpansion.png alt="External Document Expansion with docT5query">
<a href=https://github.com/castorini/docTTTTTquery target=_blank rel="noopener nofollow"><code>docT5query</code></a> is the most used document expansion model.
It is based on the <a href=https://huggingface.co/docs/transformers/en/model_doc/t5 target=_blank rel="noopener nofollow">Text-to-Text Transfer Transformer (T5)</a> model trained to
generate top-k possible queries for which the given document would be an answer.
These predicted short queries (up to ~50-60 words) can have repetitions in them,
so it also contributes to the frequency of the terms if the term frequency is considered by the retriever.</p><p>The problem with docT5query expansion is a very long inference time, as with any generative model:
it can generate only one token per run, and it spends a fair share of resources on it.</p><h4 id=external-document-expansion-with-term-independent-likelihood-model-tilde>External Document Expansion with Term Independent Likelihood MODel (TILDE)</h4><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/TILDEDocumentExpansion.png alt="External Document Expansion with Term Independent Likelihood MODel (TILDE)"></p><p><a href=https://github.com/ielab/TILDE target=_blank rel="noopener nofollow"><code>Term Independent Likelihood MODel (TILDE)</code></a> is an external expansion method that reduces the passage expansion time compared to
docT5query by 98%. It uses the assumption that words in texts are independent of each other
(as if we were inserting in our speech words without paying attention to their order), which allows for the parallelisation of document expansion.</p><p>Instead of predicting queries, TILDE predicts the most likely terms to see next after reading a passage’s text
(<strong>query likelihood paradigm</strong>). TILDE takes the probability distribution of all tokens in a BERT vocabulary based on the document’s text
and appends top-k of them to the document without repetitions.</p><p><em><strong>Problems of external document expansion:</strong></em> External document expansion might not be feasible in many production scenarios where there’s not enough time or compute to expand each and every
document you want to store in a database and then additionally do all the calculations needed for retrievers.
To solve this problem, a generation of models was developed which do everything in one go, expanding documents “internally”.</p><h3 id=internal-document-expansion>Internal Document Expansion</h3><p>Let’s assume we don’t care about the context of query terms, so we can treat them as independent words that we combine in random order to get
the result. Then, for each contextualized term in a document, we are free to pre-compute how this term affects every word in our vocabulary.</p><p>For each document, a vector of the vocabulary length is created. To fill this vector in, for each word in the vocabulary, it is checked if the
influence of any document term on it is big enough to consider it. Otherwise, the vocabulary word’s score in a document vector will be zero.
For example, by pre-computing vectors for the document “<em>pizza Margherita</em>” on a vocabulary of 50,000 most used English words,
for this small document of two words, we will get a 50,000-dimensional vector of zeros, where non-zero values will be for a “<em>pizza</em>”, “<em>pizzeria</em>”,
“<em>flower</em>”, “<em>woman</em>”, “<em>girl</em>”, &ldquo;<em>Margherita</em>&rdquo;, “<em>cocktail</em>” and “<em>pizzaiolo</em>”.</p><h3 id=sparse-neural-retriever-with-internal-document-expansion>Sparse Neural Retriever with Internal Document Expansion</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/SPARTA.png alt="Sparse Transformer Matching (SPARTA)"></p><p>The authors of the <a href=https://arxiv.org/pdf/2009.13013 target=_blank rel="noopener nofollow"><code>Sparse Transformer Matching (SPARTA)</code></a> model use BERT’s model and BERT’s vocabulary (around 30,000 tokens).
For each token in BERT vocabulary, they find the maximum dot product between it and contextualized tokens in a document
and learn a threshold of a considerable (non-zero) effect.
Then, at the inference time, the only thing to be done is to sum up all scores of query tokens in that document.</p><p><em><strong>Why is SPARTA not a perfect solution?</strong></em> Trained on the MS MARCO dataset, many sparse neural retrievers, including SPARTA,
show good results on MS MARCO test data, but when it comes to generalisation (working with other data), they
<a href=https://arxiv.org/pdf/2307.10488 target=_blank rel="noopener nofollow">could perform worse than BM25</a>.</p><h3 id=state-of-the-art-of-modern-sparse-neural-retrieval>State-of-the-Art of Modern Sparse Neural Retrieval</h3><p><img src=https://qdrant.tech/articles_data/modern-sparse-neural-retrieval/SPLADE++.png alt="Sparse Lexical and Expansion Model Plus Plus, (SPLADE++)">
The authors of the <a href=https://arxiv.org/pdf/2109.10086 target=_blank rel="noopener nofollow"><code>Sparse Lexical and Expansion Model (SPLADE)]</code></a> family of models added dense model training tricks to the
internal document expansion idea, which made the retrieval quality noticeably better.</p><ul><li>The SPARTA model is not sparse enough by construction, so authors of the SPLADE family of models introduced explicit <strong>sparsity regularisation</strong>,
preventing the model from producing too many non-zero values.</li><li>The SPARTA model mostly uses the BERT model as-is, without any additional neural network to capture the specifity of Information Retrieval problem,
so SPLADE models introduce a trainable neural network on top of BERT with a specific architecture choice to make it perfectly fit the task.</li><li>SPLADE family of models, finally, uses <strong>knowledge distillation</strong>, which is learning from a bigger
(and therefore much slower, not-so-fit for production tasks) model how to predict good representations.</li></ul><p>One of the last versions of the SPLADE family of models is <a href=https://arxiv.org/pdf/2205.04733 target=_blank rel="noopener nofollow"><code>SPLADE++</code></a>.<br>SPLADE++, opposed to SPARTA model, expands not only documents but also queries at inference time.
We’ll demonstrate this in the next section.</p><h2 id=splade-in-qdrant>SPLADE++ in Qdrant</h2><p>In Qdrant, you can use <a href=https://arxiv.org/pdf/2205.04733 target=_blank rel="noopener nofollow"><code>SPLADE++</code></a> easily with our lightweight library for embeddings called <a href=https://qdrant.tech/documentation/fastembed/ target=_blank rel="noopener nofollow">FastEmbed</a>.</p><h4 id=setup>Setup</h4><p>Install <code>FastEmbed</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pip</span> <span class=n>install</span> <span class=n>fastembed</span>
</span></span></code></pre></div><p>Import sparse text embedding models supported in FastEmbed.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fastembed</span> <span class=kn>import</span> <span class=n>SparseTextEmbedding</span>
</span></span></code></pre></div><p>You can list all sparse text embedding models currently supported.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>SparseTextEmbedding</span><span class=o>.</span><span class=n>list_supported_models</span><span class=p>()</span>
</span></span></code></pre></div><details><summary>Output with a list of supported models</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>[{</span><span class=s1>&#39;model&#39;</span>: <span class=s1>&#39;prithivida/Splade_PP_en_v1&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;vocab_size&#39;</span>: 30522,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;description&#39;</span>: <span class=s1>&#39;Independent Implementation of SPLADE++ Model for English&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;size_in_GB&#39;</span>: 0.532,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;sources&#39;</span>: <span class=o>{</span><span class=s1>&#39;hf&#39;</span>: <span class=s1>&#39;Qdrant/SPLADE_PP_en_v1&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;model_file&#39;</span>: <span class=s1>&#39;model.onnx&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl> <span class=o>{</span><span class=s1>&#39;model&#39;</span>: <span class=s1>&#39;prithvida/Splade_PP_en_v1&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;vocab_size&#39;</span>: 30522,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;description&#39;</span>: <span class=s1>&#39;Independent Implementation of SPLADE++ Model for English&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;size_in_GB&#39;</span>: 0.532,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;sources&#39;</span>: <span class=o>{</span><span class=s1>&#39;hf&#39;</span>: <span class=s1>&#39;Qdrant/SPLADE_PP_en_v1&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;model_file&#39;</span>: <span class=s1>&#39;model.onnx&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl> <span class=o>{</span><span class=s1>&#39;model&#39;</span>: <span class=s1>&#39;Qdrant/bm42-all-minilm-l6-v2-attentions&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;vocab_size&#39;</span>: 30522,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;description&#39;</span>: <span class=s1>&#39;Light sparse embedding model, which assigns an importance score to each token in the text&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;size_in_GB&#39;</span>: 0.09,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;sources&#39;</span>: <span class=o>{</span><span class=s1>&#39;hf&#39;</span>: <span class=s1>&#39;Qdrant/all_miniLM_L6_v2_with_attentions&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;model_file&#39;</span>: <span class=s1>&#39;model.onnx&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;additional_files&#39;</span>: <span class=o>[</span><span class=s1>&#39;stopwords.txt&#39;</span><span class=o>]</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;requires_idf&#39;</span>: True<span class=o>}</span>,
</span></span><span class=line><span class=cl> <span class=o>{</span><span class=s1>&#39;model&#39;</span>: <span class=s1>&#39;Qdrant/bm25&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;description&#39;</span>: <span class=s1>&#39;BM25 as sparse embeddings meant to be used with Qdrant&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;size_in_GB&#39;</span>: 0.01,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;sources&#39;</span>: <span class=o>{</span><span class=s1>&#39;hf&#39;</span>: <span class=s1>&#39;Qdrant/bm25&#39;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;model_file&#39;</span>: <span class=s1>&#39;mock.file&#39;</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;additional_files&#39;</span>: <span class=o>[</span><span class=s1>&#39;arabic.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;azerbaijani.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;basque.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;bengali.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;catalan.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;chinese.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;danish.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;dutch.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;english.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;finnish.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;french.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;german.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;greek.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;hebrew.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;hinglish.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;hungarian.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;indonesian.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;italian.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;kazakh.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;nepali.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;norwegian.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;portuguese.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;romanian.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;russian.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;slovene.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;spanish.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;swedish.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;tajik.txt&#39;</span>,
</span></span><span class=line><span class=cl>   <span class=s1>&#39;turkish.txt&#39;</span><span class=o>]</span>,
</span></span><span class=line><span class=cl>  <span class=s1>&#39;requires_idf&#39;</span>: True<span class=o>}]</span>
</span></span></code></pre></div></details><p>Load SPLADE++.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sparse_model_name</span> <span class=o>=</span> <span class=s2>&#34;prithivida/Splade_PP_en_v1&#34;</span>
</span></span><span class=line><span class=cl><span class=n>sparse_model</span> <span class=o>=</span> <span class=n>SparseTextEmbedding</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=n>sparse_model_name</span><span class=p>)</span>
</span></span></code></pre></div><p>The model files will be fetched and downloaded, with progress showing.</p><h4 id=embed-data>Embed data</h4><p>We will use a toy movie description dataset.</p><details><summary>Movie description dataset</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>descriptions</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;In 1431, Jeanne d&#39;Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend&#39;s father&#39;s pocketwatch.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A petty thief with an utter resemblance to a samurai warlord is hired as the lord&#39;s double. When the warlord later dies the thief is forced to take up arms in his place.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;When a machine that allows therapists to enter their patients&#39; dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A world-weary political journalist picks up the story of a woman&#39;s search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it&#39;s up to Woody to convince the other toys that they weren&#39;t abandoned and to return home.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;Story of 40-man Turkish task force who must defend a relay station.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;Spinal Tap, one of England&#39;s loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl> <span class=s2>&#34;Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.&#34;</span><span class=p>]</span>
</span></span></code></pre></div></details><p>Embed movie descriptions with SPLADE++.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sparse_descriptions</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>sparse_model</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=n>descriptions</span><span class=p>))</span>
</span></span></code></pre></div><p>You can check how a sparse vector generated by SPLADE++ looks in Qdrant.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sparse_descriptions</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><p>It is stored as <strong>indices</strong> of BERT tokens, weights of which are non-zero, and <strong>values</strong> of these weights.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>SparseEmbedding<span class=o>(</span>
</span></span><span class=line><span class=cl>  <span class=nv>values</span><span class=o>=</span>array<span class=o>([</span>1.57449973, 0.90787691, ..., 1.21796167, 1.1321187<span class=o>])</span>,
</span></span><span class=line><span class=cl>  <span class=nv>indices</span><span class=o>=</span>array<span class=o>([</span> 1040,  2001, ..., 28667, 29137<span class=o>])</span>
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span></code></pre></div><h4 id=upload-embeddings-to-qdrant>Upload Embeddings to Qdrant</h4><p>Install <code>qdrant-client</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pip</span> <span class=n>install</span> <span class=n>qdrant</span><span class=o>-</span><span class=n>client</span>
</span></span></code></pre></div><p>Qdrant Client has a simple in-memory mode that allows you to experiment locally on small data volumes.
Alternatively, you could use for experiments <a href=https://qdrant.tech/documentation/cloud/create-cluster/#create-a-cluster target=_blank rel="noopener nofollow">a free tier cluster</a>
in Qdrant Cloud.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>qdrant_client</span> <span class=kn>import</span> <span class=n>QdrantClient</span><span class=p>,</span> <span class=n>models</span>
</span></span><span class=line><span class=cl><span class=n>qdrant_client</span> <span class=o>=</span> <span class=n>QdrantClient</span><span class=p>(</span><span class=s2>&#34;:memory:&#34;</span><span class=p>)</span> <span class=c1># Qdrant is running from RAM.</span>
</span></span></code></pre></div><p>Now, let&rsquo;s create a <a href=https://qdrant.tech/documentation/concepts/collections/ target=_blank rel="noopener nofollow">collection</a> in which could upload our sparse SPLADE++ embeddings.<br>For that, we will use the <a href=https://qdrant.tech/documentation/concepts/vectors/#sparse-vectors target=_blank rel="noopener nofollow">sparse vectors</a> representation supported in Qdrant.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>qdrant_client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>vectors_config</span><span class=o>=</span><span class=p>{},</span>
</span></span><span class=line><span class=cl>    <span class=n>sparse_vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;film_description&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>SparseVectorParams</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>To make this collection human-readable, let&rsquo;s save movie metadata (name, description and movie&rsquo;s length) together with an embeddings.</p><details><summary>Movie metadata</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>metadata</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;The Passion of Joan of Arc&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>114</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;In 1431, Jeanne d&#39;Arc is placed on trial on charges of heresy. The ecclesiastical jurists attempt to force Jeanne to recant her claims of holy visions.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Sherlock Jr.&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>45</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A film projectionist longs to be a detective, and puts his meagre skills to work when he is framed by a rival for stealing his girlfriend&#39;s father&#39;s pocketwatch.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Heat&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>170</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A group of high-end professional thieves start to feel the heat from the LAPD when they unknowingly leave a clue at their latest heist.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Kagemusha&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>162</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A petty thief with an utter resemblance to a samurai warlord is hired as the lord&#39;s double. When the warlord later dies the thief is forced to take up arms in his place.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Kubo and the Two Strings&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>101</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A young boy named Kubo must locate a magical suit of armour worn by his late father in order to defeat a vengeful spirit from the past.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Sardar Udham&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>164</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A biopic detailing the 2 decades that Punjabi Sikh revolutionary Udham Singh spent planning the assassination of the man responsible for the Jallianwala Bagh massacre.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Paprika&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>90</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;When a machine that allows therapists to enter their patients&#39; dreams is stolen, all hell breaks loose. Only a young female therapist, Paprika, can stop it.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;After Hours&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>97</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;An ordinary word processor has the worst night of his life after he agrees to visit a girl in Soho whom he met that evening at a coffee shop.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Udta Punjab&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>148</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A story that revolves around drug abuse in the affluent north Indian State of Punjab and how the youth there have succumbed to it en-masse resulting in a socio-economic decline.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Philomena&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>98</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A world-weary political journalist picks up the story of a woman&#39;s search for her son, who was taken away from her decades ago after she became pregnant and was forced to live in a convent.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Neon Genesis Evangelion: The End of Evangelion&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>87</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;Concurrent theatrical ending of the TV series Neon Genesis Evangelion (1995).&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;The Dirty Dozen&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>150</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;During World War II, a rebellious U.S. Army Major is assigned a dozen convicted murderers to train and lead them into a mass assassination mission of German officers.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Toy Story 3&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>103</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;The toys are mistakenly delivered to a day-care center instead of the attic right before Andy leaves for college, and it&#39;s up to Woody to convince the other toys that they weren&#39;t abandoned and to return home.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Edge of Tomorrow&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>113</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A soldier fighting aliens gets to relive the same day over and over again, the day restarting every time he dies.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Some Like It Hot&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>121</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;After two male musicians witness a mob hit, they flee the state in an all-female band disguised as women, but further complications set in.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Snow White and the Seven Dwarfs&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>83</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;Exiled into the dangerous forest by her wicked stepmother, a princess is rescued by seven dwarf miners who make her part of their household.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;It Happened One Night&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>105</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;A renegade reporter trailing a young runaway heiress for a big story joins her on a bus heading from Florida to New York, and they end up stuck with each other when the bus leaves them behind at one of the stops.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Nefes: Vatan Sagolsun&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>128</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;Story of 40-man Turkish task force who must defend a relay station.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;This Is Spinal Tap&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>82</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;Spinal Tap, one of England&#39;s loudest bands, is chronicled by film director Marty DiBergi on what proves to be a fateful tour.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>{</span><span class=s2>&#34;movie_name&#34;</span><span class=p>:</span> <span class=s2>&#34;Let the Right One In&#34;</span><span class=p>,</span> <span class=s2>&#34;movie_watch_time_min&#34;</span><span class=p>:</span> <span class=mi>114</span><span class=p>,</span> <span class=s2>&#34;movie_description&#34;</span><span class=p>:</span> <span class=s2>&#34;Oskar, an overlooked and bullied boy, finds love and revenge through Eli, a beautiful but peculiar girl.&#34;</span><span class=p>}]</span>
</span></span></code></pre></div></details><p>Upload embedded descriptions with movie metadata into the collection.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>qdrant_client</span><span class=o>.</span><span class=n>upsert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>points</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>PointStruct</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=nb>id</span><span class=o>=</span><span class=n>idx</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>payload</span><span class=o>=</span><span class=n>metadata</span><span class=p>[</span><span class=n>idx</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>vector</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;film_description&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>SparseVector</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>indices</span><span class=o>=</span><span class=n>vector</span><span class=o>.</span><span class=n>indices</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>values</span><span class=o>=</span><span class=n>vector</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>vector</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>sparse_descriptions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><aside role=status>You can also implicitly generate sparse vectors using built-in FastEmbed integration.</aside><details><summary>Implicitly generate sparse vectors (Click to expand)</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>qdrant_client</span><span class=o>.</span><span class=n>upsert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>points</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>PointStruct</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=nb>id</span><span class=o>=</span><span class=n>idx</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>payload</span><span class=o>=</span><span class=n>metadata</span><span class=p>[</span><span class=n>idx</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>vector</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;film_description&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>Document</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>text</span><span class=o>=</span><span class=n>description</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>sparse_model_name</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>description</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>descriptions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div></details><h4 id=querying>Querying</h4><p>Let’s query our collection!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>sparse_model</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=s2>&#34;A movie about music&#34;</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>qdrant_client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>SparseVector</span><span class=p>(</span><span class=n>indices</span><span class=o>=</span><span class=n>query_embedding</span><span class=o>.</span><span class=n>indices</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>query_embedding</span><span class=o>.</span><span class=n>values</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;film_description&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_vectors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_payload</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div><details><summary>Implicitly generate sparse vectors (Click to expand)</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>qdrant_client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Document</span><span class=p>(</span><span class=n>text</span><span class=o>=</span><span class=s2>&#34;A movie about music&#34;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>sparse_model_name</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;film_description&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_vectors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_payload</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>)</span>
</span></span></code></pre></div></details><p>Output looks like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>points</span><span class=o>=[</span>ScoredPoint<span class=o>(</span>
</span></span><span class=line><span class=cl>  <span class=nv>id</span><span class=o>=</span>18, 
</span></span><span class=line><span class=cl>  <span class=nv>version</span><span class=o>=</span>0, 
</span></span><span class=line><span class=cl>  <span class=nv>score</span><span class=o>=</span>9.6779785, 
</span></span><span class=line><span class=cl>  <span class=nv>payload</span><span class=o>={</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;movie_name&#39;</span>: <span class=s1>&#39;This Is Spinal Tap&#39;</span>, 
</span></span><span class=line><span class=cl>    <span class=s1>&#39;movie_watch_time_min&#39;</span>: 82, 
</span></span><span class=line><span class=cl>    <span class=s1>&#39;movie_description&#39;</span>: <span class=s2>&#34;Spinal Tap, one of England&#39;s loudest bands, 
</span></span></span><span class=line><span class=cl><span class=s2>    is chronicled by film director Marty DiBergi on what proves to be a fateful tour.&#34;</span>
</span></span><span class=line><span class=cl>  <span class=o>}</span>, 
</span></span><span class=line><span class=cl>  <span class=nv>vector</span><span class=o>={</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;film_description&#39;</span>: SparseVector<span class=o>(</span>
</span></span><span class=line><span class=cl>      <span class=nv>indices</span><span class=o>=[</span>1010, 2001, ..., 25316, 25517<span class=o>]</span>, 
</span></span><span class=line><span class=cl>      <span class=nv>values</span><span class=o>=[</span>0.49717945, 0.19760133, ..., 1.2124698, 0.58689135<span class=o>])</span>
</span></span><span class=line><span class=cl>  <span class=o>}</span>, 
</span></span><span class=line><span class=cl>  <span class=nv>shard_key</span><span class=o>=</span>None, 
</span></span><span class=line><span class=cl>  <span class=nv>order_value</span><span class=o>=</span>None
</span></span><span class=line><span class=cl><span class=o>)]</span>
</span></span></code></pre></div><p>As you can see, there are no overlapping words in the query and a description of a found movie,
even though the answer fits the query, and yet we’re working with <strong>exact matching</strong>.<br>This is possible due to the <strong>internal expansion</strong> of the query and the document that SPLADE++ does.</p><h4 id=internal-expansion-by-splade>Internal Expansion by SPLADE++</h4><p>Let’s check how did SPLADE++ expand the query and the document we got as an answer.<br>For that, we will need to use the HuggingFace library called <a href=https://huggingface.co/docs/tokenizers/en/index target=_blank rel="noopener nofollow">Tokenizers</a>.
With it, we will be able to decode back to human-readable format <strong>indices</strong> of words in a vocabulary SPLADE++ uses.</p><p>Firstly we will need to install this library.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>pip</span> <span class=n>install</span> <span class=n>tokenizers</span>
</span></span></code></pre></div><p>Then, let&rsquo;s write a function which will decode SPLADE++ sparse embeddings and return words SPLADE++ uses for encoding the input.<br>We would like to return them in the descending order based on the weight (<strong>impact score</strong>), SPLADE++ assigned them.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tokenizers</span> <span class=kn>import</span> <span class=n>Tokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>Tokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s1>&#39;Qdrant/SPLADE_PP_en_v1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_tokens_and_weights</span><span class=p>(</span><span class=n>sparse_embedding</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>token_weight_dict</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>sparse_embedding</span><span class=o>.</span><span class=n>indices</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>token</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>([</span><span class=n>sparse_embedding</span><span class=o>.</span><span class=n>indices</span><span class=p>[</span><span class=n>i</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>        <span class=n>weight</span> <span class=o>=</span> <span class=n>sparse_embedding</span><span class=o>.</span><span class=n>values</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>token_weight_dict</span><span class=p>[</span><span class=n>token</span><span class=p>]</span> <span class=o>=</span> <span class=n>weight</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Sort the dictionary by weights</span>
</span></span><span class=line><span class=cl>    <span class=n>token_weight_dict</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>sorted</span><span class=p>(</span><span class=n>token_weight_dict</span><span class=o>.</span><span class=n>items</span><span class=p>(),</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>item</span><span class=p>:</span> <span class=n>item</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>reverse</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>token_weight_dict</span>
</span></span></code></pre></div><p>Firstly, we apply our function to the query.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>sparse_model</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=s2>&#34;A movie about music&#34;</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>get_tokens_and_weights</span><span class=p>(</span><span class=n>query_embedding</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>))</span>
</span></span></code></pre></div><p>That’s how SPLADE++ expanded the query:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;music&#34;</span>: 2.764289617538452,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;movie&#34;</span>: 2.674748420715332,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;film&#34;</span>: 2.3489091396331787,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;musical&#34;</span>: 2.276120901107788,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;about&#34;</span>: 2.124547004699707,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;movies&#34;</span>: 1.3825485706329346,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;song&#34;</span>: 1.2893378734588623,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;genre&#34;</span>: 0.9066758751869202,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;songs&#34;</span>: 0.8926399946212769,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;a&#34;</span>: 0.8900706768035889,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;musicians&#34;</span>: 0.5638002157211304,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sound&#34;</span>: 0.49310919642448425,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;musician&#34;</span>: 0.46415239572525024,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;drama&#34;</span>: 0.462990403175354,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;tv&#34;</span>: 0.4398191571235657,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;book&#34;</span>: 0.38950803875923157,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;documentary&#34;</span>: 0.3758136034011841,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;hollywood&#34;</span>: 0.29099565744400024,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;story&#34;</span>: 0.2697228491306305,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;nature&#34;</span>: 0.25306591391563416,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;concerning&#34;</span>: 0.205053448677063,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;game&#34;</span>: 0.1546829640865326,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;rock&#34;</span>: 0.11775632947683334,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;definition&#34;</span>: 0.08842901140451431,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;love&#34;</span>: 0.08636035025119781,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;soundtrack&#34;</span>: 0.06807517260313034,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;religion&#34;</span>: 0.053535860031843185,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;filmed&#34;</span>: 0.025964470580220222,
</span></span><span class=line><span class=cl>    <span class=s2>&#34;sounds&#34;</span>: 0.0004048719711136073
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>Then, we apply our function to the answer.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>query_embedding</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>sparse_model</span><span class=o>.</span><span class=n>embed</span><span class=p>(</span><span class=s2>&#34;A movie about music&#34;</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>qdrant_client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>SparseVector</span><span class=p>(</span><span class=n>indices</span><span class=o>=</span><span class=n>query_embedding</span><span class=o>.</span><span class=n>indices</span><span class=p>,</span> <span class=n>values</span><span class=o>=</span><span class=n>query_embedding</span><span class=o>.</span><span class=n>values</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;film_description&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_vectors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_payload</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>get_tokens_and_weights</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>points</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>vector</span><span class=p>[</span><span class=s1>&#39;film_description&#39;</span><span class=p>],</span> <span class=n>tokenizer</span><span class=p>))</span>
</span></span></code></pre></div><details><summary>Implicitly generate sparse vectors (Click to expand)</summary><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>qdrant_client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;movies&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Document</span><span class=p>(</span><span class=n>text</span><span class=o>=</span><span class=s2>&#34;A movie about music&#34;</span><span class=p>,</span> <span class=n>model</span><span class=o>=</span><span class=n>sparse_model_name</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;film_description&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_vectors</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_payload</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>get_tokens_and_weights</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>points</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>vector</span><span class=p>[</span><span class=s2>&#34;film_description&#34;</span><span class=p>],</span> <span class=n>tokenizer</span><span class=p>))</span>
</span></span></code></pre></div></details><p>And that&rsquo;s how SPLADE++ expanded the answer.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>{</span><span class=s1>&#39;spinal&#39;</span><span class=p>:</span> <span class=mf>2.6548674</span><span class=p>,</span> <span class=s1>&#39;tap&#39;</span><span class=p>:</span> <span class=mf>2.534881</span><span class=p>,</span> <span class=s1>&#39;marty&#39;</span><span class=p>:</span> <span class=mf>2.223297</span><span class=p>,</span> <span class=s1>&#39;##berg&#39;</span><span class=p>:</span> <span class=mf>2.0402722</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;##ful&#39;</span><span class=p>:</span> <span class=mf>2.0030282</span><span class=p>,</span> <span class=s1>&#39;fate&#39;</span><span class=p>:</span> <span class=mf>1.935915</span><span class=p>,</span> <span class=s1>&#39;loud&#39;</span><span class=p>:</span> <span class=mf>1.8381964</span><span class=p>,</span> <span class=s1>&#39;spine&#39;</span><span class=p>:</span> <span class=mf>1.7507898</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;di&#39;</span><span class=p>:</span> <span class=mf>1.6161551</span><span class=p>,</span> <span class=s1>&#39;bands&#39;</span><span class=p>:</span> <span class=mf>1.5897619</span><span class=p>,</span> <span class=s1>&#39;band&#39;</span><span class=p>:</span> <span class=mf>1.589473</span><span class=p>,</span> <span class=s1>&#39;uk&#39;</span><span class=p>:</span> <span class=mf>1.5385966</span><span class=p>,</span> <span class=s1>&#39;tour&#39;</span><span class=p>:</span> <span class=mf>1.4758654</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;chronicle&#39;</span><span class=p>:</span> <span class=mf>1.4577943</span><span class=p>,</span> <span class=s1>&#39;director&#39;</span><span class=p>:</span> <span class=mf>1.4423795</span><span class=p>,</span> <span class=s1>&#39;england&#39;</span><span class=p>:</span> <span class=mf>1.4301306</span><span class=p>,</span> <span class=s1>&#39;##est&#39;</span><span class=p>:</span> <span class=mf>1.3025658</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;taps&#39;</span><span class=p>:</span> <span class=mf>1.2124698</span><span class=p>,</span> <span class=s1>&#39;film&#39;</span><span class=p>:</span> <span class=mf>1.1069428</span><span class=p>,</span> <span class=s1>&#39;##berger&#39;</span><span class=p>:</span> <span class=mf>1.1044296</span><span class=p>,</span> <span class=s1>&#39;tapping&#39;</span><span class=p>:</span> <span class=mf>1.0424755</span><span class=p>,</span> <span class=s1>&#39;best&#39;</span><span class=p>:</span> <span class=mf>1.0327196</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;louder&#39;</span><span class=p>:</span> <span class=mf>0.9229055</span><span class=p>,</span> <span class=s1>&#39;music&#39;</span><span class=p>:</span> <span class=mf>0.9056678</span><span class=p>,</span> <span class=s1>&#39;directors&#39;</span><span class=p>:</span> <span class=mf>0.8887502</span><span class=p>,</span> <span class=s1>&#39;movie&#39;</span><span class=p>:</span> <span class=mf>0.870712</span><span class=p>,</span> <span class=s1>&#39;directing&#39;</span><span class=p>:</span> <span class=mf>0.8396196</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;sound&#39;</span><span class=p>:</span> <span class=mf>0.83609974</span><span class=p>,</span> <span class=s1>&#39;genre&#39;</span><span class=p>:</span> <span class=mf>0.803052</span><span class=p>,</span> <span class=s1>&#39;dave&#39;</span><span class=p>:</span> <span class=mf>0.80212915</span><span class=p>,</span> <span class=s1>&#39;wrote&#39;</span><span class=p>:</span> <span class=mf>0.7849579</span><span class=p>,</span> <span class=s1>&#39;hottest&#39;</span><span class=p>:</span> <span class=mf>0.7594193</span><span class=p>,</span> <span class=s1>&#39;filmed&#39;</span><span class=p>:</span> <span class=mf>0.750105</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;english&#39;</span><span class=p>:</span> <span class=mf>0.72807616</span><span class=p>,</span> <span class=s1>&#39;who&#39;</span><span class=p>:</span> <span class=mf>0.69502294</span><span class=p>,</span> <span class=s1>&#39;tours&#39;</span><span class=p>:</span> <span class=mf>0.6833075</span><span class=p>,</span> <span class=s1>&#39;club&#39;</span><span class=p>:</span> <span class=mf>0.6375339</span><span class=p>,</span> <span class=s1>&#39;vertebrae&#39;</span><span class=p>:</span> <span class=mf>0.58689135</span><span class=p>,</span> <span class=s1>&#39;chronicles&#39;</span><span class=p>:</span> <span class=mf>0.57296354</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;dance&#39;</span><span class=p>:</span> <span class=mf>0.57278687</span><span class=p>,</span> <span class=s1>&#39;song&#39;</span><span class=p>:</span> <span class=mf>0.50987065</span><span class=p>,</span> <span class=s1>&#39;,&#39;</span><span class=p>:</span> <span class=mf>0.49717945</span><span class=p>,</span> <span class=s1>&#39;british&#39;</span><span class=p>:</span> <span class=mf>0.4971719</span><span class=p>,</span> <span class=s1>&#39;writer&#39;</span><span class=p>:</span> <span class=mf>0.495709</span><span class=p>,</span> <span class=s1>&#39;directed&#39;</span><span class=p>:</span> <span class=mf>0.4875775</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;cork&#39;</span><span class=p>:</span> <span class=mf>0.475757</span><span class=p>,</span> <span class=s1>&#39;##i&#39;</span><span class=p>:</span> <span class=mf>0.47122696</span><span class=p>,</span> <span class=s1>&#39;##band&#39;</span><span class=p>:</span> <span class=mf>0.46837863</span><span class=p>,</span> <span class=s1>&#39;most&#39;</span><span class=p>:</span> <span class=mf>0.44112885</span><span class=p>,</span> <span class=s1>&#39;##liest&#39;</span><span class=p>:</span> <span class=mf>0.44084555</span><span class=p>,</span> <span class=s1>&#39;destiny&#39;</span><span class=p>:</span> <span class=mf>0.4264851</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;prove&#39;</span><span class=p>:</span> <span class=mf>0.41789067</span><span class=p>,</span> <span class=s1>&#39;is&#39;</span><span class=p>:</span> <span class=mf>0.40306947</span><span class=p>,</span> <span class=s1>&#39;famous&#39;</span><span class=p>:</span> <span class=mf>0.40230379</span><span class=p>,</span> <span class=s1>&#39;hop&#39;</span><span class=p>:</span> <span class=mf>0.3897451</span><span class=p>,</span> <span class=s1>&#39;noise&#39;</span><span class=p>:</span> <span class=mf>0.38770816</span><span class=p>,</span> <span class=s1>&#39;##iest&#39;</span><span class=p>:</span> <span class=mf>0.3737782</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;comedy&#39;</span><span class=p>:</span> <span class=mf>0.36903998</span><span class=p>,</span> <span class=s1>&#39;sport&#39;</span><span class=p>:</span> <span class=mf>0.35883865</span><span class=p>,</span> <span class=s1>&#39;quiet&#39;</span><span class=p>:</span> <span class=mf>0.3552795</span><span class=p>,</span> <span class=s1>&#39;detail&#39;</span><span class=p>:</span> <span class=mf>0.3397654</span><span class=p>,</span> <span class=s1>&#39;fastest&#39;</span><span class=p>:</span> <span class=mf>0.30345848</span><span class=p>,</span> <span class=s1>&#39;filmmaker&#39;</span><span class=p>:</span> <span class=mf>0.3013101</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;festival&#39;</span><span class=p>:</span> <span class=mf>0.28146765</span><span class=p>,</span> <span class=s1>&#39;##st&#39;</span><span class=p>:</span> <span class=mf>0.28040633</span><span class=p>,</span> <span class=s1>&#39;tram&#39;</span><span class=p>:</span> <span class=mf>0.27373192</span><span class=p>,</span> <span class=s1>&#39;well&#39;</span><span class=p>:</span> <span class=mf>0.2599603</span><span class=p>,</span> <span class=s1>&#39;documentary&#39;</span><span class=p>:</span> <span class=mf>0.24368097</span><span class=p>,</span> <span class=s1>&#39;beat&#39;</span><span class=p>:</span> <span class=mf>0.22953634</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;direction&#39;</span><span class=p>:</span> <span class=mf>0.22925079</span><span class=p>,</span> <span class=s1>&#39;hardest&#39;</span><span class=p>:</span> <span class=mf>0.22293334</span><span class=p>,</span> <span class=s1>&#39;strongest&#39;</span><span class=p>:</span> <span class=mf>0.2018861</span><span class=p>,</span> <span class=s1>&#39;was&#39;</span><span class=p>:</span> <span class=mf>0.19760133</span><span class=p>,</span> <span class=s1>&#39;oldest&#39;</span><span class=p>:</span> <span class=mf>0.19532987</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;byron&#39;</span><span class=p>:</span> <span class=mf>0.19360808</span><span class=p>,</span> <span class=s1>&#39;worst&#39;</span><span class=p>:</span> <span class=mf>0.18397793</span><span class=p>,</span> <span class=s1>&#39;touring&#39;</span><span class=p>:</span> <span class=mf>0.17598206</span><span class=p>,</span> <span class=s1>&#39;rock&#39;</span><span class=p>:</span> <span class=mf>0.17319143</span><span class=p>,</span> <span class=s1>&#39;clubs&#39;</span><span class=p>:</span> <span class=mf>0.16090117</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;popular&#39;</span><span class=p>:</span> <span class=mf>0.15969758</span><span class=p>,</span> <span class=s1>&#39;toured&#39;</span><span class=p>:</span> <span class=mf>0.15917331</span><span class=p>,</span> <span class=s1>&#39;trick&#39;</span><span class=p>:</span> <span class=mf>0.1530599</span><span class=p>,</span> <span class=s1>&#39;celebrity&#39;</span><span class=p>:</span> <span class=mf>0.14458777</span><span class=p>,</span> <span class=s1>&#39;musical&#39;</span><span class=p>:</span> <span class=mf>0.13888633</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;filming&#39;</span><span class=p>:</span> <span class=mf>0.1363699</span><span class=p>,</span> <span class=s1>&#39;culture&#39;</span><span class=p>:</span> <span class=mf>0.13616633</span><span class=p>,</span> <span class=s1>&#39;groups&#39;</span><span class=p>:</span> <span class=mf>0.1340591</span><span class=p>,</span> <span class=s1>&#39;ski&#39;</span><span class=p>:</span> <span class=mf>0.13049376</span><span class=p>,</span> <span class=s1>&#39;venue&#39;</span><span class=p>:</span> <span class=mf>0.12992987</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;style&#39;</span><span class=p>:</span> <span class=mf>0.12853126</span><span class=p>,</span> <span class=s1>&#39;history&#39;</span><span class=p>:</span> <span class=mf>0.12696269</span><span class=p>,</span> <span class=s1>&#39;massage&#39;</span><span class=p>:</span> <span class=mf>0.11969914</span><span class=p>,</span> <span class=s1>&#39;theatre&#39;</span><span class=p>:</span> <span class=mf>0.11673525</span><span class=p>,</span> <span class=s1>&#39;sounds&#39;</span><span class=p>:</span> <span class=mf>0.108338095</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;visit&#39;</span><span class=p>:</span> <span class=mf>0.10516077</span><span class=p>,</span> <span class=s1>&#39;editing&#39;</span><span class=p>:</span> <span class=mf>0.078659914</span><span class=p>,</span> <span class=s1>&#39;death&#39;</span><span class=p>:</span> <span class=mf>0.066746496</span><span class=p>,</span> <span class=s1>&#39;massachusetts&#39;</span><span class=p>:</span> <span class=mf>0.055702563</span><span class=p>,</span> <span class=s1>&#39;stuart&#39;</span><span class=p>:</span> <span class=mf>0.0447934</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=s1>&#39;romantic&#39;</span><span class=p>:</span> <span class=mf>0.041140396</span><span class=p>,</span> <span class=s1>&#39;pamela&#39;</span><span class=p>:</span> <span class=mf>0.03561337</span><span class=p>,</span> <span class=s1>&#39;what&#39;</span><span class=p>:</span> <span class=mf>0.016409796</span><span class=p>,</span> <span class=s1>&#39;smallest&#39;</span><span class=p>:</span> <span class=mf>0.010815808</span><span class=p>,</span> <span class=s1>&#39;orchestra&#39;</span><span class=p>:</span> <span class=mf>0.0020691194</span><span class=p>}</span>
</span></span></code></pre></div><p>Due to the expansion both the query and the document overlap in “<em>music</em>”, “<em>film</em>”, “<em>sounds</em>”,
and others, so <strong>exact matching</strong> works.</p><h2 id=key-takeaways-when-to-choose-sparse-neural-models-for-retrieval>Key Takeaways: When to Choose Sparse Neural Models for Retrieval</h2><p>Sparse Neural Retrieval makes sense:</p><ul><li><p>In areas where keyword matching is crucial but BM25 is insufficient for initial retrieval, semantic matching (e.g., synonyms, homonyms) adds significant value. This is especially true in fields such as medicine, academia, law, and e-commerce, where brand names and serial numbers play a critical role. Dense retrievers tend to return many false positives, while sparse neural retrieval helps narrow down these false positives.</p></li><li><p>Sparse neural retrieval can be a valuable option for scaling, especially when working with large datasets. It leverages exact matching using an inverted index, which can be fast depending on the nature of your data.</p></li><li><p>If you’re using traditional retrieval systems, sparse neural retrieval is compatible with them and helps bridge the semantic gap.</p></li></ul></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! 🙏</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. 😔 You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/modern-sparse-neural-retrieval.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#sparse-neural-retrieval-as-if-keyword-based-retrievers-understood-meaning>Sparse Neural Retrieval: As If Keyword-Based Retrievers Understood Meaning</a></li><li><a href=#sparse-neural-retrieval-evolution>Sparse Neural Retrieval Evolution</a><ul><li><a href=#the-pioneer-of-sparse-neural-retrieval>The Pioneer Of Sparse Neural Retrieval</a></li><li><a href=#sparse-neural-retrieval-on-relevance-objective>Sparse Neural Retrieval on Relevance Objective</a></li><li><a href=#know-thine-tokenization>Know Thine Tokenization</a></li><li><a href=#sparse-neural-retriever-which-understood-homonyms>Sparse Neural Retriever Which Understood Homonyms</a></li><li><a href=#back-to-the-roots>Back to the Roots</a></li></ul></li><li><a href=#did-we-solve-the-vocabulary-mismatch-yet>Did we Solve the Vocabulary Mismatch Yet?</a><ul><li><a href=#external-document-expansion>External Document Expansion</a></li><li><a href=#internal-document-expansion>Internal Document Expansion</a></li><li><a href=#sparse-neural-retriever-with-internal-document-expansion>Sparse Neural Retriever with Internal Document Expansion</a></li><li><a href=#state-of-the-art-of-modern-sparse-neural-retrieval>State-of-the-Art of Modern Sparse Neural Retrieval</a></li></ul></li><li><a href=#splade-in-qdrant>SPLADE++ in Qdrant</a><ul><li></li></ul></li><li><a href=#key-takeaways-when-to-choose-sparse-neural-models-for-retrieval>Key Takeaways: When to Choose Sparse Neural Models for Retrieval</a></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/modern-sparse-neural-retrieval.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>