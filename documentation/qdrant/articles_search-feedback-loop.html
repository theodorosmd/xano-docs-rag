<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>Relevance Feedback in Informational Retrieval - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Relerance feedback: from ancient history to LLMs. Why relevance feedback techniques are good on paper but not popular in neural search, and what we can do about it."><meta name=keywords content="relevance,relevant,feedback,semantic search,lexical search,search,informational retrieval,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/search-feedback-loop/#article","@type":"Article","abstract":"Relerance feedback: from ancient history to LLMs Why relevance feedback techniques are good on paper but not popular in neural search and what we can do about it","author":{"@type":"Person","name":"Evgeniya Sukhodolskaya"},"dateModified":"2025-03-27 00:00:00 +0300 +0300","datePublished":"2025-03-27 00:00:00 +0300 +0300","description":"Relerance feedback: from ancient history to LLMs Why relevance feedback techniques are good on paper but not popular in neural search and what we can do about it","headline":"Relevance Feedback in Informational Retrieval","image":["https://qdrant.tech/articles_data/search-feedback-loop/preview/social_preview.jpg"],"name":"Relevance Feedback in Informational Retrieval","url":"https://qdrant.tech/articles/search-feedback-loop/","wordCount":"2437"},{"@id":"https://qdrant.tech/articles/search-feedback-loop/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestra√üe 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["relevance","relevant","feedback","semantic search","lexical search","search","informational retrieval","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/search-feedback-loop/"><meta property="og:type" content="website"><meta property="og:title" content="Relevance Feedback in Informational Retrieval - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/search-feedback-loop/"><meta name=twitter:title content="Relevance Feedback in Informational Retrieval - Qdrant"><meta property="og:description" content="Relerance feedback: from ancient history to LLMs. Why relevance feedback techniques are good on paper but not popular in neural search, and what we can do about it."><meta name=twitter:description content="Relerance feedback: from ancient history to LLMs. Why relevance feedback techniques are good on paper but not popular in neural search, and what we can do about it."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/search-feedback-loop/preview/social_preview.jpg"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/search-feedback-loop/preview/social_preview.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/search-feedback-loop/preview/social_preview.jpg"><meta name=author content="Evgeniya Sukhodolskaya"><link rel=canonical href=https://qdrant.tech/articles/search-feedback-loop/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>Relevance Feedback in Informational Retrieval</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/machine-learning/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Machine Learning</a><h1 class=documentation-article__header-title>Relevance Feedback in Informational Retrieval</h1><div class=documentation-article__header-about><p>Evgeniya Sukhodolskaya</p><span>&#183;</span><p>March 27, 2025</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/search-feedback-loop/preview/title.webp type=image/webp><img alt="Relevance Feedback in Informational Retrieval" src=https://qdrant.tech/articles_data/search-feedback-loop/preview/title.jpg></picture></div><blockquote><p>A problem well stated is a problem half solved.</p></blockquote><p>This quote applies as much to life as it does to information retrieval.</p><p>With a well-formulated query, retrieving the relevant document becomes trivial.
In reality, however, most users struggle to precisely define what they are searching for.</p><p>While users may struggle to formulate a perfect request ‚Äî especially in unfamiliar topics ‚Äî they can easily judge whether a retrieved answer is relevant or not.</p><p><strong>Relevance is a powerful feedback mechanism for a retrieval system</strong> to iteratively refine results in the direction of user interest.</p><p>In 2025, with social media flooded with daily AI breakthroughs, it almost seems like information retrieval is solved, agents can iteratively adjust their search queries while assessing the relevance.</p><p>Of course, there&rsquo;s a catch: these models still rely on retrieval systems (<em>RAG isn&rsquo;t dead yet, despite daily predictions of its demise</em>).
They receive only a handful of top-ranked results provided by a far simpler and cheaper retriever.
As a result, the success of guided retrieval still mainly depends on the retrieval system itself.</p><p>So, we should find a way of effectively and efficiently incorporating relevance feedback directly into a retrieval system.
In this article, we&rsquo;ll explore the approaches proposed in the research literature and try to answer the following question:</p><p><em>If relevance feedback in search is so widely studied and praised as effective, why is it practically not used in dedicated vector search solutions?</em></p><h2 id=dismantling-the-relevance-feedback>Dismantling the Relevance Feedback</h2><p>Both industry and academia tend to reinvent the wheel here and there.
So, we first took some time to study and categorize different methods ‚Äî just in case there was something we could plug directly into Qdrant.
The resulting taxonomy isn&rsquo;t set in stone, but we aim to make it useful.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/relevance-feedback.png alt="Types of Relevance Feedback" width=80%><figcaption><p>Types of Relevance Feedback</p></figcaption></figure><h3 id=pseudo-relevance-feedback-prf>Pseudo-Relevance Feedback (PRF)</h3><p>Pseudo-Relevance feedback takes the top-ranked documents from the initial retrieval results and treats them as relevant. This approach might seem naive, but it provides a noticeable performance boost in lexical retrieval while being relatively cheap to compute.</p><h3 id=binary-relevance-feedback>Binary Relevance Feedback</h3><p>The most straightforward way to gather feedback is to ask users directly if document is relevant.
There are two main limitations to this approach:</p><p>First, users are notoriously reluctant to provide feedback. Did you know that <a href="https://en.wikipedia.org/wiki/Google_SearchWiki#:~:text=SearchWiki%20was%20a%20Google%20Search,for%20a%20given%20search%20query" target=_blank rel="noopener nofollow">Google once had</a> an upvote/downvote mechanism on search results but removed it because almost no one used it?</p><p>Second, even if users are willing to provide feedback, no relevant documents might be present in the initial retrieval results. In this case, the user can&rsquo;t provide a meaningful signal.</p><p>Instead of asking users, we can ask a smart model to provide binary relevance judgements, but this would limit its potential to generate granular judgements.</p><h3 id=re-scored-relevance-feedback>Re-scored Relevance Feedback</h3><p>We can also apply more sophisticated methods to extract relevance feedback from the top-ranked documents - machine learning models can provide a relevance score for each document.</p><p>The obvious concern here is twofold:</p><ol><li>How accurately can the automated judge determine relevance (or irrelevance)?</li><li>How cost-efficient is it? After all, you can‚Äôt expect GPT-4o to re-rank thousands of documents for every user query ‚Äî unless you‚Äôre filthy rich.</li></ol><p>Nevertheless, automated re-scored feedback could be a scalable way to improve search when explicit binary feedback is not accessible.</p><h2 id=has-the-problem-already-been-solved>Has the Problem Already Been Solved?</h2><p>Digging through research materials, we expected anything else but to discover that the first relevance feedback study dates back <a href=https://sigir.org/files/museum/pub-08/XXIII-1.pdf target=_blank rel="noopener nofollow"><em>sixty years</em></a>.
In the midst of the neural search bubble, it&rsquo;s easy to forget that lexical (term-based) retrieval has been around for decades. Naturally, research in that field has had enough time to develop.</p><p><strong>Neural search</strong> ‚Äî aka <a href=https://qdrant.tech/articles/neural-search-tutorial/ target=_blank rel="noopener nofollow">vector search</a> ‚Äî gained traction in the industry around 5 years ago. Hence, vector-specific relevance feedback techniques might still be in their early stages, awaiting production-grade validation and industry adoption.</p><p>As a <a href=https://qdrant.tech/articles/dedicated-vector-search/ target=_blank rel="noopener nofollow">dedicated vector search engine</a>, we would like to be these adopters.
Our focus is neural search, but approaches in both lexical and neural retrieval seem worth exploring, as cross-field studies are always insightful, with the potential to reuse well-established methods of one field in another.</p><p>We found some interesting methods applicable to neural search solutions and additionally revealed a <strong>gap in the neural search-based relevance feedback approaches</strong>. Stick around, and we&rsquo;ll share our findings!</p><h2 id=two-ways-to-approach-the-problem>Two Ways to Approach the Problem</h2><p>Retrieval as a recipe can be broken down into three main ingredients:</p><ol><li>Query</li><li>Documents</li><li>Similarity scoring between them.</li></ol><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/taxonomy-overview.png alt="Research Field Taxonomy Overview" width=80%><figcaption><p>Research Field Taxonomy Overview</p></figcaption></figure><p>Query formulation is a subjective process ‚Äì it can be done in infinite configurations, making the relevance of a document unpredictable until the query is formulated and submitted to the system.</p><p>So, adapting documents (or the search index) to relevance feedback would require per-request dynamic changes, which is impractical, considering that modern retrieval systems store billions of documents.</p><p>Thus, approaches for incorporating relevance feedback in search fall into two categories: <strong>refining a query</strong> and <strong>refining the similarity scoring function</strong> between the query and documents.</p><h2 id=query-refinement>Query Refinement</h2><p>There are several ways to refine a query based on relevance feedback.
Globally, we prefer to distinguish between two approaches: modifying the query as text and modifying the vector representation of the query.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/query.png alt="Incorporating Relevance Feedback in Query" width=80%><figcaption><p>Incorporating Relevance Feedback in Query</p></figcaption></figure><h3 id=query-as-text>Query As Text</h3><p>In <strong>term-based retrieval</strong>, an intuitive way to improve a query would be to <strong>expand it with relevant terms</strong>. It resembled the &ldquo;<em>aha, so that&rsquo;s what it&rsquo;s called</em>&rdquo; stage in the discovery search.</p><p>Before the deep learning era of this century, expansion terms were mainly selected using statistical or probabilistic models. The idea was to:</p><ol><li>Either extract the <strong>most frequent</strong> terms from (pseudo-)relevant documents;</li><li>Or the <strong>most specific</strong> ones (for example, according to IDF);</li><li>Or the <strong>most probable</strong> ones (most likely to be in query according to a relevance set).</li></ol><p>Well-known methods of those times come from the family of <a href=https://sigir.org/wp-content/uploads/2017/06/p260.pdf target=_blank rel="noopener nofollow">Relevance Models</a>, where terms for expansion are chosen based on their probability in pseudo-relevant documents (how often terms appear) and query terms likelihood given those pseudo-relevant documents - how strongly these pseudo-relevant documents match the query.</p><p>The most famous one, <code>RM3</code> ‚Äì interpolation of expansion terms probability with their probability in a query ‚Äì is still appearing in papers of the last few years as a (noticeably decent) baseline in term-based retrieval, usually as part of <a href=https://github.com/castorini/anserini target=_blank rel="noopener nofollow">anserini</a>.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/relevance-models.png alt="Simplified Query Expansion" width=100%><figcaption><p>Simplified Query Expansion</p></figcaption></figure><p>With the time approaching the modern machine learning era, <a href=https://aclanthology.org/2020.findings-emnlp.424.pdf target=_blank rel="noopener nofollow">multiple</a> <a href=https://dl.acm.org/doi/10.1145/1390334.1390377 target=_blank rel="noopener nofollow">studies</a> began claiming that these traditional ways of query expansion are not as effective as they could be.</p><p>Started with simple classifiers based on hand-crafted features, this trend naturally led to use the famous <a href=https://huggingface.co/docs/transformers/model_doc/bert target=_blank rel="noopener nofollow">BERT (Bidirectional encoder representations from transformers)</a>. For example, <code>BERT-QE</code> (Query Expansion) authors came up with this schema:</p><ol><li>Get pseudo-relevance feedback from the finetuned BERT reranker (~10 documents);</li><li>Chunk these pseudo-relevant documents (~100 words) and score query-chunk relevance with the same reranker;</li><li>Expand the query with the most relevant chunks;</li><li>Rerank 1000 documents with the reranker using the expanded query.</li></ol><p>This approach significantly outperformed BM25 + RM3 baseline in experiments (+11% NDCG@20). However, it required <strong>11.01x</strong> more computation than just using BERT for reranking, and reranking 1000 documents with BERT would take around 9 seconds alone.</p><p>Query term expansion can <em>hypothetically</em> work for neural retrieval as well. New terms might shift the query vector closer to that of the desired document. However, <a href=https://dl.acm.org/doi/10.1145/3570724 target=_blank rel="noopener nofollow">this approach isn‚Äôt guaranteed to succeed</a>. Neural search depends entirely on embeddings, and how those embeddings are generated ‚Äî consequently, how similar query and document vectors are ‚Äî depends heavily on the model‚Äôs training.</p><p>It definitely works if <strong>query refining is done by a model operating in the same vector space</strong>, which typically requires offline training of a retriever.
The goal is to extend the query encoder input to also include feedback documents, producing an adjusted query embedding. Examples include <a href=https://arxiv.org/pdf/2108.13454 target=_blank rel="noopener nofollow"><code>ANCE-PRF</code></a> and <a href=https://dl.acm.org/doi/10.1145/3572405 target=_blank rel="noopener nofollow"><code>ColBERT-PRF</code></a> ‚Äì ANCE and ColBERT fine-tuned extensions.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/updated-encoder.png alt="Generating a new relevance-aware query vector" width=100%><figcaption><p>Generating a new relevance-aware query vector</p></figcaption></figure><p>The reason why you‚Äôre most probably not familiar with these models ‚Äì their absence in the industry ‚Äì is that their <strong>training</strong> itself is a <strong>high upfront cost</strong>, and even though it was ‚Äúpaid‚Äù, these models <a href=https://arxiv.org/abs/2108.13454 target=_blank rel="noopener nofollow">struggle with generalization</a>, performing poorly on out-of-domain tasks (datasets they haven‚Äôt seen during training).
Additionally, feeding an attention-based model a lengthy input (query + documents) is not a good practice in production settings (attention is quadratic in the input length), where time and money are crucial decision factors.</p><p>Alternatively, one could skip a step ‚Äî and work directly with vectors.</p><h3 id=query-as-vector>Query As Vector</h3><p>Instead of modifying the initial query, a more scalable approach is to directly adjust the query vector.
It is easily applicable across modalities and suitable for both lexical and neural retrieval.</p><p>Although vector search has become a trend in recent years, its core principles have existed in the field for decades. For example, the SMART retrieval system used by <a href=https://sigir.org/files/museum/pub-08/XXIII-1.pdf target=_blank rel="noopener nofollow">Rocchio</a> in 1965 for his relevance feedback experiments operated on bag-of-words vector representations of text.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/Roccio.png alt="Roccio&rsquo;s Relevance Feedback Method" width=100%><figcaption><p>Roccio&rsquo;s Relevance Feedback Method</p></figcaption></figure><p><strong>Rocchio‚Äôs idea</strong> ‚Äî to update the query vector by adding a difference between the centroids of relevant and non-relevant documents ‚Äî seems to translate well to modern dual encoders-based dense retrieval systems.
Researchers seem to agree: a study from 2022 demonstrated that the <a href=https://arxiv.org/pdf/2108.11044 target=_blank rel="noopener nofollow">parametrized version of Rocchio‚Äôs method</a> in dense retrieval consistently improves Recall@1000 by 1‚Äì5%, while keeping query processing time suitable for production ‚Äî around 170 ms.</p><p>However, parameters (centroids and query weights) in the dense retrieval version of Roccio‚Äôs method must be tuned for each dataset and, ideally, also for each request.</p><h4 id=gradient-descent-based-methods>Gradient Descent-Based Methods</h4><p>The efficient way of doing so on-the-fly remained an open question until the introduction of a <strong>gradient-descent-based Roccio‚Äôs method generalization</strong>: <a href=https://arxiv.org/pdf/2205.12680 target=_blank rel="noopener nofollow"><code>Test-Time Optimization of Query Representations (TOUR)</code></a>.
TOUR adapts a query vector over multiple iterations of retrieval and reranking (<em>retrieve ‚Üí rerank ‚Üí gradient descent step</em>), guided by a reranker‚Äôs relevance judgments.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/TOUR.png alt="An overview of TOUR iteratively optimizing initial query representation based on pseudo relevance feedback.
Figure adapted from Sung et al., 2023, Optimizing Test-Time Query Representations for Dense Retrieval" width=60%><figcaption><p>An overview of TOUR iteratively optimizing initial query representation based on pseudo relevance feedback.<br>Figure adapted from Sung et al., 2023, <a href=https://arxiv.org/pdf/2205.12680 target=_blank rel="noopener nofollow">Optimizing Test-Time Query Representations for Dense Retrieval</a></p></figcaption></figure><p>The next iteration of gradient-based methods of query refinement ‚Äì <a href=https://arxiv.org/abs/2305.11744 target=_blank rel="noopener nofollow"><code>ReFit</code></a> ‚Äì proposed in 2024 a lighter, production-friendly alternative to TOUR, limiting <em>retrieve ‚Üí rerank ‚Üí gradient descent</em> sequence to only one iteration. The retriever‚Äôs query vector is updated through matching (via <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence target=_blank rel="noopener nofollow">Kullback‚ÄìLeibler divergence</a>) retriever and cross-encoder‚Äôs similarity scores distribution over feedback documents. ReFit is model- and language-independent and stably improves Recall@100 metric on 2‚Äì3%.</p><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/refit.png alt="An overview of ReFit, a gradient-based method for query refinement" width=90%><figcaption><p>An overview of ReFit, a gradient-based method for query refinement</p></figcaption></figure><p>Gradient descent-based methods seem like a production-viable option, an alternative to finetuning the retriever (distilling it from a reranker).
Indeed, it doesn&rsquo;t require in-advance training and is compatible with any re-ranking models.</p><p>However, a few limitations baked into these methods prevented a broader adoption in the industry.</p><p>The gradient descent-based methods modify elements of the query vector as if it were model parameters; therefore,
they require a substantial amount of feedback documents to converge to a stable solution.</p><p>On top of that, the gradient descent-based methods are sensitive to the choice of hyperparameters, leading to <strong>query drift</strong>, where the query may drift entirely away from the user&rsquo;s intent.</p><h2 id=similarity-scoring>Similarity Scoring</h2><figure><img src=https://qdrant.tech/articles_data/search-feedback-loop/similairty-scoring.png alt="Incorporating Relevance Feedback in Similarity Scoring" width=80%><figcaption><p>Incorporating Relevance Feedback in Similarity Scoring</p></figcaption></figure><p>Another family of approaches is built around the idea of incorporating relevance feedback directly into the similarity scoring function.
It might be desirable in cases where we want to preserve the original query intent, but still adjust the similarity score based on relevance feedback.</p><p>In <strong>lexical retrieval</strong>, this can be as simple as boosting documents that share more terms with those judged as relevant.</p><p>Its <strong>neural search counterpart</strong> is a <a href=https://aclanthology.org/2022.emnlp-main.614.pdf target=_blank rel="noopener nofollow"><code>k-nearest neighbors-based method</code></a> that adjusts the query-document similarity score by adding the sum of similarities between the candidate document and all known relevant examples.
This technique yields a significant improvement, around 5.6 percentage points in NDCG@20, but it requires explicitly labelled (by users) feedback documents to be effective.</p><p>In experiments, the knn-based method is treated as a reranker. In all other papers, we also found that adjusting similarity scores based on relevance feedback is centred around <a href=https://qdrant.tech/documentation/search-precision/reranking-semantic-search/ target=_blank rel="noopener nofollow">reranking</a> ‚Äì <strong>training or finetuning rerankers to become relevance feedback-aware</strong>.
Typically, experiments include cross-encoders, though <a href=https://arxiv.org/pdf/1904.08861 target=_blank rel="noopener nofollow">simple classifiers are also an option</a>.
These methods generally involve rescoring a broader set of documents retrieved during an initial search, guided by feedback from a smaller top-ranked subset. It is not a similarity matching function adjustment per se but rather a similarity scoring model adjustment.</p><p>Methods typically fall into two categories:</p><ol><li><strong>Training rerankers offline</strong> to ingest relevance feedback as an additional input at inference time, <a href=https://aclanthology.org/D18-1478.pdf target=_blank rel="noopener nofollow">as here</a> ‚Äî again, attention-based models and lengthy inputs: a production-deadly combination.</li><li><strong>Finetuning rerankers</strong> on relevance feedback from the first retrieval stage, <a href=https://aclanthology.org/2022.emnlp-main.614.pdf target=_blank rel="noopener nofollow">as Baumg√§rtner et al. did</a>, finetuning bias parameters of a small cross-encoder per query on 2k, k={2, 4, 8} feedback documents.</li></ol><p>The biggest limitation here is that these reranker-based methods cannot retrieve relevant documents beyond those returned in the initial search, and using rerankers on thousands of documents in production is a no-go ‚Äì it‚Äôs too expensive.
Ideally, to avoid that, a similarity scoring function updated with relevance feedback should be used directly in the second retrieval iteration. However, in every research paper we‚Äôve come across, retrieval systems are <strong>treated as black boxes</strong> ‚Äî ingesting queries, returning results, and offering no built-in mechanism to modify scoring.</p><h2 id=so-what-are-the-takeaways>So, what are the takeaways?</h2><p>Pseudo Relevance Feedback (PRF) is known to improve the effectiveness of lexical retrievers. Several PRF-based approaches ‚Äì mainly query terms expansion-based ‚Äì are successfully integrated into traditional retrieval systems. At the same time, there are <strong>no known industry-adopted analogues in neural (vector) search dedicated solutions</strong>; neural search-compatible methods remain stuck in research papers.</p><p>The gap we noticed while studying the field is that researchers have <strong>no direct access to retrieval systems</strong>, forcing them to design wrappers around the black-box-like retrieval oracles. This is sufficient for query-adjusting methods but not for similarity scoring function adjustment.</p><p>Perhaps relevance feedback methods haven&rsquo;t made it into the neural search systems for trivial reasons ‚Äî like no one having the time to find the right balance between cost and efficiency.</p><p>Getting it to work in a production setting means experimenting, building interfaces, and adapting architectures. Simply put, it needs to look worth it. And unlike 2D vector math, high-dimensional vector spaces are anything but intuitive. The curse of dimensionality is real. So is query drift. Even methods that make perfect sense on paper might not work in practice.</p><p>A real-world solution should be simple. Maybe just a little bit smarter than a rule-based approach, but still practical. It shouldn&rsquo;t require fine-tuning thousands of parameters or feeding paragraphs of text into transformers. <strong>And for it to be effective, it needs to be integrated directly into the retrieval system itself.</strong></p></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! üôè</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. üòî You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/search-feedback-loop.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#dismantling-the-relevance-feedback>Dismantling the Relevance Feedback</a><ul><li><a href=#pseudo-relevance-feedback-prf>Pseudo-Relevance Feedback (PRF)</a></li><li><a href=#binary-relevance-feedback>Binary Relevance Feedback</a></li><li><a href=#re-scored-relevance-feedback>Re-scored Relevance Feedback</a></li></ul></li><li><a href=#has-the-problem-already-been-solved>Has the Problem Already Been Solved?</a></li><li><a href=#two-ways-to-approach-the-problem>Two Ways to Approach the Problem</a></li><li><a href=#query-refinement>Query Refinement</a><ul><li><a href=#query-as-text>Query As Text</a></li><li><a href=#query-as-vector>Query As Vector</a></li></ul></li><li><a href=#similarity-scoring>Similarity Scoring</a></li><li><a href=#so-what-are-the-takeaways>So, what are the takeaways?</a></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/search-feedback-loop.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>¬© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>