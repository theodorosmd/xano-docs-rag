<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Noé Achache shares insights on vector search complexities, discussing projects on image matching, document retrieval, and handling sensitive medical data with practical solutions and industry challenges."><meta name=keywords content="vector search engine,neural network,matching,SaaS,approximate nearest neighbor search,image search,recommender system,vectors,knn algorithm,hnsw,vector search,embeddings,similarity,simaes networks,BERT,transformer,word2vec,fasttext,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/blog/vector-image-search-rag/#article","@type":"Article","abstract":"No Achache shares insights on vector search complexities discussing projects on image matching document retrieval and handling sensitive medical data with practical solutions and industry challenges","author":{"@type":"Person","name":"Demetrios Brinkmann"},"dateModified":"2024-01-09 13:51:26.168 +0000 UTC","datePublished":"2024-01-09 13:51:26.168 +0000 UTC","description":"No Achache shares insights on vector search complexities discussing projects on image matching document retrieval and handling sensitive medical data with practical solutions and industry challenges","headline":"Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks","image":[""],"name":"Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks","url":"https://qdrant.tech/blog/vector-image-search-rag/","wordCount":"6210"},{"@id":"https://qdrant.tech/blog/vector-image-search-rag/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestraße 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["vector search engine","neural network","matching","SaaS","approximate nearest neighbor search","image search","recommender system","vectors","knn algorithm","hnsw","vector search","embeddings","similarity","simaes networks","BERT","transformer","word2vec","fasttext","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/blog/vector-image-search-rag/"><meta property="og:type" content="website"><meta property="og:title" content="Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/blog/vector-image-search-rag/"><meta name=twitter:title content="Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks - Qdrant"><meta property="og:description" content="Noé Achache shares insights on vector search complexities, discussing projects on image matching, document retrieval, and handling sensitive medical data with practical solutions and industry challenges."><meta name=twitter:description content="Noé Achache shares insights on vector search complexities, discussing projects on image matching, document retrieval, and handling sensitive medical data with practical solutions and industry challenges."><meta name=image property="og:image" content="https://qdrant.tech/blog/vector-image-search-rag/preview/social_preview.jpg"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/blog/vector-image-search-rag/preview/social_preview.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/blog/vector-image-search-rag/preview/social_preview.jpg"><meta name=author content="Demetrios Brinkmann"><link rel=canonical href=https://qdrant.tech/blog/vector-image-search-rag/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script></head><body><main><header class=site-header><section class=top-banner data-start=1749013200 data-end=1750428000 style=display:none><div><span class=top-banner__icon><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><g clip-path="url(#clip0_770_2716)"><path d="M14.598 6.37199C14.486 6.14399 14.254 5.99999 14 5.99999H8.7447L9.3287.739993C9.36204.44266 9.1927.159993 8.91537.0479934 8.63737-.0653399 8.31937.0226601 8.13737.259327L1.4707 8.92599C1.31604 9.12733 1.2887 9.39933 1.40137 9.62733c.11267.22866.34467.37266.59867.37266H7.25537L6.67137 15.26c-.0333299999999994.2973.136.58.41333.692C7.16537 15.9847 7.25004 16 7.33337 16 7.53604 16 7.73337 15.9073 7.86204 15.74L14.5287 7.07333C14.6834 6.87199 14.71 6.59999 14.598 6.37199z" fill="#8547ff"/></g><defs><clipPath id="clip0_770_2716"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg> </span><span class=top-banner__text>Learn how TripAdvisor Drives 2-3x More Revenue with Qdrant-Powered AI at Enterprise Scale </span><a data-metric-loc=banner data-metric-label="Learn how TripAdvisor Drives 2-3x More Revenue with Qdrant-Powered AI at Enterprise Scale Read now" class="link link_light link_sm" href=https://qdrant.tech/blog/case-study-tripadvisor/>Read now</a></div></section><div class="main-menu z-2"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><span>Products</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/qdrant-vector-database/><img src=https://qdrant.tech/img/menu/qdrant-vector-database.svg draggable=false>
<span>Qdrant Vector Database</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/cloud/><img src=https://qdrant.tech/img/menu/qdrant-cloud.svg draggable=false>
<span>Qdrant Cloud</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/hybrid-cloud/><img src=https://qdrant.tech/img/menu/hybrid-cloud.svg draggable=false>
<span>Qdrant Hybrid Cloud</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/enterprise-solutions/><img src=https://qdrant.tech/img/menu/qdrant-enterprise-solutions.svg draggable=false>
<span>Qdrant Enterprise Solutions</span></a></li></ul></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/use-cases/>Use Cases</a><ul class=main-menu__submenu><li class=main-menu__section-link><a class="link link_neutral link_sm" href=https://qdrant.tech/use-cases/>Use Cases</a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/rag/><img src=https://qdrant.tech/img/menu/rag.svg draggable=false>
<span>RAG</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/recommendations/><img src=https://qdrant.tech/img/menu/recommendation-systems.svg draggable=false>
<span>Recommendation Systems</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/advanced-search/><img src=https://qdrant.tech/img/menu/advanced-search.svg draggable=false>
<span>Advanced Search</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/data-analysis-anomaly-detection/><img src=https://qdrant.tech/img/menu/data-analysis-anomaly-detection.svg draggable=false>
<span>Data Analysis & Anomaly Detection</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/ai-agents/><img src=https://qdrant.tech/img/menu/ai-agents.svg draggable=false>
<span>AI Agents</span></a></li></ul></li><li class=main-menu__item><span>Developers</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/documentation/><img src=https://qdrant.tech/img/menu/documentation.svg draggable=false>
<span>Documentation</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/community/><img src=https://qdrant.tech/img/menu/community.svg draggable=false>
<span>Community</span></a></li><li class=main-menu__submenu-item><a href=https://github.com/qdrant/qdrant target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/github.svg draggable=false>
<span>GitHub</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.to/roadmap target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/roadmap.svg draggable=false>
<span>Roadmap</span></a></li><li class=main-menu__submenu-item><a href=https://github.com/qdrant/qdrant/releases target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/changelog.svg draggable=false>
<span>Change Log</span></a></li></ul></li><li class=main-menu__item><span>Resources</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/benchmarks/><img src=https://qdrant.tech/img/menu/benchmarks.svg draggable=false>
<span>Benchmarks</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/blog/><img src=https://qdrant.tech/img/menu/blog.svg draggable=false>
<span>Blog</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/articles/><img src=https://qdrant.tech/img/menu/articles.svg draggable=false>
<span>Articles</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/demo/><img src=https://qdrant.tech/img/menu/demos.svg draggable=false>
<span>Demos</span></a></li><li class=main-menu__submenu-item><a href=https://try.qdrant.tech/events target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/partners.svg draggable=false>
<span>Events</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/qdrant-for-startups/><img src=https://qdrant.tech/img/menu/qdrant-for-startups.svg draggable=false>
<span>Startup Program</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/security/bug-bounty-program/><img src=https://qdrant.tech/img/menu/bug-bounty-program.svg draggable=false>
<span>Bug Bounty Program</span></a></li></ul></li><li class=main-menu__item><span>Company</span><ul class=main-menu__submenu><li class=main-menu__submenu-item><a href=https://qdrant.tech/about-us/><img src=https://qdrant.tech/img/menu/about-us.svg draggable=false>
<span>About us</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/customers/><img src=https://qdrant.tech/img/menu/customers.svg draggable=false>
<span>Customers</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/partners/><img src=https://qdrant.tech/img/menu/partners.svg draggable=false>
<span>Partners</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.join.com/ target=_blank rel="noopener noreferrer nofollow"><img src=https://qdrant.tech/img/menu/careers.svg draggable=false>
<span>Careers</span></a></li><li class=main-menu__submenu-item><a href=https://qdrant.tech/contact-us/><img src=https://qdrant.tech/img/menu/contact-us.svg draggable=false>
<span>Contact us</span></a></li></ul></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/pricing/>Pricing</a></li></ul><div class=main-menu__buttons><a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3">Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm">Get Started</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M19.0713 4.92871 4.92915 19.0708" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M19.0713 19.0708 4.9292 4.92871" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content>Products
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/qdrant-vector-database/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-vector-database.svg)></span>Qdrant Vector Database</li></a><a href=https://qdrant.tech/cloud/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-cloud.svg)></span>Qdrant Cloud</li></a><a href=https://qdrant.tech/hybrid-cloud/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/hybrid-cloud.svg)></span>Qdrant Hybrid Cloud</li></a><a href=https://qdrant.tech/enterprise-solutions/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-enterprise-solutions.svg)></span>Qdrant Enterprise Solutions</li></a></ul></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content>Use Cases
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><li class=menu-mobile__section-link><a class="link link_neutral link_sm" href=https://qdrant.tech/use-cases/>Use Cases</a></li><a href=https://qdrant.tech/rag/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/rag.svg)></span>RAG</li></a><a href=https://qdrant.tech/recommendations/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/recommendation-systems.svg)></span>Recommendation Systems</li></a><a href=https://qdrant.tech/advanced-search/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/advanced-search.svg)></span>Advanced Search</li></a><a href=https://qdrant.tech/data-analysis-anomaly-detection/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/data-analysis-anomaly-detection.svg)></span>Data Analysis & Anomaly Detection</li></a><a href=https://qdrant.tech/ai-agents/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/ai-agents.svg)></span>AI Agents</li></a></ul></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content>Developers
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/documentation/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/documentation.svg)></span>Documentation</li></a><a href=https://qdrant.tech/community/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/community.svg)></span>Community</li></a><a href=https://github.com/qdrant/qdrant><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/github.svg)></span>GitHub</li></a><a href=https://qdrant.to/roadmap><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/roadmap.svg)></span>Roadmap</li></a><a href=https://github.com/qdrant/qdrant/releases><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/changelog.svg)></span>Change Log</li></a></ul></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content>Resources
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/benchmarks/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/benchmarks.svg)></span>Benchmarks</li></a><a href=https://qdrant.tech/blog/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/blog.svg)></span>Blog</li></a><a href=https://qdrant.tech/articles/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/articles.svg)></span>Articles</li></a><a href=https://qdrant.tech/demo/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/demos.svg)></span>Demos</li></a><a href=https://try.qdrant.tech/events><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/partners.svg)></span>Events</li></a><a href=https://qdrant.tech/qdrant-for-startups/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/qdrant-for-startups.svg)></span>Startup Program</li></a><a href=https://qdrant.tech/security/bug-bounty-program/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/bug-bounty-program.svg)></span>Bug Bounty Program</li></a></ul></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content>Company
<button type=button class=menu-mobile__expand><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M2 7 12 17 22 7" stroke="#161e33" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=menu-mobile__subitems><a href=https://qdrant.tech/about-us/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/about-us.svg)></span>About us</li></a><a href=https://qdrant.tech/customers/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/customers.svg)></span>Customers</li></a><a href=https://qdrant.tech/partners/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/partners.svg)></span>Partners</li></a><a href=https://qdrant.join.com/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/careers.svg)></span>Careers</li></a><a href=https://qdrant.tech/contact-us/><li class=menu-mobile__subitem><span style=background-image:url(/img/menu/contact-us.svg)></span>Contact us</li></a></ul></li><li class=menu-mobile__item data-path=menu-5><div class=menu-mobile__item-content><a href=https://qdrant.tech/pricing/>Pricing</a></div></li></ul><div class=menu-mobile__controls><a data-metric-loc=mobile_nav href=https://cloud.qdrant.io/login class="button button_outlined button_lg menu-mobile__login">Log in</a>
<a data-metric-loc=mobile_nav href=https://cloud.qdrant.io/signup class="button button_contained button_lg">Get Started</a></div></div></header><progress id=progress class=progress-bar value=0 max=100>0</progress><section class="qdrant-post
qdrant-blog-post"><article id=article class=container><div class=qdrant-post__header><h1 class=qdrant-post__title>Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks</h1><div class=qdrant-post__about><p>Demetrios Brinkmann</p><span>&#183;</span><p>January 09, 2024</p></div><picture class=qdrant-post__preview><img src=https://qdrant.tech/blog/vector-image-search-rag/preview/title.jpg alt="Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks" loading=lazy></picture></div><div class="row qdrant-post__breadcrumbs"><div class=col-12><ul class=breadcrumbs><li class=breadcrumbs__crumb><a href=https://qdrant.tech/>Home</a></li><li class=breadcrumbs__crumb>/</li><li class=breadcrumbs__crumb><a href=https://qdrant.tech/blog/>Blog</a></li><li class=breadcrumbs__crumb>/</li><li class=breadcrumbs__crumb>Vector Search Complexities: Insights from Projects in Image Search and RAG - Noé Achache | Vector Space Talks</li></ul></div></div><div class=qdrant-post__body><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><ul><li><a href=#top-takeaways><strong>Top Takeaways:</strong></a></li><li><a href=#show-notes>Show Notes:</a></li><li><a href=#more-quotes-from-noé>More Quotes from Noé:</a></li><li><a href=#transcript>Transcript:</a></li></ul></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fqdrant.tech%2Fblog%2Fvector-image-search-rag%2F&amp;text=Vector%20Search%20Complexities:%20Insights%20from%20Projects%20in%20Image%20Search%20and%20RAG%20-%20No%c3%a9%20Achache%20%7c%20Vector%20Space%20Talks" target=_blank rel="noopener noreferrer" title=x><svg width="33" height="33" viewBox="0 0 33 33" fill="none"><path d="M14.959 20.7369l-8.581 9.798H1.625l11.114-12.696 2.22 2.898z" fill="#161e33"/><path d="M17.5508 11.5229l7.857-8.98799h4.75L19.7508 14.4369l-2.2-2.914z" fill="#161e33"/><path d="M31.9877 30.5349h-9.559L1.01172 2.53491H10.8127L31.9877 30.5349zm-8.248-2.843h2.632L9.38272 5.22891h-2.824L23.7397 27.6919z" fill="#161e33"/></svg>
Share on X</a></li><li class=table-of-contents__link><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fqdrant.tech%2Fblog%2Fvector-image-search-rag%2F" target=_blank rel="noopener noreferrer" title=LinkedIn><svg width="32" height="33" viewBox="0 0 32 33" fill="none"><g clip-path="url(#clip0_1811_16094)"><path d="M30.6667.4375H1.33333C.533333.4375.0.970833.0 1.77083V31.1042C0 31.9042.533333 32.4375 1.33333 32.4375H30.6667C31.4667 32.4375 32 31.9042 32 31.1042V1.77083C32 .970833 31.4667.4375 30.6667.4375zM9.46667 27.7708H4.8V12.4375H9.6V27.7708H9.46667zm-2.4-17.4666c-1.46667.0-2.8-1.20003-2.8-2.80003.0-1.46667 1.2-2.8 2.8-2.8 1.46666.0 2.8 1.2 2.8 2.8s-1.2 2.80003-2.8 2.80003zM27.3333 27.7708h-4.8V20.3042c0-1.7334.0-4-2.4-4-2.5333.0-2.8 1.8666-2.8 3.8666v7.6h-4.8V12.4375h4.5334v2.1333C17.7333 13.3708 19.2 12.1708 21.6 12.1708c4.8.0 5.7333 3.2 5.7333 7.3334v8.2666z" fill="#161e33"/></g><defs><clipPath id="clip0_1811_16094"><rect width="32" height="32" fill="#fff" transform="translate(0 0.4375)"/></clipPath></defs></svg>
Share on LinkedIn</a></li></ul></div><div class=qdrant-post__content><blockquote><p><em>&ldquo;I really think it&rsquo;s something the technology is ready for and would really help this kind of embedding model jumping onto the text search projects.”</em><br>&ndash; Noé Achache on the future of image embedding</p></blockquote><p>Exploring the depths of vector search? Want an analysis of its application in image search and document retrieval? Noé got you covered.</p><p>Noé Achache is a Lead Data Scientist at Sicara, where he worked on a wide range of projects mostly related to computer vision, prediction with structured data, and more recently LLMs.</p><p><em><strong>Listen to the episode on <a href="https://open.spotify.com/episode/2YgcSFjP7mKE0YpDGmSiq5?si=6BhlAMveSty4Yt7umPeHjA" target=_blank rel="noopener nofollow">Spotify</a>, Apple Podcast, Podcast addicts, Castbox. You can also watch this episode on <a href=https://youtu.be/1vKoiFAdorE target=_blank rel="noopener nofollow">YouTube</a>.</strong></em></p><iframe width=560 height=315 src="https://www.youtube.com/embed/1vKoiFAdorE?si=wupcX2v8vHNnR_QB" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe src=https://podcasters.spotify.com/pod/show/qdrant-vector-space-talk/embed/episodes/Navigating-the-Complexities-of-Vector-Search-Practical-Insights-from-Diverse-Projects-in-Image-Search-and-RAG---No-Achache--Vector-Space-Talk-008-e2diivl/a-aap4q5d height=102px width=400px frameborder=0 scrolling=no></iframe><h2 id=top-takeaways><strong>Top Takeaways:</strong></h2><p>Discover the efficacy of Dino V2 in image representation and the complexities of deploying vector databases, while navigating the challenges of fine-tuning and data safety in sensitive fields.</p><p>In this episode, Noe, shares insights on vector search from image search to retrieval augmented generation, emphasizing practical application in complex projects.</p><p>5 key insights you’ll learn:</p><ol><li>Cutting-edge Image Search: Learn about the advanced model Dino V2 and its efficacy in image representation, surpassing traditional feature transform methods.</li><li>Data Deduplication Strategies: Gain knowledge on the sophisticated process of deduplicating real estate listings, a vital task in managing extensive data collections.</li><li>Document Retrieval Techniques: Understand the challenges and solutions in retrieval augmented generation for document searches, including the use of multi-language embedding models.</li><li>Protection of Sensitive Medical Data: Delve into strategies for handling confidential medical information and the importance of data safety in health-related applications.</li><li>The Path Forward in Model Development: Hear Noe discuss the pressing need for new types of models to address the evolving needs within the industry.</li></ol><blockquote><p>Fun Fact: The best-performing model Noé mentions for image representation in his image search project is Dino V2, which interestingly didn&rsquo;t require fine-tuning to understand objects and patterns.</p></blockquote><h2 id=show-notes>Show Notes:</h2><p>00:00 Relevant experience in vector DB projects and talks.<br>05:57 Match image features, not resilient to changes.<br>07:06 Compute crop vectors, and train to converge.<br>11:37 Simple training task, improve with hard examples.<br>15:25 Improving text embeddings using hard examples.<br>22:29 Future of image embedding for document search.<br>27:28 Efficient storage and retrieval process feature.<br>29:01 Models handle varied data; sparse vectors now possible.<br>35:59 Use memory, avoid disk for CI integration.<br>37:43 Challenging metadata filtering for vector databases and new models</p><h2 id=more-quotes-from-noé>More Quotes from Noé:</h2><p><em>&ldquo;So basically what was great is that Dino manages to understand all objects and close patterns without fine tuning. So you can get an off the shelf model and get started very quickly and start bringing value very quickly without having to go through all the fine tuning processes.”</em><br>&ndash; Noé Achache</p><p><em>&ldquo;And at the end, the embeddings was not learning any very complex features, so it was not really improving it.”</em><br>&ndash; Noé Achache</p><p><em>&ldquo;When using an API model, it&rsquo;s much faster to use it in asynchronous mode like the embedding equation went something like ten times or 100 times faster. So it was definitely, it changed a lot of things.”</em><br>&ndash; Noé Achache</p><h2 id=transcript>Transcript:</h2><p>Demetrios:
Noe. Great to have you here everyone. We are back for another vector space talks and today we are joined by my man Noe, who is the lead data scientist at Sicara, and if you do not know, he is working on a wide range of projects, mostly related to computer vision. Vision. And today we are talking about navigating the complexities of vector search. We&rsquo;re going to get some practical insights from diverse projects in image search and everyone&rsquo;s favorite topic these days, retrieval augmented generation, aka rags. So noe, I think you got something for us. You got something planned for us here?</p><p>Noe Acache:
Yeah, I do. I can share them.</p><p>Demetrios:
All right, well, I&rsquo;m very happy to have you on here, man. I appreciate you doing this. And let&rsquo;s get you sharing your screen so we can start rocking, rolling.</p><p>Noe Acache:
Okay. Can you see my screen?</p><p>Demetrios:
Yeah. Awesome.</p><p>Noe Acache:
Great. Thank you, Demetrius, for the great introduction. I just completed quickly. So as you may have guessed, I&rsquo;m french. I&rsquo;m a lead data scientist at Sicara. So Secura is a service company helping its clients in data engineering and data science, so building projects for them. Before being there, I worked at realtics on optical character recognition, and I&rsquo;m now working mostly on, as you said, computer vision and also Gen AI. So I&rsquo;m leading the geni side and I&rsquo;ve been there for more than three years.</p><p>Noe Acache:
So some relevant experience on vector DB is why I&rsquo;m here today, because I did four projects, four vector soft projects, and I also wrote an article on how to choose your database in 2023, your vector database. And I did some related talks in other conferences like Pydata, DVC, all the geni meetups of London and Paris. So what are we going to talk about today? First, an overview of the vector search projects. Just to give you an idea of the kind of projects we can do with vector search. Then we will dive into the specificities of the image search project and then into the specificities of the text search project. So here are the four projects. So two in image search, two in text search. The first one is about matching objects in videos to sell them afterwards.</p><p>Noe Acache:
So basically you have a video. We first detect the object. So like it can be a lamp, it can be a piece of clothes, anything, we classify it and then we compare it to a large selection of similar objects to retrieve the most similar one to a large collection of sellable objects. The second one is about deduplicating real estate adverts. So when agencies want to sell a property, like sometimes you have several agencies coming to take pictures of the same good. So you have different pictures of the same good. And the idea of this project was to match the different pictures of the same good, the same profile.</p><p>Demetrios:
I&rsquo;ve seen that dude. I have been a victim of that. When I did a little house shopping back like five years ago, it would be the same house in many different ones, and sometimes you wouldn&rsquo;t know because it was different photos. So I love that you were thinking about it that way. Sorry to interrupt.</p><p>Noe Acache:
Yeah, so to be fair, it was the idea of my client. So basically I talk about it a bit later with aggregating all the adverts and trying to deduplicate them. And then the last two projects are about drugs retrieval, augmented generation. So the idea to be able to ask questions to your documentation. The first one was for my company&rsquo;s documentation and the second one was for a medical company. So different kind of complexities. So now we know all about this project, let&rsquo;s dive into them. So regarding the image search project, to compute representations of the images, the best performing model from the benchmark, and also from my experience, is currently Dino V two.</p><p>Noe Acache:
So a model developed by meta that you may have seen, which is using visual transformer. And what&rsquo;s amazing about it is that using the attention map, you can actually segment what&rsquo;s important in the picture, although you haven&rsquo;t told it specifically what&rsquo;s important. And as a human, it will learn to focus on the dog, on this picture and do not take into consideration the noisy background. So when I say best performing model, I&rsquo;m talking about comparing to other architecture like Resnet efficient nets models, an approach I haven&rsquo;t tried, which also seems interesting. If anyone tried it for similar project, please reach out afterwards. I&rsquo;ll be happy to talk about it. Is sift for feature transform something about feature transform. It&rsquo;s basically a more traditional method without learned features through machine learning, as in you don&rsquo;t train the model, but it&rsquo;s more traditional methods.</p><p>Noe Acache:
And you basically detect the different features in an image and then try to find the same features in an image which is supposed to post to be the same. All the blue line trying to match the different features. Of course it&rsquo;s made to match image with exactly the same content, so it wouldn&rsquo;t really work. Probably not work in the first use case, because we are trying to match similar clothes, but which are not exactly the same one. And also it&rsquo;s known to be not very resilient with the changes of angles when it changes too much, et cetera. So it may not be very good as well for the second use case, but again, I haven&rsquo;t tried it, so just leaving it here on the side. Just a quick word about how Dino works in case you&rsquo;re interested. So it&rsquo;s a vision transformer and it&rsquo;s trade in an unsupervised way, as in you don&rsquo;t have any labels provided, so you just take pictures and you first extract small crops and large crops and you augment them.</p><p>Noe Acache:
And then you&rsquo;re going to use the model to compute vectors, representations of each of these crops. And since they all represent the same image, they should all be the same. So then you can compute a loss to see how they diverge and to basically train them to become the same. So this is how it works and how it works. And the difference between the second version is just that they use more data sets and the distillation method to have a very performant model, which is also very fast to run regarding the first use case. So, matching objects in videos to sellable items for people who use Google lengths before, it&rsquo;s quite similar, where in Google lens you can take a picture of something and then it will try to find similar objects to buy. So again, you have a video and then you detect one of the objects in the video, put it and compare it to a vector database which contains a lot of objects which are similar for the representation. And then it will output the most similar lamp here.</p><p>Noe Acache:
Now we&rsquo;re going to try to analyze how this project went regarding the positive outcomes and the changes we faced. So basically what was great is that Dino manages to understand all objects and close patterns without fine tuning. So you can get an off the shelf model and get started very quickly and start bringing value very quickly without having to go through all the fine tuning processes. And it also manages to focus on the object without segmentation. What I mean here is that we&rsquo;re going to get a box of the object, and in this box there will be a very noisy background which may disturb the matching process. And since Dino really manages to focus on the object, that&rsquo;s important on the image. It doesn&rsquo;t really matter that we don&rsquo;t segmentate perfectly the image. Regarding the vector database, this project started a while ago, and I think we chose the vector database something like a year and a half ago.</p><p>Noe Acache:
And so it was before all the vector database hype. And at the time, the most famous one was Milvos, the only famous one actually. And we went for an on premise development deployment. And actually our main learning is that the DevOps team really struggled to deploy it, because basically it&rsquo;s made of a lot of pods. And the documentations about how these pods are supposed to interact together is not really perfect. And it was really buggy at this time. So the clients lost a lot of time and money in this deployment. The challenges, other challenges we faced is that we noticed that the matching wasn&rsquo;t very resilient to large distortions.</p><p>Noe Acache:
So for furnitures like lamps, it&rsquo;s fine. But let&rsquo;s say you have a trouser and a person walking. So the trouser won&rsquo;t exactly have the same shape. And since you haven&rsquo;t trained your model to specifically know, it shouldn&rsquo;t focus on the movements. It will encode this movement. And then in the matching, instead of matching trouser, which looks similar, it will just match trouser where in the product picture the person will be working as well, which is not really what we want. And the other challenges we faced is that we tried to fine tune the model, but our first fine tuning wasn&rsquo;t very good because we tried to take an open source model and, and get the labels it had, like on different furnitures, clothes, et cetera, to basically train a model to classify the different classes and then remove the classification layer to just keep the embedding parts. The thing is that the labels were not specific enough.</p><p>Noe Acache:
So the training task was quite simple. And at the end, the embeddings was not learning any very complex features, so it was not really improving it. So jumping onto the areas of improvement, knowing all of that, the first thing I would do if I had to do it again will be to use the managed milboss for a better fine tuning, it would be to labyd hard examples, hard pairs. So, for instance, you know that when you have a matching pair where the similarity score is not too high or not too low, you know, it&rsquo;s where the model kind of struggles and you will find some good matching and also some mistakes. So it&rsquo;s where it kind of is interesting to level to then be able to fine tune your model and make it learn more complex things according to your tasks. Another possibility for fine tuning will be some sort of multilabel classification. So for instance, if you consider tab close, you could say, all right, those disclose contain buttons. It have a color, it have stripes.</p><p>Noe Acache:
And for all of these categories, you&rsquo;ll get a score between zero and one. And concatenating all these scores together, you can get an embedding which you can put in a vector database for your vector search. It&rsquo;s kind of hard to scale because you need to do a specific model and labeling for each type of object. And I really wonder how Google lens does because their algorithm work very well. So are they working more like with this kind of functioning or this kind of functioning? So if anyone had any thought on that or any idea, again, I&rsquo;d be happy to talk about it afterwards. And finally, I feel like we made a lot of advancements in multimodal training, trying to combine text inputs with image. We&rsquo;ve made input to build some kind of complex embeddings. And how great would it be to have an image embeding you could guide with text.</p><p>Noe Acache:
So you could just like when creating an embedding of your image, just say, all right, here, I don&rsquo;t care about the movements, I only care about the features on the object, for instance. And then it will learn an embedding according to your task without any fine tuning. I really feel like with the current state of the arts we are able to do this. I mean, we need to do it, but the technology is ready.</p><p>Demetrios:
Can I ask a few questions before you jump into the second use case?</p><p>Noe Acache:
Yes.</p><p>Demetrios:
What other models were you looking at besides the dyno one?</p><p>Noe Acache:
I said here, compared to Resnet, efficient nets and these kind of architectures.</p><p>Demetrios:
Maybe this was too early, or maybe it&rsquo;s not actually valuable. Was that like segment anything? Did that come into the play?</p><p>Noe Acache:
So segment anything? I don&rsquo;t think they redo embeddings. It&rsquo;s really about segmentation. So here I was just showing the segmentation part because it&rsquo;s a cool outcome of the model and it shows that the model works well here we are really here to build a representation of the image we cannot really play with segment anything for the matching, to my knowledge, at least.</p><p>Demetrios:
And then on the next slide where you talked about things you would do differently, or the last slide, I guess the areas of improvement you mentioned label hard examples for fine tuning. And I feel like, yeah, there&rsquo;s one way of doing it, which is you hand picking the different embeddings that you think are going to be hard. And then there&rsquo;s another one where I think there&rsquo;s tools out there now that can kind of show you where there are different embeddings that aren&rsquo;t doing so well or that are more edge cases.</p><p>Noe Acache:
Which tools are you talking about?</p><p>Demetrios:
I don&rsquo;t remember the names, but I definitely have seen demos online about how it&rsquo;ll give you a 3d space and you can kind of explore the different embeddings and explore what&rsquo;s going on I.</p><p>Noe Acache:
Know exactly what you&rsquo;re talking about. So tensorboard embeddings is a good tool for that. I could actually demo it afterwards.</p><p>Demetrios:
Yeah, I don&rsquo;t want to get you off track. That&rsquo;s something that came to mind if.</p><p>Noe Acache:
You&rsquo;Re talking about the same tool. Turns out embedding. So basically you have an embedding of like 1000 dimensions and it just reduces it to free dimensions. And so you can visualize it in a 3d space and you can see how close your embeddings are from each other.</p><p>Demetrios:
Yeah, exactly.</p><p>Noe Acache:
But it&rsquo;s really for visualization purposes, not really for training purposes.</p><p>Demetrios:
Yeah, okay, I see.</p><p>Noe Acache:
Talking about the same thing.</p><p>Demetrios:
Yeah, I think that sounds like what I&rsquo;m talking about. So good to know on both of these. And you&rsquo;re shooting me straight on it. Mike is asking a question in here, like text embedding, would that allow you to include an image with alternate text?</p><p>Noe Acache:
An image with alternate text? I&rsquo;m not sure the question.</p><p>Demetrios:
So it sounds like a way to meet regulatory accessibility requirements if you have. I think it was probably around where you were talking about the multimodal and text to guide the embeddings and potentially would having that allow you to include an image with alternate text?</p><p>Noe Acache:
The idea is not to. I feel like the question is about inserting text within the image. It&rsquo;s what I understand. My idea was just if you could create an embedding that could combine a text inputs and the image inputs, and basically it would be trained in such a way that the text would basically be used as a guidance of the image to only encode the parts of the image which are required for your task to not be disturbed by the noisy.</p><p>Demetrios:
Okay. Yeah. All right, Mike, let us know if that answers the question or if you have more. Yes. He&rsquo;s saying, yeah, inserting text with image for people who can&rsquo;t see.</p><p>Noe Acache:
Okay, cool.</p><p>Demetrios:
Yeah, right on. So I&rsquo;ll let you keep cruising and I&rsquo;ll try not to derail it again. But that was great. It was just so pertinent. I wanted to stop you and ask some questions.</p><p>Noe Acache:
Larry, let&rsquo;s just move in. So second use case is about deduplicating real estate adverts. So as I was saying, you have two agencies coming to take different pictures of the same property. And the thing is that they may not put exactly the same price or the same surface or the same location. So you cannot just match them with metadata. So what our client was doing beforehand, and he kind of built a huge if machine, which is like, all right, if the location is not too far and if the surface is not too far. And the price, and it was just like very complex rules. And at the end there were a lot of edge cases.</p><p>Noe Acache:
It was very hard to maintain. So it was like, let&rsquo;s just do a simpler solution just based on images. So it was basically the task to match images of the same properties. Again on the positive outcomes is that the dino really managed to understand the patterns of the properties without any fine tuning. And it was resilient to read different angles of the same room. So like on the pictures I shown, I just showed, the model was quite good at identifying. It was from the same property. Here we used cudrant for this project was a bit more recent.</p><p>Noe Acache:
We leveraged a lot the metadata filtering because of course we can still use the metadata even it&rsquo;s not perfect just to say, all right, only search vectors, which are a price which is more or less 10% this price. The surface is more or less 10% the surface, et cetera, et cetera. And indexing of this metadata. Otherwise the search is really slowed down. So we had 15 million vectors and without this indexing, the search could take up to 20, 30 seconds. And with indexing it was like in a split second. So it was a killer feature for us. And we use quantization as well to save costs because the task was not too hard.</p><p>Noe Acache:
Since using the metadata we managed to every time reduce the task down to a search of 1000 vector. So it wasn&rsquo;t too annoying to quantize the vectors. And at the end for 15 million vectors, it was only $275 per month, which with the village version, which is very decent. The challenges we faced was really about bathrooms and empty rooms because all bathrooms kind of look similar. They have very similar features and same for empty rooms since there is kind of nothing in them, just windows. The model would often put high similarity scores between two bathroom of different properties and same for the empty rooms. So again, the method to overcome this thing will be to label harpers. So example were like two images where the model would think they are similar to actually tell the model no, they are not similar to allow it to improve its performance.</p><p>Noe Acache:
And again, same thing on the future of image embedding. I really think it&rsquo;s something the technology is ready for and would really help this kind of embedding model jumping onto the text search projects. So the principle of retribution generation for those of you who are not familiar with it is just you take some documents, you have an embedding model here, an embedding model trained on text and not on images, which will output representations from these documents, put it in a vector database, and then when a user will ask a question over the documentation, it will create an embedding of the request and retrieve the most similar documents. And afterwards we usually pass it to an LLM, which will generate an answer. But here in this talk, we won&rsquo;t focus on the overall product, but really on the vector search part. So the two projects was one, as I told you, a rack for my nutrition company, so endosion with around a few hundred thousand of pages, and the second one was for medical companies, so for the doctors. So it was really about the documentation search rather than the LLM, because you cannot output any mistake. The model we used was OpenAI Ada two.</p><p>Noe Acache:
Why? Mostly because for the first use case it&rsquo;s multilingual and it was off the shelf, very easy to use, so we did not spend a lot of time on this project. So using an API model made it just much faster. Also it was multilingual, approved by the community, et cetera. For the second use case, we&rsquo;re still working on it. So since we use GPT four afterwards, because it&rsquo;s currently the best LLM, it was also easier to use adatu to start with, but we may use a better one afterwards because as I&rsquo;m saying, it&rsquo;s not the best one if you refer to the MTAB. So the massive text embedding benchmark made by hugging face, which basically gathers a lot of embeddings benchmark such as retrieval for instance, and so classified the different model for these benchmarks. The M tab is not perfect because it&rsquo;s not taking into account cross language capabilities. All the benchmarks are just for one language and it&rsquo;s not as well taking into account most of the languages, like it&rsquo;s only considering English, Polish and Chinese.</p><p>Noe Acache:
And also it&rsquo;s probably biased for models trained on close source data sets. So like most of the best performing models are currently closed source APIs and hence closed source data sets, and so we don&rsquo;t know how they&rsquo;ve been trained. So they probably trained themselves on these data sets. At least if I were them, it&rsquo;s what I would do. So I assume they did it to gain some points in these data sets.</p><p>Demetrios:
So both of these rags are mainly with documents that are in French?</p><p>Noe Acache:
Yes. So this one is French and English, and this one is French only.</p><p>Demetrios:
Okay. Yeah, that&rsquo;s why the multilingual is super important for these use cases.</p><p>Noe Acache:
Exactly. Again, for this one there are models for French working much better than other two, so we may change it afterwards, but right now the performance we have is decent. Since both projects are very similar, I&rsquo;ll jump into the conclusion for both of them together. So Ada two is good for understanding diverse context, wide range of documentation, medical contents, technical content, et cetera, without any fine tuning. The cross language works quite well, so we can ask questions in English and retrieve documents in French and the other way around. And also, quick note, because I did not do it from the start, is that when using an API model, it&rsquo;s much faster to use it in asynchronous mode like the embedding equation went something like ten times or 100 times faster. So it was definitely, it changed a lot of things. Again, here we use cudrant mostly to leverage the free tier so they have a free version.</p><p>Noe Acache:
So you can pop it in a second, get the free version, and using the feature which allows to put the vectors on disk instead of storing them on ram, which makes it a bit slower, you can easily support few hundred thousand of vectors and with a very decent response time. The challenge we faced is that mostly for the notion, so like mostly in notion, we have a lot of pages which are just a title because they are empty, et cetera. And so when pages have just a title, the content is so small that it will be very similar actually to a question. So often the documents were retrieved were document with very little content, which was a bit frustrating. Chunking appropriately was also tough. Basically, if you want your retrieval process to work well, you have to divide your documents the right way to create the embeddings. So you can use matrix rules, but basically you need to divide your documents in content which semantically makes sense and it&rsquo;s not always trivial. And also for the rag, for the medical company, sometimes we are asking questions about a specific drug and it&rsquo;s just not under our search is just not retrieving the good documents, which is very frustrating because a basic search would.</p><p>Noe Acache:
So to handle these changes, a good option would be to use models handing differently question and documents like Bg or cohere. Basically they use the same model but trained differently on long documents and questions which allow them to map them differently in the space. And my guess is that using such model documents, which are only a title, et cetera, will not be as close as the question as they are right now because they will be considered differently. So I hope it will help this problem. Again, it&rsquo;s just a guess, maybe I&rsquo;m wrong. Heap research so for the keyword problem I was mentioning here, so in the recent release, Cudran just enabled sparse vectors which make actually TFEdev vectors possible. The TFEDEF vectors are vectors which are based on keywords, but basically there is one number per possible word in the data sets, and a lot of zeros, so storing them as a normal vector will make the vector search very expensive. But as a sparse vector it&rsquo;s much better.</p><p>Noe Acache:
And so you can build a debrief search combining the TFDF search for keyword search and the other search for semantic search to get the best of both worlds and overcome this issue. And finally, I&rsquo;m actually quite surprised that with all the work that is going on, generative AI and rag, nobody has started working on a model to help with chunking. It&rsquo;s like one of the biggest challenge, and I feel like it&rsquo;s quite doable to have a model which will our model, or some kind of algorithm which will understand the structure of your documentation and understand why it semantically makes sense to chunk your documents. Dude, so good.</p><p>Demetrios:
I got questions coming up. Don&rsquo;t go anywhere. Actually, it&rsquo;s not just me. Tom&rsquo;s also got some questions, so I&rsquo;m going to just blame it on Tom, throw him under the bus. Rag with medical company seems like a dangerous use case. You can work to eliminate hallucinations and other security safety concerns, but you can&rsquo;t make sure that they&rsquo;re completely eliminated, right? You can only kind of make sure they&rsquo;re eliminated. And so how did you go about handling these concerns?</p><p>Noe Acache:
This is a very good question. This is why I mentioned this project is mostly about the document search. Basically what we do is that we use chainlit, which is a very good tool for chatting, and then you can put a react front in front of it to make it very custom. And so when the user asks a question, we provide the LLM answer more like as a second thought, like something the doctor could consider as a fagon thought. But what&rsquo;s the most important is that we directly put the, instead of just citing the sources, we put the HTML of the pages the source is based on, and what bring the most value is really these HTML pages. And so we know the answer may have some problems. The fact is, based on documents, hallucinations are almost eliminated. Like, we don&rsquo;t notice any hallucinations, but of course they can happen.</p><p>Noe Acache:
So it&rsquo;s really the way, it&rsquo;s really a product problem rather than an algorithm problem, an algorithmic problem, yeah. The documents retrieved rather than the LLM answer.</p><p>Demetrios:
Yeah, makes sense. My question around it is a lot of times in the medical space, the data that is being thrown around is super sensitive. Right. And you have a lot of Pii. How do you navigate that? Are you just not touching that?</p><p>Noe Acache:
So basically we work with a provider in front which has public documentation. So it&rsquo;s public documentation. There is no PII.</p><p>Demetrios:
Okay, cool. So it&rsquo;s not like some of it.</p><p>Noe Acache:
Is private, but still there is no PII in the documents.</p><p>Demetrios:
Yeah, because I think that&rsquo;s another really incredibly hard problem is like, oh yeah, we&rsquo;re just sending all this sensitive information over to the IDA model to create embeddings with it. And then we also pass it through Chat GPT before we get it back. And next thing you know, that is the data that was used to train GPT five. And you can say things like create an unlimited poem and get that out of it. So it&rsquo;s super sketchy, right?</p><p>Noe Acache:
Yeah, of course, one way to overcome that is to, for instance, for the notion project, it&rsquo;s our private documentation. We use Ada over Azure, which guarantees data safety. So it&rsquo;s quite a good workaround. And when you have to work with different level of security, if you deal with PII, a good way is to play with metadata. Depending on the security level of the person who has the question, you play with the metadata to output only some kind of documents. The database metadata.</p><p>Demetrios:
Excellent. Well, don&rsquo;t let me stop you. I know you had some conclusionary thoughts there.</p><p>Noe Acache:
No, sorry, I was about to conclude anyway. So just to wrap it up, so we got some good models without any fine tuning. With the model, we tried to overcome them, to overcome these limitations we still faced. For MS search, fine tuning is required at the moment. There&rsquo;s no really any other way to overcome it otherwise. While for tech search, fine tuning is not really necessary, it&rsquo;s more like tricks which are required about using eBrid search, using better models, et cetera. So two kind of approaches, Qdrant really made a lot of things easy. For instance, I love the feature where you can use the database as a disk file.</p><p>Noe Acache:
You can even also use it in memory for CI integration and stuff. But since for all my experimentations, et cetera, I won&rsquo;t use it as a disk file because it&rsquo;s much easier to play with. I just like this feature. And then it allows to use the same tool for your experiment and in production. When I was playing with milverse, I had to use different tools for experimentation and for the database in production, which was making the technical stock a bit more complex. Sparse vector for Tfedef, as I was mentioning, which allows to search based on keywords to make your retrieval much better. Manage deployment again, we really struggle with the deployment of the, I mean, the DevOps team really struggled with the deployment of the milverse. And I feel like in most cases, except if you have some security requirements, it will be much cheaper to use the managed deployments rather than paying dev costs.</p><p>Noe Acache:
And also with the free cloud and on these vectors, you can really do a lot of, at least start a lot of projects. And finally, the metadata filtering and indexing. So by the way, we went into a small trap. It&rsquo;s that indexing. It&rsquo;s recommended to index on your metadata before adding your vectors. Otherwise your performance may be impacted. So you may not retrieve the good vectors that you need. So it&rsquo;s interesting thing to take into consideration.</p><p>Noe Acache:
I know that metadata filtering is something quite hard to do for vector database, so I don&rsquo;t really know how it works, but I assume there is a good reason for that. And finally, as I was mentioning before, in my view, new types of models are needed to answer industrial needs. So the model we are talking about, tech guidance to make better image embeddings and automatic chunking, like some kind of algorithm and model which will automatically chunk your documents appropriately. So thank you very much. If you still have questions, I&rsquo;m happy to answer them. Here are my social media. If you want to reach me out afterwards, twitch out afterwards, and all my writing and talks are gathered here if you&rsquo;re interested.</p><p>Demetrios:
Oh, I like how you did that. There is one question from Tom again, asking about if you did anything to handle images and tables within the documentation when you were doing those rags.</p><p>Noe Acache:
No, I did not do anything for the images and for the tables. It depends when they are well structured. I kept them because the model manages to understand them. But for instance, we did a small pock for the medical company when he tried to integrate some external data source, which was a PDF, and we wanted to use it as an HTML to be able to display the HTML otherwise explained to you directly in the answer. So we converted the PDF to HTML and in this conversion, the tables were absolutely unreadable. So even after cleaning. So we did not include them in this case.</p><p>Demetrios:
Great. Well, dude, thank you so much for coming on here. And thank you all for joining us for yet another vector space talk. If you would like to come on to the vector space talk and share what you&rsquo;ve been up to and drop some knowledge bombs on the rest of us, we&rsquo;d love to have you. So please reach out to me. And I think that is it for today. Noe, this was awesome, man. I really appreciate you doing this.</p><p>Noe Acache:
Thank you, Demetrius. Have a nice day.</p><p>Demetrios:
We&rsquo;ll see you all later. Bye.</p></div></div></article></section><section class=get-started-blogs><div class=container><div class=get-started-blogs__content><div class=row><div class="get-started-blogs__text col-12 col-lg-7"><h3 class=get-started-blogs__title>Get Started with Qdrant Free</h3><a href=https://cloud.qdrant.io/signup class="button button_contained" target=_blank>Get Started</a></div><div class="get-started-blogs__image col-12 col-lg-5"><img src=https://qdrant.tech/img/rocket.svg alt></div></div><div class=get-started-blogs__overlay-top></div></div></div></section></main><footer class=footer><div class=footer__top><div class=container><div class="row justify-content-md-between"><div class="col-12 col-md-6"><a href=https://qdrant.tech/ title="Go to Home Page"><img class=footer__top-logo src=https://qdrant.tech/img/logo-white.png alt="Qdrant Logo"></a></div><div class="col-12 col-md-6 footer__top-social-media-platforms"><a class=footer__top-social-media-link href=https://github.com/qdrant/qdrant target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g clip-path="url(#clip0_1841_958)"><path fill-rule="evenodd" clip-rule="evenodd" d="M12 .299805C5.35.299805.0 5.6498.0 12.2998c0 5.3 3.45 9.8 8.2 11.4C8.8 23.7998 9 23.4498 9 23.0998 9 22.7998 9 22.0498 9 21.0498c-3.35.75-4.05-1.6-4.05-1.6-.55-1.4-1.35-1.75-1.35-1.75-1.1-.75.1-.75.1-.75C4.9 17.0498 5.55 18.1998 5.55 18.1998c1.05 1.85 2.8 1.3 3.5 1C9.15 18.3998 9.45 17.8998 9.8 17.5998 7.15 17.2998 4.35 16.2498 4.35 11.6498c0-1.3.45-2.4 1.25-3.2C5.5 8.1498 5.05 6.9498 5.7 5.2498c0 0 1-.3 3.3 1.25C9.95 6.2498 11 6.0998 12 6.0998S14.05 6.2498 15 6.4998c2.3-1.55 3.3-1.25 3.3-1.25C18.95 6.8998 18.55 8.0998 18.4 8.4498c.75.85 1.25 1.9 1.25 3.2.0 4.6-2.8 5.6-5.5 5.9C14.6 17.8998 14.95 18.6498 14.95 19.7498c0 1.6.0 2.9.0 3.3C14.95 23.3498 15.15 23.7498 15.8 23.6498c4.75-1.55 8.2-6.05 8.2-11.35C24 5.6498 18.65.299805 12 .299805z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_1841_958"><rect width="24" height="24" fill="#fff"/></clipPath></defs></svg>
</a><a class=footer__top-social-media-link href=https://qdrant.to/linkedin target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g clip-path="url(#clip0_1841_961)"><path d="M21.75.75H2.25c-.39782.0-.77936.158035-1.06066.43934C.908035 1.47064.75 1.85218.75 2.25v19.5C.75 22.1478.908035 22.5294 1.18934 22.8107 1.47064 23.092 1.85218 23.25 2.25 23.25h19.5C22.1478 23.25 22.5294 23.092 22.8107 22.8107 23.092 22.5294 23.25 22.1478 23.25 21.75V2.25C23.25 1.85218 23.092 1.47064 22.8107 1.18934 22.5294.908035 22.1478.75 21.75.75zM7.41525 19.9455H4.0305V9.1875H7.41525v10.758zM5.7225 7.71075C5.33338 7.70956 4.95333 7.59309 4.63036 7.37604 4.30739 7.15899 4.05599 6.8511 3.9079 6.49126 3.75981 6.13141 3.72167 5.73575 3.79831 5.35425 3.87495 4.97275 4.06293 4.62251 4.3385 4.34778 4.61408 4.07304 4.96488 3.88613 5.34662 3.81065 5.72835 3.73517 6.1239 3.77451 6.48329 3.92369 6.84268 4.07288 7.1498 4.32522 7.36587 4.64885c.21606.32363.33138.70402.33138 1.09315C7.69735 6.00107 7.6463 6.25762 7.54702 6.49691 7.44774 6.73621 7.30219 6.95355 7.11872 7.13646 6.93525 7.31938 6.71746 7.46426 6.47787 7.56282 6.23827 7.66137 5.98157 7.71164 5.7225 7.71075zM19.9657 19.9455H16.65V14.742c0-1.2652.0-2.8125-1.7625-2.8125s-1.9748 1.3365-1.9748 2.742V20.016H9.6V9.1875h3.102v1.4767H12.7725C13.0924 10.1111 13.5567 9.65537 14.1156 9.34571 14.6746 9.03606 15.3072 8.88415 15.9457 8.90625c3.3848.0 4.0193 2.24995 4.0193 5.13295L19.9657 19.9455z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_1841_961"><rect width="24" height="24" fill="#fff"/></clipPath></defs></svg>
</a><a class=footer__top-social-media-link href=https://qdrant.to/twitter target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M10.8423 15.1515 4.40655 22.5H.841797L9.1773 12.978l1.665 2.1735z" fill="#f0f3fa"/><path d="M12.7881 8.241 18.6808 1.5h3.5625l-7.8052 8.9265-1.65-2.1855z" fill="#f0f3fa"/><path d="M23.6158 22.5H16.4465L.383789 1.5H7.73454l15.88126 21zm-6.186-2.1322h1.974L6.66204 3.5205h-2.118L17.4298 20.3678z" fill="#f0f3fa"/></svg>
</a><a class=footer__top-social-media-link href=https://qdrant.to/discord target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M5 1C2.79086 1 1 2.79086 1 5V19c0 2.2091 1.79086 4 4 4H19c2.2091.0 4-1.7909 4-4V5c0-2.20914-1.7909-4-4-4H5zM16.3683 18.5964S15.8027 17.9208 15.3314 17.3238C16.4701 17.0557 17.4774 16.3935 18.1749 15.4543 17.6098 15.8317 17.0037 16.1438 16.3683 16.3846 15.6374 16.6966 14.873 16.9233 14.0903 17.0602 12.7448 17.3079 11.3649 17.3026 10.0213 17.0445 9.23256 16.8901 8.45954 16.664 7.71193 16.3689 7.08195 16.1284 6.48116 15.8174 5.92097 15.442 6.59328 16.3616 7.56575 17.0173 8.67025 17.2958 8.19895 17.8928 7.61767 18.5998 7.61767 18.5998 4.14571 18.4898 2.82605 16.2091 2.82605 16.2091 2.87704 13.0242 3.65058 9.89242 5.08832 7.05004 6.35356 6.05636 7.89607 5.47998 9.5029 5.40046l.1571.18853c-1.51174.37413-2.92245 1.0768-4.13179 2.05804.0.0.34562-.18853.926900000000001-.4556C7.58438 6.67595 8.78793 6.34193 10.0213 6.20168 10.1093 6.18348 10.1986 6.17297 10.2884 6.17026 11.3412 6.0331 12.4066 6.02255 13.4619 6.13884c1.6595.18933 3.2659.70144 4.7288 1.5075C17.0426 6.71186 15.7093 6.0318 14.2788 5.65114L14.4988 5.39978C16.1056 5.4793 17.6481 6.05568 18.9133 7.04935c1.4378 2.84239 2.2113 5.97415 2.2623 9.15905.0.0-1.3354 2.278-4.8073 2.388zM9.06284 11.2616C8.62563 11.2983 8.21817 11.498 7.9212 11.821 7.62423 12.1439 7.45941 12.5667 7.45941 13.0054c0 .438800000000001.16482.8615.46179 1.1845S8.62563 14.7125 9.06284 14.7493C9.50005 14.7125 9.90751 14.5129 10.2045 14.1899 10.5015 13.8669 10.6663 13.4442 10.6663 13.0054c0-.438699999999999-.1648-.8615-.4618-1.1844C9.90751 11.498 9.50005 11.2983 9.06284 11.2616zm5.73766.0C14.4493 11.2319 14.0974 11.3089 13.7907 11.4825c-.3066.1736-.553699999999999.4358-.7089.7522-.155199999999999.3164-.2112.6723-.1608 1.021C12.9714 13.6045 13.1259 13.93 13.3644 14.1894 13.6028 14.4489 13.9141 14.6304 14.2573 14.71 14.6006 14.7897 14.96 14.7639 15.2883 14.6359c.3284-.1279.6104-.352.8093-.642899999999999C16.2965 13.702 16.4029 13.3578 16.4029 13.0054 16.4124 12.7854 16.3783 12.5657 16.3026 12.3588 16.2269 12.152 16.1112 11.9622 15.962 11.8002 15.8128 11.6381 15.6331 11.5072 15.4332 11.4148 15.2333 11.3223 15.0171 11.2703 14.7971 11.2616H14.8005z" fill="#f0f3fa"/></svg>
</a><a class=footer__top-social-media-link href=https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA target=_blank rel="noopener noreferrer nofollow"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M23.775 7.1999S23.55 5.5499 22.8 4.7999C21.9 3.8249 20.85 3.8249 20.4 3.7499c-3.375-.225-8.4-.225-8.4-.225s-5.025.0-8.4.225C3.15 3.8249 2.1 3.8249 1.2 4.7999c-.75.75-.975 2.4-.975 2.4S0 9.1499.0 11.0999v1.8c0 1.95.225 3.9.225 3.9s.225 1.65.975 2.4C2.1 20.1749 3.3 20.0999 3.825 20.2499 5.775 20.3999 12 20.4749 12 20.4749S17.025 20.4749 20.4 20.2499C20.85 20.1749 21.9 20.1749 22.8 19.1999 23.55 18.4499 23.775 16.7999 23.775 16.7999S24 14.8499 24 12.8999v-1.8C24 9.1499 23.775 7.1999 23.775 7.1999zm-14.25 7.95v-6.75l6.45 3.375-6.45 3.375z" fill="#f0f3fa"/></svg></a></div></div></div></div><div class=footer__menu><div class=container><nav class=footer__menu-content><div class=footer__menu-section><p class=footer__menu-section-title>Products</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/qdrant-vector-database/>Qdrant Vector Database</a></li><li class=footer__menu-item><a href=https://qdrant.tech/cloud/>Qdrant Cloud</a></li><li class=footer__menu-item><a href=https://qdrant.tech/hybrid-cloud/>Qdrant Hybrid Cloud</a></li><li class=footer__menu-item><a href=https://qdrant.tech/enterprise-solutions/>Qdrant Enterprise Solutions</a></li><li class=footer__menu-item><a href=https://qdrant.tech/pricing/>Pricing</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Use Cases</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/advanced-search/>Advanced Search</a></li><li class=footer__menu-item><a href=https://qdrant.tech/recommendations/>Recommendation Systems</a></li><li class=footer__menu-item><a href=https://qdrant.tech/rag/>Retrieval Augmented Generation</a></li><li class=footer__menu-item><a href=https://qdrant.tech/data-analysis-anomaly-detection/>Data Analysis & Anomaly Detection</a></li><li class=footer__menu-item><a href=https://qdrant.tech/ai-agents/>AI Agents</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Developers</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/documentation/>Documentation</a></li><li class=footer__menu-item><a href=https://qdrant.tech/community/>Community</a></li><li class=footer__menu-item><a href=https://github.com/qdrant/qdrant target=_blank rel="noopener noreferrer nofollow">GitHub</a></li><li class=footer__menu-item><a href=https://qdrant.to/roadmap target=_blank rel="noopener noreferrer nofollow">Roadmap</a></li><li class=footer__menu-item><a href=https://github.com/qdrant/qdrant/releases target=_blank rel="noopener noreferrer nofollow">Change Log</a></li><li class=footer__menu-item><a href=https://status.qdrant.io/ target=_blank rel="noopener noreferrer nofollow">Status Page</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Resources</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/blog/>Blog</a></li><li class=footer__menu-item><a href=https://qdrant.tech/benchmarks/>Benchmarks</a></li><li class=footer__menu-item><a href=https://qdrant.tech/articles/>Articles</a></li><li class=footer__menu-item><a href=https://try.qdrant.tech/events target=_blank rel="noopener noreferrer nofollow">Events</a></li><li class=footer__menu-item><a href=https://qdrant.tech/qdrant-for-startups/>Startup Program</a></li><li class=footer__menu-item><a href=https://qdrant.tech/demo/>Demos</a></li><li class=footer__menu-item><a href=https://qdrant.tech/security/bug-bounty-program/>Bug Bounty</a></li></ul></div><div class=footer__menu-section><p class=footer__menu-section-title>Company</p><ul class=footer__menu-items><li class=footer__menu-item><a href=https://qdrant.tech/about-us/>About Us</a></li><li class=footer__menu-item><a href=https://qdrant.tech/customers/>Customers</a></li><li class=footer__menu-item><a href=https://qdrant.tech/partners/>Partners</a></li><li class=footer__menu-item><a href=https://qdrant.join.com/ target=_blank rel="noopener noreferrer nofollow">Careers</a></li><li class=footer__menu-item><a href=https://qdrant.tech/contact-us/>Contact Us</a></li></ul></div></nav></div></div><div class=footer__middle><div class=container><div class="align-items-center row"><div class="col-12 col-lg-5"><p class=footer__middle-title>Sign up for Qdrant updates</p><p class=footer__middle-subtitle>We'll occasionally send you best practices for using vector data and similarity search, as well as product news.</p></div><div class="footer__middle-newsletter col-12 col-lg-7"><div id=footer-subscribe-form><script>(function(){const t={region:"eu1",portalId:"139603372",formId:"049d96c6-ef65-4e41-ba69-a3335b9334cf",cssClass:"subscribe-form",submitButtonClass:"button button_contained button_lg",submitText:"Subscribe"},n="weh",s=function(){const e=document.createElement("input");return e.classList.add(n),e.type="text",e.name="my-work-email",e.style.display="none",e.placeholder="Email",e.ariaHidden="true",e},o=function(e){const t=s();e.appendChild(t);const n=e.querySelector('[type="submit"]');t.addEventListener("input",function(){t.value.length>0&&(n.disabled=!0)})},i=["Argentina","Belgium","Canada","Czech Republic","Cyprus","Denmark","Germany","Hungary","Latvia","Liechtenstein","Luxembourg","Netherlands","Norway","France","Finland","Croatia","Bulgaria","Belarus","Bosnia and Herzegovina","Austria","Estonia","Georgia","Greenland","Hong Kong","Israel","Italy","Maldives","Moldova","Monaco","Portugal","Russia","Serbia","Slovakia","Slovenia","Sweden","Switzerland","Türkiye","Ukraine","Macedonia (FYROM)","United Kingdom"];function e(e){const a=e.querySelector('select[name="country"]'),n=e.querySelector(".legal-consent-container .hs-fieldtype-booleancheckbox"),r=e.querySelector(".legal-consent-container > div:nth-child(3)"),c=e.querySelector(".legal-consent-container > div:nth-child(2)");if(!a||!n)return;const s=a.value,l=i.includes(s),o=s&&l,t=n.querySelector('input[type="checkbox"]');n.style.display=o?"block":"none",r&&(r.style.display=o?"block":"none"),c&&(c.style.display=o?"none":"block"),s&&!l?t.checked||t.click():t.checked&&t.click()}try{hbspt.forms.create({...t,formInstanceId:"#footer-subscribe-form",pageId:"",target:"#footer-subscribe-form",onFormReady:function(t){if(!t){console.warn("Form not found.");return}o(t),e(t);const n=t.querySelector('select[name="country"]');n&&n.addEventListener("change",()=>e(t))}})}catch{document.getElementById("footer-subscribe-form").innerHTML='<p class="text-white">Here should be a form but looks like it was blocked on your side. Please, check your trackers blocking policy.</p>'}})()</script></div></div></div></div></div><div class=footer__bottom><div class=container><div class="row g-3"><div class="col-12 col-lg-6 footer__bottom-content"><span class=footer__bottom-copyright>© 2025 Qdrant.</span><div class=footer__bottom-bages><a href=http://qdrant.to/trust-center target=_blank><img src=https://qdrant.tech/img/soc2-badge.png alt=SOC2>
</a><a href=https://heydata.eu/ target=_blank><img src=https://qdrant.tech/img/gdpr-badge.png alt="heyData GDPR">
</a><a href=https://qdrant.tech/# target=_blank><img src=https://qdrant.tech/img/dark-gdpr-badge.png alt=GDPR>
</a><a href=https://qdrant.tech/# target=_blank><img src=https://qdrant.tech/img/hipaa-badge.png alt=HIPAA></a></div></div><div class="col-12 col-lg-6 footer__bottom-links"><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></div></footer><button class="d-none button button_outlined go-to-top-button" id=scrollToTopBtn title="Go to top">Up!</button>
</body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>document.addEventListener("scroll",()=>{const e=document.getElementById("article"),t=document.getElementById("progress"),n=e.scrollHeight-window.innerHeight,s=window.scrollY-e.offsetTop;t.value=Math.min(Math.max(s/n*100,0),100)})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>