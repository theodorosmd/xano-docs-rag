<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>Vector Search Resource Optimization Guide - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Learn how to get the most from Qdrant's optimization features. Discover key tricks and best practices to boost vector search performance and reduce Qdrant's resource usage."><meta name=keywords content="vector search engine,neural network,matching,SaaS,approximate nearest neighbor search,image search,recommender system,vectors,knn algorithm,hnsw,vector search,embeddings,similarity,simaes networks,BERT,transformer,word2vec,fasttext,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/vector-search-resource-optimization/#article","@type":"Article","abstract":"Learn how to get the most from Qdrants optimization features Discover key tricks and best practices to boost vector search performance and reduce Qdrants resource usage","author":{"@type":"Person","name":"David Myriel"},"dateModified":"2025-02-09 00:00:00 +0000 UTC","datePublished":"2025-02-09 00:00:00 +0000 UTC","description":"Learn how to get the most from Qdrants optimization features Discover key tricks and best practices to boost vector search performance and reduce Qdrants resource usage","headline":"Vector Search Resource Optimization Guide","image":["https://qdrant.tech/articles_data/vector-search-resource-optimization/social_preview.png"],"name":"Vector Search Resource Optimization Guide","url":"https://qdrant.tech/articles/vector-search-resource-optimization/","wordCount":"3322"},{"@id":"https://qdrant.tech/articles/vector-search-resource-optimization/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestra√üe 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["vector search engine","neural network","matching","SaaS","approximate nearest neighbor search","image search","recommender system","vectors","knn algorithm","hnsw","vector search","embeddings","similarity","simaes networks","BERT","transformer","word2vec","fasttext","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/vector-search-resource-optimization/"><meta property="og:type" content="website"><meta property="og:title" content="Vector Search Resource Optimization Guide - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/vector-search-resource-optimization/"><meta name=twitter:title content="Vector Search Resource Optimization Guide - Qdrant"><meta property="og:description" content="Learn how to get the most from Qdrant's optimization features. Discover key tricks and best practices to boost vector search performance and reduce Qdrant's resource usage."><meta name=twitter:description content="Learn how to get the most from Qdrant's optimization features. Discover key tricks and best practices to boost vector search performance and reduce Qdrant's resource usage."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/vector-search-resource-optimization/social_preview.png"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/vector-search-resource-optimization/social_preview.png"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/vector-search-resource-optimization/social_preview.png"><meta name=author content="David Myriel"><link rel=canonical href=https://qdrant.tech/articles/vector-search-resource-optimization/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>Vector Search Resource Optimization Guide</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/vector-search-manuals/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Vector Search Manuals</a><h1 class=documentation-article__header-title>Vector Search Resource Optimization Guide</h1><div class=documentation-article__header-about><p>David Myriel</p><span>&#183;</span><p>February 09, 2025</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/vector-search-resource-optimization/preview/title.webp type=image/webp><img alt="Vector Search Resource Optimization Guide" src=https://qdrant.tech/articles_data/vector-search-resource-optimization/preview/title.jpg></picture></div><h2 id=whats-in-this-guide>What‚Äôs in This Guide?</h2><p><a href=#storage-disk-vs-ram><strong>Resource Management Strategies:</strong></a> If you are trying to scale your app on a budget - this is the guide for you. We will show you how to avoid wasting compute resources and get the maximum return on your investment.</p><p><a href=#configure-indexing-for-faster-searches><strong>Performance Improvement Tricks:</strong></a> We‚Äôll dive into advanced techniques like indexing, compression, and partitioning. Our tips will help you get better results at scale, while reducing total resource expenditure.</p><p><a href=#query-optimization><strong>Query Optimization Methods:</strong></a> Improving your vector database setup isn‚Äôt just about saving costs. We‚Äôll show you how to build search systems that deliver consistently high precision while staying adaptable.</p><hr><h4 id=remember-optimization-is-a-balancing-act>Remember: Optimization is a Balancing Act</h4><p>In this guide, we will show you how to use Qdrant‚Äôs features to meet your performance needs.
However - there are resource tradeoffs and you can&rsquo;t have it all.
It is up to you to choose the optimization strategy that best fits your goals.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/optimization.png alt=optimization style=width:75%><p>Let&rsquo;s take a look at some common goals and optimization strategies:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th>Intended Result</th><th>Optimization Strategy</th></tr></thead><tbody><tr><td><a href=https://qdrant.tech/documentation/guides/optimize/#1-high-speed-search-with-low-memory-usage><strong>High Search Precision + Low Memory Expenditure</strong></a></td><td><a href=https://qdrant.tech/documentation/guides/optimize/#1-high-speed-search-with-low-memory-usage><strong>On-Disk Indexing</strong></a></td></tr><tr><td><a href=https://qdrant.tech/documentation/guides/quantization/><strong>Low Memory Expenditure + Fast Search Speed</strong></a></td><td><a href=https://qdrant.tech/documentation/guides/quantization/><strong>Quantization</strong></a></td></tr><tr><td><a href=https://qdrant.tech/documentation/guides/optimize/#3-high-precision-with-high-speed-search><strong>High Search Precision + Fast Search Speed</strong></a></td><td><a href=https://qdrant.tech/documentation/guides/optimize/#3-high-precision-with-high-speed-search><strong>RAM Storage + Quantization</strong></a></td></tr><tr><td><a href=https://qdrant.tech/documentation/guides/optimize/#balancing-latency-and-throughput><strong>Balance Latency vs Throughput</strong></a></td><td><a href=https://qdrant.tech/documentation/guides/optimize/#balancing-latency-and-throughput><strong>Segment Configuration</strong></a></td></tr></tbody></table></div><p>After this article, check out the code samples in our docs on <a href=https://qdrant.tech/documentation/guides/optimize/><strong>Qdrant‚Äôs Optimization Methods</strong></a>.</p><hr><h2 id=configure-indexing-for-faster-searches>Configure Indexing for Faster Searches</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/index.png alt=indexing></p><p>A vector index is the central location where Qdrant calculates vector similarity. It is the backbone of your search process, retrieving relevant results from vast amounts of data.</p><p>Qdrant uses the <a href=https://qdrant.tech/documentation/concepts/indexing/#vector-index><strong>HNSW (Hierarchical Navigable Small World Graph) algorithm</strong></a> as its dense vector index, which is both powerful and scalable.</p><p><strong>Figure 2:</strong> A sample HNSW vector index with three layers. Follow the blue arrow on the top layer to see how a query travels throughout the database index. The closest result is on the bottom level, nearest to the gray query point.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/hnsw.png alt=hnsw style=width:75%><h4 id=vector-index-optimization-parameters>Vector Index Optimization Parameters</h4><p>Working with massive datasets that contain billions of vectors demands significant resources‚Äîand those resources come with a price. While Qdrant provides reasonable defaults, tailoring them to your specific use case can unlock optimal performance. Here‚Äôs what you need to know.</p><p>The following parameters give you the flexibility to fine-tune Qdrant‚Äôs performance for your specific workload. You can modify them directly in Qdrant&rsquo;s <a href=https://qdrant.tech/documentation/guides/configuration/ target=_blank rel="noopener nofollow"><strong>configuration</strong></a> files or at the collection and named vector levels for more granular control.</p><p><strong>Figure 3:</strong> A description of three key HNSW parameters.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/hnsw-parameters.png alt=hnsw-parameters style=width:75%><h4 id=1-the-m-parameter-determines-edges-per-node>1. The <code>m</code> parameter determines edges per node</h4><p>This controls the number of edges in the graph. A higher value enhances search accuracy but demands more memory and build time. Fine-tune this to balance memory usage and precision.</p><h4 id=2-the-ef_construct-parameter-controls-the-index-build-range>2. The <code>ef_construct</code> parameter controls the index build range</h4><p>This parameter sets how many neighbors are considered during index construction. A larger value improves the accuracy of the index but increases the build time. Use this to customize your indexing speed versus quality.</p><p>You need to set both the <code>m</code> and <code>ef parameters</code> as you create the collection:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>update_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;my_vector&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>VectorParamsDiff</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>hnsw_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>HnswConfigDiff</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>m</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ef_construct</span><span class=o>=</span><span class=mi>123</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h4 id=3-the-ef-parameter-updates-vector-search-range>3. The <code>ef</code> parameter updates vector search range</h4><p>This determines how many neighbors are evaluated during a search query. You can adjust this to balance query speed and accuracy.</p><p>The <code>ef</code> parameter is configured during the search process:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>query</span><span class=o>=</span><span class=p>[</span><span class=o>...</span><span class=p>]</span>
</span></span><span class=line><span class=cl>   <span class=n>search_params</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>SearchParams</span><span class=p>(</span><span class=n>hnsw_ef</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span> <span class=n>exact</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><hr><p>These are just the basics of HNSW. Learn More about <a href=https://qdrant.tech/documentation/concepts/indexing/><strong>Indexing</strong></a>.</p><hr><h2 id=data-compression-techniques>Data Compression Techniques</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/compress.png alt=compression></p><p>Efficient data compression is a cornerstone of resource optimization in vector databases. By reducing memory usage, you can achieve faster query performance without sacrificing too much accuracy.</p><p>One powerful technique is <a href=https://qdrant.tech/documentation/guides/quantization/><strong>quantization</strong></a>, which transforms high-dimensional vectors into compact representations while preserving relative similarity. Let‚Äôs explore the quantization options available in Qdrant.</p><h4 id=scalar-quantization>Scalar Quantization</h4><p>Scalar quantization strikes an excellent balance between compression and performance, making it the go-to choice for most use cases.</p><p>This method minimizes the number of bits used to represent each vector component. For instance, Qdrant compresses 32-bit floating-point values (<strong>float32</strong>) into 8-bit unsigned integers (<strong>uint8</strong>), slashing memory usage by an impressive 75%.</p><p><strong>Figure 4:</strong> The top example shows a float32 vector with a size of 40 bytes. Converting it to int8 format reduces its size by a factor of four, while maintaining approximate similarity relationships between vectors. The loss in precision compared to the original representation is typically negligible for most practical applications.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/scalar-quantization.png alt=scalar-quantization style=width:75%><h4 id=benefits-of-scalar-quantization>Benefits of Scalar Quantization:</h4><div class=table-responsive><table class="table mb-5"><thead><tr><th>Benefit</th><th>Description</th></tr></thead><tbody><tr><td><strong>Memory usage will drop</strong></td><td>Compression cuts memory usage by a factor of 4. Qdrant compresses 32-bit floating-point values (float32) into 8-bit unsigned integers (uint8).</td></tr><tr><td><strong>Accuracy loss is minimal</strong></td><td>Converting from float32 to uint8 introduces a small loss in precision. Typical error rates remain below 1%, making this method highly efficient.</td></tr><tr><td><strong>Best for specific use cases</strong></td><td>To be used with high-dimensional vectors where minor accuracy losses are acceptable.</td></tr></tbody></table></div><h4 id=set-it-up-as-you-create-the-collection>Set it up as you create the collection:</h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>vectors_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=mi>1536</span><span class=p>,</span> <span class=n>distance</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Distance</span><span class=o>.</span><span class=n>COSINE</span><span class=p>),</span>
</span></span><span class=line><span class=cl>   <span class=n>quantization_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>ScalarQuantization</span><span class=p>(</span>
</span></span><span class=line><span class=cl>       <span class=n>scalar</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>ScalarQuantizationConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=nb>type</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>ScalarType</span><span class=o>.</span><span class=n>INT8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>quantile</span><span class=o>=</span><span class=mf>0.99</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>always_ram</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=p>),</span>
</span></span><span class=line><span class=cl>   <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>When working with Qdrant, you can fine-tune the quantization configuration to optimize precision, memory usage, and performance. Here‚Äôs what the key configuration options include:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th>Configuration Option</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td>Specifies the quantized vector type (currently supports only int8).</td></tr><tr><td><code>quantile</code></td><td>Sets bounds for quantization, excluding outliers. For example, 0.99 excludes the top 1% of extreme values to maintain better accuracy.</td></tr><tr><td><code>always_ram</code></td><td>Keeps quantized vectors in RAM to speed up searches.</td></tr></tbody></table></div><p>Adjust these settings to strike the right balance between precision and efficiency for your specific workload.</p><hr><p>Learn More about <a href=https://qdrant.tech/documentation/guides/quantization/><strong>Scalar Quantization</strong></a></p><hr><h4 id=binary-quantization>Binary Quantization</h4><p><strong>Binary quantization</strong> takes scalar quantization to the next level by compressing each vector component into just <strong>a single bit</strong>. This method achieves unparalleled memory efficiency and query speed, reducing memory usage by a factor of 32 and enabling searches up to 40x faster.</p><h4 id=benefits-of-binary-quantization><strong>Benefits of Binary Quantization:</strong></h4><p>Binary quantization is ideal for large-scale datasets and compatible embedding models, where compression and speed are paramount.</p><p><strong>Figure 5:</strong> This method causes maximum compression. It reduces memory usage by 32x and speeds up searches by up to 40x.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/binary-quantization.png alt=binary-quantization style=width:75%><div class=table-responsive><table class="table mb-5"><thead><tr><th>Benefit</th><th>Description</th></tr></thead><tbody><tr><td><strong>Efficient similarity calculations</strong></td><td>Emulates Hamming distance through dot product comparisons, making it fast and effective.</td></tr><tr><td><strong>Perfect for high-dimensional vectors</strong></td><td>Works well with embedding models like OpenAI‚Äôs text-embedding-ada-002 or Cohere‚Äôs embed-english-v3.0.</td></tr><tr><td><strong>Precision management</strong></td><td>Consider rescoring or oversampling to offset precision loss.</td></tr></tbody></table></div><p>Here‚Äôs how you can enable binary quantization in Qdrant:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>vectors_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span><span class=n>size</span><span class=o>=</span><span class=mi>1536</span><span class=p>,</span> <span class=n>distance</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Distance</span><span class=o>.</span><span class=n>COSINE</span><span class=p>),</span>
</span></span><span class=line><span class=cl>   <span class=n>quantization_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>BinaryQuantization</span><span class=p>(</span>
</span></span><span class=line><span class=cl>       <span class=n>binary</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>BinaryQuantizationConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=n>always_ram</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=p>),</span>
</span></span><span class=line><span class=cl>   <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><blockquote><p>By default, quantized vectors load like original vectors unless you set <code>always_ram</code> to <code>True</code> for instant access and faster queries.</p></blockquote><hr><p>Learn more about <a href=https://qdrant.tech/documentation/guides/quantization/><strong>Binary Quantization</strong></a></p><hr><h2 id=scaling-the-database>Scaling the Database</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/shards.png alt=sharding></p><p>Efficiently managing large datasets in distributed systems like Qdrant requires smart strategies for data isolation. <strong>Multitenancy</strong> and <strong>Sharding</strong> are essential tools to help you handle high volumes of user-specific data while maintaining performance and scalability.</p><h4 id=multitenancy>Multitenancy</h4><p><strong>Multitenancy</strong> is a software architecture where multiple independent users (or tenants) share the same resources or environment. In Qdrant, a single collection with logical partitioning is often the most efficient setup for multitenant use cases.</p><p><strong>Figure 5:</strong> Each individual vector is assigned a specific payload that denotes which tenant it belongs to. This is how a large number of different tenants can share a single Qdrant collection.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/multitenancy.png alt=multitenancy style=width:75%><p><strong>Why Choose Multitenancy?</strong></p><ul><li><strong>Logical Isolation</strong>: Ensures each tenant‚Äôs data remains separate while residing in the same collection.</li><li><strong>Minimized Overhead</strong>: Reduces resource consumption compared to maintaining separate collections for each user.</li><li><strong>Scalability</strong>: Handles high user volumes without compromising performance.</li></ul><p>Here‚Äôs how you can implement multitenancy efficiently in Qdrant:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_payload_index</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>field_name</span><span class=o>=</span><span class=s2>&#34;group_id&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>field_schema</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>KeywordIndexParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=nb>type</span><span class=o>=</span><span class=s2>&#34;keyword&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>is_tenant</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Creating a keyword payload index, with the <code>is_tenant</code> parameter set to <code>True</code>, modifies the way the vectors will be logically stored. Storage structure will be organized to co-locate vectors of the same tenant together.</p><p>Now, each point stored in Qdrant should have the <code>group_id</code> payload attribute set:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>upsert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>points</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>       <span class=n>models</span><span class=o>.</span><span class=n>PointStruct</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=nb>id</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>payload</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;group_id&#34;</span><span class=p>:</span> <span class=s2>&#34;user_1&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>           <span class=n>vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       <span class=p>),</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>       <span class=n>models</span><span class=o>.</span><span class=n>PointStruct</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=nb>id</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>payload</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;group_id&#34;</span><span class=p>:</span> <span class=s2>&#34;user_2&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>           <span class=n>vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.5</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>,</span> <span class=mf>0.4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       <span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><hr><p>To ensure proper data isolation in a multitenant environment, you can assign a unique identifier, such as a <strong>group_id</strong>, to each vector. This approach ensures that each user&rsquo;s data remains segregated, allowing users to access only their own data. You can further enhance this setup by applying filters during queries to restrict access to the relevant data.</p><hr><p>Learn More about <a href=https://qdrant.tech/documentation/guides/multiple-partitions/><strong>Multitenancy</strong></a></p><hr><h4 id=sharding>Sharding</h4><p>Sharding is a critical strategy in Qdrant for splitting collections into smaller units, called <strong>shards</strong>, to efficiently distribute data across multiple nodes. It‚Äôs a powerful tool for improving scalability and maintaining performance in large-scale systems.</p><h4 id=user-defined-sharding>User-Defined Sharding:</h4><p><strong>User-Defined Sharding</strong> allows you to take control of data placement by specifying a shard key. This feature is particularly useful in multi-tenant setups, as it enables the isolation of each tenant‚Äôs data within separate shards, ensuring better organization and enhanced data security.</p><p><strong>Figure 6:</strong> Users can both upsert and query shards that are relevant to them, all within the same collection. Regional sharding can help avoid cross-continental traffic.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/user-defined-sharding.png alt=user-defined-sharding style=width:75%><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my_custom_sharded_collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>shard_number</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>sharding_method</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>ShardingMethod</span><span class=o>.</span><span class=n>CUSTOM</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_shard_key</span><span class=p>(</span><span class=s2>&#34;my_custom_sharded_collection&#34;</span><span class=p>,</span> <span class=s2>&#34;tenant_id&#34;</span><span class=p>)</span>
</span></span></code></pre></div><hr><p>When implementing user-defined sharding in Qdrant, two key parameters are critical to achieving efficient data distribution:</p><ol><li><p><strong>Shard Key</strong>:</p><p>The shard key determines how data points are distributed across shards. For example, using a key like <code>tenant_id</code> allows you to control how Qdrant partitions the data. Each data point added to the collection will be assigned to a shard based on the value of this key, ensuring logical isolation of data.</p></li><li><p><strong>Shard Number</strong>:</p><p>This defines the total number of physical shards for each shard key, influencing resource allocation and query performance.</p></li></ol><p>Here‚Äôs how you can add a data point to a collection with user-defined sharding:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>upsert</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my_custom_sharded_collection&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>    <span class=n>points</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>PointStruct</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=nb>id</span><span class=o>=</span><span class=mi>1111</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>            <span class=n>vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span> 
</span></span><span class=line><span class=cl>    <span class=n>shard_key_selector</span><span class=o>=</span><span class=s2>&#34;tenant_1&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><hr><p>This code assigns the point to a specific shard based on the <code>tenant_1</code> shard key, ensuring proper data placement.</p><p>Here‚Äôs how to choose the shard_number:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th>Recommendation</th><th>Description</th></tr></thead><tbody><tr><td><strong>Match Shards to Nodes</strong></td><td>The number of shards should align with the number of nodes in your cluster to balance resource utilization and query performance.</td></tr><tr><td><strong>Plan for Scalability</strong></td><td>Start with at least <strong>2 shards per node</strong> to allow room for future growth.</td></tr><tr><td><strong>Future-Proofing</strong></td><td>Starting with around <strong>12 shards</strong> is a good rule of thumb. This setup allows your system to scale seamlessly from 1 to 12 nodes without requiring re-sharding.</td></tr></tbody></table></div><p>Learn more about <a href=https://qdrant.tech/documentation/guides/distributed_deployment/><strong>Sharding in Distributed Deployment</strong></a></p><hr><h2 id=query-optimization>Query Optimization</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/query.png alt=qdrant>
Improving vector database performance is critical when dealing with large datasets and complex queries. By leveraging techniques like <strong>filtering</strong>, <strong>batch processing</strong>, <strong>reranking</strong>, <strong>rescoring</strong>, and <strong>oversampling</strong>, so you can ensure fast response times and maintain efficiency even at scale.</p><h4 id=filtering>Filtering</h4><p>Filtering allows you to select only the required fields in your query results. By limiting the output size, you can significantly reduce response time and improve performance.</p><p>The filterable vector index is Qdrant&rsquo;s solves pre and post-filtering problems by adding specialized links to the search graph. It aims to maintain the speed advantages of vector search while allowing for precise filtering, addressing the inefficiencies that can occur when applying filters after the vector search.</p><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>search</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my_collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query_vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.1</span><span class=p>,</span> <span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>query_filter</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Filter</span><span class=p>(</span><span class=n>must</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>FieldCondition</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>key</span><span class=o>=</span><span class=s2>&#34;category&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=k>match</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>MatchValue</span><span class=p>(</span><span class=n>value</span><span class=o>=</span><span class=s2>&#34;my-category-name&#34;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p><strong>Figure 7:</strong> The filterable vector index adds specialized links to the search graph to speed up traversal.</p><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/filterable-vector-index.png alt=filterable-vector-index></p><p><a href=https://qdrant.tech/documentation/concepts/indexing/><strong>Filterable vector index</strong></a>: This technique builds additional links <strong>(orange)</strong> between leftover data points. The filtered points which stay behind are now traversible once again. Qdrant uses special category-based methods to connect these data points.</p><hr><p>Read more about <a href=https://qdrant.tech/documentation/concepts/filtering/><strong>Filtering Docs</strong></a> and check out the <a href=https://qdrant.tech/articles/vector-search-filtering/><strong>Complete Filtering Guide</strong></a>.</p><hr><h4 id=batch-processing>Batch Processing</h4><p>Batch processing consolidates multiple operations into a single execution cycle, reducing request overhead and enhancing throughput. It‚Äôs an effective strategy for both data insertion and query execution.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/batch-processing.png alt=batch-processing style=width:75%><p><strong>Batch Insertions</strong>: Instead of inserting vectors individually, group them into medium-sized batches to minimize the number of database requests and the overhead of frequent writes.</p><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>vectors</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>   <span class=p>[</span><span class=mf>.1</span><span class=p>,</span> <span class=mf>.0</span><span class=p>,</span> <span class=mf>.0</span><span class=p>,</span> <span class=mf>.0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>   <span class=p>[</span><span class=mf>.0</span><span class=p>,</span> <span class=mf>.1</span><span class=p>,</span> <span class=mf>.0</span><span class=p>,</span> <span class=mf>.0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>   <span class=p>[</span><span class=mf>.0</span><span class=p>,</span> <span class=mf>.0</span><span class=p>,</span> <span class=mf>.1</span><span class=p>,</span> <span class=mf>.0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>   <span class=p>[</span><span class=mf>.0</span><span class=p>,</span> <span class=mf>.0</span><span class=p>,</span> <span class=mf>.0</span><span class=p>,</span> <span class=mf>.1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>   <span class=err>‚Ä¶</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>upload_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;test_collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>vectors</span><span class=o>=</span><span class=n>vectors</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>This reduces write operations and ensures faster data ingestion.</p><p><strong>Batch Queries</strong>: Similarly, you can batch multiple queries together rather than executing them one by one. This reduces the number of round trips to the database, optimizing performance and reducing latency.</p><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>results</span> <span class=o>=</span> <span class=n>client</span><span class=o>.</span><span class=n>search_batch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;test_collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>requests</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>       <span class=n>SearchRequest</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=n>vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>2.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>           <span class=n>limit</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=p>),</span>
</span></span><span class=line><span class=cl>       <span class=n>SearchRequest</span><span class=p>(</span>
</span></span><span class=line><span class=cl>           <span class=n>vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>],</span>
</span></span><span class=line><span class=cl>           <span class=n>with_vector</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>           <span class=n>limit</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Batch queries are particularly useful when processing a large number of similar queries or when handling multiple user requests simultaneously.</p><hr><h4 id=hybrid-search>Hybrid Search</h4><p>Hybrid search combines <strong>keyword filtering</strong> with <strong>vector similarity search</strong>, enabling faster and more precise results. Keywords help narrow down the dataset quickly, while vector similarity ensures semantic accuracy. This search method combines <a href=https://qdrant.tech/documentation/concepts/vectors/><strong>dense and sparse vectors</strong></a>.</p><p>Hybrid search in Qdrant uses both fusion and reranking. The former is about combining the results from different search methods, based solely on the scores returned by each method. That usually involves some normalization, as the scores returned by different methods might be in different ranges.</p><p><strong>Figure 8</strong>: Hybrid Search Architecture</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/hybrid-search.png alt=hybrid-search style=width:75%><p>After that, there is a formula that takes the relevancy measures and calculates the final score that we use later on to reorder the documents. Qdrant has built-in support for the Reciprocal Rank Fusion method, which is the de facto standard in the field.</p><hr><p>Learn more about <a href=https://qdrant.tech/articles/hybrid-search/><strong>Hybrid Search</strong></a> and read out <a href=https://qdrant.tech/documentation/concepts/hybrid-queries/><strong>Hybrid Queries docs</strong></a>.</p><hr><h4 id=oversampling>Oversampling</h4><p>Oversampling is a technique that helps compensate for any precision lost due to quantization. Since quantization simplifies vectors, some relevant matches could be missed in the initial search. To avoid this, you can <strong>retrieve more candidates</strong>, increasing the chances that the most relevant vectors make it into the final results.</p><p>You can control the number of extra candidates by setting an <code>oversampling</code> parameter. For example, if your desired number of results (<code>limit</code>) is 4 and you set an <code>oversampling</code> factor of 2, Qdrant will retrieve 8 candidates (4 √ó 2).</p><p>You can adjust the oversampling factor to control how many extra vectors Qdrant includes in the initial pool. More candidates mean a better chance of obtaining high-quality top-K results, especially after rescoring with the original vectors.</p><hr><p>Learn more about <a href=https://qdrant.tech/articles/what-is-vector-quantization/#2-oversampling><strong>Oversampling</strong></a>.</p><hr><h4 id=rescoring>Rescoring</h4><p>After oversampling to gather more potential matches, each candidate is re-evaluated based on additional criteria to ensure higher accuracy and relevance to the query.</p><p>The rescoring process maps the quantized vectors to their corresponding original vectors, allowing you to consider factors like context, metadata, or additional relevance that wasn‚Äôt included in the initial search, leading to more accurate results.</p><p><strong>Example of Rescoring and Oversampling:</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my_collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>query_vector</span><span class=o>=</span><span class=p>[</span><span class=mf>0.22</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.01</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.98</span><span class=p>,</span> <span class=mf>0.37</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>search_params</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>SearchParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>quantization</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>QuantizationSearchParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>rescore</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>   <span class=c1># Enables rescoring with original vectors</span>
</span></span><span class=line><span class=cl>            <span class=n>oversampling</span><span class=o>=</span><span class=mi>2</span>  <span class=c1># Retrieves extra candidates for rescoring</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>4</span>  <span class=c1># Desired number of final results</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><hr><p>Learn more about <a href=https://qdrant.tech/articles/what-is-vector-quantization/#3-rescoring-with-original-vectors><strong>Rescoring</strong></a>.</p><hr><h4 id=reranking>Reranking</h4><p>Reranking adjusts the order of search results based on additional criteria, ensuring the most relevant results are prioritized.</p><p>This method is about taking the results from different search methods and reordering them based on some additional processing using the content of the documents, not just the scores. This processing may rely on an additional neural model, such as a cross-encoder which would be inefficient enough to be used on the whole dataset.</p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/reranking.png alt=reranking style=width:75%><p>These methods are practically applicable only when used on a smaller subset of candidates returned by the faster search methods. Late interaction models, such as ColBERT, are way more efficient in this case, as they can be used to rerank the candidates without the need to access all the documents in the collection.</p><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>       <span class=s2>&#34;collection-name&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>prefetch</span><span class=o>=</span><span class=n>prefetch</span><span class=p>,</span> <span class=c1># Previous results</span>
</span></span><span class=line><span class=cl>       <span class=n>query</span><span class=o>=</span><span class=n>late_vectors</span><span class=p>,</span> <span class=c1># Colbert converted query</span>
</span></span><span class=line><span class=cl>       <span class=n>using</span><span class=o>=</span><span class=s2>&#34;colbertv2.0&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>with_payload</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>       <span class=n>limit</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><hr><p>Learn more about <a href=https://qdrant.tech/documentation/search-precision/reranking-hybrid-search/#rerank><strong>Reranking</strong></a>.</p><hr><h2 id=storage-disk-vs-ram>Storage: Disk vs RAM</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/disk.png alt=disk></p><div class=table-responsive><table class="table mb-5"><thead><tr><th>Storage</th><th>Description</th></tr></thead><tbody><tr><td><strong>RAM</strong></td><td>Crucial for fast access to frequently used data, such as indexed vectors. The amount of RAM required can be estimated based on your dataset size and dimensionality. For example, storing <strong>1 million vectors with 1024 dimensions</strong> would require approximately <strong>5.72 GB of RAM</strong>.</td></tr><tr><td><strong>Disk</strong></td><td>Suitable for less frequently accessed data, such as payloads and non-critical information. Disk-backed storage reduces memory demands but can introduce slight latency.</td></tr></tbody></table></div><h4 id=which-disk-type>Which Disk Type?</h4><p><strong>Local SSDs</strong> are recommended for optimal performance, as they provide the fastest query response times with minimal latency. While network-attached storage is also viable, it typically introduces additional latency that can affect performance, so local SSDs are preferred when possible, particularly for workloads requiring high-speed random access.</p><h4 id=memory-management-for-vectors-and-payload>Memory Management for Vectors and Payload</h4><p>As your data scales, effective resource management becomes crucial to keeping costs low while ensuring your application remains reliable and performant. One of the key areas to focus on is <strong>memory management</strong>.</p><p>Understanding how Qdrant handles memory can help you make informed decisions about scaling your vector database. Qdrant supports two main methods for storing vectors:</p><h4 id=1-in-memory-storage>1. In-Memory Storage</h4><ul><li><strong>How it works</strong>: All data is stored in RAM, providing the fastest access times for queries and operations.</li><li><strong>When to use it</strong>: This setup is ideal for applications where performance is critical, and your RAM capacity can accommodate all data.</li><li><strong>Advantages</strong>: Maximum speed for queries and updates.</li><li><strong>Limitations</strong>: RAM usage can become a bottleneck as your dataset grows.</li></ul><h4 id=2-memmap-storage>2. Memmap Storage</h4><ul><li><strong>How it works</strong>: Instead of loading all data into memory, memmap storage maps data files directly to a virtual address space on disk. The system&rsquo;s page cache handles data access, making it highly efficient.</li><li><strong>When to use it</strong>: Perfect for storing large collections that exceed your available RAM while still maintaining near in-memory performance when enough RAM is available.</li><li><strong>Advantages</strong>: Balances performance and memory usage, allowing you to work with datasets larger than your physical RAM.</li><li><strong>Limitations</strong>: Slightly slower than pure in-memory storage but significantly more scalable.</li></ul><p>To enable memmap vector storage in Qdrant, you can set the <strong>on_disk</strong> parameter to <code>true</code> when creating or updating a collection.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>   <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>   <span class=n>vectors_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>      <span class=err>‚Ä¶</span>
</span></span><span class=line><span class=cl>      <span class=n>on_disk</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>   <span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>To do the same for payloads:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;</span><span class=si>{collection_name}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>on_disk_payload</span><span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>The general guideline for selecting a storage method in Qdrant is to use <strong>InMemory storage</strong> when high performance is a priority, and sufficient RAM is available to accommodate the dataset. This approach ensures the fastest access speeds by keeping data readily accessible in memory.</p><p>However, for larger datasets or scenarios where memory is limited, <strong>Memmap</strong> and <strong>OnDisk storage</strong> are more suitable. These methods significantly reduce memory usage by storing data on disk while leveraging advanced techniques like page caching and indexing to maintain efficient and relatively fast data access.</p><h2 id=monitoring-the-database>Monitoring the Database</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/monitor.png alt=monitoring></p><p>Continuous monitoring is essential for maintaining system health and identifying potential issues before they escalate. Tools like <strong>Prometheus</strong> and <strong>Grafana</strong> are widely used to achieve this.</p><ul><li><strong>Prometheus</strong>: An open-source monitoring and alerting toolkit, Prometheus collects and stores metrics in a time-series database. It scrapes metrics from predefined endpoints and supports powerful querying and visualization capabilities.</li><li><strong>Grafana</strong>: Often paired with Prometheus, Grafana provides an intuitive interface for visualizing metrics and creating interactive dashboards.</li></ul><p>Qdrant exposes metrics in the <strong>Prometheus/OpenMetrics</strong> format through the /metrics endpoint. Prometheus can scrape this endpoint to monitor various aspects of the Qdrant system.</p><p>For a local Qdrant instance, the metrics endpoint is typically available at:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>http</span><span class=p>:</span><span class=o>//</span><span class=n>localhost</span><span class=p>:</span><span class=mi>6333</span><span class=o>/</span><span class=n>metrics</span>
</span></span></code></pre></div><hr><p>Here are some important metrics to monitor:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th><strong>Metric Name</strong></th><th></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>collections_total</td><td></td><td>Total number of collections</td></tr><tr><td>collections_vector_total</td><td></td><td>Total number of vectors in all collections</td></tr><tr><td>rest_responses_avg_duration_seconds</td><td></td><td>Average response duration in REST API</td></tr><tr><td>grpc_responses_avg_duration_seconds</td><td></td><td>Average response duration in gRPC API</td></tr><tr><td>rest_responses_fail_total</td><td></td><td>Total number of failed responses (REST)</td></tr></tbody></table></div><p>Read more about <a href=https://qdrant.tech/documentation/guides/monitoring/><strong>Qdrant Open Source Monitoring</strong></a> and <a href=https://qdrant.tech/documentation/cloud/cluster-monitoring/><strong>Qdrant Cloud Monitoring</strong></a> for managed clusters.</p><hr><h2 id=recap-when-should-you-optimize>Recap: When Should You Optimize?</h2><p><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/solutions.png alt=solutions></p><div class=table-responsive><table class="table mb-5"><thead><tr><th>Scenario</th><th>Description</th></tr></thead><tbody><tr><td><strong>When You Scale Up</strong></td><td>As data grows and the request surge, optimizing resource usage ensures your systems stay responsive and cost-efficient, even under heavy loads.</td></tr><tr><td><strong>If Facing Budget Constraints</strong></td><td>Strike the perfect balance between performance and cost, cutting unnecessary expenses while maintaining essential capabilities.</td></tr><tr><td><strong>You Need Better Performance</strong></td><td>If you‚Äôre noticing slow query speeds, latency issues, or frequent timeouts, it‚Äôs time to fine-tune your resource allocation.</td></tr><tr><td><strong>When System Stability is Paramount</strong></td><td>To manage high-traffic environments you will need to prevent crashes or failures caused by resource exhaustion.</td></tr></tbody></table></div><h2 id=get-the-cheatsheet>Get the Cheatsheet</h2><p>Want to download a printer-friendly version of this guide? <a href=https://try.qdrant.tech/resource-optimization-guide target=_blank rel="noopener nofollow"><strong>Download it now.</strong></a>.</p><p><a href=https://try.qdrant.tech/resource-optimization-guide target=_blank rel="noopener nofollow"><img src=https://qdrant.tech/articles_data/vector-search-resource-optimization/downloadable-guide.jpg alt="downloadable vector search resource optimization guide"></a></p></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! üôè</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. üòî You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/vector-search-resource-optimization.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#whats-in-this-guide>What‚Äôs in This Guide?</a><ul><li></li></ul></li><li><a href=#configure-indexing-for-faster-searches>Configure Indexing for Faster Searches</a><ul><li></li></ul></li><li><a href=#data-compression-techniques>Data Compression Techniques</a><ul><li></li></ul></li><li><a href=#scaling-the-database>Scaling the Database</a><ul><li></li></ul></li><li><a href=#query-optimization>Query Optimization</a><ul><li></li></ul></li><li><a href=#storage-disk-vs-ram>Storage: Disk vs RAM</a><ul><li></li></ul></li><li><a href=#monitoring-the-database>Monitoring the Database</a></li><li><a href=#recap-when-should-you-optimize>Recap: When Should You Optimize?</a></li><li><a href=#get-the-cheatsheet>Get the Cheatsheet</a></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/vector-search-resource-optimization.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>¬© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>