<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>BM42: New Baseline for Hybrid Search - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Introducing BM42 - a new sparse embedding approach, which combines the benefits of exact keyword search with the intelligence of transformers."><meta name=keywords content="hybrid search,sparse embeddings,bm25,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/bm42/#article","@type":"Article","abstract":"Introducing BM42  a new sparse embedding approach which combines the benefits of exact keyword search with the intelligence of transformers","author":{"@type":"Person","name":"Andrey Vasnetsov"},"dateModified":"2024-07-01 12:00:00 +0300 +0300","datePublished":"2024-07-01 12:00:00 +0300 +0300","description":"Introducing BM42  a new sparse embedding approach which combines the benefits of exact keyword search with the intelligence of transformers","headline":"BM42: New Baseline for Hybrid Search","image":["https://qdrant.tech/articles_data/bm42/social-preview.jpg"],"name":"BM42: New Baseline for Hybrid Search","url":"https://qdrant.tech/articles/bm42/","wordCount":"2737"},{"@id":"https://qdrant.tech/articles/bm42/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestra√üe 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["hybrid search","sparse embeddings","bm25","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/bm42/"><meta property="og:type" content="website"><meta property="og:title" content="BM42: New Baseline for Hybrid Search - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/bm42/"><meta name=twitter:title content="BM42: New Baseline for Hybrid Search - Qdrant"><meta property="og:description" content="Introducing BM42 - a new sparse embedding approach, which combines the benefits of exact keyword search with the intelligence of transformers."><meta name=twitter:description content="Introducing BM42 - a new sparse embedding approach, which combines the benefits of exact keyword search with the intelligence of transformers."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/bm42/social-preview.jpg"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/bm42/social-preview.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/bm42/social-preview.jpg"><meta name=author content="Andrey Vasnetsov"><link rel=canonical href=https://qdrant.tech/articles/bm42/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>BM42: New Baseline for Hybrid Search</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/machine-learning/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Machine Learning</a><h1 class=documentation-article__header-title>BM42: New Baseline for Hybrid Search</h1><div class=documentation-article__header-about><p>Andrey Vasnetsov</p><span>&#183;</span><p>July 01, 2024</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/bm42/preview/title.webp type=image/webp><img alt="BM42: New Baseline for Hybrid Search" src=https://qdrant.tech/articles_data/bm42/preview/title.jpg></picture></div><aside role=status>Please note that the benchmark section of this article was updated after the publication due to a mistake in the evaluation script.
BM42 does not outperform BM25 implementation of other vendors.
Please consider BM42 as an experimental approach, which requires further research and development before it can be used in production.</aside><p>For the last 40 years, BM25 has served as the standard for search engines.
It is a simple yet powerful algorithm that has been used by many search engines, including Google, Bing, and Yahoo.</p><p>Though it seemed that the advent of vector search would diminish its influence, it did so only partially.
The current state-of-the-art approach to retrieval nowadays tries to incorporate BM25 along with embeddings into a hybrid search system.</p><p>However, the use case of text retrieval has significantly shifted since the introduction of RAG.
Many assumptions upon which BM25 was built are no longer valid.</p><p>For example, the typical length of documents and queries vary significantly between traditional web search and modern RAG systems.</p><p>In this article, we will recap what made BM25 relevant for so long and why alternatives have struggled to replace it. Finally, we will discuss BM42, as the next step in the evolution of lexical search.</p><h2 id=why-has-bm25-stayed-relevant-for-so-long>Why has BM25 stayed relevant for so long?</h2><p>To understand why, we need to analyze its components.</p><p>The famous BM25 formula is defined as:</p><p>$$
\text{score}(D,Q) = \sum_{i=1}^{N} \text{IDF}(q_i) \times \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
$$</p><p>Let&rsquo;s simplify this to gain a better understanding.</p><ul><li><p>The $score(D, Q)$ - means that we compute the score for each pair of document $D$ and query $Q$.</p></li><li><p>The $\sum_{i=1}^{N}$ - means that each of $N$ terms in the query contribute to the final score as a part of the sum.</p></li><li><p>The $\text{IDF}(q_i)$ - is the inverse document frequency. The more rare the term $q_i$ is, the more it contributes to the score. A simplified formula for this is:</p></li></ul><p>$$
\text{IDF}(q_i) = \frac{\text{Number of documents}}{\text{Number of documents with } q_i}
$$</p><p>It is fair to say that the <code>IDF</code> is the most important part of the BM25 formula.
<code>IDF</code> selects the most important terms in the query relative to the specific document collection.
So intuitively, we can interpret the <code>IDF</code> as <strong>term importance within the corpora</strong>.</p><p>That explains why BM25 is so good at handling queries, which dense embeddings consider out-of-domain.</p><p>The last component of the formula can be intuitively interpreted as <strong>term importance within the document</strong>.
This might look a bit complicated, so let&rsquo;s break it down.</p><p>$$
\text{Term importance in document }(q_i) = \color{red}\frac{f(q_i, D)\color{gray} \cdot \color{blue}(k_1 + 1) \color{gray} }{\color{red}f(q_i, D)\color{gray} + \color{blue}k_1\color{gray} \cdot \left(1 - \color{blue}b\color{gray} + \color{blue}b\color{gray} \cdot \frac{|D|}{\text{avgdl}}\right)}
$$</p><ul><li>The $\color{red}f(q_i, D)\color{gray}$ - is the frequency of the term $q_i$ in the document $D$. Or in other words, the number of times the term $q_i$ appears in the document $D$.</li><li>The $\color{blue}k_1\color{gray}$ and $\color{blue}b\color{gray}$ are the hyperparameters of the BM25 formula. In most implementations, they are constants set to $k_1=1.5$ and $b=0.75$. Those constants define relative implications of the term frequency and the document length in the formula.</li><li>The $\frac{|D|}{\text{avgdl}}$ - is the relative length of the document $D$ compared to the average document length in the corpora. The intuition befind this part is following: if the token is found in the smaller document, it is more likely that this token is important for this document.</li></ul><h4 id=will-bm25-term-importance-in-the-document-work-for-rag>Will BM25 term importance in the document work for RAG?</h4><p>As we can see, the <em>term importance in the document</em> heavily depends on the statistics within the document. Moreover, statistics works well if the document is long enough.
Therefore, it is suitable for searching webpages, books, articles, etc.</p><p>However, would it work as well for modern search applications, such as RAG? Let&rsquo;s see.</p><p>The typical length of a document in RAG is much shorter than that of web search. In fact, even if we are working with webpages and articles, we would prefer to split them into chunks so that
a) Dense models can handle them and
b) We can pinpoint the exact part of the document which is relevant to the query</p><p>As a result, the document size in RAG is small and fixed.</p><p>That effectively renders the term importance in the document part of the BM25 formula useless.
The term frequency in the document is always 0 or 1, and the relative length of the document is always 1.</p><p>So, the only part of the BM25 formula that is still relevant for RAG is <code>IDF</code>. Let&rsquo;s see how we can leverage it.</p><h2 id=why-splade-is-not-always-the-answer>Why SPLADE is not always the answer</h2><p>Before discussing our new approach, let&rsquo;s examine the current state-of-the-art alternative to BM25 - SPLADE.</p><p>The idea behind SPLADE is interesting‚Äîwhat if we let a smart, end-to-end trained model generate a bag-of-words representation of the text for us?
It will assign all the weights to the tokens, so we won&rsquo;t need to bother with statistics and hyperparameters.
The documents are then represented as a sparse embedding, where each token is represented as an element of the sparse vector.</p><p>And it works in academic benchmarks. Many papers report that SPLADE outperforms BM25 in terms of retrieval quality.
This performance, however, comes at a cost.</p><ul><li><p><strong>Inappropriate Tokenizer</strong>: To incorporate transformers for this task, SPLADE models require using a standard transformer tokenizer. These tokenizers are not designed for retrieval tasks. For example, if the word is not in the (quite limited) vocabulary, it will be either split into subwords or replaced with a <code>[UNK]</code> token. This behavior works well for language modeling but is completely destructive for retrieval tasks.</p></li><li><p><strong>Expensive Token Expansion</strong>: In order to compensate the tokenization issues, SPLADE uses <em>token expansion</em> technique. This means that we generate a set of similar tokens for each token in the query. There are a few problems with this approach:</p><ul><li>It is computationally and memory expensive. We need to generate more values for each token in the document, which increases both the storage size and retrieval time.</li><li>It is not always clear where to stop with the token expansion. The more tokens we generate, the more likely we are to get the relevant one. But simultaneously, the more tokens we generate, the more likely we are to get irrelevant results.</li><li>Token expansion dilutes the interpretability of the search. We can&rsquo;t say which tokens were used in the document and which were generated by the token expansion.</li></ul></li><li><p><strong>Domain and Language Dependency</strong>: SPLADE models are trained on specific corpora. This means that they are not always generalizable to new or rare domains. As they don&rsquo;t use any statistics from the corpora, they cannot adapt to the new domain without fine-tuning.</p></li><li><p><strong>Inference Time</strong>: Additionally, currently available SPLADE models are quite big and slow. They usually require a GPU to make the inference in a reasonable time.</p></li></ul><p>At Qdrant, we acknowledge the aforementioned problems and are looking for a solution.
Our idea was to combine the best of both worlds - the simplicity and interpretability of BM25 and the intelligence of transformers while avoiding the pitfalls of SPLADE.</p><p>And here is what we came up with.</p><h2 id=the-best-of-both-worlds>The best of both worlds</h2><p>As previously mentioned, <code>IDF</code> is the most important part of the BM25 formula. In fact it is so important, that we decided to build its calculation into the Qdrant engine itself.
Check out our latest <a href=https://github.com/qdrant/qdrant/releases/tag/v1.10.0 target=_blank rel="noopener nofollow">release notes</a>. This type of separation allows streaming updates of the sparse embeddings while keeping the <code>IDF</code> calculation up-to-date.</p><p>As for the second part of the formula, <em>the term importance within the document</em> needs to be rethought.</p><p>Since we can&rsquo;t rely on the statistics within the document, we can try to use the semantics of the document instead.
And semantics is what transformers are good at. Therefore, we only need to solve two problems:</p><ul><li>How does one extract the importance information from the transformer?</li><li>How can tokenization issues be avoided?</li></ul><h3 id=attention-is-all-you-need>Attention is all you need</h3><p>Transformer models, even those used to generate embeddings, generate a bunch of different outputs.
Some of those outputs are used to generate embeddings.</p><p>Others are used to solve other kinds of tasks, such as classification, text generation, etc.</p><p>The one particularly interesting output for us is the attention matrix.</p><figure><img src=https://qdrant.tech/articles_data/bm42/attention-matrix.png alt="Attention matrix" width=60%><figcaption><p>Attention matrix</p></figcaption></figure><p>The attention matrix is a square matrix, where each row and column corresponds to the token in the input sequence.
It represents the importance of each token in the input sequence for each other.</p><p>The classical transformer models are trained to predict masked tokens in the context, so the attention weights define which context tokens influence the masked token most.</p><p>Apart from regular text tokens, the transformer model also has a special token called <code>[CLS]</code>. This token represents the whole sequence in the classification tasks, which is exactly what we need.</p><p>By looking at the attention row for the <code>[CLS]</code> token, we can get the importance of each token in the document for the whole document.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sentences</span> <span class=o>=</span> <span class=s2>&#34;Hello, World - is the starting point in most programming languages&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>features</span> <span class=o>=</span> <span class=n>transformer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>sentences</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>attentions</span> <span class=o>=</span> <span class=n>transformer</span><span class=o>.</span><span class=n>auto_model</span><span class=p>(</span><span class=o>**</span><span class=n>features</span><span class=p>,</span> <span class=n>output_attentions</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span><span class=o>.</span><span class=n>attentions</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>attentions</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>,:,</span><span class=mi>0</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>                       
</span></span><span class=line><span class=cl><span class=c1>#                ‚ñ≤               ‚ñ≤  ‚ñ≤   ‚ñ≤                                 </span>
</span></span><span class=line><span class=cl><span class=c1>#                ‚îÇ               ‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ [CLS] token is the first one</span>
</span></span><span class=line><span class=cl><span class=c1>#                ‚îÇ               ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ First item of the batch         </span>
</span></span><span class=line><span class=cl><span class=c1>#                ‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Last transformer layer       </span>
</span></span><span class=line><span class=cl><span class=c1>#                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Average all 6 attention heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>weight</span><span class=p>,</span> <span class=n>token</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>token</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>weight</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># [CLS]       : 0.434 // Filter out the [CLS] token</span>
</span></span><span class=line><span class=cl><span class=c1># hello       : 0.039</span>
</span></span><span class=line><span class=cl><span class=c1># ,           : 0.039</span>
</span></span><span class=line><span class=cl><span class=c1># world       : 0.107 // &lt;-- The most important token</span>
</span></span><span class=line><span class=cl><span class=c1># -           : 0.033</span>
</span></span><span class=line><span class=cl><span class=c1># is          : 0.024</span>
</span></span><span class=line><span class=cl><span class=c1># the         : 0.031</span>
</span></span><span class=line><span class=cl><span class=c1># starting    : 0.054</span>
</span></span><span class=line><span class=cl><span class=c1># point       : 0.028</span>
</span></span><span class=line><span class=cl><span class=c1># in          : 0.018</span>
</span></span><span class=line><span class=cl><span class=c1># most        : 0.016</span>
</span></span><span class=line><span class=cl><span class=c1># programming : 0.060 // &lt;-- The third most important token</span>
</span></span><span class=line><span class=cl><span class=c1># languages   : 0.062 // &lt;-- The second most important token</span>
</span></span><span class=line><span class=cl><span class=c1># [SEP]       : 0.047 // Filter out the [SEP] token</span>
</span></span></code></pre></div><p>The resulting formula for the BM42 score would look like this:</p><p>$$
\text{score}(D,Q) = \sum_{i=1}^{N} \text{IDF}(q_i) \times \text{Attention}(\text{CLS}, q_i)
$$</p><p>Note that classical transformers have multiple attention heads, so we can get multiple importance vectors for the same document. The simplest way to combine them is to simply average them.</p><p>These averaged attention vectors make up the importance information we were looking for.
The best part is, one can get them from any transformer model, without any additional training.
Therefore, BM42 can support any natural language as long as there is a transformer model for it.</p><p>In our implementation, we use the <code>sentence-transformers/all-MiniLM-L6-v2</code> model, which gives a huge boost in the inference speed compared to the SPLADE models. In practice, any transformer model can be used.
It doesn&rsquo;t require any additional training, and can be easily adapted to work as BM42 backend.</p><h3 id=wordpiece-retokenization>WordPiece retokenization</h3><p>The final piece of the puzzle we need to solve is the tokenization issue. In order to get attention vectors, we need to use native transformer tokenization.
But this tokenization is not suitable for the retrieval tasks. What can we do about it?</p><p>Actually, the solution we came up with is quite simple. We reverse the tokenization process after we get the attention vectors.</p><p>Transformers use <a href=https://huggingface.co/learn/nlp-course/en/chapter6/6 target=_blank rel="noopener nofollow">WordPiece</a> tokenization.
In case it sees the word, which is not in the vocabulary, it splits it into subwords.</p><p>Here is how that looks:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>&#34;unbelievable&#34; -&gt; [&#34;un&#34;, &#34;##believ&#34;, &#34;##able&#34;]
</span></span></code></pre></div><p>What can merge the subwords back into the words. Luckily, the subwords are marked with the <code>##</code> prefix, so we can easily detect them.
Since the attention weights are normalized, we can simply sum the attention weights of the subwords to get the attention weight of the word.</p><p>After that, we can apply the same traditional NLP techniques, as</p><ul><li>Removing of the stop-words</li><li>Removing of the punctuation</li><li>Lemmatization</li></ul><p>In this way, we can significantly reduce the number of tokens, and therefore minimize the memory footprint of the sparse embeddings. We won&rsquo;t simultaneously compromise the ability to match (almost) exact tokens.</p><h2 id=practical-examples>Practical examples</h2><div class=table-responsive><table class="table mb-5"><thead><tr><th>Trait</th><th>BM25</th><th>SPLADE</th><th>BM42</th></tr></thead><tbody><tr><td>Interpretability</td><td>High ‚úÖ</td><td>Ok üÜó</td><td>High ‚úÖ</td></tr><tr><td>Document Inference speed</td><td>Very high ‚úÖ</td><td>Slow üêå</td><td>High ‚úÖ</td></tr><tr><td>Query Inference speed</td><td>Very high ‚úÖ</td><td>Slow üêå</td><td>Very high ‚úÖ</td></tr><tr><td>Memory footprint</td><td>Low ‚úÖ</td><td>High ‚ùå</td><td>Low ‚úÖ</td></tr><tr><td>In-domain accuracy</td><td>Ok üÜó</td><td>High ‚úÖ</td><td>High ‚úÖ</td></tr><tr><td>Out-of-domain accuracy</td><td>Ok üÜó</td><td>Low ‚ùå</td><td>Ok üÜó</td></tr><tr><td>Small documents accuracy</td><td>Low ‚ùå</td><td>High ‚úÖ</td><td>High ‚úÖ</td></tr><tr><td>Large documents accuracy</td><td>High ‚úÖ</td><td>Low ‚ùå</td><td>Ok üÜó</td></tr><tr><td>Unknown tokens handling</td><td>Yes ‚úÖ</td><td>Bad ‚ùå</td><td>Yes ‚úÖ</td></tr><tr><td>Multi-lingual support</td><td>Yes ‚úÖ</td><td>No ‚ùå</td><td>Yes ‚úÖ</td></tr><tr><td>Best Match</td><td>Yes ‚úÖ</td><td>No ‚ùå</td><td>Yes ‚úÖ</td></tr></tbody></table></div><p>Starting from Qdrant v1.10.0, BM42 can be used in Qdrant via FastEmbed inference.</p><p>Let&rsquo;s see how you can setup a collection for hybrid search with BM42 and <a href=https://jina.ai/embeddings/ target=_blank rel="noopener nofollow">jina.ai</a> dense embeddings.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-http data-lang=http><span class=line><span class=cl><span class=err>PUT collections/my-hybrid-collection
</span></span></span><span class=line><span class=cl><span class=err>{
</span></span></span><span class=line><span class=cl><span class=err>  &#34;vectors&#34;: {
</span></span></span><span class=line><span class=cl><span class=err>    &#34;jina&#34;: {
</span></span></span><span class=line><span class=cl><span class=err>      &#34;size&#34;: 768,
</span></span></span><span class=line><span class=cl><span class=err>      &#34;distance&#34;: &#34;Cosine&#34;
</span></span></span><span class=line><span class=cl><span class=err>    }
</span></span></span><span class=line><span class=cl><span class=err>  },
</span></span></span><span class=line><span class=cl><span class=err>  &#34;sparse_vectors&#34;: {
</span></span></span><span class=line><span class=cl><span class=err>    &#34;bm42&#34;: {
</span></span></span><span class=line><span class=cl><span class=err>      &#34;modifier&#34;: &#34;idf&#34; // &lt;--- This parameter enables the IDF calculation
</span></span></span><span class=line><span class=cl><span class=err>    }
</span></span></span><span class=line><span class=cl><span class=err>  }
</span></span></span><span class=line><span class=cl><span class=err>}
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>qdrant_client</span> <span class=kn>import</span> <span class=n>QdrantClient</span><span class=p>,</span> <span class=n>models</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>QdrantClient</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my-hybrid-collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;jina&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>distance</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Distance</span><span class=o>.</span><span class=n>COSINE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>sparse_vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;bm42&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>SparseVectorParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>modifier</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Modifier</span><span class=o>.</span><span class=n>IDF</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>The search query will retrieve the documents with both dense and sparse embeddings and combine the scores
using the Reciprocal Rank Fusion (RRF) algorithm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fastembed</span> <span class=kn>import</span> <span class=n>SparseTextEmbedding</span><span class=p>,</span> <span class=n>TextEmbedding</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>query_text</span> <span class=o>=</span> <span class=s2>&#34;best programming language for beginners?&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_bm42</span> <span class=o>=</span> <span class=n>SparseTextEmbedding</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=s2>&#34;Qdrant/bm42-all-minilm-l6-v2-attentions&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_jina</span> <span class=o>=</span> <span class=n>TextEmbedding</span><span class=p>(</span><span class=n>model_name</span><span class=o>=</span><span class=s2>&#34;jinaai/jina-embeddings-v2-base-en&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sparse_embedding</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>model_bm42</span><span class=o>.</span><span class=n>query_embed</span><span class=p>(</span><span class=n>query_text</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>dense_embedding</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>model_jina</span><span class=o>.</span><span class=n>query_embed</span><span class=p>(</span><span class=n>query_text</span><span class=p>))[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my-hybrid-collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>prefetch</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>      <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>sparse_embedding</span><span class=o>.</span><span class=n>as_object</span><span class=p>(),</span> <span class=n>using</span><span class=o>=</span><span class=s2>&#34;bm42&#34;</span><span class=p>,</span> <span class=n>limit</span><span class=o>=</span><span class=mi>10</span><span class=p>),</span>
</span></span><span class=line><span class=cl>      <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span><span class=n>query</span><span class=o>=</span><span class=n>dense_embedding</span><span class=o>.</span><span class=n>tolist</span><span class=p>(),</span>  <span class=n>using</span><span class=o>=</span><span class=s2>&#34;jina&#34;</span><span class=p>,</span> <span class=n>limit</span><span class=o>=</span><span class=mi>10</span><span class=p>),</span>
</span></span><span class=line><span class=cl>  <span class=p>],</span>
</span></span><span class=line><span class=cl>  <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>FusionQuery</span><span class=p>(</span><span class=n>fusion</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Fusion</span><span class=o>.</span><span class=n>RRF</span><span class=p>),</span> <span class=c1># &lt;--- Combine the scores</span>
</span></span><span class=line><span class=cl>  <span class=n>limit</span><span class=o>=</span><span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h3 id=benchmarks>Benchmarks</h3><p>To prove the point further we have conducted some benchmarks to highlight the cases where BM42 outperforms BM25.
Please note, that we didn&rsquo;t intend to make an exhaustive evaluation, as we are presenting a new approach, not a new model.</p><p>For out experiments we choose <a href=https://huggingface.co/datasets/BeIR/quora target=_blank rel="noopener nofollow">quora</a> dataset, which represents a question-deduplication task <del>the Question-Answering task</del>.</p><p>The typical example of the dataset is the following:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>{&#34;_id&#34;: &#34;109&#34;, &#34;text&#34;: &#34;How GST affects the CAs and tax officers?&#34;}
</span></span><span class=line><span class=cl>{&#34;_id&#34;: &#34;110&#34;, &#34;text&#34;: &#34;Why can&#39;t I do my homework?&#34;}
</span></span><span class=line><span class=cl>{&#34;_id&#34;: &#34;111&#34;, &#34;text&#34;: &#34;How difficult is it get into RSI?&#34;}
</span></span></code></pre></div><p>As you can see, it has pretty short texts, there are not much of the statistics to rely on.</p><p>After encoding with BM42, the average vector size is only <strong>5.6 elements per document</strong>.</p><p>With <code>datatype: uint8</code> available in Qdrant, the total size of the sparse vector index is about <strong>13MB</strong> for ~530k documents.</p><p>As a reference point, we use:</p><ul><li>BM25 with tantivy</li><li>the <a href=https://github.com/qdrant/bm42_eval/blob/master/index_bm25_qdrant.py target=_blank rel="noopener nofollow">sparse vector BM25 implementation</a> with the same preprocessing pipeline like for BM42: tokenization, stop-words removal, and lemmatization</li></ul><div class=table-responsive><table class="table mb-5"><thead><tr><th></th><th>BM25 (tantivy)</th><th>BM25 (Sparse)</th><th>BM42</th></tr></thead><tbody><tr><td><del>Precision @ 10</del> *</td><td><del>0.45</del></td><td><del>0.45</del></td><td><del>0.49</del></td></tr><tr><td>Recall @ 10</td><td><del>0.71</del> <strong>0.89</strong></td><td>0.83</td><td>0.85</td></tr></tbody></table></div><p>* - values were corrected after the publication due to a mistake in the evaluation script.</p><aside role=status>When used properly, BM25 with tantivy achieves the best results. Our initial implementation performed wrong character escaping that led to understating the value of <code>recall@10</code> for tantivy.</aside><p>To make our benchmarks transparent, we have published scripts we used for the evaluation: see <a href=https://github.com/qdrant/bm42_eval target=_blank rel="noopener nofollow">github repo</a>.</p><p>Please note, that both BM25 and BM42 won&rsquo;t work well on their own in a production environment.
Best results are achieved with a combination of sparse and dense embeddings in a hybrid approach.
In this scenario, the two models are complementary to each other.
The sparse model is responsible for exact token matching, while the dense model is responsible for semantic matching.</p><p>Some more advanced models might outperform default <code>sentence-transformers/all-MiniLM-L6-v2</code> model we were using.
We encourage developers involved in training embedding models to include a way to extract attention weights and contribute to the BM42 backend.</p><h2 id=fostering-curiosity-and-experimentation>Fostering curiosity and experimentation</h2><p>Despite all of its advantages, BM42 is not always a silver bullet.
For large documents without chunks, BM25 might still be a better choice.</p><p>There might be a smarter way to extract the importance information from the transformer. There could be a better method to weigh IDF against attention scores.</p><p>Qdrant does not specialize in model training. Our core project is the search engine itself. However, we understand that we are not operating in a vacuum. By introducing BM42, we are stepping up to empower our community with novel tools for experimentation.</p><p>We truly believe that the sparse vectors method is at exact level of abstraction to yield both powerful and flexible results.</p><p>Many of you are sharing your recent Qdrant projects in our <a href=https://discord.com/invite/qdrant target=_blank rel="noopener nofollow">Discord channel</a>. Feel free to try out BM42 and let us know what you come up with.</p></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! üôè</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. üòî You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/bm42.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#why-has-bm25-stayed-relevant-for-so-long>Why has BM25 stayed relevant for so long?</a><ul><li></li></ul></li><li><a href=#why-splade-is-not-always-the-answer>Why SPLADE is not always the answer</a></li><li><a href=#the-best-of-both-worlds>The best of both worlds</a><ul><li><a href=#attention-is-all-you-need>Attention is all you need</a></li><li><a href=#wordpiece-retokenization>WordPiece retokenization</a></li></ul></li><li><a href=#practical-examples>Practical examples</a><ul><li><a href=#benchmarks>Benchmarks</a></li></ul></li><li><a href=#fostering-curiosity-and-experimentation>Fostering curiosity and experimentation</a></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/bm42.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>¬© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>