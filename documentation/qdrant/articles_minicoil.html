<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>miniCOIL: on the Road to Usable Sparse Neural Retrieval - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Introducing miniCOIL, a lightweight sparse neural retriever capable of generalization."><meta name=keywords content="hybrid search,sparse retrieval,bm25,splade,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/minicoil/#article","@type":"Article","abstract":"Introducing miniCOIL a lightweight sparse neural retriever capable of generalization","author":{"@type":"Person","name":"Evgeniya Sukhodolskaya"},"dateModified":"2025-05-13 00:00:00 +0300 +0300","datePublished":"2025-05-13 00:00:00 +0300 +0300","description":"Introducing miniCOIL a lightweight sparse neural retriever capable of generalization","headline":"miniCOIL: on the Road to Usable Sparse Neural Retrieval","image":["https://qdrant.tech/articles_data/minicoil/preview/social_preview.jpg"],"name":"miniCOIL: on the Road to Usable Sparse Neural Retrieval","url":"https://qdrant.tech/articles/minicoil/","wordCount":"3376"},{"@id":"https://qdrant.tech/articles/minicoil/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestra√üe 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["hybrid search","sparse retrieval","bm25","splade","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/minicoil/"><meta property="og:type" content="website"><meta property="og:title" content="miniCOIL: on the Road to Usable Sparse Neural Retrieval - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/minicoil/"><meta name=twitter:title content="miniCOIL: on the Road to Usable Sparse Neural Retrieval - Qdrant"><meta property="og:description" content="Introducing miniCOIL, a lightweight sparse neural retriever capable of generalization."><meta name=twitter:description content="Introducing miniCOIL, a lightweight sparse neural retriever capable of generalization."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/minicoil/preview/social_preview.jpg"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/minicoil/preview/social_preview.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/minicoil/preview/social_preview.jpg"><meta name=author content="Evgeniya Sukhodolskaya"><link rel=canonical href=https://qdrant.tech/articles/minicoil/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>miniCOIL: on the Road to Usable Sparse Neural Retrieval</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/machine-learning/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Machine Learning</a><h1 class=documentation-article__header-title>miniCOIL: on the Road to Usable Sparse Neural Retrieval</h1><div class=documentation-article__header-about><p>Evgeniya Sukhodolskaya</p><span>&#183;</span><p>May 13, 2025</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/minicoil/preview/title.webp type=image/webp><img alt="miniCOIL: on the Road to Usable Sparse Neural Retrieval" src=https://qdrant.tech/articles_data/minicoil/preview/title.jpg></picture></div><p>Have you ever heard of sparse neural retrieval? If so, have you used it in production?</p><p>It&rsquo;s a field with excellent potential &ndash; who wouldn&rsquo;t want to use an approach that combines the strengths of dense and term-based text retrieval? Yet it&rsquo;s not so popular. Is it due to the common curse of <em>‚ÄúWhat looks good on paper is not going to work in practice‚Äù?</em>?</p><p>This article describes our path towards sparse neural retrieval <em>as it should be</em> &ndash; lightweight term-based retrievers capable of distinguishing word meanings.</p><p>Learning from the mistakes of previous attempts, we created <strong>miniCOIL</strong>, a new sparse neural candidate to take BM25&rsquo;s place in hybrid searches. We&rsquo;re happy to share it with you and are awaiting your feedback.</p><h2 id=the-good-the-bad-and-the-ugly>The Good, the Bad and the Ugly</h2><p>Sparse neural retrieval is not so well known, as opposed to methods it&rsquo;s based on &ndash; term-based and dense retrieval. Their weaknesses motivated this field&rsquo;s development, guiding its evolution. Let&rsquo;s follow its path.</p><figure><img src=https://qdrant.tech/articles_data/minicoil/models_evolution.png alt="Retrievers evolution" width=100%><figcaption><p>Retrievers evolution</p></figcaption></figure><h3 id=term-based-retrieval>Term-based Retrieval</h3><p>Term-based retrieval usually treats text as a bag of words. These words play roles of different importance, contributing to the overall relevance score between a document and a query.</p><p>Famous <strong>BM25</strong> estimates words&rsquo; contribution based on their:</p><ol><li>Importance in a particular text &ndash; Term Frequency (TF) based.</li><li>Significance within the whole corpus &ndash; Inverse Document Frequency (IDF) based.</li></ol><p>It also has several parameters reflecting typical text length in the corpus, the exact meaning of which you can check in <a href=https://qdrant.tech/articles/bm42/#why-has-bm25-stayed-relevant-for-so-long target=_blank rel="noopener nofollow">our detailed breakdown of the BM25 formula</a>.</p><p>Precisely defining word importance within a text is nontrivial.</p><p>BM25 is built on the idea that term importance can be defined statistically.
This isn&rsquo;t far from the truth in long texts, where frequent repetition of a certain word signals that the text is related to this concept. In very short texts &ndash; say, chunks for Retrieval Augmented Generation (RAG) &ndash; it&rsquo;s less applicable, with TF of 0 or 1. We approached fixing it in our <a href=https://qdrant.tech/articles/bm42/ target=_blank rel="noopener nofollow">BM42 modification of BM25 algorithm.</a></p><p>Yet there is one component of a word&rsquo;s importance for retrieval, which is not considered in BM25 at all &ndash; word meaning. The same words have different meanings in different contexts, and it affects the text&rsquo;s relevance. Think of <em>&ldquo;fruit <strong>bat</strong>&rdquo;</em> and <em>&ldquo;baseball <strong>bat</strong>"</em>‚Äîthe same importance in the text, different meanings.</p><h3 id=dense-retrieval>Dense Retrieval</h3><p>How to capture the meaning? Bag-of-words models like BM25 assume that words are placed in a text independently, while linguists say:</p><blockquote><p>&ldquo;You shall know a word by the company it keeps&rdquo; - John Rupert Firth</p></blockquote><p>This idea, together with the motivation to numerically express word relationships, powered the development of the second branch of retrieval &ndash; dense vectors. Transformer models with attention mechanisms solved the challenge of distinguishing word meanings within text context, making it a part of relevance matching in retrieval.</p><p>Yet dense retrieval didn&rsquo;t (and can&rsquo;t) become a complete replacement for term-based retrieval. Dense retrievers are capable of broad semantic similarity searches, yet they lack precision when we need results including a specific keyword.</p><p>It&rsquo;s a fool&rsquo;s errand &ndash; trying to make dense retrievers do exact matching, as they&rsquo;re built in a paradigm where every word matches every other word semantically to some extent, and this semantic similarity depends on the training data of a particular model.</p><h3 id=sparse-neural-retrieval>Sparse Neural Retrieval</h3><p>So, on one side, we have weak control over matching, sometimes leading to too broad retrieval results, and on the other‚Äîlightweight, explainable and fast term-based retrievers like BM25, incapable of capturing semantics.</p><p>Of course, we want the best of both worlds, fused in one model, no drawbacks included. Sparse neural retrieval evolution was pushed by this desire.</p><ul><li>Why <strong>sparse</strong>? Term-based retrieval can operate on sparse vectors, where each word in the text is assigned a non-zero value (its importance in this text).</li><li>Why <strong>neural</strong>? Instead of deriving an importance score for a word based on its statistics, let&rsquo;s use machine learning models capable of encoding words&rsquo; meaning.</li></ul><p><strong>So why is it not widely used?</strong><figure><img src=https://qdrant.tech/articles_data/minicoil/models_problems.png alt="Problems of modern sparse neural retrievers" width=100%><figcaption><p>Problems of modern sparse neural retrievers</p></figcaption></figure></p><p>The detailed history of sparse neural retrieval makes for <a href=https://qdrant.tech/articles/modern-sparse-neural-retrieval/ target=_blank rel="noopener nofollow">a whole other article</a>. Summing a big part of it up, there were many attempts to map a word representation produced by a dense encoder to a single-valued importance score, and most of them never saw the real world outside of research papers (<strong>DeepImpact</strong>, <strong>TILDEv2</strong>, <strong>uniCOIL</strong>).</p><p>Trained end-to-end on a relevance objective, most of the <strong>sparse encoders</strong> estimated word importance well only for a particular domain. Their out-of-domain accuracy, on datasets they hadn&rsquo;t &ldquo;seen&rdquo; during training, <a href=https://arxiv.org/pdf/2307.10488 target=_blank rel="noopener nofollow">was worse than BM25.</a></p><p>The SOTA of sparse neural retrieval is <strong>SPLADE</strong> &ndash; (Sparse Lexical and Expansion Model). This model has made its way into retrieval systems - you can <a href=https://qdrant.tech/documentation/fastembed/fastembed-splade/ target=_blank rel="noopener nofollow">use SPLADE++ in Qdrant with FastEmbed</a>.</p><p>Yet there&rsquo;s a catch. The &ldquo;expansion&rdquo; part of SPLADE&rsquo;s name refers to a technique that combats against another weakness of term-based retrieval &ndash; <strong>vocabulary mismatch</strong>. While dense encoders can successfully connect related terms like &ldquo;fruit bat&rdquo; and &ldquo;flying fox&rdquo;, term-based retrieval fails at this task.</p><p>SPLADE solves this problem by <strong>expanding documents and queries with additional fitting terms</strong>. However, it leads to SPLADE inference becoming heavy. Additionally, produced representations become not-so-sparse (so, consequently, not lightweight) and far less explainable as expansion choices are made by machine learning models.</p><blockquote><p>&ldquo;Big man in a suit of armor. Take that off, what are you?&rdquo;</p></blockquote><p>Experiments showed that SPLADE without its term expansion tells the same old story of sparse encoders ‚Äî <a href=https://arxiv.org/pdf/2307.10488 target=_blank rel="noopener nofollow">it performs worse than BM25.</a></p><h2 id=eyes-on-the-prize-usable-sparse-neural-retrieval>Eyes on the Prize: Usable Sparse Neural Retrieval</h2><p>Striving for perfection on specific benchmarks, the sparse neural retrieval field either produced models performing worse than BM25 out-of-domain(ironically, <a href=https://arxiv.org/pdf/2307.10488 target=_blank rel="noopener nofollow">trained with BM25-based hard negatives</a>) or models based on heavy document expansion, lowering sparsity.</p><p>To be usable in production, the minimal criteria a sparse neural retriever should meet are:</p><ul><li><strong>Producing lightweight sparse representations (it&rsquo;s in the name!).</strong> Inheriting the perks of term-based retrieval, it should be lightweight and simple. For broader semantic search, there are dense retrievers.</li><li><strong>Being better than BM25 at ranking in different domains.</strong> The goal is a term-based retriever capable of distinguishing word meanings ‚Äî what BM25 can&rsquo;t do ‚Äî preserving BM25&rsquo;s out-of-domain, time-proven performance.</li></ul><figure><img src=https://qdrant.tech/articles_data/minicoil/minicoil.png alt="The idea behind miniCOIL" width=100%><figcaption><p>The idea behind miniCOIL</p></figcaption></figure><h3 id=inspired-by-coil>Inspired by COIL</h3><p>One of the attempts in the field of Sparse Neural Retrieval ‚Äî <a href=https://qdrant.tech/articles/modern-sparse-neural-retrieval/#sparse-neural-retriever-which-understood-homonyms target=_blank rel="noopener nofollow">Contextualized Inverted Lists (COIL)</a> ‚Äî stands out with its approach to term weights encoding.</p><p>Instead of squishing high-dimensional token representations (usually 768-dimensional BERT embeddings) into a single number, COIL authors project them to smaller vectors of 32 dimensions. They propose storing these vectors in <strong>inverted lists</strong> of an <strong>inverted index</strong> (used in term-based retrieval) as is and comparing vector representations through dot product.</p><p>This approach captures deeper semantics, a single number simply cannot convey all the nuanced meanings a word can have.</p><p>Despite this advantage, COIL failed to gain widespread adoption for several key reasons:</p><ul><li>Inverted indexes are usually not designed to store vectors and perform vector operations.</li><li>Trained end-to-end with a relevance objective on <a href=https://microsoft.github.io/msmarco/ target=_blank rel="noopener nofollow">MS MARCO dataset</a>, COIL&rsquo;s performance is heavily domain-bound.</li><li>Additionally, COIL operates on tokens, reusing BERT&rsquo;s tokenizer. However, working at a word level is far better for term-based retrieval. Imagine we want to search for a <em>&ldquo;retriever&rdquo;</em> in our documentation. COIL will break it down into <code>re</code>, <code>#trie</code>, and <code>#ver</code> 32-dimensional vectors and match all three parts separately &ndash; not so convenient.</li></ul><p>However, COIL representations allow distinguishing homographs, a skill BM25 lacks. The best ideas don&rsquo;t start from zero. We propose an approach <strong>built on top of COIL, keeping in mind what needs fixing</strong>:</p><ol><li>We should <strong>abandon end-to-end training on a relevance objective</strong> to get a model performant on out-of-domain data. There is not enough data to train a model able to generalize.</li><li>We should <strong>keep representations sparse and reusable in a classic inverted index</strong>.</li><li>We should <strong>fix tokenization</strong>. This problem is the easiest one to solve, as it was already done in several sparse neural retrievers, and <a href=https://qdrant.tech/articles/bm42/#wordpiece-retokenization target=_blank rel="noopener nofollow">we also learned to do it in our BM42</a>.</li></ol><h3 id=standing-on-the-shoulders-of-bm25>Standing on the Shoulders of BM25</h3><p>BM25 has been a decent baseline across various domains for many years &ndash; and for a good reason. So why discard a time-proven formula?</p><p>Instead of training our sparse neural retriever to assign words&rsquo; importance scores, let&rsquo;s add a semantic COIL-inspired component to BM25 formula.</p><p>$$
\text{score}(D,Q) = \sum_{i=1}^{N} \text{IDF}(q_i) \cdot \text{Importance}^{q_i}_{D} \cdot {\color{YellowGreen}\text{Meaning}^{q_i \times d_j}} \text{, where term } d_j \in D \text{ equals } q_i
$$</p><p>Then, if we manage to capture a word&rsquo;s meaning, our solution alone could work like BM25 combined with a semantically aware reranker &ndash; or, in other words:</p><ul><li>It could see the difference between homographs;</li><li>When used with word stems, it could distinguish parts of speech.</li></ul><figure><img src=https://qdrant.tech/articles_data/minicoil/examples.png alt="Meaning component" width=100%><figcaption><p>Meaning component</p></figcaption></figure><p>And if our model stumbles upon a word it hasn&rsquo;t &ldquo;seen&rdquo; during training, we can just fall back to the original BM25 formula!</p><h3 id=bag-of-words-in-4d>Bag-of-words in 4D</h3><p>COIL uses 32 values to describe one term. Do we need this many? How many words with 32 separate meanings could we name without additional research?</p><p>Yet, even if we use fewer values in COIL representations, the initial problem of dense vectors not fitting into a classical inverted index persists.<br>Unless&mldr; We perform a simple trick!</p><figure><img src=https://qdrant.tech/articles_data/minicoil/bow_4D.png alt="miniCOIL vectors to sparse representation" width=80%><figcaption><p>miniCOIL vectors to sparse representation</p></figcaption></figure><p>Imagine a bag-of-words sparse vector. Every word from the vocabulary takes up one cell. If the word is present in the encoded text ‚Äî we assign some weight; if it isn&rsquo;t ‚Äî it equals zero.</p><p>If we have a mini COIL vector describing a word&rsquo;s meaning, for example, in 4D semantic space, we could just dedicate 4 consecutive cells for word in the sparse vector, one cell per &ldquo;meaning&rdquo; dimension. If we don&rsquo;t, we could fall back to a classic one-cell description with a pure BM25 score.</p><p><strong>Such representations can be used in any standard inverted index.</strong></p><h2 id=training-minicoil>Training miniCOIL</h2><p>Now, we&rsquo;re coming to the part where we need to somehow get this low-dimensional encapsulation of a word&rsquo;s meaning &ndash; <strong>a miniCOIL vector</strong>.</p><p>We want to work smarter, not harder, and rely as much as possible on time-proven solutions. Dense encoders are good at encoding a word&rsquo;s meaning in its context, so it would be convenient to reuse their output. Moreover, we could kill two birds with one stone if we wanted to add miniCOIL to hybrid search &ndash; where dense encoder inference is done regardless.</p><h3 id=reducing-dimensions>Reducing Dimensions</h3><p>Dense encoder outputs are high-dimensional, so we need to perform <strong>dimensionality reduction, which should preserve the word&rsquo;s meaning in context</strong>. The goal is to:</p><ul><li>Avoid relevance objective and dependence on labelled datasets;</li><li>Find a target capturing spatial relations between word‚Äôs meanings;</li><li>Use the simplest architecture possible.</li></ul><h3 id=training-data>Training Data</h3><p>We want miniCOIL vectors to be comparable according to a word&rsquo;s meaning ‚Äî <em>fruit <strong>bat</strong></em> and <em>vampire <strong>bat</strong></em> should be closer to each other in low-dimensional vector space than to <em>baseball <strong>bat</strong></em>. So, we need something to calibrate on when reducing the dimensionality of words&rsquo; contextualized representations.</p><p>It&rsquo;s said that a word&rsquo;s meaning is hidden in the surrounding context or, simply put, in any texts that include this word. In bigger texts, we risk the word&rsquo;s meaning blending out. So, let&rsquo;s work at the sentence level and assume that sentences sharing one word should cluster in a way that each cluster contains sentences where this word is used in one specific meaning.</p><p>If that&rsquo;s true, we could encode various sentences with a sophisticated dense encoder and form a reusable spatial relations target for input dense encoders. It&rsquo;s not a big problem to find lots of textual data containing frequently used words when we have datasets like the <a href=https://paperswithcode.com/dataset/openwebtext target=_blank rel="noopener nofollow">OpenWebText dataset</a>, spanning the whole web. With this amount of data available, we could afford generalization and domain independence, which is hard to achieve with the relevance objective.</p><h4 id=its-going-to-work-i-bat>It&rsquo;s Going to Work, I Bat</h4><p>Let‚Äôs test our assumption and take a look at the word <em>‚Äúbat‚Äù</em>.</p><p>We took several thousand sentences with this word, which we sampled from <a href=https://paperswithcode.com/dataset/openwebtext target=_blank rel="noopener nofollow">OpenWebText dataset</a> and vectorized with a <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> encoder. The goal was to check if we could distinguish any clusters containing sentences where <em>‚Äúbat‚Äù</em> shares the same meaning.</p><figure><img src=https://qdrant.tech/articles_data/minicoil/bat.png alt='Sentences with "bat" in 2D' width=80%><figcaption><p>Sentences with &ldquo;bat&rdquo; in 2D.<br>A very important observation: <em>Looks like a bat</em>:)</p></figcaption></figure><p>The result had two big clusters related to <em>&ldquo;bat&rdquo;</em> as an animal and <em>&ldquo;bat&rdquo;</em> as a sports equipment, and two smaller ones related to fluttering motion and the verb used in sports. Seems like it could work!</p><h3 id=architecture-and-training-objective>Architecture and Training Objective</h3><p>Let&rsquo;s continue dealing with <em>&ldquo;bats&rdquo;</em>.</p><p>We have a training pool of sentences containing the word <em>&ldquo;bat&rdquo;</em> in different meanings. Using a dense encoder of choice, we get a contextualized embedding of <em>&ldquo;bat&rdquo;</em> from each sentence and learn to compress it into a low-dimensional miniCOIL <em>&ldquo;bat&rdquo;</em> space, guided by <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> sentence embeddings.</p><p>We&rsquo;re dealing with only one word, so it should be enough to use just one linear layer for dimensionality reduction, with a <a href=https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html target=_blank rel="noopener nofollow"><code>Tanh activation</code></a> on top, mapping values of compressed vectors to (-1, 1) range. The activation function choice is made to align miniCOIL representations with dense encoder ones, which are mainly compared through <code>cosine similarity</code>.</p><figure><img src=https://qdrant.tech/articles_data/minicoil/miniCOIL_one_word.png alt="miniCOIL architecture on a word level" width=100%><figcaption><p>miniCOIL architecture on a word level</p></figcaption></figure><p>As a training objective, we can select the minimization of <a href=https://qdrant.tech/articles/triplet-loss/ target=_blank rel="noopener nofollow">triplet loss</a>, where triplets are picked and aligned based on distances between <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> sentence embeddings. We rely on the confidence (size of the margin) of <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> to guide our <em>&ldquo;bat&rdquo;</em> miniCOIL compression.</p><figure><img src=https://qdrant.tech/articles_data/minicoil/training_objective.png alt="miniCOIL training" width=80%><figcaption><p>miniCOIL training</p></figcaption></figure><aside role=status>Since miniCOIL vectors are trained to reflect spatial relationships based on cosine similarity, they should be normalized before inserting them into bag-of-words sparse vectors (compared though dot product).</aside><h4 id=eating-elephant-one-bite-at-a-time>Eating Elephant One Bite at a Time</h4><p>Now, we have the full idea of how to train miniCOIL for one word. How do we scale to a whole vocabulary?</p><p>What if we keep it simple and continue training a model per word? It has certain benefits:</p><ol><li>Extremely simple architecture: even one layer per word can suffice.</li><li>Super fast and easy training process.</li><li>Cheap and fast inference due to the simple architecture.</li><li>Flexibility to discover and tune underperforming words.</li><li>Flexibility to extend and shrink the vocabulary depending on the domain and use case.</li></ol><p>Then we could train all the words we&rsquo;re interested in and simply combine (stack) all models into one big miniCOIL.</p><figure><img src=https://qdrant.tech/articles_data/minicoil/miniCOIL_full.png alt="miniCOIL model" width=100%><figcaption><p>miniCOIL model</p></figcaption></figure><h3 id=implementation-details>Implementation Details</h3><p>The code of the training approach sketched above is open-sourced <a href=https://github.com/qdrant/miniCOIL target=_blank rel="noopener nofollow">in this repository</a>.</p><p>Here are the specific characteristics of the miniCOIL model we trained based on this approach:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th style=text-align:left>Component</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><strong>Input Dense Encoder</strong></td><td style=text-align:left><a href=https://huggingface.co/jinaai/jina-embeddings-v2-small-en target=_blank rel="noopener nofollow"><code>jina-embeddings-v2-small-en</code></a> (512 dimensions)</td></tr><tr><td style=text-align:left><strong>miniCOIL Vectors Size</strong></td><td style=text-align:left>4 dimensions</td></tr><tr><td style=text-align:left><strong>miniCOIL Vocabulary</strong></td><td style=text-align:left>List of 30,000 of the most common English words, cleaned of stop words and words shorter than 3 letters, <a href=https://github.com/arstgit/high-frequency-vocabulary/tree/master target=_blank rel="noopener nofollow">taken from here</a>. Words are stemmed to align miniCOIL with our BM25 implementation.</td></tr><tr><td style=text-align:left><strong>Training Data</strong></td><td style=text-align:left>40 million sentences ‚Äî a random subset of the <a href=https://paperswithcode.com/dataset/openwebtext target=_blank rel="noopener nofollow">OpenWebText dataset</a>. To make triplet sampling convenient, we uploaded sentences and their <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> embeddings to Qdrant and built a <a href=https://qdrant.tech/documentation/concepts/indexing/#full-text-index target=_blank rel="noopener nofollow">full-text payload index</a> on sentences with a tokenizer of type <code>word</code>.</td></tr><tr><td style=text-align:left><strong>Training Data per Word</strong></td><td style=text-align:left>We sample 8000 sentences per word and form triplets with a margin of at least <strong>0.1</strong>.<br>Additionally, we apply <strong>augmentation</strong> ‚Äî take a sentence and cut out the target word plus its 1‚Äì3 neighbours. We reuse the same similarity score between original and augmented sentences for simplicity.</td></tr><tr><td style=text-align:left><strong>Training Parameters</strong></td><td style=text-align:left><strong>Epochs</strong>: 60<br><strong>Optimizer</strong>: Adam with a learning rate of 1e-4<br><strong>Validation set</strong>: 20%</td></tr></tbody></table></div><p>Each word was <strong>trained on just one CPU</strong>, and it took approximately fifty seconds per word to train.
We included this <code>minicoil-v1</code> version in the <a href=https://github.com/qdrant/fastembed target=_blank rel="noopener nofollow">v0.7.0 release of our FastEmbed library</a>.</p><p>You can check an example of <code>minicoil-v1</code> usage with FastEmbed in the <a href=https://huggingface.co/Qdrant/minicoil-v1 target=_blank rel="noopener nofollow">HuggingFace card</a>.</p><aside role=status>To use <span style=font-weight:700>minicoil-v1</span> correctly, make sure to configure sparse vectors with <a href="https://qdrant.tech/documentation/concepts/indexing/?q=modifier#idf-modifier">Modifier.IDF</a></aside><h2 id=results>Results</h2><h3 id=validation-loss>Validation Loss</h3><p>Input transformer <a href=https://huggingface.co/jinaai/jina-embeddings-v2-small-en target=_blank rel="noopener nofollow"><code>jina-embeddings-v2-small-en</code></a> approximates the ‚Äúrole model‚Äù transformer <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> context relations with a (measured though triplets) quality of 83%. That means that in 17% of cases, <a href=https://huggingface.co/jinaai/jina-embeddings-v2-small-en target=_blank rel="noopener nofollow"><code>jina-embeddings-v2-small-en</code></a> will take a sentence triplet from <a href=https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1 target=_blank rel="noopener nofollow"><code>mxbai-embed-large-v1</code></a> and embed it in a way that the negative example from the perspective of <code>mxbai</code> will be closer to the anchor than the positive one.</p><p>The validation loss we obtained, depending on the miniCOIL vector size (4, 8, or 16), demonstrates miniCOIL correctly distinguishing from 76% (60 failed triplets on average per batch of size 256) to 85% (38 failed triplets on average per batch of size 256) triplets respectively.</p><figure><img src=https://qdrant.tech/articles_data/minicoil/validation_loss.png alt="Validation loss" width=80%><figcaption><p>Validation loss</p></figcaption></figure><h3 id=benchmarking>Benchmarking</h3><p>The benchmarking code is open-sourced in <a href=https://github.com/qdrant/mini-coil-demo/tree/master/minicoil_demo target=_blank rel="noopener nofollow">this repository</a>.</p><p>To check our 4D miniCOIL version performance in different domains, we, ironically, chose a subset of the same <a href=https://github.com/beir-cellar/beir target=_blank rel="noopener nofollow">BEIR datasets</a>, high benchmark values on which became an end in itself for many sparse neural retrievers. Yet the difference is that <strong>miniCOIL wasn&rsquo;t trained on BEIR datasets and shouldn&rsquo;t be biased towards them</strong>.</p><p>We&rsquo;re testing our 4D miniCOIL model versus <a href=https://huggingface.co/Qdrant/bm25 target=_blank rel="noopener nofollow">our BM25 implementation</a>. BEIR datasets are indexed to Qdrant using the following parameters for both methods:</p><ul><li><code>k = 1.2</code>, <code>b = 0.75</code> default values recommended to use with BM25 scoring;</li><li><code>avg_len</code> estimated on 50,000 documents from a respective dataset.</li></ul><aside role=status>BM25 results depend on implementation details, such as the choice of stemmer, tokenizer, stop word list, etc. To make miniCOIL comparable to BM25, we use our own BM25 implementation and reuse all its implementation choices for miniCOIL.</aside><p>We compare models based on the <code>NDCG@10</code> metric, as we&rsquo;re interested in the ranking performance of miniCOIL compared to BM25. Both retrieve the same subset of indexed documents based on exact matches, but miniCOIL should ideally rank this subset better based on its semantics understanding.</p><p>The result on several domains we tested is the following:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th style=text-align:left>Dataset</th><th style=text-align:left>BM25 (NDCG@10)</th><th style=text-align:left>MiniCOIL (NDCG@10)</th></tr></thead><tbody><tr><td style=text-align:left>MS MARCO</td><td style=text-align:left>0.237</td><td style=text-align:left><strong>0.244</strong></td></tr><tr><td style=text-align:left>NQ</td><td style=text-align:left>0.304</td><td style=text-align:left><strong>0.319</strong></td></tr><tr><td style=text-align:left>Quora</td><td style=text-align:left>0.784</td><td style=text-align:left><strong>0.802</strong></td></tr><tr><td style=text-align:left>FiQA-2018</td><td style=text-align:left>0.252</td><td style=text-align:left><strong>0.257</strong></td></tr><tr><td style=text-align:left>HotpotQA</td><td style=text-align:left><strong>0.634</strong></td><td style=text-align:left>0.633</td></tr></tbody></table></div><p>We can see miniCOIL performing slightly better than BM25 in four out of five tested domains. It shows that <strong>we&rsquo;re moving in the right direction</strong>.</p><aside role=status>To use any model for your specific use case, always benchmark it yourself!<br>Performance on public benchmarks doesn't secure high performance on specific data.</aside><h2 id=key-takeaways>Key Takeaways</h2><p>This article describes our attempt to make a lightweight sparse neural retriever that is able to generalize to out-of-domain data. Sparse neural retrieval has a lot of potential, and we hope to see it gain more traction.</p><h3 id=why-is-this-approach-useful>Why is this Approach Useful?</h3><p>This approach to training sparse neural retrievers:</p><ol><li>Doesn‚Äôt rely on a relevance objective because it is trained in a self-supervised way, so it doesn‚Äôt need labeled datasets to scale.</li><li>Builds on the proven BM25 formula, simply adding a semantic component to it.</li><li>Creates lightweight sparse representations that fit into a standard inverted index.</li><li>Fully reuses the outputs of dense encoders, making it adaptable to different models. This also makes miniCOIL a cheap upgrade for hybrid search solutions.</li><li>Uses an extremely simple model architecture, with one trainable layer per word in miniCOIL‚Äôs vocabulary. This results in very fast training and inference. Also, this word-level training makes it easy to expand miniCOIL‚Äôs vocabulary for a specific use case.</li></ol><h3 id=the-right-tool-for-the-right-job>The Right Tool for the Right Job</h3><p>When are miniCOIL retrievers applicable?</p><p>If you need precise term matching but BM25-based retrieval doesn&rsquo;t meet your needs, ranking higher documents with words of the right form but the wrong semantical meaning.</p><p>Say you&rsquo;re implementing search in your documentation. In this use case, keywords-based search prevails, but BM25 won&rsquo;t account for different context-based meanings of these keywords. For example, if you&rsquo;re searching for a <em>&ldquo;data <strong>point</strong>&rdquo;</em> in our documentation, you&rsquo;d prefer to see <em>&ldquo;a <strong>point</strong> is a record in Qdrant&rdquo;</em> ranked higher than <em>floating <strong>point</strong> precision</em>, and here miniCOIL-based retrieval is an alternative to consider.</p><p>Additionally, miniCOIL fits nicely as a part of a hybrid search, as it enhances sparse retrieval without any noticeable increase in resource consumption, directly reusing contextual word representations produced by a dense encoder.</p><p>To sum up, miniCOIL should work as if BM25 understood the meaning of words and ranked documents based on this semantic knowledge. It operates only on exact matches, so if you aim for documents semantically similar to the query but expressed in different words, dense encoders are the way to go.</p><h3 id=whats-next>What&rsquo;s Next?</h3><p>We will continue working on improving our approach &ndash; both in-depth, searching for ways to improve the model&rsquo;s quality, and in-width, extending it to various dense encoders and languages beyond English.</p><p>And we would love to share this road to usable sparse neural retrieval with you!</p></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! üôè</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. üòî You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/miniCOIL.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#the-good-the-bad-and-the-ugly>The Good, the Bad and the Ugly</a><ul><li><a href=#term-based-retrieval>Term-based Retrieval</a></li><li><a href=#dense-retrieval>Dense Retrieval</a></li><li><a href=#sparse-neural-retrieval>Sparse Neural Retrieval</a></li></ul></li><li><a href=#eyes-on-the-prize-usable-sparse-neural-retrieval>Eyes on the Prize: Usable Sparse Neural Retrieval</a><ul><li><a href=#inspired-by-coil>Inspired by COIL</a></li><li><a href=#standing-on-the-shoulders-of-bm25>Standing on the Shoulders of BM25</a></li><li><a href=#bag-of-words-in-4d>Bag-of-words in 4D</a></li></ul></li><li><a href=#training-minicoil>Training miniCOIL</a><ul><li><a href=#reducing-dimensions>Reducing Dimensions</a></li><li><a href=#training-data>Training Data</a></li><li><a href=#architecture-and-training-objective>Architecture and Training Objective</a></li><li><a href=#implementation-details>Implementation Details</a></li></ul></li><li><a href=#results>Results</a><ul><li><a href=#validation-loss>Validation Loss</a></li><li><a href=#benchmarking>Benchmarking</a></li></ul></li><li><a href=#key-takeaways>Key Takeaways</a><ul><li><a href=#why-is-this-approach-useful>Why is this Approach Useful?</a></li><li><a href=#the-right-tool-for-the-right-job>The Right Tool for the Right Job</a></li><li><a href=#whats-next>What&rsquo;s Next?</a></li></ul></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/miniCOIL.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>¬© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>