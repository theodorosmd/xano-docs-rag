<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>Hybrid Search Revamped - Building with Qdrant's Query API - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Our new Query API allows you to build a hybrid search system that uses different search methods to improve search quality & experience. Learn more here."><meta name=keywords content="vector search engine,neural network,matching,SaaS,approximate nearest neighbor search,image search,recommender system,vectors,knn algorithm,hnsw,vector search,embeddings,similarity,simaes networks,BERT,transformer,word2vec,fasttext,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/hybrid-search/#article","@type":"Article","abstract":"Our new Query API allows you to build a hybrid search system that uses different search methods to improve search quality  experience Learn more here","author":{"@type":"Person","name":"Kacper Łukawski"},"dateModified":"2024-07-25 00:00:00 +0000 UTC","datePublished":"2024-07-25 00:00:00 +0000 UTC","description":"Our new Query API allows you to build a hybrid search system that uses different search methods to improve search quality  experience Learn more here","headline":"Hybrid Search Revamped - Building with Qdrant\u0026#39;s Query API","image":["https://qdrant.tech/articles_data/hybrid-search/social-preview.png"],"name":"Hybrid Search Revamped - Building with Qdrant\u0026#39;s Query API","url":"https://qdrant.tech/articles/hybrid-search/","wordCount":"2292"},{"@id":"https://qdrant.tech/articles/hybrid-search/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestraße 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["vector search engine","neural network","matching","SaaS","approximate nearest neighbor search","image search","recommender system","vectors","knn algorithm","hnsw","vector search","embeddings","similarity","simaes networks","BERT","transformer","word2vec","fasttext","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/hybrid-search/"><meta property="og:type" content="website"><meta property="og:title" content="Hybrid Search Revamped - Building with Qdrant's Query API - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/hybrid-search/"><meta name=twitter:title content="Hybrid Search Revamped - Building with Qdrant's Query API - Qdrant"><meta property="og:description" content="Our new Query API allows you to build a hybrid search system that uses different search methods to improve search quality & experience. Learn more here."><meta name=twitter:description content="Our new Query API allows you to build a hybrid search system that uses different search methods to improve search quality & experience. Learn more here."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/hybrid-search/social-preview.png"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/hybrid-search/social-preview.png"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/hybrid-search/social-preview.png"><meta name=author content="Kacper Łukawski"><link rel=canonical href=https://qdrant.tech/articles/hybrid-search/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>Hybrid Search Revamped - Building with Qdrant's Query API</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/vector-search-manuals/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Vector Search Manuals</a><h1 class=documentation-article__header-title>Hybrid Search Revamped - Building with Qdrant's Query API</h1><div class=documentation-article__header-about><p>Kacper Łukawski</p><span>&#183;</span><p>July 25, 2024</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/hybrid-search/preview/title.webp type=image/webp><img alt="Hybrid Search Revamped - Building with Qdrant's Query API" src=https://qdrant.tech/articles_data/hybrid-search/preview/title.jpg></picture></div><p>It&rsquo;s been over a year since we published the original article on how to build a hybrid
search system with Qdrant. The idea was straightforward: combine the results from different search methods to improve
retrieval quality. Back in 2023, you still needed to use an additional service to bring lexical search
capabilities and combine all the intermediate results. Things have changed since then. Once we introduced support for
sparse vectors, <a href=https://qdrant.tech/articles/sparse-vectors/>the additional search service became obsolete</a>, but you were still
required to combine the results from different methods on your end.</p><p><strong>Qdrant 1.10 introduces a new Query API that lets you build a search system by combining different search methods
to improve retrieval quality</strong>. Everything is now done on the server side, and you can focus on building the best search
experience for your users. In this article, we will show you how to utilize the new <a href=https://qdrant.tech/documentation/concepts/search/#query-api>Query
API</a> to build a hybrid search system.</p><h2 id=introducing-the-new-query-api>Introducing the new Query API</h2><p>At Qdrant, we believe that vector search capabilities go well beyond a simple search for nearest neighbors.
That&rsquo;s why we provided separate methods for different search use cases, such as <code>search</code>, <code>recommend</code>, or <code>discover</code>.
With the latest release, we are happy to introduce the new Query API, which combines all of these methods into a single
endpoint and also supports creating nested multistage queries that can be used to build complex search pipelines.</p><p>If you are an existing Qdrant user, you probably have a running search mechanism that you want to improve, whether sparse
or dense. Doing any changes should be preceded by a proper evaluation of its effectiveness.</p><h2 id=how-effective-is-your-search-system>How effective is your search system?</h2><p>None of the experiments makes sense if you don&rsquo;t measure the quality. How else would you compare which method works
better for your use case? The most common way of doing that is by using the standard metrics, such as <code>precision@k</code>,
<code>MRR</code>, or <code>NDCG</code>. There are existing libraries, such as <a href=https://amenra.github.io/ranx/ target=_blank rel="noopener nofollow">ranx</a>, that can help you with
that. We need to have the ground truth dataset to calculate any of these, but curating it is a separate task.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ranx</span> <span class=kn>import</span> <span class=n>Qrels</span><span class=p>,</span> <span class=n>Run</span><span class=p>,</span> <span class=n>evaluate</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Qrels, or query relevance judgments, keep the ground truth data</span>
</span></span><span class=line><span class=cl><span class=n>qrels_dict</span> <span class=o>=</span> <span class=p>{</span> <span class=s2>&#34;q_1&#34;</span><span class=p>:</span> <span class=p>{</span> <span class=s2>&#34;d_12&#34;</span><span class=p>:</span> <span class=mi>5</span><span class=p>,</span> <span class=s2>&#34;d_25&#34;</span><span class=p>:</span> <span class=mi>3</span> <span class=p>},</span>
</span></span><span class=line><span class=cl>               <span class=s2>&#34;q_2&#34;</span><span class=p>:</span> <span class=p>{</span> <span class=s2>&#34;d_11&#34;</span><span class=p>:</span> <span class=mi>6</span><span class=p>,</span> <span class=s2>&#34;d_22&#34;</span><span class=p>:</span> <span class=mi>1</span> <span class=p>}</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Runs are built from the search results</span>
</span></span><span class=line><span class=cl><span class=n>run_dict</span> <span class=o>=</span> <span class=p>{</span> <span class=s2>&#34;q_1&#34;</span><span class=p>:</span> <span class=p>{</span> <span class=s2>&#34;d_12&#34;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span> <span class=s2>&#34;d_23&#34;</span><span class=p>:</span> <span class=mf>0.8</span><span class=p>,</span> <span class=s2>&#34;d_25&#34;</span><span class=p>:</span> <span class=mf>0.7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=s2>&#34;d_36&#34;</span><span class=p>:</span> <span class=mf>0.6</span><span class=p>,</span> <span class=s2>&#34;d_32&#34;</span><span class=p>:</span> <span class=mf>0.5</span><span class=p>,</span> <span class=s2>&#34;d_35&#34;</span><span class=p>:</span> <span class=mf>0.4</span>  <span class=p>},</span>
</span></span><span class=line><span class=cl>             <span class=s2>&#34;q_2&#34;</span><span class=p>:</span> <span class=p>{</span> <span class=s2>&#34;d_12&#34;</span><span class=p>:</span> <span class=mf>0.9</span><span class=p>,</span> <span class=s2>&#34;d_11&#34;</span><span class=p>:</span> <span class=mf>0.8</span><span class=p>,</span> <span class=s2>&#34;d_25&#34;</span><span class=p>:</span> <span class=mf>0.7</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=s2>&#34;d_36&#34;</span><span class=p>:</span> <span class=mf>0.6</span><span class=p>,</span> <span class=s2>&#34;d_22&#34;</span><span class=p>:</span> <span class=mf>0.5</span><span class=p>,</span> <span class=s2>&#34;d_35&#34;</span><span class=p>:</span> <span class=mf>0.4</span>  <span class=p>}</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># We need to create both objects, and then we can evaluate the run against the qrels</span>
</span></span><span class=line><span class=cl><span class=n>qrels</span> <span class=o>=</span> <span class=n>Qrels</span><span class=p>(</span><span class=n>qrels_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>run</span> <span class=o>=</span> <span class=n>Run</span><span class=p>(</span><span class=n>run_dict</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculating the NDCG@5 metric is as simple as that</span>
</span></span><span class=line><span class=cl><span class=n>evaluate</span><span class=p>(</span><span class=n>qrels</span><span class=p>,</span> <span class=n>run</span><span class=p>,</span> <span class=s2>&#34;ndcg@5&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=available-embedding-options-with-query-api>Available embedding options with Query API</h2><p>Support for multiple vectors per point is nothing new in Qdrant, but introducing the Query API makes it even
more powerful. The 1.10 release supports the multivectors, allowing you to treat embedding lists
as a single entity. There are many possible ways of utilizing this feature, and the most prominent one is the support
for late interaction models, such as <a href=https://qdrant.tech/documentation/fastembed/fastembed-colbert/ target=_blank rel="noopener nofollow">ColBERT</a>. Instead of having a single embedding for each document or query, this
family of models creates a separate one for each token of text. In the search process, the final score is calculated
based on the interaction between the tokens of the query and the document. Contrary to cross-encoders, document
embedding might be precomputed and stored in the database, which makes the search process much faster. If you are
curious about the details, please check out <a href=https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/ target=_blank rel="noopener nofollow">the article about ColBERT, written by our friends from Jina
AI</a>.</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/late-interaction.png alt="Late interaction"></p><p>Besides multivectors, you can use regular dense and sparse vectors, and experiment with smaller data types to reduce
memory use. Named vectors can help you store different dimensionalities of the embeddings, which is useful if you
use multiple models to represent your data, or want to utilize the Matryoshka embeddings.</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/multiple-vectors.png alt="Multiple vectors per point"></p><p>There is no single way of building a hybrid search. The process of designing it is an exploratory exercise, where you
need to test various setups and measure their effectiveness. Building a proper search experience is a
complex task, and it&rsquo;s better to keep it data-driven, not just rely on the intuition.</p><h2 id=fusion-vs-reranking>Fusion vs reranking</h2><p>We can, distinguish two main approaches to building a hybrid search system: fusion and reranking. The former is about
combining the results from different search methods, based solely on the scores returned by each method. That usually
involves some normalization, as the scores returned by different methods might be in different ranges. After that, there
is a formula that takes the relevancy measures and calculates the final score that we use later on to reorder the
documents. Qdrant has built-in support for the Reciprocal Rank Fusion method, which is the de facto standard in the
field.</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/fusion.png alt=Fusion></p><p>Reranking, on the other hand, is about taking the results from different search methods and reordering them based on
some additional processing using the content of the documents, not just the scores. This processing may rely on an
additional neural model, such as a cross-encoder which would be inefficient enough to be used on the whole dataset.
These methods are practically applicable only when used on a smaller subset of candidates returned by the faster search
methods. Late interaction models, such as ColBERT, are way more efficient in this case, as they can be used to rerank
the candidates without the need to access all the documents in the collection.</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/reranking.png alt=Reranking></p><h3 id=why-not-a-linear-combination>Why not a linear combination?</h3><p>It&rsquo;s often proposed to use full-text and vector search scores to form a linear combination formula to rerank
the results. So it goes like this:</p><p><code>final_score = 0.7 * vector_score + 0.3 * full_text_score</code></p><p>However, we didn&rsquo;t even consider such a setup. Why? Those scores don&rsquo;t make the problem linearly separable. We used
the BM25 score along with cosine vector similarity to use both of them as points coordinates in 2-dimensional space. The
chart shows how those points are distributed:</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/linear-combination.png alt="A distribution of both Qdrant and BM25 scores mapped into 2D space."></p><p><em>A distribution of both Qdrant and BM25 scores mapped into 2D space. It clearly shows relevant and non-relevant
objects are not linearly separable in that space, so using a linear combination of both scores won&rsquo;t give us
a proper hybrid search.</em></p><p>Both relevant and non-relevant items are mixed. <strong>None of the linear formulas would be able to distinguish
between them.</strong> Thus, that&rsquo;s not the way to solve it.</p><h2 id=building-a-hybrid-search-system-in-qdrant>Building a hybrid search system in Qdrant</h2><p>Ultimately, <strong>any search mechanism might also be a reranking mechanism</strong>. You can prefetch results with sparse vectors
and then rerank them with the dense ones, or the other way around. Or, if you have Matryoshka embeddings, you can start
with oversampling the candidates with the dense vectors of the lowest dimensionality and then gradually reduce the
number of candidates by reranking them with the higher-dimensional embeddings. Nothing stops you from
combining both fusion and reranking.</p><p>Let&rsquo;s go a step further and build a hybrid search mechanism that combines the results from the
Matryoshka embeddings, dense vectors, and sparse vectors and then reranks them with the late interaction model. In the
meantime, we will introduce additional reranking and fusion steps.</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/complex-search-pipeline.png alt="Complex search pipeline"></p><p>Our search pipeline consists of two branches, each of them responsible for retrieving a subset of documents that
we eventually want to rerank with the late interaction model. Let&rsquo;s connect to Qdrant first and then build the search
pipeline.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>qdrant_client</span> <span class=kn>import</span> <span class=n>QdrantClient</span><span class=p>,</span> <span class=n>models</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>QdrantClient</span><span class=p>(</span><span class=s2>&#34;http://localhost:6333&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>All the steps utilizing Matryoshka embeddings might be specified in the Query API as a nested structure:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># The first branch of our search pipeline retrieves 25 documents</span>
</span></span><span class=line><span class=cl><span class=c1># using the Matryoshka embeddings with multistep retrieval.</span>
</span></span><span class=line><span class=cl><span class=n>matryoshka_prefetch</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prefetch</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>prefetch</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=c1># The first prefetch operation retrieves 100 documents</span>
</span></span><span class=line><span class=cl>                <span class=c1># using the Matryoshka embeddings with the lowest</span>
</span></span><span class=line><span class=cl>                <span class=c1># dimensionality of 64.</span>
</span></span><span class=line><span class=cl>                <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>query</span><span class=o>=</span><span class=p>[</span><span class=mf>0.456</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.789</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mf>0.239</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;matryoshka-64dim&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>limit</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=c1># Then, the retrieved documents are re-ranked using the</span>
</span></span><span class=line><span class=cl>            <span class=c1># Matryoshka embeddings with the dimensionality of 128.</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=o>=</span><span class=p>[</span><span class=mf>0.456</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.789</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.789</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>using</span><span class=o>=</span><span class=s2>&#34;matryoshka-128dim&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>limit</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># Finally, the results are re-ranked using the Matryoshka</span>
</span></span><span class=line><span class=cl>    <span class=c1># embeddings with the dimensionality of 256.</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=p>[</span><span class=mf>0.456</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.789</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mf>0.123</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;matryoshka-256dim&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>25</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Similarly, we can build the second branch of our search pipeline, which retrieves the documents using the dense and
sparse vectors and performs the fusion of them using the Reciprocal Rank Fusion method:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># The second branch of our search pipeline also retrieves 25 documents,</span>
</span></span><span class=line><span class=cl><span class=c1># but uses the dense and sparse vectors, with their results combined</span>
</span></span><span class=line><span class=cl><span class=c1># using the Reciprocal Rank Fusion.</span>
</span></span><span class=line><span class=cl><span class=n>sparse_dense_rrf_prefetch</span> <span class=o>=</span> <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>prefetch</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>prefetch</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=c1># The first prefetch operation retrieves 100 documents</span>
</span></span><span class=line><span class=cl>                <span class=c1># using dense vectors using integer data type. Retrieval</span>
</span></span><span class=line><span class=cl>                <span class=c1># is faster, but quality is lower.</span>
</span></span><span class=line><span class=cl>                <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                    <span class=n>query</span><span class=o>=</span><span class=p>[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>63</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mi>92</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;dense-uint8&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>limit</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=c1># Integer-based embeddings are then re-ranked using the</span>
</span></span><span class=line><span class=cl>            <span class=c1># float-based embeddings. Here we just want to retrieve</span>
</span></span><span class=line><span class=cl>            <span class=c1># 25 documents.</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mf>1.234</span><span class=p>,</span> <span class=mf>0.762</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mf>1.532</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>using</span><span class=o>=</span><span class=s2>&#34;dense&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>limit</span><span class=o>=</span><span class=mi>25</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=c1># Here we just add another 25 documents using the sparse</span>
</span></span><span class=line><span class=cl>        <span class=c1># vectors only.</span>
</span></span><span class=line><span class=cl>        <span class=n>models</span><span class=o>.</span><span class=n>Prefetch</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>SparseVector</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>indices</span><span class=o>=</span><span class=p>[</span><span class=mi>125</span><span class=p>,</span> <span class=mi>9325</span><span class=p>,</span> <span class=mi>58214</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>values</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mf>0.164</span><span class=p>,</span> <span class=mf>0.229</span><span class=p>,</span> <span class=mf>0.731</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>using</span><span class=o>=</span><span class=s2>&#34;sparse&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>limit</span><span class=o>=</span><span class=mi>25</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># RRF is activated below, so there is no need to specify the</span>
</span></span><span class=line><span class=cl>    <span class=c1># query vector here, as fusion is done on the scores of the</span>
</span></span><span class=line><span class=cl>    <span class=c1># retrieved documents.</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>FusionQuery</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>fusion</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Fusion</span><span class=o>.</span><span class=n>RRF</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>The second branch could have already been called hybrid, as it combines the results from the dense and sparse vectors
with fusion. However, nothing stops us from building even more complex search pipelines.</p><p>Here is how the target call to the Query API would look like in Python:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>query_points</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;my-collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>prefetch</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>matryoshka_prefetch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>sparse_dense_rrf_prefetch</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=c1># Finally rerank the results with the late interaction model. It only </span>
</span></span><span class=line><span class=cl>    <span class=c1># considers the documents retrieved by all the prefetch operations above. </span>
</span></span><span class=line><span class=cl>    <span class=c1># Return 10 final results.</span>
</span></span><span class=line><span class=cl>    <span class=n>query</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>1.928</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.654</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mf>0.213</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=o>-</span><span class=mf>1.197</span><span class=p>,</span> <span class=mf>0.583</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mf>1.901</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=o>...</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=mf>0.112</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.473</span><span class=p>,</span> <span class=o>...</span><span class=p>,</span> <span class=mf>1.786</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>using</span><span class=o>=</span><span class=s2>&#34;late-interaction&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>with_payload</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>The options are endless, the new Query API gives you the flexibility to experiment with different setups. <strong>You
rarely need to build such a complex search pipeline</strong>, but it&rsquo;s good to know that you can do that if needed.</p><aside role=status>The example above is a simplified version of the search pipeline using <b>multi-vector representations, such as late
interaction models</b>. Practically, these methods are computationally expensive, and there are some considerations
to take into account when building a real-world system. The paragraph below will give you some hints on how to use
these methods efficiently.</aside><h2 id=lessons-learned-multi-vector-representations>Lessons learned: multi-vector representations</h2><p>Many of you have already started building hybrid search systems and reached out to us with questions and feedback.
We&rsquo;ve seen many different approaches, however one recurring idea was to utilize <strong>multi-vector representations with
ColBERT-style models as a reranking step</strong>, after retrieving candidates with single-vector dense and/or sparse methods.
This reflects the latest trends in the field, as single-vector methods are still the most efficient, but multivectors
capture the nuances of the text better.</p><p><img src=https://qdrant.tech/articles_data/hybrid-search/late-interaction-reranking.png alt="Reranking with late interaction models"></p><p>Assuming you never use late interaction models for retrieval alone, but only for reranking, this setup comes with a
hidden cost. By default, each configured dense vector of the collection will have a corresponding HNSW graph created.
Even, if it is a multi-vector.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>qdrant_client</span> <span class=kn>import</span> <span class=n>QdrantClient</span><span class=p>,</span> <span class=n>models</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>client</span> <span class=o>=</span> <span class=n>QdrantClient</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my-collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;dense&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span><span class=o>...</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;late-interaction&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>distance</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Distance</span><span class=o>.</span><span class=n>COSINE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>multivector_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>MultiVectorConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>comparator</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>MultiVectorComparator</span><span class=o>.</span><span class=n>MAX_SIM</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>sparse_vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;sparse&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>SparseVectorParams</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Reranking will never use the created graph, as all the candidates are already retrieved. Multi-vector ranking will only
be applied to the candidates retrieved by the previous steps, so no search operation is needed. HNSW becomes redundant
while still the indexing process has to be performed, and in that case, it will be quite heavy. ColBERT-like models
create hundreds of embeddings for each document, so the overhead is significant. <strong>To avoid it, you can disable the HNSW
graph creation for this kind of model</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>client</span><span class=o>.</span><span class=n>create_collection</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>collection_name</span><span class=o>=</span><span class=s2>&#34;my-collection&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;dense&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span><span class=o>...</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;late-interaction&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>VectorParams</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>size</span><span class=o>=</span><span class=mi>128</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>distance</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>Distance</span><span class=o>.</span><span class=n>COSINE</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>multivector_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>MultiVectorConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>comparator</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>MultiVectorComparator</span><span class=o>.</span><span class=n>MAX_SIM</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>hnsw_config</span><span class=o>=</span><span class=n>models</span><span class=o>.</span><span class=n>HnswConfigDiff</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>m</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span>  <span class=c1># Disable HNSW graph creation</span>
</span></span><span class=line><span class=cl>            <span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=n>sparse_vectors_config</span><span class=o>=</span><span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;sparse&#34;</span><span class=p>:</span> <span class=n>models</span><span class=o>.</span><span class=n>SparseVectorParams</span><span class=p>(</span><span class=o>...</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>You won&rsquo;t notice any difference in the search performance, but the use of resources will be significantly lower when you
upload the embeddings to the collection.</p><h2 id=some-anecdotal-observations>Some anecdotal observations</h2><p>Neither of the algorithms performs best in all cases. In some cases, keyword-based search
will be the winner and vice-versa. The following table shows some interesting examples we could find in the
<a href=https://github.com/wayfair/WANDS target=_blank rel="noopener nofollow">WANDS</a> dataset during experimentation:</p><div class=table-responsive><table class="table mb-5"><thead><th>Query</th><th>BM25 Search</th><th>Vector Search</th></thead><tbody><tr><th>cybersport desk</th><td>desk ❌</td><td>gaming desk ✅</td></tr><tr><th>plates for icecream</th><td>"eat" plates on wood wall décor ❌</td><td>alicyn 8.5 '' melamine dessert plate ✅</td></tr><tr><th>kitchen table with a thick board</th><td>craft kitchen acacia wood cutting board ❌</td><td>industrial solid wood dining table ✅</td></tr><tr><th>wooden bedside table</th><td>30 '' bedside table lamp ❌</td><td>portable bedside end table ✅</td></tr></tbody></table></div><p>Also examples where keyword-based search did better:</p><div class=table-responsive><table class="table mb-5"><thead><th>Query</th><th>BM25 Search</th><th>Vector Search</th></thead><tbody><tr><th>computer chair</th><td>vibrant computer task chair ✅</td><td>office chair ❌</td></tr><tr><th>64.2 inch console table</th><td>cervantez 64.2 '' console table ✅</td><td>69.5 '' console table ❌</td></tr></tbody></table></div><h2 id=try-the-new-query-api-in-qdrant-110>Try the New Query API in Qdrant 1.10</h2><p>The new Query API introduced in Qdrant 1.10 is a game-changer for building hybrid search systems. You don&rsquo;t need any
additional services to combine the results from different search methods, and you can even create more complex pipelines
and serve them directly from Qdrant.</p><p>Our webinar on <em>Building the Ultimate Hybrid Search</em> takes you through the process of building a hybrid search system
with Qdrant Query API. If you missed it, you can <a href="https://www.youtube.com/watch?v=LAZOxqzceEU" target=_blank rel="noopener nofollow">watch the recording</a>, or
<a href=https://github.com/qdrant/workshop-ultimate-hybrid-search target=_blank rel="noopener nofollow">check the notebooks</a>.</p><div style="max-width:640px;margin:0 auto;padding-bottom:1em"><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe width=100% height=100% src=https://www.youtube.com/embed/LAZOxqzceEU frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen style=position:absolute;top:0;left:0;width:100%;height:100%></iframe></div></div><p>If you have any questions or need help with building your hybrid search system, don&rsquo;t hesitate to reach out to us on
<a href=https://qdrant.to/discord target=_blank rel="noopener nofollow">Discord</a>.</p></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! 🙏</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. 😔 You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/hybrid-search.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#introducing-the-new-query-api>Introducing the new Query API</a></li><li><a href=#how-effective-is-your-search-system>How effective is your search system?</a></li><li><a href=#available-embedding-options-with-query-api>Available embedding options with Query API</a></li><li><a href=#fusion-vs-reranking>Fusion vs reranking</a><ul><li><a href=#why-not-a-linear-combination>Why not a linear combination?</a></li></ul></li><li><a href=#building-a-hybrid-search-system-in-qdrant>Building a hybrid search system in Qdrant</a></li><li><a href=#lessons-learned-multi-vector-representations>Lessons learned: multi-vector representations</a></li><li><a href=#some-anecdotal-observations>Some anecdotal observations</a></li><li><a href=#try-the-new-query-api-in-qdrant-110>Try the New Query API in Qdrant 1.10</a></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/hybrid-search.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>