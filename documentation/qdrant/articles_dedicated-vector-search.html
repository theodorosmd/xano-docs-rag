<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta charset=UTF-8><title>Built for Vector Search - Qdrant</title>
<link rel=icon href=https://qdrant.tech/favicon/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=https://qdrant.tech/favicon/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://qdrant.tech/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://qdrant.tech/favicon/favicon-16x16.png><link rel=manifest href=https://qdrant.tech/favicon/site.webmanifest><link rel=mask-icon href=https://qdrant.tech/favicon/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#2b5797"><meta name=msapplication-config content="/favicon/browserconfig.xml"><meta name=theme-color content="#ffffff"><meta name=partition content="learn"><link href=https://qdrant.tech/css/search/search.min.09596ee2b3c94d0ac82a1c33199cabdb3e0210b1ee46aaf1515200e9e484d05dd9e27cc1e861c58cc2e582af162f63b8e180ff2a12f7a6592b7aeaa0a7125130.css rel=stylesheet integrity="sha512-CVlu4rPJTQrIKhwzGZyr2z4CELHuRqrxUVIA6eSE0F3Z4nzB6GHFjMLlgq8WL2O44YD/KhL3plkreuqgpxJRMA=="><link href=https://qdrant.tech/css/documentation.min.de204cdcfc410689a41c17bcdad0a34f2caaa50cc8505c820adaab481cef2d69a920b0a32e0d624701c29487e3271b694559d98c5047cbfaa5aa475f638011e5.css rel=stylesheet integrity="sha512-3iBM3PxBBomkHBe82tCjTyyqpQzIUFyCCtqrSBzvLWmpILCjLg1iRwHClIfjJxtpRVnZjFBHy/qlqkdfY4AR5Q=="><link href=https://qdrant.tech/css/main.min.c1e379ad4cf03647832f6a0f040754b0be3e79934db2c29f50b396327c1ec7ac45f778abadbd93f5a8bc431850d1b0d9c9eff34c0171cd4236e83c5e324c1a0e.css rel=stylesheet integrity crossorigin=anonymous><meta name=generator content="Hugo 0.141.0"><meta name=description content="Why add-on vector search looks good — until you actually use it."><meta name=keywords content="system architecture,vector search,vector database,qdrant"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@id":"https://qdrant.tech/articles/dedicated-vector-search/#article","@type":"Article","abstract":"Why addon vector search looks good  until you actually use it","author":{"@type":"Person","name":"Evgeniya Sukhodolskaya \u0026amp; Andrey Vasnetsov"},"dateModified":"2025-02-17 10:00:00 +0300 +0300","datePublished":"2025-02-17 10:00:00 +0300 +0300","description":"Why addon vector search looks good  until you actually use it","headline":"Built for Vector Search","image":["https://qdrant.tech/articles_data/dedicated-vector-search/preview/social_preview.jpg"],"name":"Built for Vector Search","url":"https://qdrant.tech/articles/dedicated-vector-search/","wordCount":"3130"},{"@id":"https://qdrant.tech/articles/dedicated-vector-search/#organization","@type":"Organization","address":{"@type":"PostalAddress","addressCountry":"DE","addressLocality":"Berlin","addressRegion":"Berlin","postalCode":"10115","streetAddress":"Chausseestraße 86"},"contactPoint":{"@type":"ContactPoint","contactType":"customer support","email":"info@qdrant.com","telephone":"+49 3040797694"},"description":"Qdrant is an Open-Source Vector Database and Vector Search Engine written in Rust. It provides fast and scalable vector similarity search service with convenient API.","email":"info@qdrant.com","founders":[{"@type":"Person","name":"map[email:info@qdrant.tech name:Andrey Vasnetsov]"},{"@type":"Person","name":"Andre Zayarni"}],"foundingDate":"2021","keywords":["system architecture","vector search","vector database","Qdrant"],"legalName":"Qdrant Solutions GmbH","location":"Berlin, Germany","logo":"https://qdrant.tech/images/logo_with_text.png","name":"Qdrant","sameAs":["https://github.com/qdrant/qdrant","https://qdrant.to/discord","https://www.youtube.com/channel/UC6ftm8PwH1RU_LM1jwG0LQA","https://www.linkedin.com/company/qdrant/","https://twitter.com/qdrant_engine"],"url":"https://qdrant.tech"}]}</script><meta property="og:url" content="https://qdrant.tech/articles/dedicated-vector-search/"><meta property="og:type" content="website"><meta property="og:title" content="Built for Vector Search - Qdrant"><meta name=twitter:card content="summary_large_image"><meta name=twitter:domain content="qdrant"><meta name=twitter:url content="https://qdrant.tech/articles/dedicated-vector-search/"><meta name=twitter:title content="Built for Vector Search - Qdrant"><meta property="og:description" content="Why add-on vector search looks good — until you actually use it."><meta name=twitter:description content="Why add-on vector search looks good — until you actually use it."><meta name=image property="og:image" content="https://qdrant.tech/articles_data/dedicated-vector-search/preview/social_preview.jpg"><meta name=image property="og:image:secure_url" content="https://qdrant.tech/articles_data/dedicated-vector-search/preview/social_preview.jpg"><meta property="og:image:type" content="image/jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name=twitter:image:src content="https://qdrant.tech/articles_data/dedicated-vector-search/preview/social_preview.jpg"><meta name=author content="Evgeniya Sukhodolskaya & Andrey Vasnetsov"><link rel=canonical href=https://qdrant.tech/articles/dedicated-vector-search/><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script src=https://qdrant.tech/js/documentation.min.594fc9226f6fb87b8c3b0cafb9d5c0b0bcd95f47d292b929aab04315e0506706b2f376dc4fae9961dc3082138cb19d45b2a3370ee8e881bbf4b8c91dcb735c32.js></script></head><body><main><header class=docs-header><div class="main-menu z-5"><a href=https://qdrant.tech/><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div></a><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=main-menu__trigger><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M1 12H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 5H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/><path d="M1 19H23" stroke="#e1e5f0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/></svg></button></div><ul class=main-menu__links><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/>Qdrant</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/cloud-intro/>Cloud</a></li><li class=main-menu__item><a class=menu-link href=https://qdrant.tech/documentation/build/>Build</a></li><li class=main-menu__item><a class=menu-linkactive href=https://qdrant.tech/articles/>Learn</a></li><li class=main-menu__item><a class=menu-link href=https://api.qdrant.tech/api-reference target=_blank>API Reference</a></li></ul><div class=main-menu__buttons><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><button role=button class=theme-switch></button>
<a data-metric-loc=nav href=https://cloud.qdrant.io/login class="menu-link mx-3" target=_blank>Log in</a>
<a data-metric-loc=nav href=https://cloud.qdrant.io/signup class="button button_contained button_sm" target=_blank>Start Free</a></div></div><div class=menu-mobile><div class=menu-mobile__header><div class=logo><img class=logo__img src=https://qdrant.tech/img/qdrant-logo.svg alt=logo></div><div class="d-flex d-xl-none justify-content-end align-items-center gap-4"><div class="d-block d-xl-none"><button role=button class=theme-switch></button></div><button type=button class=menu-mobile__close><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><g id="Close"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M19.778 5.63606C20.1685 5.24554 20.1685 4.61237 19.778 4.22185 19.3874 3.83132 18.7543 3.83132 18.3637 4.22185L11.9998 10.5858 5.63586 4.22185c-.39052-.39053-1.02369-.39053-1.41421.0C3.83112 4.61237 3.83112 5.24554 4.22165 5.63606L10.5856 12l-6.364 6.364C3.83108 18.7545 3.83108 19.3877 4.2216 19.7782 4.61213 20.1687 5.24529 20.1687 5.63582 19.7782l6.36398-6.364 6.364 6.364C18.7543 20.1687 19.3875 20.1687 19.778 19.7782 20.1685 19.3877 20.1685 18.7545 19.778 18.364L13.414 12l6.364-6.36394z" fill="#e1e5f0"/></g></svg></button></div></div><div class=main-menu__buttons-input><button class="qdr-search-input-btn q-input input_md input_light-bg" type=button name=search data-target=#searchModal>
Search</button></div><ul class=menu-mobile__items><li class=menu-mobile__item data-path=menu-0><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Documentation"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M4.25 1c-1.10092.0-2.12247.23681-2.88689.64443C.620129 2.04063.0 2.67297.0 3.5v11c0 .2761.223858.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.83364-.9732C2.42647 13.2107 3.27992 13 4.25 13 5.22008 13 6.07353 13.2107 6.66636 13.5268c.61427.3276.83364.6952.83364.9732.0.2761.22386.5.5.5s.5-.2239.5-.5c0-.278.21937-.6456.833640000000001-.9732C9.92647 13.2107 10.7799 13 11.75 13S13.5735 13.2107 14.1664 13.5268C14.7806 13.8544 15 14.222 15 14.5c0 .2761.2239.5.5.5s.5-.2239.5-.5V3.5C16 2.67297 15.3799 2.04063 14.6369 1.64443 13.8725 1.23681 12.8509 1 11.75 1s-2.12247.23681-2.88689.64443C8.53799 1.8178 8.2364 2.03638 8 2.29671c-.2364-.26033-.53799-.47891-.86311-.65228C6.37247 1.23681 5.35092 1 4.25 1zM1.36311 12.6444C1.23778 12.7113 1.11595 12.7848 1 12.8649V3.5c0-.27797.21937-.64563.83364-.97318C2.42647 2.21069 3.27992 2 4.25 2c.97008.0 1.82353.21069 2.41636.52682C7.28063 2.85437 7.5 3.22203 7.5 3.5v9.3649C7.38405 12.7848 7.26222 12.7113 7.13689 12.6444 6.37247 12.2368 5.35092 12 4.25 12 3.14908 12 2.12753 12.2368 1.36311 12.6444zM8.5 12.8649C8.61595 12.7848 8.73778 12.7113 8.86311 12.6444 9.62753 12.2368 10.6491 12 11.75 12S13.8725 12.2368 14.6369 12.6444C14.7622 12.7113 14.8841 12.7848 15 12.8649V3.5C15 3.22203 14.7806 2.85437 14.1664 2.52682 13.5735 2.21069 12.7201 2 11.75 2s-1.82353.21069-2.41636.52682C8.71937 2.85437 8.5 3.22203 8.5 3.5v9.3649z"/></g></svg>
Qdrant</a></div></li><li class=menu-mobile__item data-path=menu-1><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/cloud-intro/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Cloud"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M7.5 2C4.57416 2 2.18128 4.28566 2.00983 7.16851.8403 7.57763.0 8.68965.0 10c0 1.6571 1.34286 3 3 3H13c1.6571.0 3-1.3429 3-3 0-1.65714-1.3429-3-3-3L12.9776 7.00009C12.7249 4.19714 10.3686 2 7.5 2zM3.00107 7.48137 3.00008 7.47308C3.01457 5.0006 5.02412 3 7.5 3c2.47599.0 4.4856 2.00079 4.4999 4.47342L11.9987 7.4836C11.9969 7.50027 11.995 7.52348 11.995 7.551 11.995 7.701 12.0623 7.84308 12.1785 7.93804 12.2946 8.03301 12.4472 8.07082 12.5942 8.04106 12.726 8.01438 12.8617 8 13 8c1.1049.0 2 .89514 2 2C15 11.1049 14.1049 12 13 12H3C1.89514 12 1 11.1049 1 10 1 9.03225 1.68901 8.22422 2.60364 8.04017 2.83708 7.9932 3.005 7.78812 3.005 7.55 3.005 7.52154 3.00291 7.49773 3.00107 7.48137z"/></g></svg>
Cloud</a></div></li><li class=menu-mobile__item data-path=menu-2><div class=menu-mobile__item-content><a href=https://qdrant.tech/documentation/build/><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Blog"><path id="Union" fill-rule="evenodd" clip-rule="evenodd" d="M0 1.5C0 1.22386.223858 1 .5 1h5c.27614.0.5.22386.5.5v5c0 .27614-.22386.5-.5.5H.5C.223858 7 0 6.77614.0 6.5v-5zM1 2V6H5V2H1zm7 .5c0-.27614.22386-.5.5-.5h7c.2761.0.5.22386.5.5s-.2239.5-.5.5h-7C8.22386 3 8 2.77614 8 2.5zM8.5 6c-.27614.0-.5.22386-.5.5s.22386.5.5.5h7c.2761.0.5-.22386.5-.5s-.2239-.5-.5-.5h-7zM0 10.5c0-.2761.223858-.5.5-.5h15c.2761.0.5.2239.5.5s-.2239.5-.5.5H.5c-.276142.0-.5-.2239-.5-.5zM.5 14c-.276142.0-.5.2239-.5.5s.223858.5.5.5h15c.2761.0.5-.2239.5-.5s-.2239-.5-.5-.5H.5z"/></g></svg>
Build</a></div></li><li class=menu-mobile__item data-path=menu-3><div class=menu-mobile__item-content><a class=active href=https://qdrant.tech/articles/><svg width="16" height="16" viewBox="0 0 16 16" fill="none" stroke="#e1e5f0"><g id="Group"><path id="Vector" d="M10.084 1.84094l4.059 4.059" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_2" d="M8.49188 2.72406c-1.22044-.53042-2.59304-.59475-3.85774-.1808-1.2647.41395-2.33363 1.27742-3.00426 2.4268l2.34 2.34" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_3" d="M13.2657 7.47302C13.8062 8.69691 13.8769 10.077 13.4645 11.3497c-.4125 1.2728-1.2792 2.3491-2.4348 3.0233l-2.35001-2.35" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_4" d="M6.81217 13.049l-3.861-3.861S6.24917.983 15.5002.5c-.523 9.211-8.68803 12.549-8.68803 12.549z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_5" d="M3.59892 12.4008C3.34181 12.1441 2.99331 12 2.62995 12c-.36335.0-.71186.1441-.96897.4008-.46665.4664-.63131 1.941-.65931 2.2322C.997021 14.6793 1.0021 14.726 1.01659 14.7701 1.03108 14.8143 1.05465 14.8549 1.08579 14.8895 1.11693 14.924 1.15495 14.9516 1.19741 14.9705 1.23987 14.9895 1.28583 14.9994 1.33233 14.9995 1.34298 15.0002 1.35367 15.0002 1.36433 14.9995 1.65565 14.9715 3.1316 14.8063 3.59759 14.3405 3.85501 14.0834 3.99975 13.7346 4 13.3709 4.00025 13.0072 3.85599 12.6582 3.59892 12.4008z" stroke-linecap="round" stroke-linejoin="round"/><path id="Vector_6" d="M9 8c.55228.0 1-.44772 1-1S9.55228 6 9 6 8 6.44772 8 7s.44772 1 1 1z"/></g></svg>
Learn</a></div></li><li class=menu-mobile__item data-path=menu-4><div class=menu-mobile__item-content><a href=https://api.qdrant.tech/api-reference target=_blank><svg width="16" height="16" viewBox="0 0 16 16" fill="#e1e5f0"><g id="Menu/Roadmap" clip-path="url(#clip0_7182_4311)"><path id="Vector" fill-rule="evenodd" clip-rule="evenodd" d="M13.5 1c-.8284.0-1.5.67157-1.5 1.5S12.6716 4 13.5 4 15 3.32843 15 2.5 14.3284 1 13.5 1zM11 2.5C11 1.11929 12.1193.0 13.5.0S16 1.11929 16 2.5 14.8807 5 13.5 5 11 3.88071 11 2.5zM2.5 12c-.82843.0-1.5.6716-1.5 1.5S1.67157 15 2.5 15 4 14.3284 4 13.5 3.32843 12 2.5 12zM0 13.5C0 12.1193 1.11929 11 2.5 11S5 12.1193 5 13.5 3.88071 16 2.5 16 0 14.8807.0 13.5zM3.65901 1.65901C4.08097 1.23705 4.65326 1 5.25 1s1.16903.23705 1.59099.65901C7.26295 2.08097 7.5 2.65326 7.5 3.25v9.5c0 .862.34241 1.6886.9519 2.2981C9.0614 15.6576 9.88805 16 10.75 16c.862.0 1.6886-.3424 2.2981-.9519S14 13.612 14 12.75V6.5c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v6.25C13 13.3467 12.7629 13.919 12.341 14.341 11.919 14.7629 11.3467 15 10.75 15S9.58097 14.7629 9.15901 14.341C8.73705 13.919 8.5 13.3467 8.5 12.75V3.25C8.5 2.38805 8.15759 1.5614 7.5481.951903 6.9386.34241 6.11195.0 5.25.0S3.5614.34241 2.9519.951903C2.34241 1.5614 2 2.38805 2 3.25V9.5c0 .27614.22386.5.5.5s.5-.22386.5-.5V3.25c0-.59674.23705-1.16903.65901-1.59099z"/></g><defs><clipPath id="clip0_7182_4311"><rect width="16" height="16" fill="#fff"/></clipPath></defs></svg>
API Reference</a></div></li></ul><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div></div></header><section class="docs documentation"><div class=documentation__container><div class="d-flex flex-column flex-xl-row"><div class=docs-menu><div id=sidebar class=docs-menu__content><h3 class=docs-menu__links-title>Learn</h3><nav><div class=docs-menu__links-group><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/vector-search-manuals/>Vector Search Manuals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/qdrant-internals/>Qdrant Internals</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/data-exploration/>Data Exploration</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/machine-learning/>Machine Learning</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/rag-and-genai/>RAG & GenAI</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/practicle-examples/>Practical Examples</a></div><div class="docs-menu__articles-link docs-menu__links-group-heading"><a href=https://qdrant.tech/articles/ecosystem/>Ecosystem</a></div></div></nav></div></div><div class=documentation__content><div class=documentation__content-wrapper><div class=documentation__article-wrapper><ul class=docs-breadcrumbs><li class=docs-breadcrumbs__crumb><a href=https://qdrant.tech/articles/>Articles</a></li><li class=docs-breadcrumbs__crumb-separator></li><li class=docs-breadcrumbs__crumb>Built for Vector Search</li></ul><article class=documentation-article><div class=documentation-article__header><a href=https://qdrant.tech/articles/qdrant-internals/ class=documentation-article__header-link><svg width="16" height="16" viewBox="0 0 16 16" fill="none"><path d="M14.6668 8.00004H1.3335m0 0L6.00016 12.6667M1.3335 8.00004 6.00016 3.33337" stroke="#8f98b3" stroke-width="1.33333" stroke-linecap="round" stroke-linejoin="round"/></svg>
Back to Qdrant Internals</a><h1 class=documentation-article__header-title>Built for Vector Search</h1><div class=documentation-article__header-about><p>Evgeniya Sukhodolskaya & Andrey Vasnetsov</p><span>&#183;</span><p>February 17, 2025</p></div><picture class=documentation-article__header-preview><source srcset=https://qdrant.tech/articles_data/dedicated-vector-search/preview/title.webp type=image/webp><img alt="Built for Vector Search" src=https://qdrant.tech/articles_data/dedicated-vector-search/preview/title.jpg></picture></div><p>Any problem with even a bit of complexity requires a specialized solution. You can use a Swiss Army knife to open a bottle or poke a hole in a cardboard box, but you will need an axe to chop wood — the same goes for software.</p><p>In this article, we will describe the unique challenges vector search poses and why a dedicated solution is the best way to tackle them.</p><h2 id=vectors>Vectors</h2><p><img src=https://qdrant.tech/articles_data/dedicated-vector-search/image1.jpg alt=vectors></p><p>Let&rsquo;s look at the central concept of vector databases — <a href=https://qdrant.tech/documentation/concepts/vectors/><strong>vectors</strong></a>.</p><p>Vectors (also known as embeddings) are high-dimensional representations of various data points — texts, images, videos, etc. Many state-of-the-art (SOTA) embedding models generate representations of over 1,500 dimensions. When it comes to state-of-the-art PDF retrieval, the representations can reach <a href=https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/><strong>over 100,000 dimensions per page</strong></a>.</p><p>This brings us to the first challenge of vector search — vectors are heavy.</p><h3 id=vectors-are-heavy>Vectors are Heavy</h3><p>To put this in perspective, consider one million records stored in a relational database. It&rsquo;s a relatively small amount of data for modern databases, which a free tier of many cloud providers could easily handle.</p><p>Now, generate a 1536-dimensional embedding with OpenAI&rsquo;s <code>text-embedding-ada-002</code> model from each record, and you are looking at around <strong>6GB of storage</strong>. As a result, vector search workloads, especially if not optimized, will quickly dominate the main use cases of a non-vector database.</p><p>Having vectors as a part of a main database is a potential issue for another reason — vectors are always a transformation of other data.</p><h3 id=vectors-are-a-transformation>Vectors are a Transformation</h3><p>Vectors are obtained from some other source-of-truth data. They can be restored if lost with the same embedding model previously used. At the same time, even small changes in that model can shift the geometry of the vector space, so if you update or change the embedding model, you need to update and reindex all the data to maintain accurate vector comparisons.</p><p>If coupled with the main database, this update process can lead to significant complications and even unavailability of the whole system.</p><aside role=status>Decouple vector workloads even if you plan to use a general-purpose database for vectors.</aside><p>However, vectors have positive properties as well. One of the most important is that vectors are fixed-size.</p><h3 id=vectors-are-fixed-size>Vectors are Fixed-Size</h3><p>Embedding models are designed to produce vectors of a fixed size. We have to use it to our advantage.</p><p>For fast search, vectors need to be instantly accessible. Whether in <a href=https://qdrant.tech/documentation/concepts/storage/><strong>RAM or disk</strong></a>, vectors should be stored in a format that allows quick access and comparison. This is essential, as vector comparison is a very hot operation in vector search workloads. It is often performed thousands of times per search query, so even a small overhead can lead to a significant slowdown.</p><p>For dedicated storage, vectors&rsquo; fixed size comes as a blessing. Knowing how much space one data point needs, we don&rsquo;t have to deal with the usual overhead of locating data — the location of elements in storage is straightforward to calculate.</p><p>Everything becomes far less intuitive if vectors are stored together with other data types, for example, texts or JSONs. The size of a single data point is not fixed anymore, so accessing it becomes non-trivial, especially if data is added, updated, and deleted over time.</p><figure><img src=https://qdrant.tech/articles_data/dedicated-vector-search/dedicated_storage.png alt="Fixed size columns VS Variable length table" width=80%><figcaption><p>Fixed size columns VS Variable length table</p></figcaption></figure><p><strong>Storing vectors together with other types of data, we lose all the benefits of their characteristics</strong>; however, we fully &ldquo;enjoy&rdquo; their drawbacks, polluting the storage with an extremely heavy transformation of data already existing in that storage.</p><h2 id=vector-search>Vector Search</h2><p><img src=https://qdrant.tech/articles_data/dedicated-vector-search/image2.jpg alt=vector-search></p><p>Unlike traditional databases that serve as data stores, <strong>vector databases are more like search engines</strong>. They are designed to be <strong>scalable</strong>, always <strong>available</strong>, and capable of delivering high-speed search results even under heavy loads. Just as Google or Bing can handle billions of queries at once, vector databases are designed for scenarios where rapid, high-throughput, low-latency retrieval is a must.</p><figure><img src=https://qdrant.tech/articles_data/dedicated-vector-search/compass.png alt="Database Compass" width=80%><figcaption><p>Database Compass</p></figcaption></figure><h3 id=pick-any-two>Pick Any Two</h3><p>Distributed systems are perfect for scalability — horizontal scaling in these systems allows you to add more machines as needed. In the world of distributed systems, one well-known principle — the <strong>CAP theorem</strong> — illustrates that you cannot have it all. The theorem states that a distributed system can guarantee only two out of three properties: <strong>Consistency</strong>, <strong>Availability</strong>, and <strong>Partition Tolerance</strong>.</p><p>As network partitions are inevitable in any real-world distributed system, all modern distributed databases are designed with partition tolerance in mind, forcing a trade-off between <strong>consistency</strong> (providing the most up-to-date data) and <strong>availability</strong> (remaining responsive).</p><aside role=status><strong>CP systems</strong> are still available to clients under normal operation — they prioritize data correctness over availability during failures.<br><strong>AP systems</strong> deliver quick responses by relaxing immediate consistency guarantees but eventually converge to a correct state.</aside><p>There are two main design philosophies for databases in this context:</p><h3 id=acid-prioritizing-consistency>ACID: Prioritizing Consistency</h3><p>The ACID model ensures that every transaction (a group of operations treated as a single unit, such as transferring money between accounts) is executed fully or not at all (reverted), leaving the database in a valid state. When a system is distributed, achieving ACID properties requires complex coordination between nodes. Each node must communicate and agree on the state of a transaction, which can <strong>limit system availability</strong> — if a node is uncertain about the state of another, it may refuse to process a transaction until consistency is assured. This coordination also makes <strong>scaling more challenging</strong>.</p><p>Financial institutions use ACID-compliant databases when dealing with money transfers, where even a momentary discrepancy in an account balance is unacceptable.</p><h3 id=base-prioritizing-availability>BASE: Prioritizing Availability</h3><p>On the other hand, the BASE model favors high availability and partition tolerance. BASE systems distribute data and workload across multiple nodes, enabling them to respond to read and write requests immediately. They operate under the principle of <strong>eventual consistency</strong> — although data may be temporarily out-of-date, the system will converge on a consistent state given time.</p><p>Social media platforms, streaming services, and search engines all benefit from the BASE approach. For these applications, having immediate responsiveness is more critical than strict consistency.</p><h3 id=based-vector-search>BASEd Vector Search</h3><p>Considering the specifics of vector search — its nature demanding availability & scalability — it should be served on BASE-oriented architecture. This choice is made due to the need for horizontal scaling, high availability, low latency, and high throughput. For example, having BASE-focused architecture allows us to <a href=https://qdrant.tech/documentation/cloud/cluster-scaling/#resharding><strong>easily manage resharding</strong></a>.</p><p>A strictly consistent transactional approach also loses its attractiveness when we remember that vectors are heavy transformations of data at our disposal — what&rsquo;s the point in limiting data protection mechanisms if we can always restore vectorized data through a transformation?</p><h2 id=vector-index>Vector Index</h2><p><img src=https://qdrant.tech/articles_data/dedicated-vector-search/image3.jpg alt=vector-index></p><p><a href=https://qdrant.tech/documentation/concepts/search/><strong>Vector search</strong></a> relies on high-dimensional vector mathematics, making it computationally heavy at scale. A brute-force similarity search would require comparing a query against every vector in the database. In a database with 100 million 1536-dimensional vectors, performing 100 million comparisons per one query is unfeasible for production scenarios. Instead of a brute-force approach, vector databases have specialized approximate nearest neighbour (ANN) indexes that balance search precision and speed. These indexes require carefully designed architectures to make their maintenance in production feasible.</p><figure><img src=https://qdrant.tech/articles_data/dedicated-vector-search/hnsw.png alt="HNSW Index" width=80%><figcaption><p>HNSW Index</p></figcaption></figure><p>One of the most popular vector indexes is <strong>HNSW (Hierarchical Navigable Small World)</strong>, which we picked for its capability to provide simultaneously high search speed and accuracy. High performance came with a cost — implementing it in production is untrivial due to several challenges, so to make it shine all the system&rsquo;s architecture has to be structured around it, serving the capricious index.</p><h3 id=index-complexity>Index Complexity</h3><p><a href=https://qdrant.tech/documentation/concepts/indexing/><strong>HNSW</strong></a> is structured as a multi-layered graph. With a new data point inserted, the algorithm must compare it to existing nodes across several layers to index it. As the number of vectors grows, these comparisons will noticeably slow down the construction process, making updates increasingly time-consuming. The indexing operation can quickly become the bottleneck in the system, slowing down search requests.</p><p>Building an HNSW monolith means limiting the scalability of your solution — its size has to be capped, as its construction time scales <strong>non-linearly</strong> with the number of elements. To keep the construction process feasible and ensure it doesn&rsquo;t affect the search time, we came up with a layered architecture that breaks down all data management into small units called <strong>segments</strong>.</p><figure><img src=https://qdrant.tech/articles_data/dedicated-vector-search/segments.png alt="Storage structure" width=80%><figcaption><p>Storage structure</p></figcaption></figure><p>Each segment isolates a subset of vectorized corpora and supports all collection-level operations on it, from searching to indexing, for example segments build their own index on the subset of data available to them. For users working on a collection level, the specifics of segmentation are unnoticeable. The search results they get span the whole collection, as sub-results are gathered from segments and then merged & deduplicated.</p><p>By balancing between size and number of segments, we can ensure the right balance between search speed and indexing time, making the system flexible for different workloads.</p><h3 id=immutability>Immutability</h3><p>With index maintenance divided between segments, Qdrant can ensure high performance even during heavy load, and additional optimizations secure that further. These optimizations come from an idea that working with immutable structures introduces plenty of benefits: the possibility of using internally fixed sized lists (so no dynamic updates), ordering stored data accordingly to access patterns (so no unpredictable random accesses). With this in mind, to optimize search speed and memory management further, we use a strategy that combines and manages <a href=https://qdrant.tech/articles/immutable-data-structures/><strong>mutable and immutable segments</strong></a>.</p><div class=table-responsive><table class="table mb-5"><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Mutable Segments</strong></td><td>These are used for quickly ingesting new data and handling changes (updates) to existing data.</td></tr><tr><td><strong>Immutable Segments</strong></td><td>Once a mutable segment reaches a certain size, an optimization process converts it into an immutable segment, constructing an HNSW index – you could <a href=https://qdrant.tech/documentation/concepts/optimizer/#optimizer><strong>read about these optimizers here</strong></a> in detail. This immutability trick allowed us, for example, to ensure effective <a href=https://qdrant.tech/documentation/concepts/indexing/#tenant-index><strong>tenant isolation</strong></a>.</td></tr></tbody></table></div><p>Immutable segments are an implementation detail transparent for users — they can delete vectors at any time, while additions and updates are applied to a mutable segment instead. This combination of mutability and immutability allows search and indexing to smoothly run simultaneously, even under heavy loads. This approach minimizes the performance impact of indexing time and allows on-the-fly configuration changes on a collection level (such as enabling or disabling data quantization) without downtimes.</p><h3 id=filterable-index>Filterable Index</h3><p>Vector search wasn&rsquo;t historically designed for filtering — imposing strict constraints on results. It&rsquo;s inherently fuzzy; every document is, to some extent, both similar and dissimilar to any query — there&rsquo;s no binary &ldquo;<em>fits/doesn&rsquo;t fit</em>&rdquo; segregation. As a result, vector search algorithms weren&rsquo;t originally built with filtering in mind.</p><p>At the same time, filtering is unavoidable in many vector search applications, such as <a href=https://qdrant.tech/recommendations/><strong>e-commerce search/recommendations</strong></a>. Searching for a Christmas present, you might want to filter out everything over 100 euros while still benefiting from the vector search&rsquo;s semantic nature.</p><p>In many vector search solutions, filtering is approached in two ways: <strong>pre-filtering</strong> (computes a binary mask for all vectors fitting the condition before running HNSW search) or <strong>post-filtering</strong> (running HNSW as usual and then filtering the results).</p><div class=table-responsive><table class="table mb-5"><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>❌</td><td><strong>Pre-filtering</strong></td><td>Has the linear complexity of computing the vector mask and becomes a bottleneck for large datasets.</td></tr><tr><td>❌</td><td><strong>Post-filtering</strong></td><td>The problem with <strong>post-filtering</strong> is tied to vector search &ldquo;<em>everything fits and doesn&rsquo;t at the same time</em>&rdquo; nature: imagine a low-cardinality filter that leaves only a few matching elements in the database. If none of them are similar enough to the query to appear in the top-X retrieved results, they&rsquo;ll all be filtered out.</td></tr></tbody></table></div><p>Qdrant <a href=https://qdrant.tech/articles/vector-search-filtering/><strong>took filtering in vector search further</strong></a>, recognizing the limitations of pre-filtering & post-filtering strategies. We developed an adaptation of HNSW — <a href=https://qdrant.tech/articles/filtrable-hnsw/><strong>filterable HNSW</strong></a> — that also enables <strong>in-place filtering</strong> during graph traversal. To make this possible, we condition HNSW index construction on possible filtering conditions reflected by <a href=https://qdrant.tech/documentation/concepts/indexing/#payload-index><strong>payload indexes</strong></a> (inverted indexes built on vectors&rsquo; <a href=https://qdrant.tech/documentation/concepts/payload/><strong>metadata</strong></a>).</p><p><strong>Qdrant was designed with a vector index being a central component of the system.</strong> That made it possible to organize optimizers, payload indexes and other components around the vector index, unlocking the possibility of building a filterable HNSW.</p><figure><img src=https://qdrant.tech/articles_data/dedicated-vector-search/filterable-vector-index.png alt="Filterable Vector Index" width=80%><figcaption><p>Filterable Vector Index</p></figcaption></figure><p>In general, optimizing vector search requires a custom, finely tuned approach to data and index management that secures high performance even as data grows and changes dynamically. This specialized architecture is the key reason why <strong>dedicated vector databases will always outperform general-purpose databases in production settings</strong>.</p><h2 id=vector-search-beyond-rag>Vector Search Beyond RAG</h2><figure><img src=https://qdrant.tech/articles_data/dedicated-vector-search/venn-diagram.png alt="Vector Search is not Text Search Extension" width=80%><figcaption><p>Vector Search is not Text Search Extension</p></figcaption></figure><p>Many discussions about the purpose of vector databases focus on Retrieval-Augmented Generation (RAG) — or its more advanced variant, agentic RAG — where vector databases are used as a knowledge source to retrieve context for large language models (LLMs). This is a legitimate use case, however, the hype wave of RAG solutions has overshadowed the broader potential of vector search, which goes <a href=https://qdrant.tech/articles/vector-similarity-beyond-search/><strong>beyond augmenting generative AI</strong></a>.</p><h3 id=discovery>Discovery</h3><p>The strength of vector search lies in its ability to facilitate <a href=https://qdrant.tech/articles/discovery-search/><strong>discovery</strong></a>. Vector search allows you to refine your choices as you search rather than starting with a fixed query. Say, <a href=https://qdrant.tech/articles/food-discovery-demo/><strong>you&rsquo;re ordering food not knowing exactly what you want</strong></a> — just that it should contain meat & not a burger, or that it should be meat with cheese & not tacos. Instead of searching for a specific dish, vector search helps you navigate options based on similarity and dissimilarity, guiding you toward something that matches your taste without requiring you to define it upfront.</p><h3 id=recommendations>Recommendations</h3><p>Vector search is perfect for <a href=https://qdrant.tech/documentation/concepts/explore/#recommendation-api><strong>recommendations</strong></a>. Imagine browsing for a new book or movie. Instead of searching for an exact match, you might look for stories that capture a certain mood or theme but differ in key aspects from what you already know. For example, you may <a href="https://www.youtube.com/watch?v=O5mT8M7rqQQ" target=_blank rel="noopener nofollow"><strong>want a film featuring wizards without the familiar feel of the &ldquo;Harry Potter&rdquo; series</strong></a>. This flexibility is possible because vector search is not tied to the binary &ldquo;match/not match&rdquo; concept but operates on distances in a vector space.</p><h3 id=big-unstructured-data-analysis>Big Unstructured Data Analysis</h3><p>Vector search nature makes it also ideal for <a href="https://www.youtube.com/watch?v=_BQTnXpuH-E" target=_blank rel="noopener nofollow"><strong>big unstructured data analysis</strong></a>, for instance, anomaly detection. In large, unstructured, and often unlabelled datasets, vector search can help identify clusters and outliers by analyzing distance relationships between data points.</p><h3 id=fundamentally-different>Fundamentally Different</h3><p><strong>Vector search beyond RAG isn&rsquo;t just another feature — it&rsquo;s a fundamental shift in how we interact with data</strong>. Dedicated solutions integrate these capabilities natively and are designed from the ground up to handle high-dimensional math and (dis-)similarity-based retrieval. In contrast, databases with vector extensions are built around a different data paradigm, making it impossible to efficiently support advanced vector search capabilities.</p><p>Even if you want to retrofit these capabilities, it&rsquo;s not just a matter of adding a new feature — it&rsquo;s a structural problem. Supporting advanced vector search requires <strong>dedicated interfaces</strong> that enable flexible usage of vector search from multi-stage filtering to dynamic exploration of high-dimensional spaces.</p><p>When the underlying architecture wasn&rsquo;t initially designed for this kind of interaction, integrating interfaces is a <strong>software engineering team nightmare</strong>. You end up breaking existing assumptions, forcing inefficient workarounds, and often introducing backwards-compatibility problems. It&rsquo;s why attempts to patch vector search onto traditional databases won&rsquo;t match the efficiency of purpose-built systems.</p><h2 id=making-vector-search-state-of-the-art>Making Vector Search State-of-the-Art</h2><p><img src=https://qdrant.tech/articles_data/dedicated-vector-search/image4.jpg alt=vector-search-state-of-the-art></p><p>Now, let&rsquo;s shift focus to another key advantage of dedicated solutions — their ability to keep up with state-of-the-art solutions in the field.</p><p><a href=https://qdrant.tech/qdrant-vector-database/><strong>Vector databases</strong></a> are purpose-built for vector retrieval, and as a result, they offer cutting-edge features that are often critical for AI businesses relying on vector search. Vector database engineers invest significant time and effort into researching and implementing the most optimal ways to perform vector search. Many of these innovations come naturally to vector-native architectures, while general-purpose databases with added vector capabilities may struggle to adapt and replicate these benefits efficiently.</p><p>Consider some of the advanced features implemented in Qdrant:</p><ul><li><p><a href=https://qdrant.tech/blog/qdrant-1.13.x/#gpu-accelerated-indexing><strong>GPU-Accelerated Indexing</strong></a><br>By offloading index construction tasks to the GPU, Qdrant can significantly speed up the process of data indexing while keeping costs low. This becomes especially valuable when working with large datasets in hot data scenarios.</p><p>GPU acceleration in Qdrant is a custom solution developed by an enthusiast from our core team. It&rsquo;s vendor-free and natively supports all Qdrant&rsquo;s unique architectural features, from FIlterable HNSW to multivectors.</p></li><li><p><a href="https://qdrant.tech/documentation/concepts/vectors/?q=multivectors#multivectors"><strong>Multivectors</strong></a><br>Some modern embedding models produce an entire matrix (a list of vectors) as output rather than a single vector. Qdrant supports multivectors natively.</p><p>This feature is critical when using state-of-the-art retrieval models such as <a href=https://qdrant.tech/documentation/fastembed/fastembed-colbert/><strong>ColBERT</strong></a>, ColPali, or ColQwen. For instance, ColPali and ColQwen produce multivector outputs, and supporting them natively is crucial for <a href=https://qdrant.tech/documentation/advanced-tutorials/pdf-retrieval-at-scale/><strong>state-of-the-art (SOTA) PDF-retrieval</strong></a>.</p></li></ul><p>In addition to that, we continuously look for improvements in:</p><div class=table-responsive><table class="table mb-5"><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><strong>Memory Efficiency & Compression</strong></td><td>Techniques such as <a href=documentation/guides/quantization/><strong>quantization</strong></a> and <a href=https://qdrant.tech/blog/qdrant-1.13.x/#hnsw-graph-compression><strong>HNSW compression</strong></a> to reduce storage requirements</td></tr><tr><td><strong>Retrieval Algorithms</strong></td><td>Support for the latest retrieval algorithms, including <a href=https://qdrant.tech/articles/modern-sparse-neural-retrieval/><strong>sparse neural retrieval</strong></a>, <a href=https://qdrant.tech/documentation/concepts/hybrid-queries/><strong>hybrid search</strong></a> methods, and <a href=https://qdrant.tech/documentation/fastembed/fastembed-rerankers/><strong>re-rankers</strong></a>.</td></tr><tr><td><strong>Vector Data Analysis & Visualization</strong></td><td>Tools like the <a href=https://qdrant.tech/blog/qdrant-1.12.x/#distance-matrix-api-for-data-insights><strong>distance matrix API</strong></a> provide insights into vectorized data, and a <a href=https://qdrant.tech/blog/qdrant-1.11.x/#web-ui-search-quality-tool><strong>Web UI</strong></a> allows for intuitive exploration of data.</td></tr><tr><td><strong>Search Speed & Scalability</strong></td><td>Includes optimizations for <a href=https://qdrant.tech/articles/multitenancy/><strong>multi-tenant environments</strong></a> to ensure efficient and scalable search.</td></tr></tbody></table></div><p><strong>These advancements are not just incremental improvements — they define the difference between a system optimized for vector search and one that accommodates it.</strong></p><p>Staying at the cutting edge of vector search is not just about performance — it&rsquo;s also about keeping pace with an evolving AI landscape.</p><h2 id=summing-up>Summing up</h2><p><img src=https://qdrant.tech/articles_data/dedicated-vector-search/image5.jpg alt=conclusion-vector-search></p><p>When it comes to vector search, there&rsquo;s a clear distinction between using a dedicated vector search solution and extending a database to support vector operations.</p><p><strong>For small-scale applications or prototypes handling up to a million data points, a non-optimized architecture might suffice.</strong> However, as the volume of vectors grows, an unoptimized solution will quickly become a bottleneck — slowing down search operations and limiting scalability. Dedicated vector search solutions are engineered from the ground up to handle massive amounts of high-dimensional data efficiently.</p><p>State-of-the-art (SOTA) vector search evolves rapidly. If you plan to build on the latest advances, using a vector extension will eventually hold you back. Dedicated vector search solutions integrate these features natively, ensuring that you benefit from continuous innovations without compromising performance.</p><p>The power of vector search extends into areas such as big data analysis, recommendation systems, and discovery-based applications, and to support these vector search capabilities, a dedicated solution is needed.</p><h3 id=when-to-choose-a-dedicated-database-over-an-extension>When to Choose a Dedicated Database over an Extension:</h3><ul><li><strong>High-Volume, Real-Time Search</strong>: Ideal for applications with many simultaneous users who require fast, continuous access to search results—think search engines, e-commerce recommendations, social media, or media streaming services.</li><li><strong>Dynamic, Unstructured Data</strong>: Perfect for scenarios where data is continuously evolving and where the goal is to discover insights from data patterns.</li><li><strong>Innovative Applications</strong>: If you&rsquo;re looking to implement advanced use cases such as recommendation engines, hybrid search solutions, or exploratory data analysis where traditional exact or token-based searches hold short.</li></ul><p>Investing in a dedicated vector search engine will deliver the performance and flexibility necessary for success if your application relies on vector search at scale, keeps up with trends, or requires more than just a simple small-scale similarity search.</p></article><section class="docs-feedback d-print-none"><h5 class=docs-feedback__title>Was this page useful?</h5><div class=docs-feedback__buttons><button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_yes>
<img src=https://qdrant.tech/icons/outline/thumb-up.svg alt="Thumb up icon">
Yes
</button>
<button class="button button_outlined button_sm docs-feedback__button" id=feedback__answer_no>
<img src=https://qdrant.tech/icons/outline/thumb-down.svg alt="Thumb down icon">
No</button></div></section><section class="docs-feedback__responses d-none"><p class="docs-feedback__response docs-feedback__response_yes" id=feedback__response_yes>Thank you for your feedback! 🙏</p><p class="docs-feedback__response docs-feedback__response_no" id=feedback__response_no>We are sorry to hear that. 😔 You can <a class=text-brand-p href=https:/github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/dedicated-vector-search.md target=_blank>edit</a> this page on GitHub, or <a class=text-brand-p href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank>create</a> a GitHub issue.</p></section><script>const buttonsSection=document.querySelector(".docs-feedback"),responsesSection=document.querySelector(".docs-feedback__responses"),yesButton=document.querySelector("#feedback__answer_yes"),noButton=document.querySelector("#feedback__answer_no"),yesResponse=document.querySelector("#feedback__response_yes"),noResponse=document.querySelector("#feedback__response_no"),toggleResponses=()=>{buttonsSection.classList.add("d-none"),responsesSection.classList.remove("d-none")};yesButton.addEventListener("click",()=>{yesResponse.classList.add("feedback__response_visible"),toggleResponses()}),noButton.addEventListener("click",()=>{noResponse.classList.add("feedback__response_visible"),toggleResponses()})</script></div><div class=table-of-contents><p class=table-of-contents__head>On this page:</p><nav id=TableOfContents><ul><li><a href=#vectors>Vectors</a><ul><li><a href=#vectors-are-heavy>Vectors are Heavy</a></li><li><a href=#vectors-are-a-transformation>Vectors are a Transformation</a></li><li><a href=#vectors-are-fixed-size>Vectors are Fixed-Size</a></li></ul></li><li><a href=#vector-search>Vector Search</a><ul><li><a href=#pick-any-two>Pick Any Two</a></li><li><a href=#acid-prioritizing-consistency>ACID: Prioritizing Consistency</a></li><li><a href=#base-prioritizing-availability>BASE: Prioritizing Availability</a></li><li><a href=#based-vector-search>BASEd Vector Search</a></li></ul></li><li><a href=#vector-index>Vector Index</a><ul><li><a href=#index-complexity>Index Complexity</a></li><li><a href=#immutability>Immutability</a></li><li><a href=#filterable-index>Filterable Index</a></li></ul></li><li><a href=#vector-search-beyond-rag>Vector Search Beyond RAG</a><ul><li><a href=#discovery>Discovery</a></li><li><a href=#recommendations>Recommendations</a></li><li><a href=#big-unstructured-data-analysis>Big Unstructured Data Analysis</a></li><li><a href=#fundamentally-different>Fundamentally Different</a></li></ul></li><li><a href=#making-vector-search-state-of-the-art>Making Vector Search State-of-the-Art</a></li><li><a href=#summing-up>Summing up</a><ul><li><a href=#when-to-choose-a-dedicated-database-over-an-extension>When to Choose a Dedicated Database over an Extension:</a></li></ul></li></ul></nav><ul class=table-of-contents__external-links><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/tree/master/qdrant-landing/content/articles/dedicated-vector-search.md target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_955)"><path fill-rule="evenodd" clip-rule="evenodd" d="M8 .700012c-4.4.0-8 3.599998-8 7.999998C0 12.2333 2.26667 15.2333 5.46667 16.3 5.86667 16.3667 6 16.1 6 15.9S6 15.2333 6 14.5667C3.8 15.0333 3.33333 13.5 3.33333 13.5 3 12.5667 2.46667 12.3 2.46667 12.3 1.66667 11.8333 2.46667 11.8333 2.46667 11.8333 3.26667 11.9 3.66667 12.6333 3.66667 12.6333 4.4 13.8333 5.53333 13.5 6 13.3 6.06667 12.7667 6.26667 12.4333 6.53333 12.2333 4.73333 12.0333 2.86667 11.3667 2.86667 8.30001c0-.86666.33333-1.6.8-2.13333.0-.26667-.33334-1.06667.13333-2.13333.0.0.66667-.200000000000001 2.2.8.66667-.2 1.33333-.26667 2-.26667S9.33333 4.63335 10 4.83335c1.5333-1.06667 2.2-.8 2.2-.8C12.6667 5.16668 12.3333 5.96668 12.2667 6.16668 12.8 6.70001 13.0667 7.43335 13.0667 8.30001c0 3.06669-1.8667 3.73329-3.6667 3.93329C9.66667 12.5 9.93333 12.9667 9.93333 13.7c0 1.0667.0 1.9333.0 2.2C9.93333 16.1 10.0667 16.3667 10.4667 16.3c3.2-1.0667 5.4666-4.0667 5.4666-7.59999C16 4.30001 12.4.700012 8 .700012z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_955"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Edit on Github</a></li><li class=table-of-contents__link><a href=https://github.com/qdrant/landing_page/issues/new/choose target=_blank><svg width="16" height="17" viewBox="0 0 16 17" fill="none"><g clip-path="url(#clip0_6269_1540)"><path d="M16 9.83331V8.49998H13.9347C13.79 7.33465 13.3947 6.24065 12.8 5.31065l1.476-1.478L13.3327 2.89065 11.9633 4.26198c-.24-.248-.496600000000001-.474-.772-.676C11.0967 3.25665 10.9507 2.95398 10.7673 2.67598l1.038-1.038L10.8627.695312 9.82867 1.72931C9.29867 1.37798 8.672 1.16665 8 1.16665c-.672.0-1.29867.21133-1.828.56266L5.138.695312l-.94267.942668 1.038 1.038c-.18333.27733-.32933.58-.424.90867-.27533.202-.532.42933-.77266.67733L2.66733 2.89065l-.94333.942L3.2 5.30998c-.59467.93-.99 2.02467-1.13467 3.19H0V9.83331H2.01467C2.07333 11.276 2.50533 12.6026 3.204 13.688l-1.48 1.478L2.666 16.1093l1.37-1.368C4.93733 15.67 6.07667 16.288 7.33333 16.452V6.49998H8.66667V16.452C9.92333 16.2873 11.0627 15.67 11.964 14.7413l1.37 1.368L14.276 15.166l-1.48-1.478C13.4947 12.602 13.9267 11.2753 13.9853 9.83331H16z" fill="#f0f3fa"/></g><defs><clipPath id="clip0_6269_1540"><rect width="16" height="16" fill="#fff" transform="translate(0 0.5)"/></clipPath></defs></svg>
Create an issue</a></li></ul></div><footer class="docs-footer w-100 py-0 pt-0 pb-4 pb-xl-0"><div class=docs-footer__cta><h4>Ready to get started with Qdrant?</h4><a href=https://qdrant.to/cloud/ class="button button_outlined button_sm" target=_blank>Start Free</a></div><div class=docs-footer__bottom><div class=docs-footer__bottom-content><span>© 2025 Qdrant.</span><div class=footer__bottom-links><a href=https://qdrant.tech/legal/terms_and_conditions/>Terms</a>
<a href=https://qdrant.tech/legal/privacy-policy/>Privacy Policy</a>
<a href=https://qdrant.tech/legal/impressum/>Impressum</a></div></div></div></footer></div></div></div></div></section></main></body><script src=https://cdn.cookielaw.org/scripttemplates/otSDKStub.js type=text/javascript data-domain-script=01960152-5e40-782d-af01-f7f5768a214e></script><script async type=text/javascript>function OptanonWrapper(){const e=new CustomEvent("onetrust_loaded");document.dispatchEvent(e)}</script><script>!function(){const o="eQYi1nZE2zQSmnHuxjMRlDd2uLl65oHe";var n,s,t="analytics",e=window[t]=window[t]||[];if(!e.initialize)if(e.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{e.invoked=!0,e.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","screen","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware","register"],e.factory=function(n){return function(){if(window[t].initialized)return window[t][n].apply(window[t],arguments);var o,s=Array.prototype.slice.call(arguments);return["track","screen","alias","group","page","identify"].indexOf(n)>-1&&(o=document.querySelector("link[rel='canonical']"),s.push({__t:"bpc",c:o&&o.getAttribute("href")||void 0,p:location.pathname,u:location.href,s:location.search,t:document.title,r:document.referrer})),s.unshift(n),e.push(s),e}};for(n=0;n<e.methods.length;n++)s=e.methods[n],e[s]=e.factory(s);e.load=function(n,s){var i,o=document.createElement("script");o.type="text/javascript",o.async=!0,o.setAttribute("data-global-segment-analytics-key",t),o.src="https://evs.analytics.qdrant.tech/5caWuitPgcGFN5Q7HMpTaj/vEkmzjuRSqeXGbhGAFTWex.min.js",i=document.getElementsByTagName("script")[0],i.parentNode.insertBefore(o,i),e._loadOptions=s},e._writeKey=o,e._cdn="https://evs.analytics.qdrant.tech",e.SNIPPET_VERSION="5.2.0",e.load(o)}}()</script><script src=https://qdrant.tech/js/google-setup.min.14728a3ae9bd931593645b6ddbfe801d400cd2970006e782cb67f3966654248ca962dbaf4cf371493a2d326861b3a691a99d6d8349c8b2752339ab91d3787069.js></script><script src=https://qdrant.tech/js/index.min.e37feda1952d752f5568e1ebc55e183629e4d361f4df09edeed70b4a4327d601161c4147bdbe4149b4a3cdc1ed43f1bf1a2f2340a421f1aa821103980dfe0bda.js></script><script src=https://qdrant.tech/js/search/search.min.e58e7ea98cb549ea9d3ed7e68876abe323cfb323432038820f38a71c40e309deed0fba615e2d3346825229a81e0a76bbadf6f8a816ddad3438abdabfef4ebc09.js type=module></script><script src=https://qdrant.tech/js/search/scroll.min.f4ed4453a3af0adb0d4f49333cbb7892ff5430483a567d0d2dfbd4655fb203d22f22f46aff3d9f38fa5fd96dde5f30227afd0ce1d9f838f5cd5295fc83856ec9.js></script><script src=https://cdn.jsdelivr.net/npm/@popperjs/core@2.10.2/dist/umd/popper.min.js></script><script src=https://qdrant.tech/js/copy-code.min.95b03a45b2ab4b7a608ecfd4b5919c8572a5b2cd1463f52561cd10976abf3b74334f91324c0e620cb80afac46026a0b18cce6fb753d8495f74c6d478c8ca1d03.js></script><script src=https://qdrant.tech/js/lang-switcher.min.63da11cef09772078425b1ee56415b9c37842a9399f2ba2d2f5efbf72fb751686c410bd50e9bcd283c8231cb97c1973199d4923beef29d8b06e59d9983a6ba51.js></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src=https://qdrant.tech/js/vendor/anchor.min.0e138e17ebcf1c1147a7a3a81d9ac3c601622eedc479f1636470eb2552470f4e27c9a8efe6e8378995dedc1ab67029d8989536ea76f0857d45d1615ae772b8a1.js></script><script>document.addEventListener("keydown",function(e){if((e.metaKey||e.ctrlKey)&&e.key==="k"){e.preventDefault();let t=document.querySelector('[data-target="#searchModal"]');t&&t?.click()}})</script><script>window.addEventListener("message",e=>{if(e.data.type==="hsFormCallback"&&e.data.eventName==="onFormReady"){const t=document.querySelector(`form[data-form-id="${e.data.id}"]`);if(t){const e=t.querySelector('[name="last_form_fill_url"]');e&&(e.value=window.location.href);const n=t.querySelector('[name="referrer_url"]');n&&(n.value=document.referrer)}}})</script></html>